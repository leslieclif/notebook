- To get the version of K8s , run `kubectl get nodes`
- `cat /etc/*release* ` - Shows the OS version
- Hit Ctrl + D to exit out of SSH session when moving to Nodes and coming back or type `exit`
- `journalctl -u etcd.service -l` - List the service logs
## ETCD
- ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3. 
- By default its set to use Version 2. Each version has different sets of commands.
- To set the right version of API set the environment variable `ETCDCTL_API` command `export ETCDCTL_API=3`
- When API version is not set, it is assumed to be set to version 2. And version 3 commands listed below don't work. When API version is set to version 3, version 2 commands listed below don't work.
```BASH
# ETCDCTL version 2
etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set

# ETCDCTL version 3
etcdctl snapshot save 
etcdctl endpoint health
etcdctl get
etcdctl put

# This command sets the ETCD version to 3 and then shows all the keys in ETCD database and also sets the certificates
kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 
```
## API Server
- Api Server Configuration is stored
1. Using Kubeadm - Inside API Server Pod - `/etc/kubernetes/manifests/kube-apiserver.yaml`
2. As a Service - Inside the Master Node - `/etc/systemd/system/kube-apiserver.service`
```BASH
ps -ef | grep kube-apiserver        # To see all kube apiserver configuration
```
## Kube Controller Manager
- Watch Status
- Remediate Situation
1. Node Controller
2. Replicaton Controller
- Api Server Configuration is stored
1. Using Kubeadm - Inside API Server Pod - `/etc/kubernetes/manifests/kube-controller-manager.yaml`
2. As a Service - Inside the Master Node - `/etc/systemd/system/kube-controller-manager.service`
```BASH
ps -ef | grep kube-controller-manager        # To see all kube controller-manager configuration
```
## Kube Scheduler
- Assigning a Pod to a Node:
1. Filter Nodes
2. Rank Nodes
1. Using Kubeadm - Inside API Server Pod - `/etc/kubernetes/manifests/kube-scheduler.yaml`
2. As a Service - Inside the Master Node - `/etc/systemd/system/kube-scheduler.service`
```BASH
ps -ef | grep kube-scheduler       # To see all kube scheduler configuration
```
## Kubelet
- **NOTE**: Kubeadm does not install kubelet. Always install kubelet manually on the worker nodes.
- Always runs as a service on the worker nodes.
```BASH
ps -ef | grep kubelet       # To see all kubelet configuration
```
## Kube Proxy
- Process running on the worker node as a service.
## Manual Scheduling
- Add `nodeName` property in the pod definition to schedule a pod at creation time if there is no scheduler.

## Explain Commands
```BASH
# When is it useful: sometimes when editing/creating yaml files, it is not clear where exactly rsource should be placed (indented) in the file. Using this command gives a quick overview of resources structure as well as helpful explaination. Sometimes this is faster then looking up in k8s docs.
kubectl explian pods --recursive | grep envFrom -A3   # Prints lines after a match is found
# Output would be 
envFrom        <[]Object>         # This is an array of Objects, so the next line will be start with -
  configMapRef        <Object>    # This is an Object
    name     <string>             # This is a dictionary
    optional <boolean>
# 
kubectl explain cronjob.spec.jobTemplate --recursive | less
kubectl explain pods.spec.containers --recursive | less
```
## Pods
```BASH
kubetcl get pods -o wide        # Get the Node on which pod is running
```
- **Remember**: You **CANNOT** edit specifications of an existing POD other than the below.
1. spec.containers[*].image
1. spec.initContainers[*].image
1. spec.activeDeadlineSeconds
1. spec.tolerations
- For example: when you edit a pod in vi editor for environment variables, service accounts, resource limits. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable.
1. A copy of the file with your changes is saved in a temporary location when it fails. You can then delete the existing pod. Then create a new pod with your changes using the temporary file which was saved earlier in `/tmp`.
2. The second option is to extract the pod definition in YAML format to a file. Then make the changes to the exported file using an editor and save the file. Then delete the existing pod. Then create a new pod with the edited file. 
3. Force replace the pod using `kubectl replace -f <filename> --force`
## Replicasets
- `selector` is the difference between ReplicaSet and ReplicationController apart from apiVersion. 
```BASH
# Scaling RS
kubectl replace -f rs.yaml
kubectl scale --replicas=6 rs myapp-rs      # <Type> <Name of RS> format 
kubectl delete rs myapp-rs                  # Also deletes the underlying pods
### IMPORTANT
# Either delete and recreate the ReplicaSet or Update the existing ReplicaSet and then delete all PODs, so new ones with the correct image will be created.
```
## Deployments
```BASH
kubectl create deployment mydeploy --image=nginx --replicas=3
kubectl scale deployment mydeploy --replicas=6
```
- Edit Deployments - With Deployments you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification,  with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment.

## Formatting kubectl Output
```BASH
-o json         # Output a JSON formatted API object.
-o name         # Print only the resource name and nothing else.
-o wide         # Output in the plain-text format with any additional information.
-o yaml         # Output a YAML formatted API object.
```
## Namespaces
```BASH
kubectl config set-context $(kubectl config current-context) --namespace=test
# OR
alias kns=kubectl config set-context --current --namespace
kns test
# Testing services
# DNS resolution
<svc name>.<namespace>.svc.cluster.local:<svc port>  # cluster.local - Domain name, svc - subdomain name
```
## Imperative Commands
```BASH
`--dry-run`: # By default as soon as the command is run, the resource will be created. 
`-dry-run=client`: # This will not create the resource, instead, tell you whether the resource can be created and if your command is right.
# Generate POD Manifest 
kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml
# Generate Deployment with 4 Replicas
kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > deploy.yaml
# Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
# This will automatically use the pod's labels as selectors
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml > svc.yaml
# OR
# This will not use the pods labels as selectors, instead it will assume selectors as app=redis.
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
# So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service

# Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes
# This will automatically use the pod's labels as selectors, but you cannot specify the node port. 
kubectl expose pod nginx --port=80 --name nginx-service --type=NodePort --dry-run=client -o yaml
# OR
# This will not use the labels as selectors
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
# I would recommend going with the `kubectl expose` command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.

# Create Pod and Svc in one command
kubetcl run nginx --image=nginx --port=8080 --expose
```
##  Docker Commands
```BASH
CMD ["command","parameters"]            # Process which is executed in Docker container continuously
# example
CMD ["sleep", "5"]                      # Sleep is executed every 5 secs
# What is you want to pass parameters to Docker during execution, Use ENTRYPOINT
ENTRYPOINT ["sleep"]                    # Process invoked at startup
# To execute
docker run sleeper-image 10             # This will pass 10 to the Sleep process
# If no parameter is passed to Docker command, it will fail.
# Passing default parameter when no parameter is passed, Use ENTRYPOINT and CMD both in Dockerfile
ENTRYPOINT ["sleep"]                    # Process invoked at startup
CMD ["5"]                      # Default parameter passed to sleep, if not given during docker execution
# Suppose you want to override the default sleep process during execution
docker run --entrypoint ping sleeper-image 8.8.8.8  # Override sleep with ping process
```
##  K8s Commands and Arguments
```BASH
# Docker           # K8s
#---------------------------#
# ENTRYPOINT  -->  command
# CMD         -->  args

# Example
# In Dockerfile
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]
# In K8s, args is overriding the input
command: ["python", "app.py"]
args: ["--color", "pink"]
```
##  Environment Variables
- Passed as an array in key value format
- 3 Types of setting Env variables
1. Direct
1. Config Map
1. Secrets
```BASH
# Direct
env:
  - name: APP_COLOR
    value: pink
# ConfigMap
env:
  - name: APP_COLOR
    valueFrom: 
        configMapKeyRef:
# Secret
env:
  - name: APP_COLOR
    valueFrom:
        secretKeyRef:
```
## ConfigMap
```BASH
# From Literal
kubectl create configmap <config-map name> --from-literal=<key>-<value>
# From File
kubectl create configmap <config-map name> --from-file=<path-to-file>
# To reference a configMap file in pod definition
# ConfigMap with apiVersion etc defined
envFrom:
  - configMapRef:
      name: <configMap Name in Metadata>
# To reference a configMap volume in pod definition
volumes:
    - name: app-config-volume
      configMap:
        name:  <configMap Name in Metadata>
```
## Secrets
- The way kubernetes handles secrets. Such as:
- A secret is only sent to a node if a pod on that node requires it.
- Kubelet stores the secret into a `tmpfs` so that the secret is not written to disk storage.
- Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.
- Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault. 
```BASH
# From Literal
kubectl create secret generic <secret name> --from-literal=<key>-<value> # Note: generic is added
# From File
kubectl create secret generic <secret name> --from-file=<path-to-file>
# To encode text to base64
echo -n "Hello" | base64
# To view the secret information 
kubectl get secret app-secret -o yaml     # Output in yaml, then decode using base64
echo -n "aTsfgs*#" | base64 -d
# To reference a secret in pod definition
envFrom:
  - secretRef:
      name: <secret Name in Metadata>
# To reference a secret in volume definition
volumes:
    - name: app-secret-volume
      secret:
        secretName:  <secret Name in Metadata>    # Note the change of Key
```
## Security Context
- Security Context can be added at Pod and Container level.
- If defined at both levels, container configuration overrides the security context defined at pod level.
- **Note**: Capabilities are only supported at container level and NOT at Pod level.
```BASH
# Adding additional Linux capability during container execution in Docker
docker run --cap-add MAC_ADMIN ubuntu       # Adds additional capability to the container apart from defaults
# Adding security context to Pod in container section
securityContext:
  runAsUser: 1000
  capabilities:
    add: ["MAC_ADMIN"]
```
## Service Accounts
- Service Accounts are used by applications or services and not by humans.
- **Note:** SA cannot be added to existing Pod. Always Delete and add SA to Pod definition to recreate.
- K8s automatically mounts the `default` namespace SA. To override this behavior, set `automountServiceAccountToken: false` in the Pod definition.
```BASH
kubectl create sa dashboard-sa            # Create a SA
kubectl get secret dashboard-sa-token-kdbm  # K8s creates a secret to store the token to auth the service
# You can use the token to run K8s API calls
curl https://192.168.0.10:6443/api -insecure --header "Authorization: Bearer <sa token>"
```
## Resource Requirments
```BASH
1 Gi   - Gibibyte
1 Mi   - Mebibyte
1 Ki   - Kibibyte

1 CPU  = 1 vCPU or 1 Hyperthread

# If a Pod uses more CPU than its limit, it will be throttled.
# If a Pod used more Mem than its limit, it will be terminated.
```
- When a pod is created the containers are assigned a default CPU request of .5 and memory of 256Mi". For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a `LimitRange` in that namespace.
```YAML
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
---
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container
```
## Taints and Tolerations
- If a taint is placed, by default no pods will be scheduled on the Node.
- Only when a Pod has tolerations matching the taint, will the K8s scheduler place the pod on the tainted node.
- 3 taint-effects:
1. NoSchedule
2. PreferNoSchedule
3. NoExecute
- **Important**: Taints does not neccessarily mean that Pods matching the tolerations will be placed always on that tainted node. It can be placed on another Node which is not tainted. Taints and Tolerations is used only for restricting certain pods from being placed in it. To always place a pod on a tainted node, use `Node Affinity`.
```BASH
# Node Taint
kubectl taint nodes <node name> key=value:taint-effect
# Example
kubectl taint nodes node1 app=blue:NoSchedule
# Tolerations added to Pod definition
tolerations:
- key: "app"            # Note: This is placed under Pod not containers section
  operator: "Equal"     # Note: All values should be doube quoted
  value: "blue"
  effect: "NoSchedule"
# By default, master Node is always tainted. To see the taint
kubectl describe node kubemaster | grep Taint
# To remove the taint from a node, add a minus (-) symbol at the end of the taint with NO spaces in between
kubectl taint nodes node1 app=blue:NoSchedule-    # Note the - with no spaces
```
## Node Selectors
- Labels placed on the Nodes which help scheduler place the pods matching the labels
```BASH
# Adding the lable to the node
kubectl label node <node name> key=value
# Example
kubectl label node node1 size=Large
# NodeSelectors added to Pod definition
nodeSelector:
  size: Large
# Important: Labels are simple and can't be used for complex selection using OR or NOT operators.
# For example: Place Pods in Large or Medium Nodes. OR Place Pods in Nodes which are not Small.
# OR
# Example
kubectl label node node1 size=
# NodeSelectors added to Pod definition
nodeSelector:
  size: ""
```
## Node Affinity
- To overcome NodeSelector limitations, Affinity and Anti-Affinity is used.
- Node Affinity Types:
1. `requiredDuringSchedulingIgnoredDuringExecution`: The scheduler can't schedule the Pod unless the rule is met. This functions like nodeSelector, but with a more expressive syntax.
2. `preferredDuringSchedulingIgnoredDuringExecution`: The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod.
- 2 Types of Operators - `In` and `Exists`
- **Important**: Use a combination of taints and tolerations to Deny Pods from being placed on to it. Then Add labels to the Nodes. After that add NodeAffinity to ensure the matching Pod goes to the correct Node.
```YAML
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/os
          operator: In
          values:
          - linux
```
## Multi-Container Pods
- Design Patterns 
1. Side car - Example is a logging container which ships the logs to a central logging service
2. Adapter - Example is a logging container which converts the logs to a standard format before shipping
3. Ambassador - Outsourcing the database connection to a separate container based on environments which acts as a proxy to the database service. The application always refers to the database using a standard dns name.  
## Init Pods
- In a multi-container pod, each container is expected to run a process that stays alive as long as the POD's lifecycle. For example in the multi-container pod that we talked about earlier that has a web application and logging agent, both the containers are expected to stay alive at all times. The process running in the log agent container is expected to stay alive as long as the web application is running. If any of them fails, the POD restarts.
- But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or binary from a repository that will be used by the main web application. That is a task that will be run only one time when the pod is first created. Or a process that waits for an external service or database to be up before the actual application starts. That's where initContainers comes in.
- When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts.
- You can configure multiple such initContainers as well, like how we did for multi-pod containers. In that case each init container is **run one at a time in sequential order**.
- If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.
```YAML
containers:
- name: myapp-container
  image: busybox:1.28
  command: ['sh', '-c', 'echo The app is running! && sleep 3600']
initContainers:
- name: init-myservice
  image: busybox
  command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ;']
# Another Example of Sequential execution of Init Containers
containers:
- name: myapp-container
  image: busybox:1.28
  command: ['sh', '-c', 'echo The app is running! && sleep 3600']
initContainers:
- name: init-myservice
  image: busybox:1.28
  command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
- name: init-mydb
  image: busybox:1.28
  command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
```
```BASH
# To debug the Init container logs
kubectl logs -c <Init container name>     # Shows the exact error
```
## Readiness and Liveness Probes
- Status of a Pod Lifecycle: `PodScheduled --> Initialized --> ConatinersReady --> Ready` 
- Liveness - Test for checking if your application is working
- Liveness and Readiness Probes have the same configuration.
```BASH
# Readiness probes based on the protocol
# Http
readinessProbe:
  httpGet:
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10     # Tells to wait before checking
  periodSeconds: 5            # Interval between each attempt
  failureThreshold: 8         # How many attempts
# TCP
readinessProbe:
  tcpSocket:
    port: 3306
# Exec
readinesProbe:
  exec:
    command:
      - cat
      - /app/is_ready
```
## Container Logging
```BASH
# Docker Logs
docker run -d kodekloud /event-simulator      # Logs are not streamed as its running in detached mode
docker logs -f <container id>                 # Shows the container logs
# K8s logs
kubectl logs -f event-simulator-pod           # -f = Live streaming of logs
# Multiple containers in pod
# Get pods and see if there are more than 1 containers and then after -c do a tab to see the container names
kubectl logs -f event-simulator-pod -c event-simulator  # You can skip -c and directy mention container name
```
## Monitor and Debug Aplications
- Open sources projects to monitor clusters, Metrics server, Prometheus
- You can have **1 Metrics server per cluster**. It is a In-memory solution and does not store data in disk.
- Kubelet agent has cAdvisor (container Advisor) component which extracts performance metrics.
- cAdvisor then makes this data available via the K8s API to the Metrics server.
```BASH
# Download the Metrics server from Github
git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git
# Apply the metric server components
kubectl apply -f .
# OR
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
# After metrics is collected after some time lag, run the commands
# To wait for the output to come
watch "kubectl top node" # Note: the command in double quote. Ctrl + c to exit      
kubectl top node         # Get the CPU and memory consumption of each node
kubectl top pod          # Pod performance       
```
## Labels, Selectors and Annotations
```BASH
kubectl get pods --show-labels        # List labels
kubectl get pods -l env=dev           # Filter Labels using short form
kubectl get all --selector=env=prod --no-headers | wc -l   # Filter all objects and remove headers 
kubectl get pods --selector=env=prod,bu=finance,tier=frontend # Logical AND
# Tip
# In ReplicaSet or Service, the matchLabels in the spec.selector section should always match the pod labels in the spec.template.metadata.labels.
# Error: "selector" does not match template 'labels'
```
## Update and Rollback Deployments
- 2 Deployment strategy - RollingUpdate (default) and Recreate
```BASH
# Create Deployments
kubectl create -f deployments-definition.yml          # Using yaml format
# OR
kubectl create deployment my-app-deployment --image=nginx
kubectl apply -f deployments-definition.yml           # To update a deployment
# OR Without changing definition file, updating parameters, 
# NOTE nginx is the container name in existing pod/deployment
kubectl set image deployment/my-app-deployment nginx=nginx:1.9  # Image is upgraded
# Roll-out strategy
kubectl rollout status deployment/my-app-deployment   # Shows rollout status
kubectl rollout history deployment/my-app-deployment  # Shows rollout history and revisions
# You can check the status of each revision individually by using the --revision flag:
kubectl rollout history deployment/my-app-deployment --revision=1 # Shows detailed history
# We can use the --record flag to save the command used to create/update a deployment against the revision number. Change is recorded as annotation in the deployment as "change-cause".
kubectl set image deployment/my-app-deployment nginx=nginx:1.7 --record
# OR
kubectl edit deployments my-app-deployment --record
kubectl rollout undo deployment/my-app-deployment     # Rolls back to previous version
```
## DaemonSets
```BASH
kubectl get daemonsets
# TIP: There is no kubectl create daemonset, so do a create deployment, get this into a yaml.
# Format the yaml for Kind, Remove replicas, strategy and save the file.
```
## Static Pods
- When there is no Master Node and its components, you can create Pods on standalone Worker Node.
- Such a pod is called `Static Pod`
- To inspect the path where the definition is defined, look at the kubelet.service. It could be in 2 places:
1. --config=<file name>     - kubeadm installation
2. --pod-manifest-path=<file path>  - manual installation 
- The pod definitions have to be placed for example in `/etc/kubernetes/manifests` in a yaml file.
- Deleting the yaml file, removes the pod from the node.
- To identify static pods, `pod name` will have the `node name` appended at the end.
- kubeadm deploys the cluster components as static pods, which have `controlplane` appended in the pod name.
- Another way to identify static pod is to get the yaml of the pod and then searching for `ownerReferences`. In that if `kind: Node` then its a static pod.
```BASH
# To view the pods after creation, as kubectl will not work
docker ps
# To kill the pod
docker container rm <id>
# Kubelet config
/var/lib/kubelet/config.yaml
```
## Jobs
```BASH
# Docker execution of mathematical problem
docker run ubuntu expr 3 + 2          # Task is completed and container exits
docker ps -a                          # Shows the exit status of the container
# For batch processing Jobs are used in K8s
kubectl create job throw-dice --image=kodekloud/throw-dice --dry-run=client -o yaml > job.yml
# NOTE: Add backofflimit parameter if its not in the generated template to avoid job from quiting before it succeeds
kubectl get jobs          # list the jobs
kubectl get pods          # lists the pods created by the job
kubectl logs <pod-name>   # shows the pod output
# Running multiple pods in sequence, add completions parameter to the Job spec. 
# NOTE: This is the successful pod completion count, it will keep on recreating pods till this number matches.
# Running multiple pods in parallel, add parallelism parameter to the Job spec along with completion. 
```
## Cronjobs
```BASH
# Min-Hour-DOM-Month-Day of Week (0-6)    # Sun - 0 & 7 both, Sat -6
# spec.schedule is the additional parameter added.
# NOTE: schedule is at the first spec
kubectl create job throw-dice --image=kodekloud/throw-dice --schedule="30 21 * * *"--dry-run=client -o yaml > cronjob.yml
kubectl get cronjob       # list the cronjob
```
## Services
- 3 Types to access a service
1. NodePort: Mapping a port on the Node to a port on the pod
2. ClusterIP: Internal Virtual IP not exposed out of the Node
3. LoadBalancer: External IP which load balances multiple ports on the Node
- NodePort: K8s takes care of deploying the service across all nodes, even though the pod is not on those nodes. This helps in getting the same Nodeport exposed on all the Nodes. When a http call hits a nodeport on a node, K8s will route traffic internally to the Pod on the correct Node. 
```BASH
# NodePort (Node) --> Port (Svc) --> TargetPort (Pod)
# Target Port is the Pod port where the service forwards requests to
# Port is on the Service
kubectl create deployment frontend --replicas=2 \
    --labels=run=load-balancer-example --image=busybox  --port=8080
kubectl expose deployment frontend --type=NodePort --name=frontend-service --port=6262 --target-port=8080 --dry-run=client -o yaml > svc.yml
# This is because we cant control the value of NodePort in imperative command. Edit the yaml file and add the nodePort parameter under spec.ports.port.nodePort 
kubectl get services      # List services
# Take the NodePort and use the Node IP to hit the service from outside the machine 
```

## Multiple Kube Schedulers
![K8s Multiple Scheduler Config](../assets/images/k8s-multiple-scheduler-config.png)
![K8s Multiple Scheduler Pod Config](../assets/images/k8s-multiple-scheduler-pod-config.png)
![K8s Multiple Scheduler Events](../assets/images/k8s-multiple-scheduler-events.png)
![K8s Multiple Scheduler Logs](../assets/images/k8s-multiple-scheduler-logs.png)

- Advanced Scheduling
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md

https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/


https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/

https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work

## OS Upgrades
```BASH
kubectl drain node1         # Gracefully evict the pods
kubectl cordon node1        # Makes the node unschedulable
kubectl uncordon node1      # Makes its schedulable after maintenance
kubectl drain node01 --ignore-daemonsets        # Incase you get the ds exist error
# Even with ignore ds options, you can get error when there are pods which are not managed by a replicaSet
# error: unable to drain node "node01" due to error:cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override)
# In this case, copy the pod data into yaml and apply it again after draining the nodes using --force
```
## K8s Release Strategy
https://kubernetes.io/docs/concepts/overview/kubernetes-api/
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md

## Cluster Upgrade
- First upgrade master, then the nodes
- Kubeadm does not update the kubelet, so it needs to be updated manually
```BASH
cat /etc/*release*          # Shows the OS version
# When you do apt-cache update, it will show which kubeadm minor version is present
# TIP: Copy the upgrade commands to notepad, before updating versions and pasting
kubeadm upgrade plan        # Shows the upgrade plan
apt-get upgrade -y kubeadm=1.12.0-00        # 1st Update kubeadm as per plan
kubeadm upgrade apply v1.12.0   # Upgrade the cluster controlplane 
# NOTE: Master will show version of kubelet, which has not been updated yet.
kubectl get nodes           # Master is still shown with V1.11 version
# Drain the master and then perform kubelet upgrade
apt-get upgrade -y kubelet=1.12.0-00  # Upgrade the kubelet
systemctl restart kubelet      # Make the change permanent
kubectl get nodes           # Master is now shown with V1.12 version
# Now follow the process of upgrading Nodes using drain and cordon
# IMPORTANT: the Node Drain command needs to be run on the master and not Node01. .e. all kubelet commands like drain and uncordon needs to be run on master
# Follow the upgrade process, upgrade kubeadm, upgrade kubelet , upgrade node and then restart kubelet
# Uncordon the Node
# NOTE: the Node upgrade command
kubeadm upgrade node config --kubelet-version v1.12.0   # Upgrade the node
```
# Backup and Restore
- To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3. `export ETCDCTL_API=3`
- For example, if you want to take a snapshot of etcd, use: `etcdctl snapshot save -h` and keep a note of the mandatory global options.
- Since our ETCD database is TLS-Enabled, the following options are mandatory:
1. `--cacert` - verify certificates of TLS-enabled secure servers using this CA bundle
2. `--cert` - identify secure client using this TLS certificate file
3. `--endpoints=https://127.0.0.1:2379` - This is the default as ETCD is running on master node and exposed on localhost 2379.
4. `--key` - identify secure client using this TLS key file
- Similarly use the help option for snapshot restore to see all available options for restoring the backup.
`etcdctl snapshot restore -h`
```BASH
# Manual Backup of applications
kubectl get all --all-namespaces -o yaml > all-deployed-services.yaml
# Backup ETCD
ETCDCTL_API=3 etcdctl snapshot save snapshot.db
ETCDCTL_API=3 etcdctl snapshot status snapshot.db       # View backup status
# To restore ETCD
# First stop the kube-apiserver 
service kube-apiserver stop
# Execute restore from the saved state
ETCDCTL_API=3 etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup
# Here the data-dir is a new dir where the ETCD will start storing the data.
# This is done so that the existing data is not overwritten, in case of any issues during restore
# Reload the service daemon and restart etcd service
systemctl daemon-reload
service etcd restart
# Finally start the kube-apiserver 
service kube-apiserver start

# Tips: To check the version of etcd, check the etcd pod logs or the image version in the etcd pod
kubectl describe pod etcd-controlplane -n kube-system
# Get the below 4 parameters after describe etcd pod
ETCDCTL_API=3 etcdctl snapshot save /opt/snapshot-pre-boot.db \
	 --cacert="/etc/kubernetes/pki/etcd/ca.crt" \
	  --cert="/etc/kubernetes/pki/etcd/server.crt" \
	  --endpoints=https://127.0.0.1:2379  \
	  --key="/etc/kubernetes/pki/etcd/server.key" 
# Restore the etcd to a new directory from the snapshot 
# So move the backed up data to a new dir
ETCDCTL_API=3 etcdctl snapshot restore /opt/snapshot-pre-boot.db \
	 --data-dir="/var/lib/backup-from-etcd" 
# Certificate details are not required in restore as the file in in local directory.
# Next, update the /etc/kubernetes/manifests/etcd.yaml:
# We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory (/var/lib/etcd-from-backup).
volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
```
https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md
https://www.youtube.com/watch?v=qRPNuT080Hk

## Security
![K8s Security Across Components](../assets/images/k8s-security-across-components.png)
![K8s Security Actors](../assets/images/k8s-security-actors.png)
![K8s Security basics](../assets/images/k8s-security-basics.png)
### TLS
- There are 2 types of Encryption mechanisms: `Symmetric and Asymmetric`
- For TLS both the mechanims are used:
1. Symmetric - To encrypt the client data
2. Asymmetric - To encrypt the symmetric key after server auth
- In symmetric, one key is used to encrypt and decrypt the data, while in asymmetic 2 keys are used.
![TLS Key Types](../assets/images/tls-key-types.png)
- Public Key (will be refered as Lock). You can encrypt data with either private or public key.
- **NOTE**: Decryption is only done using the opposite key. For example, if you encrypt the data with public key, you cannot decrypt it using public key, you **MUST** using the private key.
![TLS Key Naming Convention](../assets/images/tls-key-naming.png)
- Keys have naming convention, private key will always have `key` in the name to identify it as private key.
- There are 3 usecases of using Asymmetric Encyption in the TLS process
1. Using SSH to login to any server. In this case, user private key is used to unlock the server access having the user's public key stored on the server.
![Server Auth using TLS](../assets/images/tls-auth-for-servers.png)
2. Using asymmetric keys to transfer client's symetric key over the Internet. These are called **Server Certificates**. This should ensure no hacker is allowed to decrypt the data in transit. 
![Websites Auth using TLS](../assets/images/tls-auth-for-websites.png)
![Websites Auth Usecase using TLS](../assets/images/tls-website-auth-usecase.png)
3. Using asymmetric keys to authenticate the website. These are called **Root Certificates**. In this case, the website presents a CA certificate which is having the CA pulic key embedded. The client presents this via the browser which has CA's public keys installed, which decrypts the certificate using the public key to authenticate the website. 
![CA Auth Usecase using TLS](../assets/images/tls-ca-auth-usecase.png)
![CA Functions using TLS](../assets/images/tls-ca-functions.png)
- **TLS Overview**
![TLS Process Overview](../assets/images/tls-process-overview.png)
- A system admin generates private and public key to enforce SSH authenication. The public key is stored in the server.
- Web Server generates private and public key to encrypt HTTPS trafic. For this, the web server generates a certificate signing request using its public key. This CSR (which has the public key of the server) is sent to the CA for signing the certificate.
- The CA signs the certificate using its private key and sends it back to the server after completing its validation process.
- When a client request comes, the web server sends its certificate having its encrypted public key back.
- The client presents the certificate to the browser. The browser using the CA public key, decrypts the certificate, thus authenticating the web server and thus the website.
- The decrypted certificate has the server's public key. The client generates its `symmetric key` and then encrypts this using the web server's public key and sends the request back to the web server.
- The web server decrypts the request using its private key and gets access to the client's symmetric key.
- Now the communication between the client and web server will continue happening using this symmetric key.
### TLS in Kubernetes
- There are 2 types of certificates used in Kubernetes components
1. Server Certificates - Used by the server components
2. Client Certificates - These are certificates used by Users or process to authenticate themselves.
![K8s TLS Certificate Types](../assets/images/ks8-tls-cert-types.png)
![K8s TLS Server Certificate](../assets/images/k8s-tls-server-certs.png)
- Types of Clients for the API server:
1. Admin Users
2. Kube Scheduler
3. Kube Controller Manager
4. Kube Proxy
- There is only one client for the ETCD server: API Server
- There is also one client for the Kubelet server: API Server
![K8s TLS Client Certificate](../assets/images/k8s-tls-client-certs.png)
- All the certificates (Server and Client) need to be signed by a Root certificate that is issued by a CA.
![K8s TLS CA Certificate](../assets/images/k8s-tls-ca-certs.png)
### Certificate Creation
```BASH
# We require only Private Keys and Certificates
# Generate CA certificates
openssl genrsa -out ca.key 2048
openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr
# Sign the CSR to generate the Root Cert
openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt

# Generating Client Key and Certificates
openssl genrsa -out admin.key 2048
openssl req -new -key admin.key -subj "/CN=kube-admin/O=system:masters" -out admin.csr
# Remember CN will the user name that is used to login to API server
# O parameter should link to the user account, in this case its the admin group
openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt
# Here the CA crt and CA key is used to sign the user csr

# Once the cluster is configured, you an use the certificates in Rest API calls
curl https://kube-apiserver:6443/api/v1/pods \
--key admin.key --cert admin.crt
--cacert ca.crt

# Generating Server Key and Certificates
openssl genrsa -out apiserver.key 2048
openssl req -new -key apiserver.key -subj "/CN=kube-apiserver" -out apiserver.csr -config openssl.cnf
# Additional openssl.cnf contains DNS Alias or IP address which refers back to the api-server
openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -out apiserver.crt
```
### Debugging Certificates
```BASH
# View existing certificates
openssl x509 -req -in /etc/kubernetes/pki/apiserver.crt -text -noout
# Verify the Subject, Subject Alternative Names (Alias), Not After (to check validity), Issuer (CA)

# Debugging Certificate Issues
# When installed as service
journalctl -u etcd.service -l 		# List the service logs
# When installed using kubeadm
kubectl logs etcd-master
## Failure logs - Failed to dial 127.0.0.1:2379: connection error

# If ETCD or Apiserver is down, use docker
docker ps -a		# To view the containers
docker logs <container id>		# To view the logs
```
### Certificates API
![K8s Certificate Functions](../assets/images/k8s-certificate-functions.png)
![K8s Certificate Object](../assets/images/k8s-certificate-object.png)
- **NOTE**: To view the CA certificates, as its stored on the master look at the Controller Manager configuration and check the path mentioned under `--cluster-signing-cert-file` and `--cluster-signing-key-file`
```BASH
kubectl get csr				# Get pending CSR 
kubectl certificate approve jane # Approve CSR
kubectl get csr jane -o yaml	# View the CSR
echo "data" | base64 --decode	# Extract the Certificate from CSR after it is approved and send to user jane
# Now she will be able to login to the cluster
```
## Image Security
```BASH
# Create a Docker Registry secret in the cluster for Kubelet to pass this to the Docker Runtime on the worker nodes
kubectl create secret docker-registry regcred \
--docker-server=<URL> \
--docker-username=<> \
--docker-password=<> \
--docker-email=<email>
# Once the secret is created, use this in the Pod definition
imagePullSecrets:
- name: regcred
```

## Deployment Strategies
1. Recreate
2. RollingUpdate
- The above 2 deployments are native to K8s. While the below 2 needs several steps to complete.
3. Blue Green Deployment
![Blue Deployment](../assets/images/k8s-blue-deploy.png)
![Green Deployment](../assets/images/k8s-green-deploy.png)
![Blue Green Deployment](../assets/images/k8s-blue-green-deploy.png)
- Switch is done in the service object before the blue deployment is killed completely.
4. Canary Deployments
- Reduce the number of replicas in the canary deployment initially, then scale up after testing
![Canary Deployment](../assets/images/k8s-canary-deploy.png)
![Canary Deployment Manifest](../assets/images/k8s-canary-manifest.png)

## Docker Volume
- Docker creates `Read Only` layers for the image
![Docker Image Layer](../assets/images/docker-image-layer.png)
- For the container a transient `Read-Write` layer is created.
![Docker Container Layer](../assets/images/docker-container-layer.png)
- When a volume is created and then it is mounted using the `-v` option, its is called `Volume Mount`.
- If a volume is not present, but a volume is mounted, Docker will create the volume at run time.
![Docker Volume Mount](../assets/images/docker-volume-mount.png)
- When a host directory is mounted, it is called `Bind Mount`
- Use the new convention `--mount` instead of `-v` in the new version of Docker.
![Docker Bind Mount](../assets/images/docker-bind-mount.png)

## Container Storage Interface
![K8s Interfaces](../assets/images/k8s-interfaces.png)
![K8s Storage Interface](../assets/images/k8s-storage-interface.png)


## Volumes
![Volumes and Host Path](../assets/images/volumes-hostpath.png)
## Persistent Volumes and Claims
![PV and PVC](../assets/images/pv-pvc.png)
- Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this:
```YAML
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
```
```BASH
kubectl get persistentvolume
kubectl get persistentvolumeclaim
```

## Storage Class
- When we create PV, then it is called `Static Provisioning` of Storage.
- In cloud, there is need to create storage dynamically when PVC is used. 
- In this case, only Storage Class Objects are created. When a claim is made wihich references the Storage class, a PV is **dynamically** created. This is `Dynamic Provisioning` of Storage.
![K8s Storage Class](../assets/images/k8s-storage-class.png)
- The Storage Class makes use of `VolumeBindingMode` set to `WaitForFirstConsumer`. This will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created. Till then the PVC will be in `Pending State`.
```BASH
kubectl get storageclass
```
## Networking Basics
- Switching allows to configure machines to talk to each other in the same network
![Switching](../assets/images/switching.png)
- Routing allows machines to talk across 2 or more networks
![Routing](../assets/images/routing.png)
- Gateway allows machines to reach other network using a single entry point made in the routing table of each machine
![Gateway](../assets/images/gateway.png)
- When a machine wants to reach to the internet, manual routes can be made to reach the destination on each machine
![Manual Routing](../assets/images/manual-routing.png)
- To avoid the hassle of making entry for each IP address available on the Internet on each machine, a default route entry is made. When a route does not match, it goes through the default route.
- To separate Internal and External routing, you can make separate entries in the routing table of the machine.
![Default Gateway](../assets/images/default-gateway.png)
## Networking Native Commands
![K8s CNI Functions](../assets/images/k8s-cni-functions.png)
![K8s Create Container Network](../assets/images/k8s-create-container-network.png)
![K8s Virtual Bridge Network Connection](../assets/images/k8s-virtual-network-connection.png)
![K8s Port](../assets/images/k8s-ports.png)
![K8s Native Network Commands](../assets/images/k8s-native-network-commands.png)

- What is the network interface configured for cluster connectivity on the controlplane node? Run the `ip a / ip link `command and identify the interface.
- What is the MAC address of the interface on the controlplane node? Run the command: `ip link show eth0`
- What is the MAC address assigned to node01? Run the command: `arp node01` on the controlplane node.
- What is the state of the interface docker0? Run the command: `ip link show docker0` and look for the state.
- If you were to ping google from the controlplane node, which route does it take? What is the IP address of the Default Gateway? Run the command: `ip route show default` and look at for default gateway.
- What is the port the `kube-scheduler` is listening on in the `controlplane node`? Use the command: `netstat -tunlp`
- Notice that ETCD is listening on two ports. Which of these have more client connections established? Run the command: `netstat -anp | grep etcd`. That's because 2379 is the port of ETCD to which all control plane components connect to. 2380 is only for etcd peer-to-peer connectivity. When you have multiple controlplane nodes.
## CNI
```BASH
ps aux | grep kubelet			# Show kubelet configuration for CNI
ls /opt/cni/bin					# Shows all CNI executables
ls /etc/cni/net.d				# Shows the configuration files

kubectl exec busybox -- ip route	# To show the routes a pod can take. Deploy a busybox pod first
```
- What binary executable file will be run by kubelet after a container and its associated namespace are created. Look at the type field in file /`etc/cni/net.d/10-flannel.conflist`.
## Deploy Weave for Pod Networking
```BASH
# Deploy Weave CNI, use the bookmark to find the command
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
# To Trouble shoot the deployment
kubectl logs -n kube-system <weave pod name> -c weave 

# If CNI is not deployed you get the following error in App Pod
# failed to set up sandbox container network for pod "app": networkPlugin cni failed to set up pod "app_default" network: unable to allocate IP address
ip a | grep eth0			# Identify the Host Network
kubectl logs -n kube-system weave-net 	# Show the current failure logs
# If error says "Network 10.232.0.0/12 overlaps with the existing route" of the host network, you need to allocate Weave Network to a differentIP range.
# In this case, add &env.IPALLOC_RANGE=10.50.0.0/16 in the kubectl apply command. #NOTE: this env should be inside the quotes.

# Once deployed, you can view the interface that weave creates, by
ip link show weave
# The network is configured with weave. Check the weave pods logs using command kubectl logs <weave-pod-name> weave -n kube-system and look for ipalloc-range
# The above command will show POD IP address range configured by weave

# What is the default gateway configured on the PODs scheduled on node01 by weave?
kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- sleep 1000 > po.yaml
# Add nodeName: node01 to install the pod in Node01 and then exec to find the default gateway used by the Pod
kubectl exec busybox -- route
# OR
kubectl exec busybox -- ip route
```
## Service Networking
- Kube-Proxy assigns service ip and port combination to the iptables to enable networking.
```BASH
ps aux | grep kube-api-server 		# API server has the service ip range configured
# search for --service-cluster-ip-range value to get the service ip range. Pod ip will come from the CNI plugin
# OR
cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range
kubectl get service			# Shows the service ip
iptables -L -t nat | grep <service-name>		# Shows the IP table rules for the service
# Check the kube-proxy logs on the node
cat /var/log/kube-proxy.log			# Shows the logs of what iptable proxy it uses and also when a service entry is made to iptables
```
## DNS
- You need a DNS server and it can help manage name resolution in large environments with many hostnames and Ips and then configure your hosts to point to a DNS server. Domain names and the IP address of all the host in the network are added to the `/etc/hosts` file of the DNS server.
![DNS Basics](../assets/images/dns-basics.png)
- host file of the individual server is made to point to the DNS server for getting server names resolved to IP addresses. 
![DNS Basics 1](../assets/images/dns-basics-1.png)
- When the servers need to resolve a name which is not part of the internal network, the internal DNS server can point to an `external` DNS server to handle the name resolution. In this case, ping to facebook.com will get correctly resolved using Google DNS.
![DNS Basics 2](../assets/images/dns-basics-2.png)
- High level domain names are categorized based on their functions
![DNS Domains](../assets/images/dns-domains.png)
- Domains are further sub-divided into sub domains. Root (.) being the top most level. 
![DNS Domain Structure](../assets/images/dns-domain-structure.png)
- DNS resolution request originating from an internal company DNS traverses through the Root domain servers till it gets a match and then the IP address is sent back to the calling DNS, where the response is cached based on Time To Live parameter.
![DNS Domain Name Resolution](../assets/images/dns-domain-name-resolution.png)
- `A` records are stored which maps the DNS to the IP address. When you want yone DNS to map to multiple alias, you use the `CNAME` record.
- Using `search` inside an organization you can alias your domains. In this way, you can address your servers with only the sub-domain. search will append the domain name and then resolve the IP address
![DNS Search](../assets/images/dns-search.png)
## Cluster DNS
![K8s DNS Resolution](../assets/images/k8s-dns-resolution.png)
- Pod IP is replaced by `-` to derive its POD name in the DNS
## CoreDNS
```BASH
# Where is the configuration file located for configuring the CoreDNS service? Inspect the Args field of the coredns deployment and check the file used.
cat /etc/coredns/Corefile		# Shows the plugins configured and the error handling
kubectl get svc -n kube-system	# Shows the core dns service which is made available to all the pods
# The IP address shown as the Cluster-IP of this service is the Nameservice that is configured inside all the pods `/etc/resolv.conf` file which then knows which DNS they need to point to.
# Kubelet makes sure this entry is made inside all pods.
# View the kubelet service config yaml to see the DNS config
cat /var/lib/kubelet/config.yaml		# clusterDNS section points to the coreDNS service cluster IP

# How is the Corefile passed in to the CoreDNS POD? Use the kubectl get configmap command for kube-system namespace and inspect the correct ConfigMap. It will be passed as a Config Volume
# What is the root domain/zone configured for this kubernetes cluster? Run the command: kubectl describe configmap coredns -n kube-system and look for the entry after kubernetes. This is where cluster.local as root domain is configured
# Check the FQDN of any service
host web-service		# DNS will return the FQDN
# Checking the FQDN of pod is not possible, you need to specify the FQDN
host 10-244-4-5.default.pod.cluster.local

# hr app POD is hr namespace and mysql Service is in payroll namespace. From the hr pod nslookup the mysql service and redirect the output to a file /root/CKA/nslookup.out
kubectl exec -it hr -- nslookup mysql.payroll > /root/CKA/nslookup.out
# NOTE: the DNS name of the service is used along with namespace
```
## Ingress Networking
![Ingress Rules](../assets/images/ingress-rules.png)
- Create a `default-backend` deployment to handle routes that are not managed.
![Ingress Default Backend](../assets/images/ingress-default-backend.png)
- Create a service `default-backend-service` to manage 404 error handling and link to the ingress resource.
![Ingress 404 Error](../assets/images/ingress-default-404.png)
![Ingress Host Based](../assets/images/ingress-host-based.png)
![Ingress Definition](../assets/images/ingress-definition.png)
- **NOTE**: Ingress needs to be deployed in the same namespace as the deployment & service object.
```BASH
# Imperative command from K8s 1.20
kubectl create ingress <ingress-name> --rule="host/path=service:port"
# Example of Imperative
kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"
kubectl get ingress         # list the ingress
```
### Rewrite Target Option
- Our `watch` app displays the video streaming webpage at `http://<watch-service>:<port>/`
- Our `wear` app displays the apparel webpage at `http://<wear-service>:<port>/`
- We must configure Ingress to achieve the below. When user visits the URL on the left, his request should be forwarded internally to the URL on the right. Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forwarded users to the appropriate application in the backend. 
```HTML
http://<ingress-service>:<ingress-port>/watch` --> http://<watch-service>:<port>/
http://<ingress-service>:<ingress-port>/wear --> http://<wear-service>:<port>/
```
- Without the rewrite-target option, this is what would happen:
```HTML
http://<ingress-service>:<ingress-port>/watch --> http://<watch-service>:<port>/watch
http://<ingress-service>:<ingress-port>/wear --> http://<wear-service>:<port>/wear
```
- **Notice** `watch` and `wear` at the end of the target URLs. The target applications are not configured with `/watch` or `/wear` paths. They are different applications built specifically for their purpose, so they don't expect `/watch` or `/wear` in the URLs. And as such the requests would fail and throw a 404 not found error.
- To fix that we want to **"ReWrite"** the URL when the request is passed on to the `watch` or `wear` applications. We don't want to pass in the same path that user typed in. So we specify the `rewrite-target` option. This rewrites the URL by replacing whatever is under `rules->http->paths->path` which happens to be `/pay` in this case with the value in rewrite-target. This works just like a **search and replace function**.
```BASH
For example: replace(path, rewrite-target)
In our case: replace("/path","/")
```
```YAML
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /pay
        backend:
          serviceName: pay-service
          servicePort: 8282
```
```BASH
replace("/something(/|$)(.*)", "/$2")
```
- In this ingress definition, any characters captured by (.*) will be assigned to the placeholder $2, which is then used as a parameter in the rewrite-target annotation.
```BASH
rewrite.bar.com/something rewrites to rewrite.bar.com/
rewrite.bar.com/something/new rewrites to rewrite.bar.com/new
```
```YAML
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80
        path: /something(/|$)(.*)
```
## Network Policies
![Network Traffic](../assets/images/network-traffic.png)
![Network Traffic Details](../assets/images/network-traffic-details.png)
![Network Pod Traffic](../assets/images/network-pod-traffic.png)
- **Important**: Always look at Network policy from the perspective of the Pod. For Rules - Always pay attention to the **Request** (Ingress) and not the Response (Egress) as that may be already blocked due to cluster wide network policy.
- Example: If DB pod needs to be accessed by API pod, then in DB pod the traffic is Ingress.
- Flannel does not support Network Policy. If Network policy is still applied to this network, it will not have any effect.
- Usecase 1: Rule - Apply Ingress policy using Pod Labels
![Network Policy With Pod Selector](../assets/images/network-policy-1.png)
- Usecase 2: Rule - Apply Ingress policy using Pod Labels and Namespaces
- This is a Logical AND operation where pods have same labels in other namespaces are ignored.
![Network Policy With Pod And Namespace](../assets/images/network-policy-2.png)
- Usecase 3: Rule - Apply Ingress policy using Namespace only
![Network Policy With Namespace](../assets/images/network-policy-3.png)
- Usecase 4: Rule - Apply Ingress policy using External IP
- This is required, where the service resides outside the cluster and the service needs to connect to it.
- Along with the Pod label AND Namesace, External service is an OR. So the Pod should have the correct label AND in the namespace OR extrenal service should have the IP. Either of these 2 rule matches traffic will be allowed.
![Network Policy With External Service](../assets/images/network-policy-4.png)
- Usecase 5: Rule - Apply Ingress policy using 3 Rules and OR operation
- **NOTE**: There is a hypen to `namespaceSelector. This makes it as a separate rule. 
![Network Policy with Logical OR](../assets/images/network-policy-5.png)
- Usecase 6: Rule - Apply Egress policy using External Service
- The traffic is allowed from pod to the external service
![Network Polciy for Egress](../assets/images/network-policy-6.png)
```BASH
kubectl get networkpolicy
```

## Kubeconfig
![Api Server Curl Call](../assets/images/k8s-api-server-curl.png)
![Kubeconfig Default data](../assets/images/kubeconfig-default-data.png)
![Kubeconfig Data Mapping](../assets/images/kubeconfig-mapping.png)
![Kubeconfig Data Mapping with Namespaces](../assets/images/kubeconfig-mapping-namespace.png)
```BASH
kubectl view config     # $HOME/.kube/config file which is default is read
kubectl view config --kubeconfig=my-custom-config   # Pass a config file not in .kube dir
kubectl config use-context prod-user@production   # Sets the current context
```
## API Versions
![K8s API Groups](../assets/images/k8s-apis.png)
![K8s Core API Group](../assets/images/k8s-core-api-group-resources.png)
![K8s Named API Group](../assets/images/k8s-named-api-group-resources.png)
- **Important**: kube proxy != kubectl proxy. 
```BASH
# To access the K8s API from a local server, start the kubectl proxy.
# This proxy will use the kubeconfig data in the default kubeconfig
kubectl proxy           # start the proxy service on port 8001. Use 8001 instead of 6443
kubectl http://localhost:8001 -k    # Shows all the API groups
kubectl http://localhost:8001 -k | grep "name"    # Shows the named API groups
```
## Deprecated AP Versions
![K8s API Versions](../assets/images/k8s-api-versions.png)
![K8s API Versions Identification](../assets/images/k8s-api-version-explain.png)
```BASH
# Convert an old API formatted file to a new stable version
kubectl convert -f <old-file> --output-version <new api version>    
kubectl convert -f nginx.yaml --output-version apps/v1
```

## Authentication and Authorization
- Authentication
![K8s User Account](../assets/images/k8s-user-interaction.png)
![K8s Auth Mechanisms](../assets/images/k8s-auth-mechanisms.png)
![K8s Basic Auth Setup](../assets/images/k8s-basic-auth.png)
![K8s Basic Auth Rest Call](../assets/images/k8s-basic-auth-curl.png)
![K8s Basic Auth Token Rest Call](../assets/images/k8s-basic-auth-token.png)
- Authorization
![K8s Authorization Overview](../assets/images/k8s-author-overview.png)
![K8s Authorization Types](../assets/images/k8s-author-mechanisms.png)
![K8s Node Authorizer](../assets/images/k8s-node-authorizer.png)
![K8s ABAC](../assets/images/k8s-abac.png)
![K8s RBAC](../assets/images/k8s-rbac.png)
![K8s Webhook](../assets/images/k8s-webhook.png)
![K8s Authorization Chaining](../assets/images/k8s-author-chaining.png)
## Roles and Rolebindings
![K8s Roles and Rolebindings](../assets/images/k8s-roles.png)
- **IMPORTANT**: Roles and RoleBindings are namespaced
```BASH
kubectl get roles
kubectl get rolebindngs
# Check access
kubect auth can-i create deployments        # As current user
kubectl auth can-i delete nodes --as dev-user # As an Admin user, you can impersonate and test for another user
kubectl auth can-i delete pods --as system:serviceaccount:<namespace>:<sa name>> # As an service account in a namespace
```
## Cluster Roles and Cluster RoleBindings
![K8s Roles and ClusterRole Scope](../assets/images/k8s-role-scope.png)
![K8s ClusterRoles and ClusterRolebindings](../assets/images/k8s-clusterrole.png)
```BASH
kubectl api-resources --namespaced=true     # Get resources which can be added to roles
kubectl api-resources --namespaced=false    # Get resources which can be added to clusterroles
kubectl get clusterroles
kubectl get clusterrolebindngs
```
## Admission Controllers
![K8s Authorization Drawbacks](../assets/images/k8s-author-drawbacks.png)
- Helps implement better security measures.
- Validates configuration.
- Performs additional operations before a pod is created.
![K8s Admission Controllers](../assets/images/k8s-admission-controllers.png)
- **Note**: The `NamespaceExists` and `NamespaceAutoProvision` admission controllers are deprecated and now replaced by `NamespaceLifecycle` admission controller.
- The `NamespaceLifecycle` admission controller will make sure that requests to a non-existent namespace is rejected and that the default namespaces such as `default`, `kube-system` and `kube-public` cannot be deleted.
```BASH
##NOTE:
# Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins.
ps -ef | grep kube-apiserver | grep admission-plugins

# To check all the values that are valid for kube-apiserver
kube-apiserver -h | grep enable-admission-plugins   # Shows enabled admin controllers
# Incase kube-apiserver is running as a pod managed by kubeadm
kubectl exec kube-apiserver-controlplane -n kube-system \   # Exec into pod
  -- kube-apiserver -h | grep enable-admission-plugins      # NOTE: -- for command execution
```
### Validating and Mutating Addmission Controllers
- **Validating AC**: Are the controllers which validate the request that is submitted to the API server
- **Mutating AC**: Are the controllers which change or mutate the request that is submitted to the API server if it does not meet the standards defined.
- Mutating AC are always invoked before Validating AC otherwise some requests may be rejected otherwise.
![K8s Webhook Admission Controller](../assets/images/k8s-webhook-admission-controller.png)
![K8s Webhook Admission Controller Configuration](../assets/images/k8s-webhook-admission-controller-config.png)
```YAML
# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
```
## Helm
```BASH
# Install helm
sudo snap install helm
# OR
sudo snap install helm --classic

# This is the default helm repo
helm install wordpress
helm upgrade wordpress
helm rollback wordpress
helm uninstall wordpress

# To work with a custom helm repo
helm repo add bitnami https://charts.bitnami.com/bitnami  # Add a custom helm repository
helm search repo wordpress     # Search for a chart in a named repository
helm install release-1 bitnami/wordpress
helm list
helm uninstall release-1
helm pull --untar bitnami/wordpress   # Just downloads the chart, does not install it
ls wordpress      # to check the chart contents
helm install release-2 ./wordpress
```

## Troubleshooting
### Application Troubleshooting
```BASH
# Check the Front End app service if that is reachable
kubectl describe web-service 		# Get the Endpoint and Nodeport
curl http://web-service:30080		
# OR If you are in the same machine
curl https://localhost:30080
# NOTE: When you describe the service, there should be an ENDPOINTS which is detected. If its None, then selector is not correct
# If there is no response, check the labels on the service and the pod that is its serving traffic to
# Check the Pod restarts and why it is failing
kubectl logs web -f					# -f to tail the live logs and wait for it to fail
# OR
kubectl logs web -f --previous		# --previous to get the last pod's logss
```
### Cluster Troubleshooting
```BASH
# Check Node Status
kubectl get nodes
# Check Control plane pods
kubectl get pods -n kube-system
# Check Service Health of Control plane 
service kube-apiserver status
service kube-controller-manager status
service kube-scheduler status
# Check Service Health of Workers
service kubelet status
service kube-proxy status
# Check the logs
kubectl logs -n kube-system kube-apiserver-master
# Check the service logs
sudo journalctl -u kube-apiserver	-l	# Remember this as its native 
# NOTE: The certificates inside the cluster components are mapped as Volumes from the host. The path needs to be correct
```
### Worker Troubleshooting
```BASH
# Check Node Status
kubectl get nodes
# If the Nodes are not Ready, describe the node to get the error reason
kubectl describe nodes node01
# Check for memory and disk space on the node
top
df -h
# Check the service logs for kubelet
sudo journalctl -u kubelet -f			# -f to follow live logs
# Start the service without change node
ssh node01 "service kubelet start"
# OR SSH to node01 and then run
systemctl start kubelet
# Check the kubelet certificates, make sure they are not expired and are of the right group and issued by the right CA
openssl x509 -text -in /var/lib/kubelet/worker-1.crt 
# Kubelet Configuration Mismatch
# Another issue from the worker node, watch the logs for kubelet
# Check the kubelet.conf file at /etc/kubernetes/kubelet.conf on the worker node.
# kubelet is trying to connect to the API server on the controlplane node on port 6553. This is incorrect.
# Update the conf to port 6443. Restart kubelet

# Kubelet Service Configuration mismatch
# There appears to be a mistake path used for the CA certificate in the kubelet configuration. This can be corrected by updating the file /var/lib/kubelet/config.yaml.
# Once this is fixed, restart the kubelet service.
```
### Network Troubleshooting
```BASH
Kubernetes uses CNI plugins to setup network. The kubelet is responsible for executing plugins as we mention the following parameters in kubelet configuration.
- cni-bin-dir:   Kubelet probes this directory for plugins on startup
- network-plugin: The network plugin to use from cni-bin-dir. It must match the name reported by a plugin probed from the plugin directory.
Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order.

# DNS in Kubernetes
# Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS.
# In large scale Kubernetes clusters, CoreDNS's memory usage is predominantly affected by the number of Pods and Services in the cluster. Other factors include the size of the filled DNS answer cache, and the rate of queries received (QPS) per CoreDNS instance.

Kubernetes resources for coreDNS are:   
1. a service account named coredns,
2. cluster-roles named coredns and kube-dns
3. clusterrolebindings named coredns and kube-dns, 
4. a deployment named coredns,
5. a configmap named coredns and a
6. service named kube-dns.
While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap.

This is the backend to k8s for cluster.local and reverse domains.
proxy . /etc/resolv.conf
Forward out of cluster domains directly to right authoritative DNS server.

1. If you find CoreDNS pods in pending state first check network plugin is installed.
2. coredns pods have CrashLoopBackOff or Error state
If you have nodes that are running SELinux with an older version of Docker you might experience a scenario where the coredns pods are not starting. To solve that you can try one of the following options:
a)Upgrade to a newer version of Docker.
b)Disable SELinux.
c)Modify the coredns deployment to set allowPrivilegeEscalation to true:
kubectl -n kube-system get deployment coredns -o yaml | \
  sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
  kubectl apply -f -
d)Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop.
There are many ways to work around this issue, some are listed here:
- Add the following to your kubelet config yaml: resolvConf: <path-to-your-real-resolv-conf-file> This flag tells kubelet to pass an alternate resolv.conf to Pods. For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the "real" resolv.conf, although this can be different depending on your distribution.
- Disable the local DNS cache on host nodes, and restore /etc/resolv.conf to the original.
3. If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints.
kubectl -n kube-system get ep kube-dns
If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports.

kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. These network rules allow network communication to the Pods from network sessions inside or outside of the cluster.
In a cluster configured with kubeadm, you can find kube-proxy as a daemonset.

kubeproxy is responsible for watching services and endpoint associated with each service. When the client is going to connect to the service using the virtual IP the kubeproxy is responsible for sending traffic to actual pods.
If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following command inside the kube-proxy container.

    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)

So it fetches the configuration from a configuration file ie, /var/lib/kube-proxy/config.conf and we can override the hostname with the node name of at which the pod is running.

1. Check kube-proxy pod in the kube-system namespace is running.
2. Check kube-proxy logs.
3. Check configmap is correctly defined and the config file for running kube-proxy binary is correct.
4. kube-config is defined in the config map.
5. check kube-proxy is running inside the container
netstat -plan | grep kube-proxy
```
## JSONPath
- NOTE: use `jq` to see the data in proper format
- kubectl get nodes -o json | jq -c 'paths' | grep <search parameter>
- Output of this can be used in jsonpath search query
- jsonpath data can be filtered using jq
```BASH
# Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json.
# Retrieve just the first 2 columns of pv output and store it in /opt/outputs/pv-and-capacity-sorted.txt.
# The columns should be named NAME and CAPACITY. Use the custom-columns option and remember, it should still be sorted based on storage capacity.
# Use the command 
kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt
# Use a JSON PATH query to identify the context configured for the aws-user in the my-kube-config context file and store the result in /opt/outputs/aws-context-name.
kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name
```