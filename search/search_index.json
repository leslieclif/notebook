{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Leslie's Notebook \u00b6 Install mkdocs using command pip install mkdocs Topics \u00b6 Server - Setup basic Linux server Kubernetes - Setup Kubernetes IDE - Tips and Tricks","title":"Installation"},{"location":"#welcome-to-leslies-notebook","text":"Install mkdocs using command pip install mkdocs","title":"Welcome to Leslie's Notebook"},{"location":"#topics","text":"Server - Setup basic Linux server Kubernetes - Setup Kubernetes IDE - Tips and Tricks","title":"Topics"},{"location":"developer/","text":"Windows \u00b6 Update WSL2 first (by default WLS1 is enabled) Install Ubuntu from Microsoft Stores Install Visual Studio Code Update Linux packages sudo apt update sudo apt -y upgrade To find the home directory in Ubuntu explorer.exe . Install Windows Terminal for Miscrosoft Store Install Menlo font (from Powerlevel10k site) To test the terminal color output, run this code in the terminal for code in { 30 ..37 } ; do \\ echo -en \"\\e[ ${ code } m\" '\\\\e[' \" $code \" 'm' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;1m\" '\\\\e[' \" $code \" ';1m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;3m\" '\\\\e[' \" $code \" ';3m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;4m\" '\\\\e[' \" $code \" ';4m' \"\\e[0m\" ; \\ echo -e \" \\e[ $(( code+60 )) m\" '\\\\e[' \" $(( code+60 )) \" 'm' \"\\e[0m\" ; \\ done Generate SSH keys ssh-keygen -t rsa -b 4096 -f ~/.ssh/raddit-user -C raddit-user ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user Using the .ssh config files (~/.ssh/config) # 1. Generate public & private ssh keys: ` ssh-keygen -t rsa ` # Type in a name which will be put in `~/.ssh` directory # 2. To bypass password prompt, you should add the `foo.pub` file to the `authorized_keys` file on the # server's `~/.ssh` directory. You can do a pipe via ssh: ` cat mykey.pub | ssh myuser@mysite.com -p 123 'cat >> .ssh/authorized_keys' ` # 3. Add the publickey name to the `~/.ssh/config` file like this: Host bitbucket.org IdentityFile ~/.ssh/myprivatekeyfile # the leading spaces are important! Port 123 # 4. Verify and then SSH into the remote server. To check if your config is right type: `ssh -T git@github.com` ssh root@mysite.com or ssh mysite.com # if you setup the User setting in config Ubuntu \u00b6 Use bootable USB created using ventoy Press F12 at startup and select the bootable USB, select Ubuntu is image to begin installation Configure linux partitions as encrypted Add swap partion instead of efi as we will dual boot into same system. Part 2 Change root password. sudo passwd root Move Windows above Ubuntu in boot menu. Use Grub Customizer Part 3 Backup and restore data using rsync. Install programs using dotfiles. Adding SSH Keys to servers SSH Client Config Test SSH connections Edit setings on the new terminal to make Ubuntu as the default terminal. Also set the fontFace and https://www.the-digital-life.com/en/awesome-wsl-wsl2-terminal/ Create Sudo User Securing Sudoers #sudo visudo /etc/sudoers.d/leslie leslie ALL =( ALL:ALL ) NOPASSWD: /usr/bin/docker, /usr/sbin/reboot, /usr/sbin/shutdown, /usr/bin/apt-get, /usr/local/bin/docker-compose Switching remote URLs from HTTPS to SSH \u00b6 List your existing remotes in order to get the name of the remote you want to change. $ git remote -v > origin https://github.com/USERNAME/REPOSITORY.git ( fetch ) > origin https://github.com/USERNAME/REPOSITORY.git ( push ) Change your remote's URL from HTTPS to SSH with the git remote set-url command. $ git remote set-url origin git@github.com:USERNAME/REPOSITORY.git git remote set-url origin git@github.com :leslieclif/notebook.git Inspirational dotfile repos \u00b6 https://www.freecodecamp.org/news/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/ Dotfiles Intial Automation Tmux and Otherconfig nickjj/dotfiles https://github.com/jieverson/dotfiles-win/blob/master/install.sh Bashrc Automation \u00b6 https://victoria.dev/blog/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/ https://victoria.dev/blog/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/ Vagrant setup \u00b6 https://www.techdrabble.com/ansible/36-install-ansible-molecule-vagrant-on-windows-wsl Tmux \u00b6 Every Hacker should have a great terminal | TMUX - Medium \u00b6 Tmux Basics Tmux Config VSCode \u00b6 Key Shortcuts Mastering Terminal Examples \u00b6 TLS Certificate - Manual PI \u00b6 TFTP Boot Boot Methods Network Boot from Ubuntu K3s Cluster with Netboot DHCP, TFTP and NFS","title":"Developer Setup"},{"location":"developer/#windows","text":"Update WSL2 first (by default WLS1 is enabled) Install Ubuntu from Microsoft Stores Install Visual Studio Code Update Linux packages sudo apt update sudo apt -y upgrade To find the home directory in Ubuntu explorer.exe . Install Windows Terminal for Miscrosoft Store Install Menlo font (from Powerlevel10k site) To test the terminal color output, run this code in the terminal for code in { 30 ..37 } ; do \\ echo -en \"\\e[ ${ code } m\" '\\\\e[' \" $code \" 'm' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;1m\" '\\\\e[' \" $code \" ';1m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;3m\" '\\\\e[' \" $code \" ';3m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;4m\" '\\\\e[' \" $code \" ';4m' \"\\e[0m\" ; \\ echo -e \" \\e[ $(( code+60 )) m\" '\\\\e[' \" $(( code+60 )) \" 'm' \"\\e[0m\" ; \\ done Generate SSH keys ssh-keygen -t rsa -b 4096 -f ~/.ssh/raddit-user -C raddit-user ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user Using the .ssh config files (~/.ssh/config) # 1. Generate public & private ssh keys: ` ssh-keygen -t rsa ` # Type in a name which will be put in `~/.ssh` directory # 2. To bypass password prompt, you should add the `foo.pub` file to the `authorized_keys` file on the # server's `~/.ssh` directory. You can do a pipe via ssh: ` cat mykey.pub | ssh myuser@mysite.com -p 123 'cat >> .ssh/authorized_keys' ` # 3. Add the publickey name to the `~/.ssh/config` file like this: Host bitbucket.org IdentityFile ~/.ssh/myprivatekeyfile # the leading spaces are important! Port 123 # 4. Verify and then SSH into the remote server. To check if your config is right type: `ssh -T git@github.com` ssh root@mysite.com or ssh mysite.com # if you setup the User setting in config","title":"Windows"},{"location":"developer/#ubuntu","text":"Use bootable USB created using ventoy Press F12 at startup and select the bootable USB, select Ubuntu is image to begin installation Configure linux partitions as encrypted Add swap partion instead of efi as we will dual boot into same system. Part 2 Change root password. sudo passwd root Move Windows above Ubuntu in boot menu. Use Grub Customizer Part 3 Backup and restore data using rsync. Install programs using dotfiles. Adding SSH Keys to servers SSH Client Config Test SSH connections Edit setings on the new terminal to make Ubuntu as the default terminal. Also set the fontFace and https://www.the-digital-life.com/en/awesome-wsl-wsl2-terminal/ Create Sudo User Securing Sudoers #sudo visudo /etc/sudoers.d/leslie leslie ALL =( ALL:ALL ) NOPASSWD: /usr/bin/docker, /usr/sbin/reboot, /usr/sbin/shutdown, /usr/bin/apt-get, /usr/local/bin/docker-compose","title":"Ubuntu"},{"location":"developer/#switching-remote-urls-from-https-to-ssh","text":"List your existing remotes in order to get the name of the remote you want to change. $ git remote -v > origin https://github.com/USERNAME/REPOSITORY.git ( fetch ) > origin https://github.com/USERNAME/REPOSITORY.git ( push ) Change your remote's URL from HTTPS to SSH with the git remote set-url command. $ git remote set-url origin git@github.com:USERNAME/REPOSITORY.git git remote set-url origin git@github.com :leslieclif/notebook.git","title":"Switching remote URLs from HTTPS to SSH"},{"location":"developer/#inspirational-dotfile-repos","text":"https://www.freecodecamp.org/news/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/ Dotfiles Intial Automation Tmux and Otherconfig nickjj/dotfiles https://github.com/jieverson/dotfiles-win/blob/master/install.sh","title":"Inspirational dotfile repos"},{"location":"developer/#bashrc-automation","text":"https://victoria.dev/blog/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/ https://victoria.dev/blog/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/","title":"Bashrc Automation"},{"location":"developer/#vagrant-setup","text":"https://www.techdrabble.com/ansible/36-install-ansible-molecule-vagrant-on-windows-wsl","title":"Vagrant setup"},{"location":"developer/#tmux","text":"","title":"Tmux"},{"location":"developer/#every-hacker-should-have-a-great-terminal--tmux---medium","text":"Tmux Basics Tmux Config","title":"Every Hacker should have a great terminal | TMUX - Medium"},{"location":"developer/#vscode","text":"Key Shortcuts Mastering Terminal","title":"VSCode"},{"location":"developer/#examples","text":"TLS Certificate - Manual","title":"Examples"},{"location":"developer/#pi","text":"TFTP Boot Boot Methods Network Boot from Ubuntu K3s Cluster with Netboot DHCP, TFTP and NFS","title":"PI"},{"location":"devops/","text":"Devops Origin Devops Metrics Trunk based Development behind Feature Flags Feature Flag Platform Cloud Native Paradigm \u00b6 1. Devops: (How to Build) - It assumes you have embraced Agile ways of working and have broken down silos in your organization that allow teams to work in harmony to deliver software. - From a practise perspective, you have abandoned waterfall , work in short cycles and are hyper focused on delivering software quickly. 2. Microservices: (What to Build) - Cloud Native approaches advocate for buidling applications with a microservices architecture . You aim to build systems that are modular, service based and lossely coupled. - This can be achieved by directing integrations through API interfaces and by applying design principles similar to those defined by the 12-Factor App model. - In general, cloud native applications are made of small services that can be developed and released independently. 3. Automation: (How to Deploy) - Cloud Native apps rely heavily on automation . - The switch to building modular systems with many services makes automated CI/CD pipelines more critical for building and packaging software into an artifact. - After automated tests and quality assurance gates pass, the artifacts are stored in a repository where they wait for an automated agent to deploy and release them into an environment . - What is released into the system and the system\u2019s configuration is determined by code that describes the immutable versions of the infrastructure. 4. Containers: (Where to run) - Cloud native centers around the idea that you run applications using containers on scalable infrastructure. - Container runtimes like Docker allows us run software based on images. An image packages an applications and its environment together which makes the software portable and easier to run in any environment. - To manage how we scale infrastructure and deploy containerized applications, the cloud native aproach uses a container orchestrator where K8s is the most popular. - Taking it a step further, service mesh can be used if advanced management of our distributed application's security, policies and traffic is required. 5. GitOps: (How to Operate) - GitOps tells us how to operate cloud native applications. - GitOps influences how we use automation to deliver , deploy and orchestrate running containers the most. Why Devops \u00b6 \"Firms today experience a much higher velocity of business change. Market opportunities appear or dissolve in months or weeks instead of years.\" Annual updates are no longer feasible against modern competition. Updates and bug fixes need to be available right away. It isn't just our management that wants to speed up our releases. Management is simply reacting to the demands of our customers. If customers can't get what they want from us, they'll go somewhere else. The rules have changed, and organizations around the world are now adapting their approach to software development accordingly. Agile methods and practices don't promise to solve every problem. But they do promise to establish a culture and environment where solutions emerge through collaboration, continual planning and learning, and a desire to ship high quality software more often. Devops \u00b6 DevOps is a union of people, processes and products to enable continuous delivery of value to our customers DevOps is not : A methodology A specific piece of software A quick fix for an organization's challenges Just a team or a job title (although these titles are reasonably common in the industry) Our goal is to give our customers value continously . We do that by working together with a shared set of practices and tools. Devops Practices Agile planning . Together, we'll create a backlog of work that everyone on the team and in management can see. We'll prioritize the items so we know what we need to work on first. The backlog can include user stories, bugs, and any other information that helps us. Continuous integration (CI) . We'll automate how we build and test our code. We'll run that every time a team member commits changes to version control. Continuous delivery (CD) . CD is how we test, configure, and deploy from a build to a QA or production environment. Monitoring . We'll use telemetry to get information about an application's performance and usage patterns. We can use that information to improve as we iterate. Devops Benefits - Deployment Frequency, Lead Time for Changes, Change Failure Rate, and Time to Restore DevOps helps companies experiment with ways to increase customer adoption and satisfaction. It can lead to better organizational performance, and often to higher profitability and market share. 1. Deploy more frequently . Practices such as monitoring, continuous testing, database change management, and integrating security earlier in the software development process help elite performers deploy more frequently, and with greater predictability and security. 1. Reduce lead time from commit to deploy . Lead time is the time it takes for a feature to make it to the customer. By working in smaller batches, automating manual processes, and deploying more frequently, elite performers can achieve in hours or days what once took weeks or even months. 1. Reduce change failure rate . A new feature that fails in production or that causes other features to break can create a lost opportunity between you and your users. As high-performing teams mature, they reduce their change failure rate over time. 1. Recover from incidents more quickly . When incidents do occur, elite performers are able to recover more quickly. Acting on metrics helps elite performers recover more quickly while also deploying more frequently. - How you implement cloud infrastructure also matters. The cloud improves software delivery performance , and teams that adopt essential cloud characteristics are more likely to become elite performers. - DevOps is a key reason many elite performers are able to deliver value to customers, in the form of new features and improvements, more quickly than their competitors. - In a comparison between elite performers and low performers, elite performers deploy more frequently, more quickly, and with fewer failures. This mindset helps them better adapt to changing market conditions, experiment with new features, and recover from incidents with greater resiliency. DevOps gives you a path to becoming an elite performer. - Even for elite performers, change happens gradually, often starting with the most immediate challenges or pain points. Adopting DevOps practices takes time. Process \u00b6 Value stream maps (VSMs) \u00b6 The purpose of a VSM is to visually show where in the process a team creates value and where there's waste. The goal, of course, is to arrive at a process that delivers maximum value to the customer with minimum waste. A VSM can help you pinpoint those areas that either don't contribute any value or that actually reduce the value of the product. The first step to setting up a DevOps practice is to assess your current process. This means analyzing: Your existing artifacts, such as deployment packages and NuGet, as well as your container repositories. Your existing test management tools. Your existing work management tools. Recommending migration and integration strategies. With a VSM, you'll get a sense of where the team fits into the DevOps maturity model. As it turns out, more mature teams typically release faster, with greater confidence, and with fewer bugs than less mature teams. Existing Process They use a waterfall approach. Management sets the priorities. Developers write code and hand the build off to QA. QA tests and then hands off to ops for deployment. (Development processes) Waterfall could be acceptable for a small team, but here the goals aren't always clear and they seem to change frequently. Testing is delayed until late in the process. That means it's harder and more expensive to fix bugs and make changes. (Test processes) There's no clear definition of what \"done\" means. Each team member has their own idea. There's no overall business goal that everyone agrees on. Some code is in a centralized version-control system. Many tools and scripts exist only on network file shares. There are many manual processes. a. Deploy builds to the pre-production servers for more testing. b. Often, the pre-production servers are out of sync with the latest patches and updates that are needed to run the website. c. Deploying to pre-production doesn't add value, it's necessary Communication is haphazard and depends on email, Word docs, and spreadsheets. Feedback is also infrequent and inconsistent. Total lead time is the time it takes for a feature to make it to the customer. Process time is the time spent on a feature that has value to the customer. Here, the process time includes four days for coding plus one day to deploy the feature, which gives a total of five days. Activity Ratio (Efficiency) = Process Time / Total lead time We want to minimize the time we spend that has no value to the customer. We can really improve our efficiency by adopting a DevOps approach . Choose an Agile approach to software development \u00b6 Being Agile means learning from experience and continually improving. Agile is an approach to software development. Agile is a term that's used to describe approaches to software development, emphasizing incremental delivery, team collaboration, continual planning, and continual learning. Agile isn't a process as much as it is a philosophy or mindset for planning the work that a team will do. It's based on iterative development and helps a team better plan for and react to the inevitable changes that occur in software development. Iterative software development shortens the DevOps lifecycle by executing against work in smaller increments, usually called sprints. Sprints are typically 1-4 weeks long. Agile development is often contrasted with traditional or waterfall development, where larger projects are planned up front and executed against that plan. Agile requires both a Definition of Done and explicit value delivered to customers in every sprint. Agile Manifesto We value: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan Example of Agile Mindset A team delivers value to the customer, gets feedback, and then modifies their backlog based on that feedback. They learn that their automated builds are missing key tests and include work in their next sprint to address it. They find that certain features perform poorly in production and make plans to improve performance. Someone on the team hears of a new practice and the team decides to try it out for a few sprints. Recommendations for adopting Agile Create an organizational structure that supports Agile practices. Vertical teams span the architecture and are aligned with product outcomes. Mentor team members on Agile techniques and practices. Train team members in Agile techniques such as how to run stand-up and review meetings. Enable in-team and cross-team collaboration. Cultural change . It's important that team members have a quiet, comfortable place to work. They need spaces where they can focus, without a lot of distractions and noise. To give team members more control, meetings need an agenda and strict time frames. Cross-functional teams . Cross-functional teams add new skills and perspectives that can broaden everyone's ability to solve challenges creatively. Cross-functional teams also make the entire organization more cohesive. They reduce turf wars and increase the sense that everyone is working toward a common goal. Tools for collaboration . Good tools can help your Agile team members collaborate more effectively, both within the team and with other teams. For example: Teams, Slack, Skype, Google Hangouts, Asana, Trello, GoToMeeting and monday.com. Scrum is a framework used by teams to manage their work. Scrum implements the principles of Agile as a concrete set of artifacts, practices, and roles. A sprint is the amount of time we have to complete our tasks. Sprints help keep us focused. At the end, we can have a short retrospective meeting to share what we've accomplished. After that, we can plan the next one. The product of a sprint is called the increment or potentially shippable increment . - All coding, testing, and quality verification must be done each and every sprint. Unless a team is properly set up, the results can fall short of expectations. Key success factors for Agile development teams: Diligent backlog refinement . An Agile development team works off of a backlog of requirements, often called user stories. The backlog is prioritized so the most important user stories are at the top. The product owner owns the backlog and adds, changes, and reprioritizes user stories based on the customer's needs. Integrate early and often . Continuous integration and continuous delivery (CI/CD) sets your team up for the fast pace of Agile development. As soon as possible, automate the build, test, and deployment pipelines. This should be one of the first things a team sets up when starting a new project. CI/CD forces a team to fix deployment issues as they occur, ensuring the product is always ready to ship. Minimize technical debt . Technical debt includes anything the team must do to deploy production quality code and keep it running in production. Examples are bugs, performance issues, operational issues, accessibility, and others. When refining the backlog, there are some key considerations to remember. Refining user stories is often a long-lead activity. A user story is not refined unless the team says it is. User stories further down the backlog can remain ambiguous. There are some key CI/CD activities that are critially important to effective Agile development. Unit testing . Unit tests are the first defense against human error. Unit tests should be considered part of coding and checked in with the code. Executing unit tests should be part of every build. Failed unit tests mean a failed build. Build automation . The build system should automatically pull code and tests directly from source control when builds execute. Branch and build policies . Configure branch and build policies to build automatically as the team checks code in to a specific branch. Deploy to an environment . Set up a release pipeline that automatically deploys built projects to an environment that mimics production. To establish an Agile culture, start by trying to ship the product at the end of every sprint. It won't be easy at first, but when a team attempts it, they quickly discover all the things that should be happening, but aren't. Continuous Integration \u00b6 Continuous Integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. - CI encourages developers to share their code and unit tests by merging their changes into a shared version control repository after every small task completion. - Committing code triggers an automated build system to grab the latest code from the shared repository and to build, test, and validate the full main, or trunk, branch. - Teams can leverage modern version control systems such as Git to create short-lived feature branches to isolate their work. A developer submits a pull request when the feature is complete and, on approval of the pull request, the changes get merged into the main branch. Then the developer can delete the previous feature branch. Development teams repeat the process for additional work. The team can establish branch policies to ensure the main branch meets desired quality criteria. Teams use build definitions to ensure that every commit to the main branch triggers the automated build and testing processes. Implementing CI this way ensures bugs are caught earlier in the development cycle, which makes them less expensive to fix. Automated tests run for every build to ensure builds maintain a consistent quality. Test Principles Tests should be written at the lowest level possible . The majority of tests should run as part of the build, so focus on making that as easy as possible. It's not feasible to test every aspect of a service at this level, but the principle to keep in mind is that heavier functional tests should not be used where lighter unit tests could produce the same results. Consider a parallel build system that can run unit tests for an assembly as soon as that assembly and associated test assembly drop. Write once, run anywhere, including the production system . It's a best practice for functional tests to only use the public API of the product. Design the product for testability . Shifting the balance strongly in favor of unit testing over functional testing requires teams to make design and implementation choices that support testability. The principle to keep clearly in mind is that designing for testability must become a primary part of the discussion about design and code quality. Test code is product code, and only reliable tests survive . Apply the same level of care in the design and implementation of tests and test frameworks. Maintain a very high bar for reliability and discourage the use of UI tests as they tend to be unreliable. Testing infrastructure is a shared service . Testing should be viewed as a shared service for the entire team. If the tests can be run in every environment from local development through production, then they will have the same reliability as the product code. Test ownership follows product ownership . Tests should sit right next to the product code in a repo. If there are components to be tested at that component boundary, don't rely on others to test the component. Push the accountability to the person who is writing the code. Shift left The goal for shifting left is to move quality upstream by performing testing tasks earlier in the pipeline. Through a combination of test and process improvements, this both reduces the time it takes for tests to be run, as well as the impact of failures later on. Most importantly, it ensures that most of the testing is completed even before a change is merged into main . Continuous Delivery \u00b6 Continuous Delivery (CD) is the process to build, test, configure, and deploy from a build to a production environment. - Multiple testing or staging environments create a Release Pipeline to automate the creation of infrastructure and deployment of a new build. Successive environments support progressively longer-running activities of integration, load, and user acceptance testing. Modern release pipelines allow development teams to deploy new features fast and safely. Issues found in production can be remediated quickly by rolling forward with a new deployment. In this way, CD creates a continuous stream of customer value. - Without CD, software release cycles were previously a bottleneck for application and operation teams. Manual processes led to unreliable releases that produced delays and errors. These teams often relied on handoffs that resulted in issues during release cycles. - The automated release pipeline allows a \"fail fast\" approach to validation, where the tests most likely to fail quickly are run first and longer-running tests happen only after the faster ones complete successfully. - CD is a lean practice with the goal to keep production fresh by achieving the shortest path from the availability of new code in version control or new components in package management to deployment. - By automation, CD minimizes the time to deploy and time to mitigate or time to remediate production incidents (TTM and TTR). In lean terms, this optimizes process time and eliminates idle time. - CD is helped considerably by the complementary practices of Infrastructure as Code and monitoring. Deployment Patterns 1. Sequential rings : CD may sequence multiple deployment rings for progressive exposure (also known as \"controlling the blast radius\"). Progressive exposure groups users who get to try new releases to monitor their experience in rings. The first deployment ring is often a canary used to test new versions in production before a broader rollout. CD automates deployment from one ring to the next and may optionally depend on an approval step, in which a decision maker signs off on the changes electronically. - CD may create an auditable record of the approval in order to satisfy regulatory procedures or other control objectives. - CD also supports two other patterns for progressive exposure beside sequential rings. 2. Blue/Green deployment relies on keeping an existing (blue) version live while a new (green) one is deployed. Typically, this uses load balancing to direct increasing amounts of traffic to the green deployment. If monitoring discovers an incident, traffic can be rerouted to the blue deployment still running. 3. Feature flags (or feature toggles) comprise another technique used for experimentation and dark launches. Feature flags turn features on or off for different end users based on their identity and group membership. Key takeaways With the right practices, it's possible to make delivery a productive and painless part of the DevOps cycle. 1. Deploy often 1. Stay green throughout the sprint 1. Use consistent deployment tooling in development, test, and production 1. Use a continuous delivery platform that allows automation and authorization 1. Follow safe deployment practices Shift right to test in production - One of the most effective ways DevOps teams can improve velocity is by shifting their quality goals left. In this sense, they are pushing aspects of testing earlier in the pipeline in order to ultimately reduce the amount of time it takes for new code investments to reach production and operate reliably. - The full breadth and diversity of the production environment is hard to replicate in a lab. The real workload of customer traffic is also hard to simulate. And even if tests are built and optimized, it becomes a significant responsibility to maintain those profiles and behaviors as the production demand evolves over time. - Moreover, the production environment keeps changing. It's never constant and, even if your app doesn't change, everything underneath it is constantly changing. The infrastructure it relies on keeps changing. So over a period of time, teams find that certain types of testing just needs to happen in production. Testing in production is the practice of using real deployments to validate and measure an application's behavior and performance in the production environment. It serves two important purposes: It validates the quality of a given production deployment. It validates the health and quality of the constantly changing production environment. To safeguard the production environment, it's necessary to roll out changes in a progressive and controlled manner. This is typically done via the ring model of deployments and with feature flags. The first ring should be the smallest size necessary to run the standard integration suite. This is where obvious errors, such as misconfigurations, will be discovered before any customers are impacted. Once the initial ring is validated, the next ring can broaden to include a subset of real users. The usage of the new production services by real customers becomes the test run. For example, a bug that prevents a shopper from completing their purchase is very bad, so it would always be better to catch that issue when less than 1% of customers are on that ring, as opposed to a different model where all customers were switched at once. If everything looks good so far, the deployment can progress through further rings and tests until it's used by everyone. However, full deployment doesn't mean that testing is over; tracking telemetry is crticially important for testing in production. It's arguably the highest quality test data because it's literally the test results of the real customer workload. It tracks failures, exceptions, performance metrics, security events, etc. The telemetry also helps detect anomalies. Fault injection and chaos engineering . Teams often employ fault injection and chaos engineering to see how a system behaves under failure conditions. This helps to validate that the resiliency mechanisms implemented actually work. It also helps to validate that a failure starting in one subsystem is contained within that subsystem and doesn't cascade to produce a major outage for the entire product. Fault injection also helps create more realistic training drills for live site engineers so that they can be better prepared to deal with real incidents. Fault testing with a circuit breaker A circuit breaker is a mechanism that cuts off a given component from a larger system. Circuit breakers can be intentionally triggered to evaluate two important scenarios: When the circuit breaker opens, does the fallback work? It may work with unit tests, but there's no way to know for sure that it will behave as expected in production without injecting a fault to trigger it. Does the circuit breaker open when it needs to? Does it have the right sensitivity threshold configured? Fault injection may force latency and/or disconnect dependencies in order to observe breaker responsiveness. In addition to evaluating that the right behavior is occurring, it's important to determine whether it happens quickly enough. Chaos engineering can be an effective tool, but it should be limited to canary environments. For example, it should only be used against environments that have little or no customer impact. It's a good practice to automate fault injection experiments because they are expensive tests and the system is always changing. Infrastructure as Code \u00b6 Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. - IaC is a key DevOps practice and is used in conjunction with continuous delivery. - Infrastructure as Code evolved to solve the problem of environment drift in the release pipeline. - Without IaC, teams must maintain the settings of individual deployment environments. Over time, each environment becomes a snowflake , that is, a unique configuration that cannot be reproduced automatically. Inconsistency among environments leads to issues during deployments. With snowflakes, administration and maintenance of infrastructure involves manual processes which were hard to track and contributed to errors. - Idempotence is a principle of Infrastructure as Code. Idempotence is the property that a deployment command always sets the target environment into the same configuration, regardless of the environment's starting state. - Idempotency is achieved by either automatically configuring an existing target or by discarding the existing target and recreating a fresh environment. Benefits of IaC 1. Teams who implement IaC can deliver stable environments rapidly and at scale. 2. Teams avoid manual configuration of environments and enforce consistency by representing the desired state of their environments via code. 3. Infrastructure deployments with IaC are repeatable and prevent runtime issues caused by configuration drift or missing dependencies. 4. DevOps teams can work together with a unified set of practices and tools to deliver applications and their supporting infrastructure rapidly, reliably, and at scale. Monitoring \u00b6 Monitoring provides feedback from production. Monitoring delivers information about an application's performance and usage patterns. Effective monitoring is essential to allow DevOps teams to deliver at speed, get feedback from production, and increase customers satisfaction, acquisition and retention. One goal of monitoring is to achieve high availability by minimizing key metrics that are measured in terms of time When performance or other issues arise, rich diagnostic data about the issues are fed back to development teams via automated monitoring. That's time to detect (TTD) . DevOps teams act on the information to mitigate the issues as quickly as possible so that users are no longer affected. That's time to mitigate (TTM) . Resolution times are measured, and teams work to improve over time. After mitigation, teams work on how to remediate problems at root cause so that they do not recur. That's time to remediate (TTR) . A second goal of monitoring is to enable validated learning by tracking usage. The core concept of validated learning is that every deployment is an opportunity to track experimental results that support or diminish the hypotheses that led to the deployment. Tracking usage and differences between versions allows teams to measure the impact of change and drive business decisions. If a hypothesis is diminished, the team can fail fast or pivot . If the hypothesis is supported, then the team can double down or persevere . These data-informed decisions lead to new hypotheses and prioritization of the backlog. Telemetry is the mechanism for collecting data from monitoring. - Telemetry can use agents that are installed in the deployment environments, an SDK that relies on markers inserted into source code, server logging, or a combination of these. Typically, telemetry will distinguish between the data pipeline optimized for real-time alerting and dashboards and higher-volume data needed for troubleshooting or usage analytics. Synthetic monitoring uses a consistent set of transactions to assess performance and availability. - Synthetic transactions are predictable tests that have the advantage of allowing comparison from release to release in a highly predictable manner. Real user monitoring (RUM) , on the other hand, means measurement of experience from the user's browser, mobile device or desktop, and accounts for last mile conditions such as cellular networks, internet routing, and caching. - A well-monitored deployment streams the data about its health and performance so that the team can spot production incidents immediately. - Combined with a continuous deployment release pipeline, monitoring will detect new anomalies and allow for prompt mitigation. This allows discovery of the unknown unknowns in application behavior that cannot be foreseen in pre-production environments. DevSecOps \u00b6 \"Fundamentally, if somebody wants to get in, they're getting in\u2026accept that. What we tell clients is: number one, you're in the fight, whether you thought you were or not. Number two, you almost certainly are penetrated.\" \u2013 Michael Hayden, Former Director of NSA and CIA The mindset shift to a DevSecOps culture includes an important thinking about not only preventing breaches, but assuming them as well. ============================================================ Preventing breaches Assuming breaches Threat models War game exercises Code reviews Central security monitors Security testing Live site penetration tests Security development lifecycle (SDL) The most important thing to focus on is that practicing techniques that assume breaches helps the team answer questions about their security on their own time, so they don't have to figure it all out during a real security emergency. Common questions the team needs to think through: How will we detect an attack? How will respond if there is an attack or penetration? How will we recover from an attack, such as when data has been leaked or tampered with? Key DevSecOps practices First, teams should focus on improving their mean time to detection and mean time to recovery . These are metrics that indicate how long it takes to detect a breach and how long it takes to recover, respectively. They can be tracked through ongoing live site testing of security response plans. When evaluating potential policies, improving these metrics should be an important consideration. Teams should also practice defense in depth . When a breach happens, it often results in the attacker getting access to internal networks and everything they have to offer. While it would be ideal to stop them before it gets that far, a policy of assuming breaches would drive teams to minimize their exposure from an attacker who has already gotten in. Finally, teams should perform periodic post-breach assessments of the practices and environments. After a breach has been resolved, the team should evaluate the performance of the policies, as well as their own adherence to them. This serves to not only ensure the policies are effective, but also that the team is actually following them. Every breach, whether real or practiced, should be seen as an opportunity to improve. Strategies for mitigating threats - Some security holes are due to issues in dependencies like operating systems and libraries, so keeping them up-to-date is critical. - Others are due to bugs in system code that require careful analysis to find and fix. - Poor secret management is the cause of many breaches, as is social engineering. > Attack vectors \u00b6 Privilege Attack \u00b6 Can they send emails? Phish colleagues Can they access other machines? Log on, mimikatz, repeat Can they modify source Inject code Can they modify the build/release process? Inject code, run scripts Can they access a test environment? If a production environment takes a dependency on the test environment, exploit it Can they access the production environment? So many options\u2026 How can the blue team defend against this? Store secrets in protected vaults Remove local admin accounts Restrict SAMR Credential Guard Remove dual-homed servers Separate subscriptions Multi-factor authentication Privileged access workstations Detect with ATP & Azure Security Center Secret management - Use a hierarchy of vaults to eliminate the duplication of secrets. - Also consider how and when secrets are accessed. Some are used at deploy-time when building environment configurations, whereas others are accessed at run-time. Deploy-time secrets typically require a new deployment in order to pick up new settings, whereas run-time secrets are accessed when needed and can be updated at any time. The red team should include some security-minded engineers and developers deeply familiar with the code. It's also helpful to augment the team with a penetration testing specialist, if possible. If there are no specialists in-house, many companies provide this service along with mentoring. The blue team should be made up of ops-minded engineers who have a deep understanding of the systems and logging available. They have the best chance of detecting and addressing suspicious behavior. Expect the red team to be effective in the early war games. They should be able to succeed through fairly simple attacks, such as by finding poorly protected secrets, SQL injection, and successful phishing campaigns. Take plenty of time between rounds to apply fixes and feedback on policies. This will vary by organization, but you don't want to start the next round until everyone is confident that the previous round has been mined for all it's worth. After a few rounds, the red team will need to rely on more sophisticated techniques, such as cross-site scripting (XSS), deserialization exploits, and engineering system vulnerabilities. it will also help to bring in additional outside security experts in areas like Active Directory in order to attack more obscure exploits. By this time, the blue team should not only have a hardened platform to defend, but will also make use of comprehensive, centralized logging for post-breach forensics. \"Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.\" \u2013 John Lambert (MSTIC) Over time, the red team will take much longer to reach objectives. When they do, it will often requiring discovery and chaining of multiple vulnerabilities to have a limited impact. Through the use of real-time monitoring tools, the blue team should start to catch them in real-time. Any security risks or lessons learned should be documented in a backlog of repair items. Teams should define a service level agreement (SLA) for how quickly security risks will be addressed. Severe risks should be addressed as soon as possible, whereas minor issues may have a two-sprint deadline. Lessons learned War games are a really effective way to change DevSecOps culture and keep security top-of-mind. Phishing attacks are very effective for attackers and should not be underestimated. The impact can be contained by limiting production access and requiring two-factor authentication. Control of the engineering system leads to control of everything. Be sure to strictly control access to the build/release agent, queue, pool, and definition. Practice defense in depth to make it harder for attackers. Every boundary they have to breach slows them down and offers another opportunity to catch them. Don't ever cross trust realms. Production should never trust anything in test.","title":"Index"},{"location":"devops/#cloud-native-paradigm","text":"1. Devops: (How to Build) - It assumes you have embraced Agile ways of working and have broken down silos in your organization that allow teams to work in harmony to deliver software. - From a practise perspective, you have abandoned waterfall , work in short cycles and are hyper focused on delivering software quickly. 2. Microservices: (What to Build) - Cloud Native approaches advocate for buidling applications with a microservices architecture . You aim to build systems that are modular, service based and lossely coupled. - This can be achieved by directing integrations through API interfaces and by applying design principles similar to those defined by the 12-Factor App model. - In general, cloud native applications are made of small services that can be developed and released independently. 3. Automation: (How to Deploy) - Cloud Native apps rely heavily on automation . - The switch to building modular systems with many services makes automated CI/CD pipelines more critical for building and packaging software into an artifact. - After automated tests and quality assurance gates pass, the artifacts are stored in a repository where they wait for an automated agent to deploy and release them into an environment . - What is released into the system and the system\u2019s configuration is determined by code that describes the immutable versions of the infrastructure. 4. Containers: (Where to run) - Cloud native centers around the idea that you run applications using containers on scalable infrastructure. - Container runtimes like Docker allows us run software based on images. An image packages an applications and its environment together which makes the software portable and easier to run in any environment. - To manage how we scale infrastructure and deploy containerized applications, the cloud native aproach uses a container orchestrator where K8s is the most popular. - Taking it a step further, service mesh can be used if advanced management of our distributed application's security, policies and traffic is required. 5. GitOps: (How to Operate) - GitOps tells us how to operate cloud native applications. - GitOps influences how we use automation to deliver , deploy and orchestrate running containers the most.","title":"Cloud Native Paradigm"},{"location":"devops/#why-devops","text":"\"Firms today experience a much higher velocity of business change. Market opportunities appear or dissolve in months or weeks instead of years.\" Annual updates are no longer feasible against modern competition. Updates and bug fixes need to be available right away. It isn't just our management that wants to speed up our releases. Management is simply reacting to the demands of our customers. If customers can't get what they want from us, they'll go somewhere else. The rules have changed, and organizations around the world are now adapting their approach to software development accordingly. Agile methods and practices don't promise to solve every problem. But they do promise to establish a culture and environment where solutions emerge through collaboration, continual planning and learning, and a desire to ship high quality software more often.","title":"Why Devops"},{"location":"devops/#devops","text":"DevOps is a union of people, processes and products to enable continuous delivery of value to our customers DevOps is not : A methodology A specific piece of software A quick fix for an organization's challenges Just a team or a job title (although these titles are reasonably common in the industry) Our goal is to give our customers value continously . We do that by working together with a shared set of practices and tools. Devops Practices Agile planning . Together, we'll create a backlog of work that everyone on the team and in management can see. We'll prioritize the items so we know what we need to work on first. The backlog can include user stories, bugs, and any other information that helps us. Continuous integration (CI) . We'll automate how we build and test our code. We'll run that every time a team member commits changes to version control. Continuous delivery (CD) . CD is how we test, configure, and deploy from a build to a QA or production environment. Monitoring . We'll use telemetry to get information about an application's performance and usage patterns. We can use that information to improve as we iterate. Devops Benefits - Deployment Frequency, Lead Time for Changes, Change Failure Rate, and Time to Restore DevOps helps companies experiment with ways to increase customer adoption and satisfaction. It can lead to better organizational performance, and often to higher profitability and market share. 1. Deploy more frequently . Practices such as monitoring, continuous testing, database change management, and integrating security earlier in the software development process help elite performers deploy more frequently, and with greater predictability and security. 1. Reduce lead time from commit to deploy . Lead time is the time it takes for a feature to make it to the customer. By working in smaller batches, automating manual processes, and deploying more frequently, elite performers can achieve in hours or days what once took weeks or even months. 1. Reduce change failure rate . A new feature that fails in production or that causes other features to break can create a lost opportunity between you and your users. As high-performing teams mature, they reduce their change failure rate over time. 1. Recover from incidents more quickly . When incidents do occur, elite performers are able to recover more quickly. Acting on metrics helps elite performers recover more quickly while also deploying more frequently. - How you implement cloud infrastructure also matters. The cloud improves software delivery performance , and teams that adopt essential cloud characteristics are more likely to become elite performers. - DevOps is a key reason many elite performers are able to deliver value to customers, in the form of new features and improvements, more quickly than their competitors. - In a comparison between elite performers and low performers, elite performers deploy more frequently, more quickly, and with fewer failures. This mindset helps them better adapt to changing market conditions, experiment with new features, and recover from incidents with greater resiliency. DevOps gives you a path to becoming an elite performer. - Even for elite performers, change happens gradually, often starting with the most immediate challenges or pain points. Adopting DevOps practices takes time.","title":"Devops"},{"location":"devops/#process","text":"","title":"Process"},{"location":"devops/#value-stream-maps-vsms","text":"The purpose of a VSM is to visually show where in the process a team creates value and where there's waste. The goal, of course, is to arrive at a process that delivers maximum value to the customer with minimum waste. A VSM can help you pinpoint those areas that either don't contribute any value or that actually reduce the value of the product. The first step to setting up a DevOps practice is to assess your current process. This means analyzing: Your existing artifacts, such as deployment packages and NuGet, as well as your container repositories. Your existing test management tools. Your existing work management tools. Recommending migration and integration strategies. With a VSM, you'll get a sense of where the team fits into the DevOps maturity model. As it turns out, more mature teams typically release faster, with greater confidence, and with fewer bugs than less mature teams. Existing Process They use a waterfall approach. Management sets the priorities. Developers write code and hand the build off to QA. QA tests and then hands off to ops for deployment. (Development processes) Waterfall could be acceptable for a small team, but here the goals aren't always clear and they seem to change frequently. Testing is delayed until late in the process. That means it's harder and more expensive to fix bugs and make changes. (Test processes) There's no clear definition of what \"done\" means. Each team member has their own idea. There's no overall business goal that everyone agrees on. Some code is in a centralized version-control system. Many tools and scripts exist only on network file shares. There are many manual processes. a. Deploy builds to the pre-production servers for more testing. b. Often, the pre-production servers are out of sync with the latest patches and updates that are needed to run the website. c. Deploying to pre-production doesn't add value, it's necessary Communication is haphazard and depends on email, Word docs, and spreadsheets. Feedback is also infrequent and inconsistent. Total lead time is the time it takes for a feature to make it to the customer. Process time is the time spent on a feature that has value to the customer. Here, the process time includes four days for coding plus one day to deploy the feature, which gives a total of five days. Activity Ratio (Efficiency) = Process Time / Total lead time We want to minimize the time we spend that has no value to the customer. We can really improve our efficiency by adopting a DevOps approach .","title":"Value stream maps (VSMs)"},{"location":"devops/#choose-an-agile-approach-to-software-development","text":"Being Agile means learning from experience and continually improving. Agile is an approach to software development. Agile is a term that's used to describe approaches to software development, emphasizing incremental delivery, team collaboration, continual planning, and continual learning. Agile isn't a process as much as it is a philosophy or mindset for planning the work that a team will do. It's based on iterative development and helps a team better plan for and react to the inevitable changes that occur in software development. Iterative software development shortens the DevOps lifecycle by executing against work in smaller increments, usually called sprints. Sprints are typically 1-4 weeks long. Agile development is often contrasted with traditional or waterfall development, where larger projects are planned up front and executed against that plan. Agile requires both a Definition of Done and explicit value delivered to customers in every sprint. Agile Manifesto We value: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan Example of Agile Mindset A team delivers value to the customer, gets feedback, and then modifies their backlog based on that feedback. They learn that their automated builds are missing key tests and include work in their next sprint to address it. They find that certain features perform poorly in production and make plans to improve performance. Someone on the team hears of a new practice and the team decides to try it out for a few sprints. Recommendations for adopting Agile Create an organizational structure that supports Agile practices. Vertical teams span the architecture and are aligned with product outcomes. Mentor team members on Agile techniques and practices. Train team members in Agile techniques such as how to run stand-up and review meetings. Enable in-team and cross-team collaboration. Cultural change . It's important that team members have a quiet, comfortable place to work. They need spaces where they can focus, without a lot of distractions and noise. To give team members more control, meetings need an agenda and strict time frames. Cross-functional teams . Cross-functional teams add new skills and perspectives that can broaden everyone's ability to solve challenges creatively. Cross-functional teams also make the entire organization more cohesive. They reduce turf wars and increase the sense that everyone is working toward a common goal. Tools for collaboration . Good tools can help your Agile team members collaborate more effectively, both within the team and with other teams. For example: Teams, Slack, Skype, Google Hangouts, Asana, Trello, GoToMeeting and monday.com. Scrum is a framework used by teams to manage their work. Scrum implements the principles of Agile as a concrete set of artifacts, practices, and roles. A sprint is the amount of time we have to complete our tasks. Sprints help keep us focused. At the end, we can have a short retrospective meeting to share what we've accomplished. After that, we can plan the next one. The product of a sprint is called the increment or potentially shippable increment . - All coding, testing, and quality verification must be done each and every sprint. Unless a team is properly set up, the results can fall short of expectations. Key success factors for Agile development teams: Diligent backlog refinement . An Agile development team works off of a backlog of requirements, often called user stories. The backlog is prioritized so the most important user stories are at the top. The product owner owns the backlog and adds, changes, and reprioritizes user stories based on the customer's needs. Integrate early and often . Continuous integration and continuous delivery (CI/CD) sets your team up for the fast pace of Agile development. As soon as possible, automate the build, test, and deployment pipelines. This should be one of the first things a team sets up when starting a new project. CI/CD forces a team to fix deployment issues as they occur, ensuring the product is always ready to ship. Minimize technical debt . Technical debt includes anything the team must do to deploy production quality code and keep it running in production. Examples are bugs, performance issues, operational issues, accessibility, and others. When refining the backlog, there are some key considerations to remember. Refining user stories is often a long-lead activity. A user story is not refined unless the team says it is. User stories further down the backlog can remain ambiguous. There are some key CI/CD activities that are critially important to effective Agile development. Unit testing . Unit tests are the first defense against human error. Unit tests should be considered part of coding and checked in with the code. Executing unit tests should be part of every build. Failed unit tests mean a failed build. Build automation . The build system should automatically pull code and tests directly from source control when builds execute. Branch and build policies . Configure branch and build policies to build automatically as the team checks code in to a specific branch. Deploy to an environment . Set up a release pipeline that automatically deploys built projects to an environment that mimics production. To establish an Agile culture, start by trying to ship the product at the end of every sprint. It won't be easy at first, but when a team attempts it, they quickly discover all the things that should be happening, but aren't.","title":"Choose an Agile approach to software development"},{"location":"devops/#continuous-integration","text":"Continuous Integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. - CI encourages developers to share their code and unit tests by merging their changes into a shared version control repository after every small task completion. - Committing code triggers an automated build system to grab the latest code from the shared repository and to build, test, and validate the full main, or trunk, branch. - Teams can leverage modern version control systems such as Git to create short-lived feature branches to isolate their work. A developer submits a pull request when the feature is complete and, on approval of the pull request, the changes get merged into the main branch. Then the developer can delete the previous feature branch. Development teams repeat the process for additional work. The team can establish branch policies to ensure the main branch meets desired quality criteria. Teams use build definitions to ensure that every commit to the main branch triggers the automated build and testing processes. Implementing CI this way ensures bugs are caught earlier in the development cycle, which makes them less expensive to fix. Automated tests run for every build to ensure builds maintain a consistent quality. Test Principles Tests should be written at the lowest level possible . The majority of tests should run as part of the build, so focus on making that as easy as possible. It's not feasible to test every aspect of a service at this level, but the principle to keep in mind is that heavier functional tests should not be used where lighter unit tests could produce the same results. Consider a parallel build system that can run unit tests for an assembly as soon as that assembly and associated test assembly drop. Write once, run anywhere, including the production system . It's a best practice for functional tests to only use the public API of the product. Design the product for testability . Shifting the balance strongly in favor of unit testing over functional testing requires teams to make design and implementation choices that support testability. The principle to keep clearly in mind is that designing for testability must become a primary part of the discussion about design and code quality. Test code is product code, and only reliable tests survive . Apply the same level of care in the design and implementation of tests and test frameworks. Maintain a very high bar for reliability and discourage the use of UI tests as they tend to be unreliable. Testing infrastructure is a shared service . Testing should be viewed as a shared service for the entire team. If the tests can be run in every environment from local development through production, then they will have the same reliability as the product code. Test ownership follows product ownership . Tests should sit right next to the product code in a repo. If there are components to be tested at that component boundary, don't rely on others to test the component. Push the accountability to the person who is writing the code. Shift left The goal for shifting left is to move quality upstream by performing testing tasks earlier in the pipeline. Through a combination of test and process improvements, this both reduces the time it takes for tests to be run, as well as the impact of failures later on. Most importantly, it ensures that most of the testing is completed even before a change is merged into main .","title":"Continuous Integration"},{"location":"devops/#continuous-delivery","text":"Continuous Delivery (CD) is the process to build, test, configure, and deploy from a build to a production environment. - Multiple testing or staging environments create a Release Pipeline to automate the creation of infrastructure and deployment of a new build. Successive environments support progressively longer-running activities of integration, load, and user acceptance testing. Modern release pipelines allow development teams to deploy new features fast and safely. Issues found in production can be remediated quickly by rolling forward with a new deployment. In this way, CD creates a continuous stream of customer value. - Without CD, software release cycles were previously a bottleneck for application and operation teams. Manual processes led to unreliable releases that produced delays and errors. These teams often relied on handoffs that resulted in issues during release cycles. - The automated release pipeline allows a \"fail fast\" approach to validation, where the tests most likely to fail quickly are run first and longer-running tests happen only after the faster ones complete successfully. - CD is a lean practice with the goal to keep production fresh by achieving the shortest path from the availability of new code in version control or new components in package management to deployment. - By automation, CD minimizes the time to deploy and time to mitigate or time to remediate production incidents (TTM and TTR). In lean terms, this optimizes process time and eliminates idle time. - CD is helped considerably by the complementary practices of Infrastructure as Code and monitoring. Deployment Patterns 1. Sequential rings : CD may sequence multiple deployment rings for progressive exposure (also known as \"controlling the blast radius\"). Progressive exposure groups users who get to try new releases to monitor their experience in rings. The first deployment ring is often a canary used to test new versions in production before a broader rollout. CD automates deployment from one ring to the next and may optionally depend on an approval step, in which a decision maker signs off on the changes electronically. - CD may create an auditable record of the approval in order to satisfy regulatory procedures or other control objectives. - CD also supports two other patterns for progressive exposure beside sequential rings. 2. Blue/Green deployment relies on keeping an existing (blue) version live while a new (green) one is deployed. Typically, this uses load balancing to direct increasing amounts of traffic to the green deployment. If monitoring discovers an incident, traffic can be rerouted to the blue deployment still running. 3. Feature flags (or feature toggles) comprise another technique used for experimentation and dark launches. Feature flags turn features on or off for different end users based on their identity and group membership. Key takeaways With the right practices, it's possible to make delivery a productive and painless part of the DevOps cycle. 1. Deploy often 1. Stay green throughout the sprint 1. Use consistent deployment tooling in development, test, and production 1. Use a continuous delivery platform that allows automation and authorization 1. Follow safe deployment practices Shift right to test in production - One of the most effective ways DevOps teams can improve velocity is by shifting their quality goals left. In this sense, they are pushing aspects of testing earlier in the pipeline in order to ultimately reduce the amount of time it takes for new code investments to reach production and operate reliably. - The full breadth and diversity of the production environment is hard to replicate in a lab. The real workload of customer traffic is also hard to simulate. And even if tests are built and optimized, it becomes a significant responsibility to maintain those profiles and behaviors as the production demand evolves over time. - Moreover, the production environment keeps changing. It's never constant and, even if your app doesn't change, everything underneath it is constantly changing. The infrastructure it relies on keeps changing. So over a period of time, teams find that certain types of testing just needs to happen in production. Testing in production is the practice of using real deployments to validate and measure an application's behavior and performance in the production environment. It serves two important purposes: It validates the quality of a given production deployment. It validates the health and quality of the constantly changing production environment. To safeguard the production environment, it's necessary to roll out changes in a progressive and controlled manner. This is typically done via the ring model of deployments and with feature flags. The first ring should be the smallest size necessary to run the standard integration suite. This is where obvious errors, such as misconfigurations, will be discovered before any customers are impacted. Once the initial ring is validated, the next ring can broaden to include a subset of real users. The usage of the new production services by real customers becomes the test run. For example, a bug that prevents a shopper from completing their purchase is very bad, so it would always be better to catch that issue when less than 1% of customers are on that ring, as opposed to a different model where all customers were switched at once. If everything looks good so far, the deployment can progress through further rings and tests until it's used by everyone. However, full deployment doesn't mean that testing is over; tracking telemetry is crticially important for testing in production. It's arguably the highest quality test data because it's literally the test results of the real customer workload. It tracks failures, exceptions, performance metrics, security events, etc. The telemetry also helps detect anomalies. Fault injection and chaos engineering . Teams often employ fault injection and chaos engineering to see how a system behaves under failure conditions. This helps to validate that the resiliency mechanisms implemented actually work. It also helps to validate that a failure starting in one subsystem is contained within that subsystem and doesn't cascade to produce a major outage for the entire product. Fault injection also helps create more realistic training drills for live site engineers so that they can be better prepared to deal with real incidents. Fault testing with a circuit breaker A circuit breaker is a mechanism that cuts off a given component from a larger system. Circuit breakers can be intentionally triggered to evaluate two important scenarios: When the circuit breaker opens, does the fallback work? It may work with unit tests, but there's no way to know for sure that it will behave as expected in production without injecting a fault to trigger it. Does the circuit breaker open when it needs to? Does it have the right sensitivity threshold configured? Fault injection may force latency and/or disconnect dependencies in order to observe breaker responsiveness. In addition to evaluating that the right behavior is occurring, it's important to determine whether it happens quickly enough. Chaos engineering can be an effective tool, but it should be limited to canary environments. For example, it should only be used against environments that have little or no customer impact. It's a good practice to automate fault injection experiments because they are expensive tests and the system is always changing.","title":"Continuous Delivery"},{"location":"devops/#infrastructure-as-code","text":"Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. - IaC is a key DevOps practice and is used in conjunction with continuous delivery. - Infrastructure as Code evolved to solve the problem of environment drift in the release pipeline. - Without IaC, teams must maintain the settings of individual deployment environments. Over time, each environment becomes a snowflake , that is, a unique configuration that cannot be reproduced automatically. Inconsistency among environments leads to issues during deployments. With snowflakes, administration and maintenance of infrastructure involves manual processes which were hard to track and contributed to errors. - Idempotence is a principle of Infrastructure as Code. Idempotence is the property that a deployment command always sets the target environment into the same configuration, regardless of the environment's starting state. - Idempotency is achieved by either automatically configuring an existing target or by discarding the existing target and recreating a fresh environment. Benefits of IaC 1. Teams who implement IaC can deliver stable environments rapidly and at scale. 2. Teams avoid manual configuration of environments and enforce consistency by representing the desired state of their environments via code. 3. Infrastructure deployments with IaC are repeatable and prevent runtime issues caused by configuration drift or missing dependencies. 4. DevOps teams can work together with a unified set of practices and tools to deliver applications and their supporting infrastructure rapidly, reliably, and at scale.","title":"Infrastructure as Code"},{"location":"devops/#monitoring","text":"Monitoring provides feedback from production. Monitoring delivers information about an application's performance and usage patterns. Effective monitoring is essential to allow DevOps teams to deliver at speed, get feedback from production, and increase customers satisfaction, acquisition and retention. One goal of monitoring is to achieve high availability by minimizing key metrics that are measured in terms of time When performance or other issues arise, rich diagnostic data about the issues are fed back to development teams via automated monitoring. That's time to detect (TTD) . DevOps teams act on the information to mitigate the issues as quickly as possible so that users are no longer affected. That's time to mitigate (TTM) . Resolution times are measured, and teams work to improve over time. After mitigation, teams work on how to remediate problems at root cause so that they do not recur. That's time to remediate (TTR) . A second goal of monitoring is to enable validated learning by tracking usage. The core concept of validated learning is that every deployment is an opportunity to track experimental results that support or diminish the hypotheses that led to the deployment. Tracking usage and differences between versions allows teams to measure the impact of change and drive business decisions. If a hypothesis is diminished, the team can fail fast or pivot . If the hypothesis is supported, then the team can double down or persevere . These data-informed decisions lead to new hypotheses and prioritization of the backlog. Telemetry is the mechanism for collecting data from monitoring. - Telemetry can use agents that are installed in the deployment environments, an SDK that relies on markers inserted into source code, server logging, or a combination of these. Typically, telemetry will distinguish between the data pipeline optimized for real-time alerting and dashboards and higher-volume data needed for troubleshooting or usage analytics. Synthetic monitoring uses a consistent set of transactions to assess performance and availability. - Synthetic transactions are predictable tests that have the advantage of allowing comparison from release to release in a highly predictable manner. Real user monitoring (RUM) , on the other hand, means measurement of experience from the user's browser, mobile device or desktop, and accounts for last mile conditions such as cellular networks, internet routing, and caching. - A well-monitored deployment streams the data about its health and performance so that the team can spot production incidents immediately. - Combined with a continuous deployment release pipeline, monitoring will detect new anomalies and allow for prompt mitigation. This allows discovery of the unknown unknowns in application behavior that cannot be foreseen in pre-production environments.","title":"Monitoring"},{"location":"devops/#devsecops","text":"\"Fundamentally, if somebody wants to get in, they're getting in\u2026accept that. What we tell clients is: number one, you're in the fight, whether you thought you were or not. Number two, you almost certainly are penetrated.\" \u2013 Michael Hayden, Former Director of NSA and CIA The mindset shift to a DevSecOps culture includes an important thinking about not only preventing breaches, but assuming them as well. ============================================================ Preventing breaches Assuming breaches Threat models War game exercises Code reviews Central security monitors Security testing Live site penetration tests Security development lifecycle (SDL) The most important thing to focus on is that practicing techniques that assume breaches helps the team answer questions about their security on their own time, so they don't have to figure it all out during a real security emergency. Common questions the team needs to think through: How will we detect an attack? How will respond if there is an attack or penetration? How will we recover from an attack, such as when data has been leaked or tampered with? Key DevSecOps practices First, teams should focus on improving their mean time to detection and mean time to recovery . These are metrics that indicate how long it takes to detect a breach and how long it takes to recover, respectively. They can be tracked through ongoing live site testing of security response plans. When evaluating potential policies, improving these metrics should be an important consideration. Teams should also practice defense in depth . When a breach happens, it often results in the attacker getting access to internal networks and everything they have to offer. While it would be ideal to stop them before it gets that far, a policy of assuming breaches would drive teams to minimize their exposure from an attacker who has already gotten in. Finally, teams should perform periodic post-breach assessments of the practices and environments. After a breach has been resolved, the team should evaluate the performance of the policies, as well as their own adherence to them. This serves to not only ensure the policies are effective, but also that the team is actually following them. Every breach, whether real or practiced, should be seen as an opportunity to improve. Strategies for mitigating threats - Some security holes are due to issues in dependencies like operating systems and libraries, so keeping them up-to-date is critical. - Others are due to bugs in system code that require careful analysis to find and fix. - Poor secret management is the cause of many breaches, as is social engineering.","title":"DevSecOps"},{"location":"devops/#-attack-vectors","text":"","title":"&gt; Attack vectors"},{"location":"devops/#privilege-------------------------------attack","text":"Can they send emails? Phish colleagues Can they access other machines? Log on, mimikatz, repeat Can they modify source Inject code Can they modify the build/release process? Inject code, run scripts Can they access a test environment? If a production environment takes a dependency on the test environment, exploit it Can they access the production environment? So many options\u2026 How can the blue team defend against this? Store secrets in protected vaults Remove local admin accounts Restrict SAMR Credential Guard Remove dual-homed servers Separate subscriptions Multi-factor authentication Privileged access workstations Detect with ATP & Azure Security Center Secret management - Use a hierarchy of vaults to eliminate the duplication of secrets. - Also consider how and when secrets are accessed. Some are used at deploy-time when building environment configurations, whereas others are accessed at run-time. Deploy-time secrets typically require a new deployment in order to pick up new settings, whereas run-time secrets are accessed when needed and can be updated at any time. The red team should include some security-minded engineers and developers deeply familiar with the code. It's also helpful to augment the team with a penetration testing specialist, if possible. If there are no specialists in-house, many companies provide this service along with mentoring. The blue team should be made up of ops-minded engineers who have a deep understanding of the systems and logging available. They have the best chance of detecting and addressing suspicious behavior. Expect the red team to be effective in the early war games. They should be able to succeed through fairly simple attacks, such as by finding poorly protected secrets, SQL injection, and successful phishing campaigns. Take plenty of time between rounds to apply fixes and feedback on policies. This will vary by organization, but you don't want to start the next round until everyone is confident that the previous round has been mined for all it's worth. After a few rounds, the red team will need to rely on more sophisticated techniques, such as cross-site scripting (XSS), deserialization exploits, and engineering system vulnerabilities. it will also help to bring in additional outside security experts in areas like Active Directory in order to attack more obscure exploits. By this time, the blue team should not only have a hardened platform to defend, but will also make use of comprehensive, centralized logging for post-breach forensics. \"Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.\" \u2013 John Lambert (MSTIC) Over time, the red team will take much longer to reach objectives. When they do, it will often requiring discovery and chaining of multiple vulnerabilities to have a limited impact. Through the use of real-time monitoring tools, the blue team should start to catch them in real-time. Any security risks or lessons learned should be documented in a backlog of repair items. Teams should define a service level agreement (SLA) for how quickly security risks will be addressed. Severe risks should be addressed as soon as possible, whereas minor issues may have a two-sprint deadline. Lessons learned War games are a really effective way to change DevSecOps culture and keep security top-of-mind. Phishing attacks are very effective for attackers and should not be underestimated. The impact can be contained by limiting production access and requiring two-factor authentication. Control of the engineering system leads to control of everything. Be sure to strictly control access to the build/release agent, queue, pool, and definition. Practice defense in depth to make it harder for attackers. Every boundary they have to breach slows them down and offers another opportunity to catch them. Don't ever cross trust realms. Production should never trust anything in test.","title":"Privilege                               Attack"},{"location":"devops/cd/","text":"What is continuous delivery? \u00b6 Continuous Delivery (CD) is the process to build, test, configure, and deploy from a build to a production environment. CD by itself is a set of processes, tools, and techniques that enable rapid, reliable, and continuous delivery of software. So CD isn't only about setting up a pipeline, although that part is important. - CD is about setting up a working environment where: 1. We have a reliable and repeatable process for releasing and deploying software. 1. We automate as much as possible. 1. We don't put off doing something that's difficult or painful. Instead, we do it more often so that we figure out how to make it routine. 1. We keep everything in source control. 1. We all agree that done means released . 1. We build quality into the process. Quality is never an afterthought. 1. We're all responsible for the release process. We no longer work in silos. 1. We always try to improve. - CD helps software teams deliver reliable software updates to their customers at a rapid cadence. - CD also helps ensure that both customers and stakeholders have the latest features and fixes quickly. - This enables the full the benefits of CD - repeatable builds which are verified in a consistent way across environments. Plan a release pipeline \u00b6 Goal : We already have the build artifact. It's the .zip file that our existing build pipeline creates. But how do we deploy it to a live environment A basic CD pipeline contains a trigger to get the process going and at least one stage, or deployment phase. What is a pipeline stage? \u00b6 A stage is a part of the pipeline that can run independently and be triggered by different mechanisms. A mechanism might be the success of the previous stage, a schedule, or even a manual trigger. A stage is made up of jobs. A job is a series of steps that defines how to build, test, or deploy your software. Every stage is independent of every other stage. We could have a stage that builds the app and another stage that runs tests. What is an environment? \u00b6 You've likely used the term environment to refer to where your application or service is running. For example, your production environment might be where your end users access your application. - Following this example, your production environment might be: 1. A physical machine or virtual machine (VM). 1. A containerized environment, such as Kubernetes. 1. A managed service, such as Azure App Service. 1. A serverless environment, such as Azure Functions. - An artifact is deployed to an environment. How does pipelines perform deployment steps? \u00b6 To deploy your software, pipelines first needs to authenticate with the target environment. Pipelines provides different authentication mechanisms. To deploy your app to an Azure resource, such as a virtual machine or App Service, you need a service connection. A service connection provides secure access to your Azure subscription by using one of two methods: Service principal authentication Managed identities for Azure resources A service principal is an identity with a limited role that can access Azure resources. Think of a service principal as a service account that can do automated tasks on your behalf. Managed identities for Azure resources are a feature of Azure Active Directory (Azure AD). Managed identities simplify the process of working with service principals. Because managed identities exist on the Azure AD tenant, Azure infrastructure can automatically authenticate the service and manage the account for you. What information does pipeline analytics provide? \u00b6 Every pipeline provides reports that include metrics, trends, and insights. These reports can help you improve the efficiency of your pipeline. Reports include: The overall pass rate of your pipeline. The pass rate of any tests that are run in the pipeline. The average duration of your pipeline runs, including the build tasks, which take the most time to complete. Design the pipeline \u00b6 When you plan a release pipeline, you usually begin by identifying the stages, or major divisions, of that pipeline. Each stage typically maps to an environment. After you define which stages you need, consider how changes are promoted from one stage to the next. Each stage can define the success criteria that must be met before the build can move to the next stage. As a whole, these approaches are used for release management . What pipeline stages do you need? \u00b6 When you want to implement a release pipeline, it's important to first identify which stages you need. The stages you choose depend on your requirements. When a change is pushed to GitHub, a trigger causes the Build stage to run. The Build stage produces a build artifact as its output. Dev stage should be the first stop for the artifact after it's built. Developers can't always run the entire service from their local development environment. For example, an e-commerce system might require the website, the products database, a payment system, and so on. We need a stage that includes everything the app needs. Setting up a Dev stage would give us an environment where we can integrate the web app with a real database. That database might still hold fictitious data, but it brings us one step closer to our final app. We build the app each time we push a change to GitHub. Does that mean each build is promoted to the Dev stage after it finishes? Building continuously gives us important feedback about our build and test health. But we want to promote to the Dev stage only when we merge code into some central branch: either main or some other release branch . We can define a condition that promotes to the Dev stage only when changes happen on a release branch. The Dev stage runs only when the change happens in the release branch. You use a condition to specify this requirement. Test stage could be the next stop and runs only after the Dev stage succeeds. It can be deployed once a day or on demand to a test environment. Staging stage could be used to run additional stress tests in a preproduction environment. It can also be used to demo a working application before production deployment. This staging environment is often the last stop before a feature or bug fix reaches our users. Best way to handle promotion would be a release approval. A release approval lets you manually promote a change from one stage to the next. After management approves the build, we can deploy the build artifact to a production environment. We can define the criteria that promote changes from one stage to the next. But we've defined some manual criteria in our pipeline. I thought DevOps was about automating everything. DevOps is really about automating repetitive and error-prone tasks. Sometimes human intervention is necessary. For example, we get approval from management before we release new features. As we get more experience with our automated deployments, we can automate more of our manual steps to speed up the process. We can automate additional quality checks in the Test stage so we don't have to approve each build, for example. We test only one release at a time. We never change releases in the middle of the pipeline. We use the same release in the Dev stage as in the Staging stage, and every release has its own version number. If the release breaks in one of the stages, we fix it and build it again with a new version number. That new release then goes through the pipeline from the very beginning. How do you measure the quality of your release process? \u00b6 Team's design a pipeline that takes their app all the way from build to staging. The whole point of this pipeline isn't just to make their lives easier. It's to ensure the quality of the software they're delivering to their customers. The quality of your release process can't be measured directly. What you can measure is how well your process works. If you're constantly changing the process, this might be an indication that there's something wrong. Releases that fail consistently at a particular point in the pipeline might also indicate that there's a problem with the release process. Do they always fail after you deploy to a particular environment? Look for these and other patterns to see if some aspects of the release process are dependent or related. A good way to keep track of your release process quality is to create visualizations of the quality of the releases. For example, add a dashboard widget that shows you the status of every release. When you want to measure the quality of a release itself, you can perform all kinds of checks within the pipeline. For example, you can execute different types of tests, such as load tests and UI tests while running your pipeline. Using a quality gate is also a great way to check the quality of your release. There are many different quality gates. For example, work item gates can verify the quality of your requirements process. You can also add additional security and compliance checks. For example, do you comply with the 4-eyes principle, or do you have the proper traceability? Lastly, when you design a quality release process, think about what kind of documentation or release notes that you'll need to provide to the user. Keeping your documentation current can be difficult. You might want to consider using a tool, such as the Azure DevOps Release Notes Generator. The generator is a function app that contains a HTTP-triggered function. It creates a Markdown file whenever a new release is created in Azure DevOps, using Azure Blob Storage. Run functional tests in Pipelines \u00b6 We incorporated unit and code coverage tests into the build process . These tests help avoid regression bugs and ensure that the code meets the company's standards for quality and style. But what kinds of tests can you run after a service is operational and deployed to an environment? What is functional testing? \u00b6 Functional tests verify that each function of the software does what it should. How the software implements each function isn't important in these tests. What's important is that the software behaves correctly. You provide an input and check that the output is what you expect. - The team first defines what a functional test covers. They explore some types of functional tests. Then they decide on the first test to add to their pipeline. - UI tests are considered to be functional tests. I have to click through every step to make sure I get the correct result. And I have to do that for every browser we support. It's very time consuming. And as the website grows in complexity, UI testing won't be practical in the long run. Nonfunctional tests check characteristics like performance and reliability. - An example of a nonfunctional test is checking to see how many people can sign in to the app simultaneously. Load testing is another example of a nonfunctional test. What kinds of functional tests can we run? \u00b6 There are many kinds of functional tests. They vary by the functionality that you need to test and the time or effort that they typically require to run. Smoke testing verifies the most basic functionality of your application or service. These tests are often run before more complete and exhaustive tests. Smoke tests should run quickly. For example, say you're developing a website. Your smoke test might use curl to verify that the site is reachable and that fetching the home page produces a 200 (OK) HTTP status. If fetching the home page produces another status code, such as 404 (Not Found) or 500 (Internal Server Error), you know that the website isn't working. You also know that there's no reason to run other tests. Instead, you diagnose the error, fix it, and restart your tests. Unit testing verifies the most fundamental components of your program or library, such as an individual function or method. You specify one or more inputs along with the expected results. The test runner performs each test and checks to see whether the actual results match the expected results. As an example, let's say you have a function that performs an arithmetic operation that includes division. You might specify a few values that you expect your users to enter. You also specify edge-case values such as 0 and -1. If you expect a certain input to produce an error or exception, you can verify that the function produces that error. The UI tests that you'll run are also unit tests. Integration testing verifies that multiple software components work together to form a complete system. For example, an e-commerce system might include a website, a products database, and a payment system. You might write an integration test that adds items to the shopping cart and then purchases the items. The test verifies that the web application can connect to the products database and then fulfill the order. You can combine unit tests and integration tests to create a layered testing strategy . For example, you might run unit tests on each of your components before you run the integration tests. If all unit tests pass, you can move on to the integration tests with greater confidence. Regression testing helps determine whether code, configuration, or other changes affect the software's overall behavior. A regression occurs when existing behavior either changes or breaks after you add or change a feature. Regression testing is important because a change in one component can affect the behavior of another component. For example, say you optimize a database for write performance. The read performance of that database, which is handled by another component, might unexpectedly drop. The drop in read performance is a regression. You can use various strategies to test for regression. These strategies typically vary by the number of tests you run to verify that a new feature or bug fix doesn't break existing functionality. However, when you automate the tests, regression testing might involve just running all unit tests and integration tests each time the software changes. Sanity testing involves testing each major component of a piece of software to verify that the software appears to be working and can undergo more thorough testing. You can think of sanity tests as being less thorough than regression tests or unit tests. But sanity tests are broader than smoke tests. Although sanity testing can be automated, it's often done manually in response to a feature change or a bug fix . For example, a software tester who is validating a bug fix might also verify that other features are working by entering some typical values. If the software appears to be working as expected, it can then go through a more thorough test pass. User interface (UI) testing verifies the behavior of an application's user interface. UI tests help verify that the sequence, or order, of user interactions leads to the expected result. A unit test or integration test might verify that the UI receives data correctly. But UI testing helps verify that the user interface displays correctly and that the result functions as expected for the user. For example, a UI test might verify that the correct animation appears in response to a button click. A second test might verify that the same animation appears correctly when the window is resized. You can also use a capture-and-replay system to automatically build your UI tests. Usability testing is a form of manual testing that verifies an application's behavior from the user's perspective. Usability testing is typically done by the team that builds the software. Whereas UI testing focuses on whether a feature behaves as expected, usability testing helps verify that the software is intuitive and meets the user's needs. In other words, usability testing helps verify whether the software is \"usable.\" For example, say you have a website that includes a link to the user's profile. A UI test can verify that the link is present and that it brings up the user's profile when the link is clicked. However, if humans can't easily locate this link, they might become frustrated when they try to access their profile. User acceptance testing (UAT) , like usability testing, focuses on an application's behavior from the user's perspective. Unlike acceptance testing, UAT is typically done by real end users. Depending on the software, end users might be asked to complete specific tasks. Or they might be allowed to explore the software without following any specific guidelines. For custom software, UAT typically happens directly with the client. For more general-purpose software, teams might run beta tests. In beta tests, users from different geographic regions or users who have certain interests receive early access to the software. Feedback from testers can be direct or indirect. Direct feedback might come in the form of verbal comments. Indirect feedback can come in the form of measuring testers' body language, eye movements, or the time they take to complete certain tasks. How do I run functional tests in the pipeline? \u00b6 Ask yourself: In which stage will the tests run? On what system will the tests run? Will they run on the agent or on the infrastructure that hosts the application? - We can run them in the Test stage of our pipeline. We test the website from her Windows laptop because that's how most of our users visit the site. But we build on Linux and then deploy Azure App Service on Linux. How do we handle that? We can run them: On the agent : either a Microsoft agent or an agent that we host. On test infrastructure : either on-premises or in the cloud. Create a functional test plan \u00b6 If your team is just starting to incorporate functional tests into their pipeline (or even if you're already doing that), remember that you always need a plan. Many times, when someone asks team members about their performance testing plan, it's common for them to respond with a list of tools they are going to use. However, a list of tools isn't a plan . You also must work out how the testing environments will be configured, you need to determine the processes to be used, and you need to determine what success or failure looks like. Make sure your plan: Takes the expectations of the business into account. Takes the expectations of the target users into account. Defines the metrics you will use. Defines the KPIs you will use. It is also important to work out how you will monitor performance once the application has been deployed, and not just measure performance before it's released. Write the UI tests \u00b6 Once the test plan is ready, we're ready to write our tests. Create an NUnit project that includes Selenium. The project will be stored in the directory along with the app's source code. Write a test case that uses automation to click the specified link. The test case verifies that the expected modal window appears. Use the id attribute we saved to specify the parameters to the test case method. This task creates a sequence, or series, of tests. Configure the tests to run on Chrome, Firefox, and Microsoft Edge. This task creates a matrix of tests. Run the tests and watch each web browser come up automatically. Watch Selenium automatically run through the series of tests for each browser. In the console window, verify that all the tests pass. Run the UI tests in the pipeline \u00b6 The Build stage publishes only the app package as the build artifact. Publish task in the Build stage generates two build artifacts : the app package and the compiled UI tests. We build the UI tests during the Build stage to ensure that they'll compile during the Test stage. But we don't need to publish the compiled test code. We build it again during the Test stage when the tests run. The Test stage includes a second job that builds and runs the tests. Although we use a Linux agent to build the application, here we use a Windows agent to run the UI tests. We use a Windows agent because we runs manual tests on Windows, and that's what most customers use. Remember, tests that you repeatedly run manually are good candidates for automation. Run nonfunctional tests in Pipelines \u00b6 After your service is operational and deployed to an environment, how can you determine the application's performance under both realistic and heavy loads? Does your application expose any loopholes or weaknesses that might cause an information breach? What is nonfunctional testing? \u00b6 With automated functional tests, is there anything we should do in Staging to help increase the quality of our releases? Normally, after our sites are in production, I run performance, load, and stress tests. But I'd like to start running other kinds of tests as well, such as compliance tests and security tests. All of those tests are difficult to run manually. By using automation, we can run them both earlier and more frequently. - Nonfunctional testing always tests something that's measurable. The goal is to improve the product. - You might do that, for example, by improving how efficiently the application uses resources or by improving response times when many customers use it simultaneously. Here are some of the questions that nonfunctional tests can answer: 1. How does the application perform under normal circumstances? 1. How does the application perform when many users sign in concurrently? 1. How secure is the application? What kinds of nonfunctional tests can I run? \u00b6 There are many kinds of nonfunctional tests. Many of them fit in the broad categories of performance testing and security testing. Performance testing : The goal of performance testing is to improve the speed, scalability, and stability of an application . Testing for speed determines how quickly an application responds. Testing for scalability determines the maximum user load an application can handle. Testing for stability determines whether the application remains stable under different loads. Two common types of performance tests are load tests and stress tests. a. Load tests determine the performance of an application under realistic loads. For example, load tests can determine how well an application performs at the upper limit of its service-level agreement (SLA). Basically, load testing determines the behavior of the application when multiple users need it at the same time. Users aren't necessarily people. A load test for printer software, for example, might send the application large amounts of data. A load test for a mail server might simulate thousands of concurrent users. Load testing is also a good way to uncover problems that exist only when the application is operating at its limits. That's when issues such as buffer overflow and memory leaks can surface. b. Stress tests determine the stability and robustness of an application under heavy loads. The loads go beyond what's specified for the application. The stress tests determine whether the application will crash under these loads. If the application fails, the stress test checks to ensure that it fails gracefully. A graceful failure might, for example, issue an appropriate, informative error message. Scenarios in which applications must operate under abnormally heavy loads are common. For example, in case your video goes viral, you'll want to know how well the servers can handle the extra load. Another typical scenario is high traffic on shopping websites during holiday seasons. Security testing ensures that applications are free from vulnerabilities, threats, and risks. Thorough security testing finds all the possible loopholes and weaknesses of the system that might cause an information breach or a loss of revenue. There are many types of security testing. Two of them are penetration testing and compliance testing. a. Penetration testing, or pen testing , is a type of security testing that tests the insecure areas of the application. In particular, it tests for vulnerabilities that an attacker could exploit. An authorized, simulated cyber attack is usually a part of penetration testing. b. Compliance testing determines whether an application is compliant with some set of requirements, inside or outside the company. For example, healthcare organizations usually need to comply with HIPAA (Health Insurance Portability and Accountability Act of 1996), which provides data privacy and security provisions for safeguarding medical information. For example, on Linux systems, the default user mask must be 027 or more restrictive. A security test needs to prove that this requirement is met. Just as you did when you incorporated functional tests into your pipeline, focus on the types of nonfunctional tests that matter most. For example, if your team must adhere to certain compliance requirements, consider adding automated tests that provide a detailed status report. Manage release cadence in Pipelines by using deployment patterns \u00b6 Choose and implement a deployment pattern that helps you smoothly roll out new application features to your users. - You help the team solve another problem. How do they implement a deployment pattern that lets them release to production in a way that's best both for the company and for their users? You'll help them evaluate the possibilities and then implement the one that they choose. - Ops thinks it takes too long to release new features. They can't do anything until management approves the release and, right now, there's no smooth way to roll out the features after they give the OK. The process is not only long but messy. It's manual, and there's downtime. What are deployment patterns? \u00b6 A deployment pattern is an automated way to smoothly roll out new application features to your users. An appropriate deployment pattern helps you minimize downtime . - Some patterns also enable you to roll out new features progressively. That way, you can validate new features with select users before you make those features available to everyone. - A deployment pattern is an automated way to do the cutover. It's how we move the software from the final preproduction stage to live production. - Another advantage of a deployment pattern is that it gives us a chance to run tests that should really happen in production. Types of deployment patterns \u00b6 1. Blue-green deployment : A blue-green deployment reduces risk and downtime by running two identical environments. These environments are called blue and green. At any time, only one of the environments is live. A blue-green deployment typically involves a router or load balancer that helps control the flow of traffic. - Let's say blue is live. As we prepare a new release, we do our final tests in the green environment. After the software is working in the green environment, we just switch the router so that all incoming requests go to the green environment. - Blue-green deployment also gives us a fast way to do a rollback. If anything goes wrong in the green environment, then we just switch the router back to the blue environment. - A blue-green deployment is something Ops can control. Switching a router is straightforward. It's easy and sounds safe. And in a blue-green deployment, management has an environment to evaluate. When they give the OK, we can easily switch. 2. Canary releases : A canary release is a way to identify potential problems early without exposing all users to the issue. The idea is that we expose a new feature to only a small subset of users before we make it available to everyone. - In a canary release, we monitor what happens when we release the feature. If the release has problems, then we apply a fix. After the canary release is known to be stable, we move it to the actual production environment. - For example: You have a new feature for your website, and you're ready to deploy it. However, this feature is risky because it changes the way your users interact with the site. You can use canary release to a small group of early adopters who have signed up to see new features. 3. Feature toggles : Feature toggles let us \"flip a switch\" at runtime. We can deploy new software without exposing any other new or changed functionality to our users. - In this deployment pattern, we build new features behind a toggle. When a release occurs, the feature is \"off\" so that it doesn't affect the production software. Depending on how we configure the toggle, we can flip the switch to \"on\" and expose it how we want. - The big advantage to the feature toggles pattern is that it helps us avoid too much branching. Merging branches can be painful. 4. Dark launches : A dark launch is similar to a canary release or switching a feature toggle. Rather than expose a new feature to everyone, in a dark launch we release the feature to a small set of users. - Those users don't know they're testing the feature for us. We don't even highlight the new feature to them. That's why it's called a dark launch. The software is gradually or unobtrusively released to users so we can get feedback and can test performance. - For example: You're not sure how your users will react to your new feature. You want to release your feature to a small, random sample of users to see how they react. 5. A/B testing : A/B testing compares two versions of a webpage or app to determine which one performs better. A/B testing is like an experiment. - In A/B testing, we randomly show users two or more variations of a page. Then we use statistical analysis to decide which variation performs better for our goals. - For example: The marketing team has asked you to add a banner to your company's website. They have two versions of this banner. They want to know which version produces more clickthroughs. You can use A/B testing deployment pattern to help the marketing team identify the better version. 6. Progressive-exposure deployment : Progressive-exposure deployment is sometimes called ring-based deployment. It's another way to limit how changes affect users while making sure that those changes are valid in a production environment. - Rings are basically an extension of the canary stage. The canary release releases to a stage to measure effect. Adding another ring is essentially the same idea. - In a ring-based deployment, we deploy changes to risk-tolerant customers first. Then we progressively roll out to a larger set of customers. Choosing the right deployment pattern \u00b6 A good deployment pattern can help you minimize downtime. It can also enable you to roll out new features progressively to your users. The deployment pattern that you choose depends on your reasons for the deployment as well as your resources . Do you have canary testers in place? Will you employ a dark launch and choose testers who don't know that they are testers? If you have a trusted set of testers that progressively increases from a small set to a larger set, then could choose a progressive-exposure deployment. Or if you want to know if one version performs better than another version, you could choose A/B testing. Why are containers important? \u00b6 Dependency versioning challenges for QA QA test for multiple teams, and it can be challenging since each team uses their own technology stack. And even when they use the same underlying platforms, like .NET or Java, they often target different versions. QA sometimes spend half of their day simply getting test environments in a state where they can run the code they need to test. When something doesn't work, it's hard to tell whether there's a bug in the code or if they accidentally configured platform version 4.2.3 instead of 4.3.2. Overhead due to solving app isolation with VMs We have a few teams that have unique version requirements, so we have to publish their apps on their own virtual machines just to make sure their version and component requirements don't conflict with our other apps. - Besides the overhead involved in maintaining the extra set of VMs, it also costs us more than it would if those apps could run side by side. Configuration inconsistencies between deployment stages I was working on the peer-to-peer update system and had it all working on my machine. But when I handed it off for deployment, it didn't work in production. I had forgotten that I needed to open port 315 as part of the service. It took us over a day of troubleshooting to realize what was going on. Once we opened that up in production, things worked as expected. What is a container? It's more like a lightweight virtual machine designed to run directly on the host operating system. When you build your project, the output is a container that includes your software along with its dependencies. However, it's not a complete virtualized system, so it can spin up in a little as less than one second. Security and isolation are handled by the host operating system . When your container runs in a host process, the container is isolated from the other processes on that same host machine. This isolation allows your container to load whatever versions of components it needs, regardless of what other containers are doing. It also means you can easily run multiple containers on the same host simultaneously. Instead of shipping source code or a set of binaries, the entire container becomes the artifact. Containers already includes all the required versions of its dependencies. Containers are more of a packaging and deployment technology . They don't impact the fundamental software we're writing. We can just instruct our tools to produce a Docker container at the end of the build. What is Docker? \u00b6 Docker is a technology for automating the packaging and deployment of portable, self-sufficient containers. - Docker containers can be run anywhere a Docker host is found, whether on a development machine, a departmental server, an enterprise datacenter, or in the cloud. Azure provides multiple ways to run container-based applications, including App Service or as part of clusters managed with orchestration technologies like Kubernetes. Dependency versioning challenges for QA Applications are packaged as containers that bring the correct versions of their dependencies with them. Overhead due to solving app isolation with VMs Many isolated containers can be run on the same host with benefits over virtual machines including faster startup time to greater resource efficiency. Configuration inconsistencies between DevOps stages Containers ship with manifests that automate configuration requirements, such as which ports need to be exposed. Adopting Docker containers can be a key step on the path towards a microservices architecture. Why is container orchestration important? \u00b6 As development organizations scale, so do the complexities of the solutions they deliver. Over time, different products and services take increasing dependence on each other. This can result in different development and operations requirements for different components within a given application. Refactoring those components as their own distinct microservices can improve architectural efficiency. A microservice is a small, autonomous service designed to be self-contained and to fulfill a specific business capability. Microservices are typical apps, like our web app. The main difference is that instead of building and deploying a single monolithic app, we refactor any components that would be better maintained and managed as autonomous services. We then build those services to be good at what they do and deploy them to operate independently. Microservices are an architectural concept and not a deployment technology . Containers provide a great technical foundation for building and deploying these services, but this leads to a new question: how do we manage all of these containers floating around? This is where orchestration technologies like Kubernetes come in. Kubernetes is a platform for managing containerized workloads and services. It's a great option for organizations that have a growing number of containers that need to be deployed, integrated, and monitored in any environment. Kubernetes offers a way for us to deploy to different namespaces. This enables us to partition our deployments so that we can have entire environments dedicated to testing versus production. And since they all run in the same cluster and use the same containers, the testing experience should offer what we expect to see in production. Assuming we have our projects set up to build Docker containers, all we need to deploy to Kubernetes are some manifest files that describe the services and their deployments. What is Kubernetes? \u00b6 Kubernetes is a technology for orchestrating multi-container deployments. It provides a framework for running distributed systems in a declarative, responsive fashion . It automatically applies and enforces your deployment patterns to ensure that containers are deployed and run as intended. It also offers support for specialized release cadences, such as those using canary deployments . Complexity of multi-container deployments Kubernetes is designed, first and foremost, to automate the processes around deploying and maintaining container deployments. Consistency across environments and stages Just as containers ensure a consistent deployment for the apps they contain, Kubernetes ensures a consistent deployment for the containers a cluster manages. Azure DevOps support Azure DevOps offers first-class support for working with Kubernetes. Ease of development The impact of Kubernetes on a source project is comparable to that of adding Docker support, which is minimal and limited to declarative configuration. Benefits of Continous Delivery in the Release process \u00b6 The time it takes to get the build into the Test stage. The team achieved this improvement by using a scheduled trigger to deploy to Test every day at 3:00 AM. The time it takes to get the tested build into Staging . The team achieved this improvement by adding Selenium UI tests, a form of functional testing, to the Test stage. These automated tests are much faster than the manual versions. The time it takes to get the approved build from Staging to live . The team achieved this improvement by adding manual approval checks to the pipeline. When management signs off, Ops can release the changes from Staging to live.","title":"CD"},{"location":"devops/cd/#what-is-continuous-delivery","text":"Continuous Delivery (CD) is the process to build, test, configure, and deploy from a build to a production environment. CD by itself is a set of processes, tools, and techniques that enable rapid, reliable, and continuous delivery of software. So CD isn't only about setting up a pipeline, although that part is important. - CD is about setting up a working environment where: 1. We have a reliable and repeatable process for releasing and deploying software. 1. We automate as much as possible. 1. We don't put off doing something that's difficult or painful. Instead, we do it more often so that we figure out how to make it routine. 1. We keep everything in source control. 1. We all agree that done means released . 1. We build quality into the process. Quality is never an afterthought. 1. We're all responsible for the release process. We no longer work in silos. 1. We always try to improve. - CD helps software teams deliver reliable software updates to their customers at a rapid cadence. - CD also helps ensure that both customers and stakeholders have the latest features and fixes quickly. - This enables the full the benefits of CD - repeatable builds which are verified in a consistent way across environments.","title":"What is continuous delivery?"},{"location":"devops/cd/#plan-a-release-pipeline","text":"Goal : We already have the build artifact. It's the .zip file that our existing build pipeline creates. But how do we deploy it to a live environment A basic CD pipeline contains a trigger to get the process going and at least one stage, or deployment phase.","title":"Plan a release pipeline"},{"location":"devops/cd/#what-is-a-pipeline-stage","text":"A stage is a part of the pipeline that can run independently and be triggered by different mechanisms. A mechanism might be the success of the previous stage, a schedule, or even a manual trigger. A stage is made up of jobs. A job is a series of steps that defines how to build, test, or deploy your software. Every stage is independent of every other stage. We could have a stage that builds the app and another stage that runs tests.","title":"What is a pipeline stage?"},{"location":"devops/cd/#what-is-an-environment","text":"You've likely used the term environment to refer to where your application or service is running. For example, your production environment might be where your end users access your application. - Following this example, your production environment might be: 1. A physical machine or virtual machine (VM). 1. A containerized environment, such as Kubernetes. 1. A managed service, such as Azure App Service. 1. A serverless environment, such as Azure Functions. - An artifact is deployed to an environment.","title":"What is an environment?"},{"location":"devops/cd/#how-does-pipelines-perform-deployment-steps","text":"To deploy your software, pipelines first needs to authenticate with the target environment. Pipelines provides different authentication mechanisms. To deploy your app to an Azure resource, such as a virtual machine or App Service, you need a service connection. A service connection provides secure access to your Azure subscription by using one of two methods: Service principal authentication Managed identities for Azure resources A service principal is an identity with a limited role that can access Azure resources. Think of a service principal as a service account that can do automated tasks on your behalf. Managed identities for Azure resources are a feature of Azure Active Directory (Azure AD). Managed identities simplify the process of working with service principals. Because managed identities exist on the Azure AD tenant, Azure infrastructure can automatically authenticate the service and manage the account for you.","title":"How does pipelines perform deployment steps?"},{"location":"devops/cd/#what-information-does-pipeline-analytics-provide","text":"Every pipeline provides reports that include metrics, trends, and insights. These reports can help you improve the efficiency of your pipeline. Reports include: The overall pass rate of your pipeline. The pass rate of any tests that are run in the pipeline. The average duration of your pipeline runs, including the build tasks, which take the most time to complete.","title":"What information does pipeline analytics provide?"},{"location":"devops/cd/#design-the-pipeline","text":"When you plan a release pipeline, you usually begin by identifying the stages, or major divisions, of that pipeline. Each stage typically maps to an environment. After you define which stages you need, consider how changes are promoted from one stage to the next. Each stage can define the success criteria that must be met before the build can move to the next stage. As a whole, these approaches are used for release management .","title":"Design the pipeline"},{"location":"devops/cd/#what-pipeline-stages-do-you-need","text":"When you want to implement a release pipeline, it's important to first identify which stages you need. The stages you choose depend on your requirements. When a change is pushed to GitHub, a trigger causes the Build stage to run. The Build stage produces a build artifact as its output. Dev stage should be the first stop for the artifact after it's built. Developers can't always run the entire service from their local development environment. For example, an e-commerce system might require the website, the products database, a payment system, and so on. We need a stage that includes everything the app needs. Setting up a Dev stage would give us an environment where we can integrate the web app with a real database. That database might still hold fictitious data, but it brings us one step closer to our final app. We build the app each time we push a change to GitHub. Does that mean each build is promoted to the Dev stage after it finishes? Building continuously gives us important feedback about our build and test health. But we want to promote to the Dev stage only when we merge code into some central branch: either main or some other release branch . We can define a condition that promotes to the Dev stage only when changes happen on a release branch. The Dev stage runs only when the change happens in the release branch. You use a condition to specify this requirement. Test stage could be the next stop and runs only after the Dev stage succeeds. It can be deployed once a day or on demand to a test environment. Staging stage could be used to run additional stress tests in a preproduction environment. It can also be used to demo a working application before production deployment. This staging environment is often the last stop before a feature or bug fix reaches our users. Best way to handle promotion would be a release approval. A release approval lets you manually promote a change from one stage to the next. After management approves the build, we can deploy the build artifact to a production environment. We can define the criteria that promote changes from one stage to the next. But we've defined some manual criteria in our pipeline. I thought DevOps was about automating everything. DevOps is really about automating repetitive and error-prone tasks. Sometimes human intervention is necessary. For example, we get approval from management before we release new features. As we get more experience with our automated deployments, we can automate more of our manual steps to speed up the process. We can automate additional quality checks in the Test stage so we don't have to approve each build, for example. We test only one release at a time. We never change releases in the middle of the pipeline. We use the same release in the Dev stage as in the Staging stage, and every release has its own version number. If the release breaks in one of the stages, we fix it and build it again with a new version number. That new release then goes through the pipeline from the very beginning.","title":"What pipeline stages do you need?"},{"location":"devops/cd/#how-do-you-measure-the-quality-of-your-release-process","text":"Team's design a pipeline that takes their app all the way from build to staging. The whole point of this pipeline isn't just to make their lives easier. It's to ensure the quality of the software they're delivering to their customers. The quality of your release process can't be measured directly. What you can measure is how well your process works. If you're constantly changing the process, this might be an indication that there's something wrong. Releases that fail consistently at a particular point in the pipeline might also indicate that there's a problem with the release process. Do they always fail after you deploy to a particular environment? Look for these and other patterns to see if some aspects of the release process are dependent or related. A good way to keep track of your release process quality is to create visualizations of the quality of the releases. For example, add a dashboard widget that shows you the status of every release. When you want to measure the quality of a release itself, you can perform all kinds of checks within the pipeline. For example, you can execute different types of tests, such as load tests and UI tests while running your pipeline. Using a quality gate is also a great way to check the quality of your release. There are many different quality gates. For example, work item gates can verify the quality of your requirements process. You can also add additional security and compliance checks. For example, do you comply with the 4-eyes principle, or do you have the proper traceability? Lastly, when you design a quality release process, think about what kind of documentation or release notes that you'll need to provide to the user. Keeping your documentation current can be difficult. You might want to consider using a tool, such as the Azure DevOps Release Notes Generator. The generator is a function app that contains a HTTP-triggered function. It creates a Markdown file whenever a new release is created in Azure DevOps, using Azure Blob Storage.","title":"How do you measure the quality of your release process?"},{"location":"devops/cd/#run-functional-tests-in-pipelines","text":"We incorporated unit and code coverage tests into the build process . These tests help avoid regression bugs and ensure that the code meets the company's standards for quality and style. But what kinds of tests can you run after a service is operational and deployed to an environment?","title":"Run functional tests in Pipelines"},{"location":"devops/cd/#what-is-functional-testing","text":"Functional tests verify that each function of the software does what it should. How the software implements each function isn't important in these tests. What's important is that the software behaves correctly. You provide an input and check that the output is what you expect. - The team first defines what a functional test covers. They explore some types of functional tests. Then they decide on the first test to add to their pipeline. - UI tests are considered to be functional tests. I have to click through every step to make sure I get the correct result. And I have to do that for every browser we support. It's very time consuming. And as the website grows in complexity, UI testing won't be practical in the long run. Nonfunctional tests check characteristics like performance and reliability. - An example of a nonfunctional test is checking to see how many people can sign in to the app simultaneously. Load testing is another example of a nonfunctional test.","title":"What is functional testing?"},{"location":"devops/cd/#what-kinds-of-functional-tests-can-we-run","text":"There are many kinds of functional tests. They vary by the functionality that you need to test and the time or effort that they typically require to run. Smoke testing verifies the most basic functionality of your application or service. These tests are often run before more complete and exhaustive tests. Smoke tests should run quickly. For example, say you're developing a website. Your smoke test might use curl to verify that the site is reachable and that fetching the home page produces a 200 (OK) HTTP status. If fetching the home page produces another status code, such as 404 (Not Found) or 500 (Internal Server Error), you know that the website isn't working. You also know that there's no reason to run other tests. Instead, you diagnose the error, fix it, and restart your tests. Unit testing verifies the most fundamental components of your program or library, such as an individual function or method. You specify one or more inputs along with the expected results. The test runner performs each test and checks to see whether the actual results match the expected results. As an example, let's say you have a function that performs an arithmetic operation that includes division. You might specify a few values that you expect your users to enter. You also specify edge-case values such as 0 and -1. If you expect a certain input to produce an error or exception, you can verify that the function produces that error. The UI tests that you'll run are also unit tests. Integration testing verifies that multiple software components work together to form a complete system. For example, an e-commerce system might include a website, a products database, and a payment system. You might write an integration test that adds items to the shopping cart and then purchases the items. The test verifies that the web application can connect to the products database and then fulfill the order. You can combine unit tests and integration tests to create a layered testing strategy . For example, you might run unit tests on each of your components before you run the integration tests. If all unit tests pass, you can move on to the integration tests with greater confidence. Regression testing helps determine whether code, configuration, or other changes affect the software's overall behavior. A regression occurs when existing behavior either changes or breaks after you add or change a feature. Regression testing is important because a change in one component can affect the behavior of another component. For example, say you optimize a database for write performance. The read performance of that database, which is handled by another component, might unexpectedly drop. The drop in read performance is a regression. You can use various strategies to test for regression. These strategies typically vary by the number of tests you run to verify that a new feature or bug fix doesn't break existing functionality. However, when you automate the tests, regression testing might involve just running all unit tests and integration tests each time the software changes. Sanity testing involves testing each major component of a piece of software to verify that the software appears to be working and can undergo more thorough testing. You can think of sanity tests as being less thorough than regression tests or unit tests. But sanity tests are broader than smoke tests. Although sanity testing can be automated, it's often done manually in response to a feature change or a bug fix . For example, a software tester who is validating a bug fix might also verify that other features are working by entering some typical values. If the software appears to be working as expected, it can then go through a more thorough test pass. User interface (UI) testing verifies the behavior of an application's user interface. UI tests help verify that the sequence, or order, of user interactions leads to the expected result. A unit test or integration test might verify that the UI receives data correctly. But UI testing helps verify that the user interface displays correctly and that the result functions as expected for the user. For example, a UI test might verify that the correct animation appears in response to a button click. A second test might verify that the same animation appears correctly when the window is resized. You can also use a capture-and-replay system to automatically build your UI tests. Usability testing is a form of manual testing that verifies an application's behavior from the user's perspective. Usability testing is typically done by the team that builds the software. Whereas UI testing focuses on whether a feature behaves as expected, usability testing helps verify that the software is intuitive and meets the user's needs. In other words, usability testing helps verify whether the software is \"usable.\" For example, say you have a website that includes a link to the user's profile. A UI test can verify that the link is present and that it brings up the user's profile when the link is clicked. However, if humans can't easily locate this link, they might become frustrated when they try to access their profile. User acceptance testing (UAT) , like usability testing, focuses on an application's behavior from the user's perspective. Unlike acceptance testing, UAT is typically done by real end users. Depending on the software, end users might be asked to complete specific tasks. Or they might be allowed to explore the software without following any specific guidelines. For custom software, UAT typically happens directly with the client. For more general-purpose software, teams might run beta tests. In beta tests, users from different geographic regions or users who have certain interests receive early access to the software. Feedback from testers can be direct or indirect. Direct feedback might come in the form of verbal comments. Indirect feedback can come in the form of measuring testers' body language, eye movements, or the time they take to complete certain tasks.","title":"What kinds of functional tests can we run?"},{"location":"devops/cd/#how-do-i-run-functional-tests-in-the-pipeline","text":"Ask yourself: In which stage will the tests run? On what system will the tests run? Will they run on the agent or on the infrastructure that hosts the application? - We can run them in the Test stage of our pipeline. We test the website from her Windows laptop because that's how most of our users visit the site. But we build on Linux and then deploy Azure App Service on Linux. How do we handle that? We can run them: On the agent : either a Microsoft agent or an agent that we host. On test infrastructure : either on-premises or in the cloud.","title":"How do I run functional tests in the pipeline?"},{"location":"devops/cd/#create-a-functional-test-plan","text":"If your team is just starting to incorporate functional tests into their pipeline (or even if you're already doing that), remember that you always need a plan. Many times, when someone asks team members about their performance testing plan, it's common for them to respond with a list of tools they are going to use. However, a list of tools isn't a plan . You also must work out how the testing environments will be configured, you need to determine the processes to be used, and you need to determine what success or failure looks like. Make sure your plan: Takes the expectations of the business into account. Takes the expectations of the target users into account. Defines the metrics you will use. Defines the KPIs you will use. It is also important to work out how you will monitor performance once the application has been deployed, and not just measure performance before it's released.","title":"Create a functional test plan"},{"location":"devops/cd/#write-the-ui-tests","text":"Once the test plan is ready, we're ready to write our tests. Create an NUnit project that includes Selenium. The project will be stored in the directory along with the app's source code. Write a test case that uses automation to click the specified link. The test case verifies that the expected modal window appears. Use the id attribute we saved to specify the parameters to the test case method. This task creates a sequence, or series, of tests. Configure the tests to run on Chrome, Firefox, and Microsoft Edge. This task creates a matrix of tests. Run the tests and watch each web browser come up automatically. Watch Selenium automatically run through the series of tests for each browser. In the console window, verify that all the tests pass.","title":"Write the UI tests"},{"location":"devops/cd/#run-the-ui-tests-in-the-pipeline","text":"The Build stage publishes only the app package as the build artifact. Publish task in the Build stage generates two build artifacts : the app package and the compiled UI tests. We build the UI tests during the Build stage to ensure that they'll compile during the Test stage. But we don't need to publish the compiled test code. We build it again during the Test stage when the tests run. The Test stage includes a second job that builds and runs the tests. Although we use a Linux agent to build the application, here we use a Windows agent to run the UI tests. We use a Windows agent because we runs manual tests on Windows, and that's what most customers use. Remember, tests that you repeatedly run manually are good candidates for automation.","title":"Run the UI tests in the pipeline"},{"location":"devops/cd/#run-nonfunctional-tests-in-pipelines","text":"After your service is operational and deployed to an environment, how can you determine the application's performance under both realistic and heavy loads? Does your application expose any loopholes or weaknesses that might cause an information breach?","title":"Run nonfunctional tests in Pipelines"},{"location":"devops/cd/#what-is-nonfunctional-testing","text":"With automated functional tests, is there anything we should do in Staging to help increase the quality of our releases? Normally, after our sites are in production, I run performance, load, and stress tests. But I'd like to start running other kinds of tests as well, such as compliance tests and security tests. All of those tests are difficult to run manually. By using automation, we can run them both earlier and more frequently. - Nonfunctional testing always tests something that's measurable. The goal is to improve the product. - You might do that, for example, by improving how efficiently the application uses resources or by improving response times when many customers use it simultaneously. Here are some of the questions that nonfunctional tests can answer: 1. How does the application perform under normal circumstances? 1. How does the application perform when many users sign in concurrently? 1. How secure is the application?","title":"What is nonfunctional testing?"},{"location":"devops/cd/#what-kinds-of-nonfunctional-tests-can-i-run","text":"There are many kinds of nonfunctional tests. Many of them fit in the broad categories of performance testing and security testing. Performance testing : The goal of performance testing is to improve the speed, scalability, and stability of an application . Testing for speed determines how quickly an application responds. Testing for scalability determines the maximum user load an application can handle. Testing for stability determines whether the application remains stable under different loads. Two common types of performance tests are load tests and stress tests. a. Load tests determine the performance of an application under realistic loads. For example, load tests can determine how well an application performs at the upper limit of its service-level agreement (SLA). Basically, load testing determines the behavior of the application when multiple users need it at the same time. Users aren't necessarily people. A load test for printer software, for example, might send the application large amounts of data. A load test for a mail server might simulate thousands of concurrent users. Load testing is also a good way to uncover problems that exist only when the application is operating at its limits. That's when issues such as buffer overflow and memory leaks can surface. b. Stress tests determine the stability and robustness of an application under heavy loads. The loads go beyond what's specified for the application. The stress tests determine whether the application will crash under these loads. If the application fails, the stress test checks to ensure that it fails gracefully. A graceful failure might, for example, issue an appropriate, informative error message. Scenarios in which applications must operate under abnormally heavy loads are common. For example, in case your video goes viral, you'll want to know how well the servers can handle the extra load. Another typical scenario is high traffic on shopping websites during holiday seasons. Security testing ensures that applications are free from vulnerabilities, threats, and risks. Thorough security testing finds all the possible loopholes and weaknesses of the system that might cause an information breach or a loss of revenue. There are many types of security testing. Two of them are penetration testing and compliance testing. a. Penetration testing, or pen testing , is a type of security testing that tests the insecure areas of the application. In particular, it tests for vulnerabilities that an attacker could exploit. An authorized, simulated cyber attack is usually a part of penetration testing. b. Compliance testing determines whether an application is compliant with some set of requirements, inside or outside the company. For example, healthcare organizations usually need to comply with HIPAA (Health Insurance Portability and Accountability Act of 1996), which provides data privacy and security provisions for safeguarding medical information. For example, on Linux systems, the default user mask must be 027 or more restrictive. A security test needs to prove that this requirement is met. Just as you did when you incorporated functional tests into your pipeline, focus on the types of nonfunctional tests that matter most. For example, if your team must adhere to certain compliance requirements, consider adding automated tests that provide a detailed status report.","title":"What kinds of nonfunctional tests can I run?"},{"location":"devops/cd/#manage-release-cadence-in-pipelines-by-using-deployment-patterns","text":"Choose and implement a deployment pattern that helps you smoothly roll out new application features to your users. - You help the team solve another problem. How do they implement a deployment pattern that lets them release to production in a way that's best both for the company and for their users? You'll help them evaluate the possibilities and then implement the one that they choose. - Ops thinks it takes too long to release new features. They can't do anything until management approves the release and, right now, there's no smooth way to roll out the features after they give the OK. The process is not only long but messy. It's manual, and there's downtime.","title":"Manage release cadence in Pipelines by using deployment patterns"},{"location":"devops/cd/#what-are-deployment-patterns","text":"A deployment pattern is an automated way to smoothly roll out new application features to your users. An appropriate deployment pattern helps you minimize downtime . - Some patterns also enable you to roll out new features progressively. That way, you can validate new features with select users before you make those features available to everyone. - A deployment pattern is an automated way to do the cutover. It's how we move the software from the final preproduction stage to live production. - Another advantage of a deployment pattern is that it gives us a chance to run tests that should really happen in production.","title":"What are deployment patterns?"},{"location":"devops/cd/#types-of-deployment-patterns","text":"1. Blue-green deployment : A blue-green deployment reduces risk and downtime by running two identical environments. These environments are called blue and green. At any time, only one of the environments is live. A blue-green deployment typically involves a router or load balancer that helps control the flow of traffic. - Let's say blue is live. As we prepare a new release, we do our final tests in the green environment. After the software is working in the green environment, we just switch the router so that all incoming requests go to the green environment. - Blue-green deployment also gives us a fast way to do a rollback. If anything goes wrong in the green environment, then we just switch the router back to the blue environment. - A blue-green deployment is something Ops can control. Switching a router is straightforward. It's easy and sounds safe. And in a blue-green deployment, management has an environment to evaluate. When they give the OK, we can easily switch. 2. Canary releases : A canary release is a way to identify potential problems early without exposing all users to the issue. The idea is that we expose a new feature to only a small subset of users before we make it available to everyone. - In a canary release, we monitor what happens when we release the feature. If the release has problems, then we apply a fix. After the canary release is known to be stable, we move it to the actual production environment. - For example: You have a new feature for your website, and you're ready to deploy it. However, this feature is risky because it changes the way your users interact with the site. You can use canary release to a small group of early adopters who have signed up to see new features. 3. Feature toggles : Feature toggles let us \"flip a switch\" at runtime. We can deploy new software without exposing any other new or changed functionality to our users. - In this deployment pattern, we build new features behind a toggle. When a release occurs, the feature is \"off\" so that it doesn't affect the production software. Depending on how we configure the toggle, we can flip the switch to \"on\" and expose it how we want. - The big advantage to the feature toggles pattern is that it helps us avoid too much branching. Merging branches can be painful. 4. Dark launches : A dark launch is similar to a canary release or switching a feature toggle. Rather than expose a new feature to everyone, in a dark launch we release the feature to a small set of users. - Those users don't know they're testing the feature for us. We don't even highlight the new feature to them. That's why it's called a dark launch. The software is gradually or unobtrusively released to users so we can get feedback and can test performance. - For example: You're not sure how your users will react to your new feature. You want to release your feature to a small, random sample of users to see how they react. 5. A/B testing : A/B testing compares two versions of a webpage or app to determine which one performs better. A/B testing is like an experiment. - In A/B testing, we randomly show users two or more variations of a page. Then we use statistical analysis to decide which variation performs better for our goals. - For example: The marketing team has asked you to add a banner to your company's website. They have two versions of this banner. They want to know which version produces more clickthroughs. You can use A/B testing deployment pattern to help the marketing team identify the better version. 6. Progressive-exposure deployment : Progressive-exposure deployment is sometimes called ring-based deployment. It's another way to limit how changes affect users while making sure that those changes are valid in a production environment. - Rings are basically an extension of the canary stage. The canary release releases to a stage to measure effect. Adding another ring is essentially the same idea. - In a ring-based deployment, we deploy changes to risk-tolerant customers first. Then we progressively roll out to a larger set of customers.","title":"Types of deployment patterns"},{"location":"devops/cd/#choosing-the-right-deployment-pattern","text":"A good deployment pattern can help you minimize downtime. It can also enable you to roll out new features progressively to your users. The deployment pattern that you choose depends on your reasons for the deployment as well as your resources . Do you have canary testers in place? Will you employ a dark launch and choose testers who don't know that they are testers? If you have a trusted set of testers that progressively increases from a small set to a larger set, then could choose a progressive-exposure deployment. Or if you want to know if one version performs better than another version, you could choose A/B testing.","title":"Choosing the right deployment pattern"},{"location":"devops/cd/#why-are-containers-important","text":"Dependency versioning challenges for QA QA test for multiple teams, and it can be challenging since each team uses their own technology stack. And even when they use the same underlying platforms, like .NET or Java, they often target different versions. QA sometimes spend half of their day simply getting test environments in a state where they can run the code they need to test. When something doesn't work, it's hard to tell whether there's a bug in the code or if they accidentally configured platform version 4.2.3 instead of 4.3.2. Overhead due to solving app isolation with VMs We have a few teams that have unique version requirements, so we have to publish their apps on their own virtual machines just to make sure their version and component requirements don't conflict with our other apps. - Besides the overhead involved in maintaining the extra set of VMs, it also costs us more than it would if those apps could run side by side. Configuration inconsistencies between deployment stages I was working on the peer-to-peer update system and had it all working on my machine. But when I handed it off for deployment, it didn't work in production. I had forgotten that I needed to open port 315 as part of the service. It took us over a day of troubleshooting to realize what was going on. Once we opened that up in production, things worked as expected. What is a container? It's more like a lightweight virtual machine designed to run directly on the host operating system. When you build your project, the output is a container that includes your software along with its dependencies. However, it's not a complete virtualized system, so it can spin up in a little as less than one second. Security and isolation are handled by the host operating system . When your container runs in a host process, the container is isolated from the other processes on that same host machine. This isolation allows your container to load whatever versions of components it needs, regardless of what other containers are doing. It also means you can easily run multiple containers on the same host simultaneously. Instead of shipping source code or a set of binaries, the entire container becomes the artifact. Containers already includes all the required versions of its dependencies. Containers are more of a packaging and deployment technology . They don't impact the fundamental software we're writing. We can just instruct our tools to produce a Docker container at the end of the build.","title":"Why are containers important?"},{"location":"devops/cd/#what-is-docker","text":"Docker is a technology for automating the packaging and deployment of portable, self-sufficient containers. - Docker containers can be run anywhere a Docker host is found, whether on a development machine, a departmental server, an enterprise datacenter, or in the cloud. Azure provides multiple ways to run container-based applications, including App Service or as part of clusters managed with orchestration technologies like Kubernetes. Dependency versioning challenges for QA Applications are packaged as containers that bring the correct versions of their dependencies with them. Overhead due to solving app isolation with VMs Many isolated containers can be run on the same host with benefits over virtual machines including faster startup time to greater resource efficiency. Configuration inconsistencies between DevOps stages Containers ship with manifests that automate configuration requirements, such as which ports need to be exposed. Adopting Docker containers can be a key step on the path towards a microservices architecture.","title":"What is Docker?"},{"location":"devops/cd/#why-is-container-orchestration-important","text":"As development organizations scale, so do the complexities of the solutions they deliver. Over time, different products and services take increasing dependence on each other. This can result in different development and operations requirements for different components within a given application. Refactoring those components as their own distinct microservices can improve architectural efficiency. A microservice is a small, autonomous service designed to be self-contained and to fulfill a specific business capability. Microservices are typical apps, like our web app. The main difference is that instead of building and deploying a single monolithic app, we refactor any components that would be better maintained and managed as autonomous services. We then build those services to be good at what they do and deploy them to operate independently. Microservices are an architectural concept and not a deployment technology . Containers provide a great technical foundation for building and deploying these services, but this leads to a new question: how do we manage all of these containers floating around? This is where orchestration technologies like Kubernetes come in. Kubernetes is a platform for managing containerized workloads and services. It's a great option for organizations that have a growing number of containers that need to be deployed, integrated, and monitored in any environment. Kubernetes offers a way for us to deploy to different namespaces. This enables us to partition our deployments so that we can have entire environments dedicated to testing versus production. And since they all run in the same cluster and use the same containers, the testing experience should offer what we expect to see in production. Assuming we have our projects set up to build Docker containers, all we need to deploy to Kubernetes are some manifest files that describe the services and their deployments.","title":"Why is container orchestration important?"},{"location":"devops/cd/#what-is-kubernetes","text":"Kubernetes is a technology for orchestrating multi-container deployments. It provides a framework for running distributed systems in a declarative, responsive fashion . It automatically applies and enforces your deployment patterns to ensure that containers are deployed and run as intended. It also offers support for specialized release cadences, such as those using canary deployments . Complexity of multi-container deployments Kubernetes is designed, first and foremost, to automate the processes around deploying and maintaining container deployments. Consistency across environments and stages Just as containers ensure a consistent deployment for the apps they contain, Kubernetes ensures a consistent deployment for the containers a cluster manages. Azure DevOps support Azure DevOps offers first-class support for working with Kubernetes. Ease of development The impact of Kubernetes on a source project is comparable to that of adding Docker support, which is minimal and limited to declarative configuration.","title":"What is Kubernetes?"},{"location":"devops/cd/#benefits-of-continous-delivery-in-the-release-process","text":"The time it takes to get the build into the Test stage. The team achieved this improvement by using a scheduled trigger to deploy to Test every day at 3:00 AM. The time it takes to get the tested build into Staging . The team achieved this improvement by adding Selenium UI tests, a form of functional testing, to the Test stage. These automated tests are much faster than the manual versions. The time it takes to get the approved build from Staging to live . The team achieved this improvement by adding manual approval checks to the pipeline. When management signs off, Ops can release the changes from Staging to live.","title":"Benefits of Continous Delivery in the Release process"},{"location":"devops/ci/","text":"Introduction \u00b6 Continuous integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. A pipeline defines the continuous integration process for the app. It's made up of steps called tasks . It can be thought of as a script that defines how your build, test, and deployment steps are run. The pipeline runs when you submit code changes. You can configure the pipeline to run automatically, or you can run it manually. You connect your pipeline to a source repository like GitHub, Bitbucket, or Subversion. A build agent builds or deploys the code. An agent is installable software that runs one build or deployment job at a time. The final product of the pipeline is a build artifact . Think of an artifact as the smallest compiled unit that we need to test or deploy the app. For example, an artifact can be: A Java or .NET app packaged into a .jar or .zip file. A C++ or JavaScript library. A virtual machine, cloud, or Docker image. Pipeline as code refers to the concept of expressing your build definitions as code. - Build definition in YAML file to configure the build and release pipeline and stored directly with your app's source code. - When source code is checked in, pipeline is triggered and the build agent downloads the build defintion stored in the source code and then triggers the build and release tasks mentioned in the build defintion. - A continuous integration (CI) build is a build that runs when you push a change to a branch. - A pull request (PR) build is a build that runs when you open a pull request or when you push additional changes to an existing pull request. - A final CI build happens after the pull request is merged to main. The final CI build verifies that the changes are still good after the PR was merged. What is a build badge? - A badge is part of Microsoft Azure Pipelines. It has methods you can use to add an SVG image that shows the status of the build on your GitHub repository. - Most GitHub repositories include a file named README.md, which is a Markdown file that includes essential details and documentation about your project. GitHub renders this file on your project's home page. Choose a code flow strategy \u00b6 If your team is doing continuous delivery of software, I would suggest to adopt a much simpler workflow like Github flow. If, however, you are building software that is explicitly versioned , or if you need to support multiple versions of your software in the wild, then git-flow may still be as good of a fit to your team. Github-flow \u00b6 What does a branching workflow look like (a.k.a Github Flow )? Step 1 : When you begin to work on a new feature or bug fix, the first thing you want to do is make sure you're starting with the latest stable codebase. To do this, you can synchronize your local copy of the main branch with the server's copy. This pulls down all other developers' changes that have been pushed up to the main branch on the server since your last synchronization. Step 2 : To make sure you're working safely on your copy of the code, you create a new branch just for that feature or bug fix. Before you make changes to a file, you check out a new branch so that you know you're working on the files from that branch and not a different branch. You can switch branches anytime by checking out that branch. Step 3 : You're now safe to make whatever changes you want, because these changes are only in your branch. As you work, you can commit your changes to your branch to make sure you don't lose any work and to provide a way to roll back any changes you've made to previous versions. Before you can commit changes, you need to stage your files so that Git knows which ones you're ready to commit. Step 4 : The next step is to push , or upload, your local branch up to the remote repository (such as GitHub) so that others can see what you're working on. Don't worry, this won't merge your changes yet. You can push up your work as often as you'd like. In fact, that's a good way to back up your work or enable yourself to work from multiple computers. Step 5 : This step is a common one, but not required. When you're satisfied that your code is working as you want it to, you can pull , or merge, the remote main branch back into your local main branch. Changes have been taking place there that your local main branch doesn't have yet. After you've synchronized the remote main branch with yours, merge your local main branch into your working branch and test your build again. This process helps ensure that your feature works with the latest code. It also helps ensure that your work will integrate smoothly when you submit your pull request. Step 6 : You're finally ready to propose your changes to the remote main branch. To do this, you begin a pull request . When configured in Azure Pipelines or another CI/CD system, this step triggers the build process and you can watch your changes move through the pipeline. After the build succeeds and others approve your pull request, your code can be merged into the remote main branch. (It's still up to a human to merge the changes.) A remote is a Git repository where team members collaborate (like a repository on GitHub). git remote -v - You see that you have both fetch (download) and push (upload) access to your repository. Origin specifies your repository on GitHub. HEAD is the pointer to the current branch reference, which is in turn a pointer to the last commit made on that branch. That means HEAD will be the parent of the next commit that is created. It\u2019s generally simplest to think of HEAD as the snapshot of your last commit on that branch . The index is your proposed next commit . We\u2019ve also been referring to this concept as Git\u2019s \u201cStaging Area\u201d as this is what Git looks at when you run git commit . When you fork code from another repository, it's common to name the original remote (the one you forked from) as upstream. git remote add upstream <forked repo> . You also now have fetch access from the forked repository, which is beneficial when the forked repo changes . git fetch - The command goes out to that remote project and pulls down all the data from that remote project that you don\u2019t have yet. After you do this, you should have references to all the branches from that remote, which you can merge in or inspect at any time. So, git fetch origin fetches any new work that has been pushed to that server since you cloned (or last fetched from) it. It\u2019s important to note that the git fetch command only downloads the data to your local repository - it doesn\u2019t automatically merge it with any of your work or modify what you\u2019re currently working on. You have to merge it manually into your work when you\u2019re ready. git pull generally fetches data from the server you originally cloned from and automatically tries to merge it into the code you\u2019re currently working on. git pull is essentially a git fetch immediately followed by a git merge in most cases. It\u2019s better to simply use the fetch and merge commands explicitly as the magic of git pull can often be confusing. git push pushes code to a server to which you have write access and if nobody has pushed in the meantime. If you and someone else clone at the same time and they push upstream and then you push upstream, your push will rightly be rejected. You\u2019ll have to fetch their work first and incorporate it into yours before you\u2019ll be allowed to push. With the rebase command, you can take all the changes that were committed on one branch and replay them on a different branch. # This operation works by going to the common ancestor of the two branches (the one you\u2019re on and the one you\u2019re rebasing onto), getting the diff introduced by each commit of the branch you\u2019re on, saving those diffs to temporary files, resetting the current branch to the same commit as the branch you are rebasing onto, and finally applying each change in turn. git checkout server git rebase main # At this point, you can go back to the main branch and do a fast-forward merge. git checkout main git merge server There is no difference in the end product of the integration, but rebasing makes for a cleaner history. If you examine the log of a rebased branch, it looks like a linear history: it appears that all the work happened in series, even when it originally happened in parallel. You can rebase the server branch onto the main branch without having to check it out first by running git rebase <basebranch> <topicbranch> which checks out the topic branch (in this case, server ) for you and replays it onto the base branch ( main ): git rebase main server Then, you can fast-forward the base branch (main): git checkout main git merge server # You can remove the server branches because all the work is integrated and you don\u2019t need them anymore. git branch -d server Do not rebase commits that exist outside your repository and that people may have based work on. You can also simplify this by running a git pull --rebase instead of a normal git pull . If you are using git pull and want to make --rebase the default, you can set the pull.rebase config value with something like git config --global pull.rebase true . You can get the best of both worlds (Rebase vs. Merge ): rebase local changes before pushing to clean up your work, but never rebase anything that you\u2019ve pushed somewhere. Git Reset \u2192 3 Options Move HEAD \u2192 git reset --soft HEAD~ . When you reset back to HEAD~ (the parent of HEAD), you are moving the branch back to where it was, without changing the index or working directory. You could now update the index and run git commit again to accomplish what git commit --amend would have done Updating the Index \u2192 git reset --mixed HEAD~ . This is also the default, so if you specify no option at all (just git reset HEAD~ in this case). It undid your last commit, but also unstaged everything. You rolled back to before you ran all your git add and git commit commands. Updating the Working Directory (\u2013hard) \u2192 git reset --hard HEAD~ . It undid your last commit, the git add and git commit commands, and all the work you did in your working directory. It\u2019s important to note that this flag (\u2013hard) is the only way to make the reset command dangerous, and one of the very few cases where Git will actually destroy data. Any other invocation of reset can be pretty easily undone, but the --hard option cannot, since it forcibly overwrites files in the working directory. Reset With a Path \u2192 git reset file.txt . This has the practical effect of unstaging the file. Squashing. Say you have a series of commits with messages like \u201coops.\u201d, \u201cWIP\u201d and \u201cforgot this file\u201d. You can use reset to quickly and easily squash them into a single commit that makes you look really smart. Let\u2019s say you have a project where the first commit has one file, the second commit added a new file and changed the first, and the third commit changed the first file again. The second commit was a work in progress and you want to squash it down. You can run git reset --soft HEAD~2 to move the HEAD branch back to an older commit (the most recent commit you want to keep). And then simply run git commit again. Git-flow \u00b6 At the core, the development model is greatly inspired by existing models out there. The central repo holds two main branches with an infinite lifetime: main develop The main branch at origin should be familiar to every Git user. Parallel to the main branch, another branch exists called develop . We consider origin/main to be the main branch where the source code of HEAD always reflects a production-ready state . We consider origin/develop to be the main branch where the source code of HEAD always reflects a state with the latest delivered development changes for the next release. Some would call this the \u201cintegration branch\u201d. This is where any automatic nightly builds are built from. When the source code in the develop branch reaches a stable point and is ready to be released, all of the changes should be merged back into main somehow and then tagged with a release number. Therefore, each time when changes are merged back into main, this is a new production release by definition. Next to the main branches main and develop, our development model uses a variety of supporting branches to aid parallel development between team members, ease tracking of features, prepare for production releases and to assist in quickly fixing live production problems. Unlike the main branches, these branches always have a limited life time, since they will be removed eventually. The different types of branches we may use are: Feature branches Release branches Hotfix branches Feature branches May branch off from: develop . Must merge back into: develop . Branch naming convention: anything except main, develop, release- , or hotfix- . - When starting development of a feature, the target release in which this feature will be incorporated may well be unknown at that point. The essence of a feature branch is that it exists as long as the feature is in development, but will eventually be merged back into develop (to definitely add the new feature to the upcoming release) or discarded (in case of a disappointing experiment). - When starting work on a new feature, branch off from the develop branch. git checkout -b myfeature develop - Finished features may be merged into the develop branch to definitely add them to the upcoming release. git checkout develop # Switched to branch 'develop' git merge --no-ff myfeature # The --no-ff flag causes the merge to always create a new commit object, even if the merge could be performed with a fast-forward. This avoids losing information about the historical existence of a feature branch and groups together all commits that together added the feature. git branch -d myfeature #Deleted branch myfeature (was 05e9557). git push origin develop Release branches May branch off from: develop . Must merge back into: develop and main . Branch naming convention: release- . - Release branches support preparation of a new production release. Furthermore, they allow for minor bug fixes and preparing meta-data for a release (version number, build dates, etc.). By doing all of this work on a release branch, the develop branch is cleared to receive features for the next big release. - The * key moment** to branch off a new release branch from develop is when develop (almost) reflects the desired state of the new release. At least all features that are targeted for the release-to-be-built must be merged in to develop at this point in time. All features targeted at future releases may not\u2014they must wait until after the release branch is branched off. git checkout -b release-1.2 develop - This new branch may exist there for a while, until the release may be rolled out definitely. During that time, bug fixes may be applied in this branch (rather than on the develop branch). Adding large new features here is strictly prohibited. They must be merged into develop, and therefore, wait for the next big release. - When the state of the release branch is ready to become a real release, some actions need to be carried out. First, the release branch is merged into main .Next, that commit on main must be tagged for easy future reference to this historical version. Finally, the changes made on the release branch need to be merged back into develop , so that future releases also contain these bug fixes. git checkout main # Switched to branch 'main' git merge --no-ff release-1.2 # Merge made by recursive. git tag -a 1 .2 # The release is now done, and tagged for future reference. # To keep the changes made in the release branch, we need to merge those back into develop. git checkout develop # Switched to branch 'develop' git merge --no-ff release-1.2 # Delete release branch git branch -d release-1.2 Hotfix Branches May branch off from: main . Must merge back into: develop and main . Branch naming convention: hotfix-* - Hotfix branches are very much like release branches in that they are also meant to prepare for a new production release, albeit unplanned. - They arise from the necessity to act immediately upon an undesired state of a live production version. When a critical bug in a production version must be resolved immediately, a hotfix branch may be branched off from the corresponding tag on the master branch that marks the production version. git checkout -b hotfix-1.2.1 master - When finished, the bugfix needs to be merged back into master , but also needs to be merged back into develop , in order to safeguard that the bugfix is included in the next release as well. git checkout master # Switched to branch 'master' git merge --no-ff hotfix-1.2.1 # Merge made by recursive. git tag -a 1 .2.1 # Next, include the bugfix in develop git checkout develop # Switched to branch 'develop' git merge --no-ff hotfix-1.2.1 # Delete hot-fix branch git branch -d hotfix-1.2.1 - The one exception to the rule here is that, when a release branch currently exists, the hotfix changes need to be merged into that release branch, instead of develop . Back-merging the bugfix into the release branch will eventually result in the bugfix being merged into develop too, when the release branch is finished. - If work in develop immediately requires this bugfix and cannot wait for the release branch to be finished, you may safely merge the bugfix into develop now already as well. Pull Request Strategy \u00b6 Smaller PRs are easier to review, therefore more bugs and code flaws could be caught, which leads to a better quality of the code. Spilting PR Writing good and clear commit messages Automated Testing \u00b6 Automated testing uses software to execute your code and compare the actual results with the results you expect. Compare this with exploratory or manual testing , where a human typically follows instructions in a test plan to verify that software functions as expected. - Documentation and the ability to more easily refactor your code are two added benefits of automated testing. - Manual testing has its benefits. But as your code base grows in size, testing all features manually (including edge cases) can become repetitive, tedious, and error prone. Automated testing can help eliminate some of this burden and enable manual testers to focus on what they do best : ensuring that your users will have a positive experience with your software . - Focus most of your effort on writing tests that verify the foundational levels of your software, such as functions, classes, and methods. - You focus progressively less effort as features are combined, such as at the user interface (UI) layer. - The idea is that if you can verify that each lower-level component works as expected in isolation, tests at the higher levels need only verify that multiple components work together to get the expected result. - Unit tests are a great way to automatically test for regression bugs. Continuous testing means tests are run early in the development process and as every change moves through the pipeline. Shifting left means considering software quality and testing earlier in the development process. Shifting left often requires testers to get involved in the design process, even before any code for the feature is written. Automated tests can serve as a type of documentation as to how software should behave and why certain features exist. Automated test code often uses a human-readable format. The set of inputs you provide represent values your users might enter. Each associated output specifies the result your users should expect. Many developers follow the test-driven development , or TDD, method by writing their test code before implementing a new feature. The idea is to write a set of tests, often called specs, that initially fail. Then, the developer incrementally writes code to implement the feature until all tests pass. Not only do the specs document the requirements, but the TDD process helps ensure that only the necessary amount of code is written to implement the feature. When you have a set of passing tests, you're better able to experiment and refactor your code. When you make a change, all you need to do is run your tests and verify that they continue to pass. After you've met your refactoring goals, you can submit your change to the build pipeline so that everyone can benefit, but with a lower risk of something breaking. Types of Testing \u00b6 Development testing refers to tests you can run before you deploy the application to a test or production environment. lint testing , a form of static code analysis, checks your source code to determine whether it conforms to your team's style guide. Unit testing verifies the most fundamental components of your program or library, such as an individual function or method. You specify one or more inputs along with the expected results. The test runner performs each test and checks to see whether the actual and expected results match. Code coverage testing computes the percentage of your code that's covered by your unit tests. Code coverage testing can include conditional branches in your code to ensure that a function is completely covered. What makes a good test? \u00b6 Don't test for the sake of testing : Your tests should serve a purpose beyond being a checklist item to cross off. Write tests that verify that your critical code works as intended and doesn't break existing functionality. Keep your tests short : Tests should finish as quickly as possible, especially those that happen during the development and build phases. When tests are run as each change moves through the pipeline, you don't want them to be the bottleneck. Ensure that your tests are repeatable : Test runs should produce the same results each time, whether you run them on your computer, a coworker's computer, or in the build pipeline. Keep your tests focused : A common misconception is that tests are meant to cover code written by others. Ordinarily, your tests should cover only your code. For example, if you're using an open-source graphics library in your project, you don't need to test that library. Choose the right granularity : For example, if you're performing unit testing, an individual test shouldn't combine or test multiple functions or methods. Test each function separately and later write integration tests that verify that multiple components interact properly. Plan build dependencies for your pipeline \u00b6 What is a package? \u00b6 A package contains reusable code that other developers can use in their own projects, even though they didn't write it. - For compiled languages, a package typically contains the compiled binary code, such as .dll files in .NET, or .class files in Java. - For languages that are interpreted instead of compiled, such as JavaScript or Python, a package might include source code. - Either way, packages are typically compressed to ZIP or a similar format. Package systems will often define a unique file extension, such as .nupkg or .jar, to make the package's use clear. Compression can help reduce download time, and also produces a single file to make management simpler. - Packages also often contain one or more files that provide metadata, or information, about the package. This metadata might describe what the package does, specify its license terms, the author's contact information, and the package's version. Why should I build a package? \u00b6 One reason to create a package instead of duplicating code is to prevent drift . When code is duplicated, each copy can quickly diverge to satisfy the requirements of a particular app. It becomes difficult to migrate changes from one copy to the others. In other words, you lose the ability to improve the code in ways that benefit everyone. Packages also group related functionality into one reusable component. Depending on the programming language, a package can provide apps with access to certain types and functions, while restricting access to their implementation details. Another reason to build a package is to provide a consistent way to build and test that package's functionality. When code is duplicated, each app might build and test that code in different ways. One set of tests might include checks that another set could benefit from. One tradeoff is that with a package, you have another codebase to test and maintain. You must also be careful when adding features. How can I identify dependencies? \u00b6 If the goal is to reorganize your code into separate components, you need to identify those pieces of your app that can be removed, packaged to be reusable, stored in a central location, and versioned. You may even want to replace your own code with third-party components that are either open source or that you license. Here are some ways to identify dependencies: Duplicate code . If certain pieces of code appear in several places, that's a good indication that this code can be reused. Centralize these duplicate pieces of code and repackage them appropriately. High cohesion and low coupling . A second approach is to look for code elements that have a high cohesion to each other and low coupling with other parts of the code. In essence, high cohesion means keeping parts of a codebase that are related to each other in a single place. Low coupling, at the same time, is about separating unrelated parts of the code base as much as possible. Individual lifecycle . Look for parts of the code that have a similar lifecycle and can be deployed and released individually. If this code can be maintained by a separate team, it's a good indication that it can be packaged as a component outside of the solution. Stable parts . Some parts of your codebase might be stable and change infrequently. Check your code repository to find code with a low change frequency. Independent code and components . Whenever code and components are independent and unrelated to other parts of the system, they can potentially be isolated into separate dependencies. What kinds of packages are there? \u00b6 Each programming language or framework provides its own way to build packages. Popular package systems provide documentation about how the process works. NuGet : packages .NET libraries NPM : packages JavaScript libraries Maven : packages Java libraries Docker : packages software in isolated units called containers Where are packages hosted? \u00b6 You can host packages on your own network, or you can use a hosting service. A hosting service is often called a package repository or package registry. Many of these services provide free hosting for open source projects. A package feed refers to your package repository server. This server can be on the internet or behind your firewall on your network. When you host packages behind the firewall, you can include feeds to your own packages. You can also cache packages that you trust on your network when your systems can't connect to the internet. What elements make up a good dependency management strategy? \u00b6 A good dependency management strategy depends on these three elements: Standardization . Standardizing how you declare and resolve dependencies will help your automated release process remain repeatable and predictable. Packaging formats and sources . Each dependency should be packaged using the applicable format and stored in a central location. Versioning . You need to keep track of the changes that occur over time in dependencies just as you do with your own code. This means that dependencies should be versioned. How are packages versioned? \u00b6 Semantic Versioning is a popular versioning scheme. Here's the format: Major.Minor.Patch[-Suffix] A new Major version introduces breaking changes. Apps typically need to update how they use the package to work with a new major version. A new Minor version introduces new features, but is backward compatible with earlier versions. A new Patch introduces backward compatible bug fixes, but not new features. The -Suffix part is optional and identifies the package as a pre-release version. For example, 1.0.0-beta1 might identify the package as the first beta pre-release build for the 1.0.0 release. Include a versioning strategy in your build pipeline \u00b6 When you use a build pipeline, packages need versions before they can be consumed and tested. However, only after you've tested the package can you know its quality. Because package versions should never be changed, it becomes challenging to choose a certain version beforehand. A common use is to share package versions that have been tested, validated, or deployed but hold back packages still under development and not ready for public consumption. This approach works well with semantic versioning, which is useful for predicting the intent of a particular version. Essentially, allow a consumer to make a conscious decision to choose from released packages, or opt-in to prereleases of a certain quality level. Package security \u00b6 Ensuring the security of your packages is as important as ensuring the security of the rest of your code. One aspect of package security is securing access to the package feeds where a feed is where you store packages. - Setting permissions on the feed allows you to share your packages with as many or as few people as your scenario requires. Configure the pipeline to access security and license ratings There are several tools available from third parties to help you assess the security and license rating of the software packages you use. Some of these tools scan the packages as they are included in the build or CD pipeline. During the build process, the tool scans the packages and gives instantaneous feedback. During the CD process, the tool uses the build artifacts and performs scans. Two examples of such tools are WhiteSource Bolt and Black Duck . Benefits of Continous Integration in the Build process \u00b6 The time it takes to set up source control for new features. The team achieved this improvement by moving from centralized source control to Git, a form of distributed source control. By using distributed source control, they don't need to wait for files to be unlocked. The time it takes to deliver code to the tester. The team achieved this improvement by moving their build process to CI Pipelines. CI Pipelines automatically notifies the tester when a build is available. Developers no longer need to update spreadsheet to notify testers. The time it takes to test new features. The team achieved this improvement by unit-testing their code. They run unit tests each time a change moves through the build pipeline, so fewer bugs and regressions reach testers. The reduced workload means that testers can complete each manual test faster.","title":"CI"},{"location":"devops/ci/#introduction","text":"Continuous integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. A pipeline defines the continuous integration process for the app. It's made up of steps called tasks . It can be thought of as a script that defines how your build, test, and deployment steps are run. The pipeline runs when you submit code changes. You can configure the pipeline to run automatically, or you can run it manually. You connect your pipeline to a source repository like GitHub, Bitbucket, or Subversion. A build agent builds or deploys the code. An agent is installable software that runs one build or deployment job at a time. The final product of the pipeline is a build artifact . Think of an artifact as the smallest compiled unit that we need to test or deploy the app. For example, an artifact can be: A Java or .NET app packaged into a .jar or .zip file. A C++ or JavaScript library. A virtual machine, cloud, or Docker image. Pipeline as code refers to the concept of expressing your build definitions as code. - Build definition in YAML file to configure the build and release pipeline and stored directly with your app's source code. - When source code is checked in, pipeline is triggered and the build agent downloads the build defintion stored in the source code and then triggers the build and release tasks mentioned in the build defintion. - A continuous integration (CI) build is a build that runs when you push a change to a branch. - A pull request (PR) build is a build that runs when you open a pull request or when you push additional changes to an existing pull request. - A final CI build happens after the pull request is merged to main. The final CI build verifies that the changes are still good after the PR was merged. What is a build badge? - A badge is part of Microsoft Azure Pipelines. It has methods you can use to add an SVG image that shows the status of the build on your GitHub repository. - Most GitHub repositories include a file named README.md, which is a Markdown file that includes essential details and documentation about your project. GitHub renders this file on your project's home page.","title":"Introduction"},{"location":"devops/ci/#choose-a-code-flow-strategy","text":"If your team is doing continuous delivery of software, I would suggest to adopt a much simpler workflow like Github flow. If, however, you are building software that is explicitly versioned , or if you need to support multiple versions of your software in the wild, then git-flow may still be as good of a fit to your team.","title":"Choose a code flow strategy"},{"location":"devops/ci/#github-flow","text":"What does a branching workflow look like (a.k.a Github Flow )? Step 1 : When you begin to work on a new feature or bug fix, the first thing you want to do is make sure you're starting with the latest stable codebase. To do this, you can synchronize your local copy of the main branch with the server's copy. This pulls down all other developers' changes that have been pushed up to the main branch on the server since your last synchronization. Step 2 : To make sure you're working safely on your copy of the code, you create a new branch just for that feature or bug fix. Before you make changes to a file, you check out a new branch so that you know you're working on the files from that branch and not a different branch. You can switch branches anytime by checking out that branch. Step 3 : You're now safe to make whatever changes you want, because these changes are only in your branch. As you work, you can commit your changes to your branch to make sure you don't lose any work and to provide a way to roll back any changes you've made to previous versions. Before you can commit changes, you need to stage your files so that Git knows which ones you're ready to commit. Step 4 : The next step is to push , or upload, your local branch up to the remote repository (such as GitHub) so that others can see what you're working on. Don't worry, this won't merge your changes yet. You can push up your work as often as you'd like. In fact, that's a good way to back up your work or enable yourself to work from multiple computers. Step 5 : This step is a common one, but not required. When you're satisfied that your code is working as you want it to, you can pull , or merge, the remote main branch back into your local main branch. Changes have been taking place there that your local main branch doesn't have yet. After you've synchronized the remote main branch with yours, merge your local main branch into your working branch and test your build again. This process helps ensure that your feature works with the latest code. It also helps ensure that your work will integrate smoothly when you submit your pull request. Step 6 : You're finally ready to propose your changes to the remote main branch. To do this, you begin a pull request . When configured in Azure Pipelines or another CI/CD system, this step triggers the build process and you can watch your changes move through the pipeline. After the build succeeds and others approve your pull request, your code can be merged into the remote main branch. (It's still up to a human to merge the changes.) A remote is a Git repository where team members collaborate (like a repository on GitHub). git remote -v - You see that you have both fetch (download) and push (upload) access to your repository. Origin specifies your repository on GitHub. HEAD is the pointer to the current branch reference, which is in turn a pointer to the last commit made on that branch. That means HEAD will be the parent of the next commit that is created. It\u2019s generally simplest to think of HEAD as the snapshot of your last commit on that branch . The index is your proposed next commit . We\u2019ve also been referring to this concept as Git\u2019s \u201cStaging Area\u201d as this is what Git looks at when you run git commit . When you fork code from another repository, it's common to name the original remote (the one you forked from) as upstream. git remote add upstream <forked repo> . You also now have fetch access from the forked repository, which is beneficial when the forked repo changes . git fetch - The command goes out to that remote project and pulls down all the data from that remote project that you don\u2019t have yet. After you do this, you should have references to all the branches from that remote, which you can merge in or inspect at any time. So, git fetch origin fetches any new work that has been pushed to that server since you cloned (or last fetched from) it. It\u2019s important to note that the git fetch command only downloads the data to your local repository - it doesn\u2019t automatically merge it with any of your work or modify what you\u2019re currently working on. You have to merge it manually into your work when you\u2019re ready. git pull generally fetches data from the server you originally cloned from and automatically tries to merge it into the code you\u2019re currently working on. git pull is essentially a git fetch immediately followed by a git merge in most cases. It\u2019s better to simply use the fetch and merge commands explicitly as the magic of git pull can often be confusing. git push pushes code to a server to which you have write access and if nobody has pushed in the meantime. If you and someone else clone at the same time and they push upstream and then you push upstream, your push will rightly be rejected. You\u2019ll have to fetch their work first and incorporate it into yours before you\u2019ll be allowed to push. With the rebase command, you can take all the changes that were committed on one branch and replay them on a different branch. # This operation works by going to the common ancestor of the two branches (the one you\u2019re on and the one you\u2019re rebasing onto), getting the diff introduced by each commit of the branch you\u2019re on, saving those diffs to temporary files, resetting the current branch to the same commit as the branch you are rebasing onto, and finally applying each change in turn. git checkout server git rebase main # At this point, you can go back to the main branch and do a fast-forward merge. git checkout main git merge server There is no difference in the end product of the integration, but rebasing makes for a cleaner history. If you examine the log of a rebased branch, it looks like a linear history: it appears that all the work happened in series, even when it originally happened in parallel. You can rebase the server branch onto the main branch without having to check it out first by running git rebase <basebranch> <topicbranch> which checks out the topic branch (in this case, server ) for you and replays it onto the base branch ( main ): git rebase main server Then, you can fast-forward the base branch (main): git checkout main git merge server # You can remove the server branches because all the work is integrated and you don\u2019t need them anymore. git branch -d server Do not rebase commits that exist outside your repository and that people may have based work on. You can also simplify this by running a git pull --rebase instead of a normal git pull . If you are using git pull and want to make --rebase the default, you can set the pull.rebase config value with something like git config --global pull.rebase true . You can get the best of both worlds (Rebase vs. Merge ): rebase local changes before pushing to clean up your work, but never rebase anything that you\u2019ve pushed somewhere. Git Reset \u2192 3 Options Move HEAD \u2192 git reset --soft HEAD~ . When you reset back to HEAD~ (the parent of HEAD), you are moving the branch back to where it was, without changing the index or working directory. You could now update the index and run git commit again to accomplish what git commit --amend would have done Updating the Index \u2192 git reset --mixed HEAD~ . This is also the default, so if you specify no option at all (just git reset HEAD~ in this case). It undid your last commit, but also unstaged everything. You rolled back to before you ran all your git add and git commit commands. Updating the Working Directory (\u2013hard) \u2192 git reset --hard HEAD~ . It undid your last commit, the git add and git commit commands, and all the work you did in your working directory. It\u2019s important to note that this flag (\u2013hard) is the only way to make the reset command dangerous, and one of the very few cases where Git will actually destroy data. Any other invocation of reset can be pretty easily undone, but the --hard option cannot, since it forcibly overwrites files in the working directory. Reset With a Path \u2192 git reset file.txt . This has the practical effect of unstaging the file. Squashing. Say you have a series of commits with messages like \u201coops.\u201d, \u201cWIP\u201d and \u201cforgot this file\u201d. You can use reset to quickly and easily squash them into a single commit that makes you look really smart. Let\u2019s say you have a project where the first commit has one file, the second commit added a new file and changed the first, and the third commit changed the first file again. The second commit was a work in progress and you want to squash it down. You can run git reset --soft HEAD~2 to move the HEAD branch back to an older commit (the most recent commit you want to keep). And then simply run git commit again.","title":"Github-flow"},{"location":"devops/ci/#git-flow","text":"At the core, the development model is greatly inspired by existing models out there. The central repo holds two main branches with an infinite lifetime: main develop The main branch at origin should be familiar to every Git user. Parallel to the main branch, another branch exists called develop . We consider origin/main to be the main branch where the source code of HEAD always reflects a production-ready state . We consider origin/develop to be the main branch where the source code of HEAD always reflects a state with the latest delivered development changes for the next release. Some would call this the \u201cintegration branch\u201d. This is where any automatic nightly builds are built from. When the source code in the develop branch reaches a stable point and is ready to be released, all of the changes should be merged back into main somehow and then tagged with a release number. Therefore, each time when changes are merged back into main, this is a new production release by definition. Next to the main branches main and develop, our development model uses a variety of supporting branches to aid parallel development between team members, ease tracking of features, prepare for production releases and to assist in quickly fixing live production problems. Unlike the main branches, these branches always have a limited life time, since they will be removed eventually. The different types of branches we may use are: Feature branches Release branches Hotfix branches Feature branches May branch off from: develop . Must merge back into: develop . Branch naming convention: anything except main, develop, release- , or hotfix- . - When starting development of a feature, the target release in which this feature will be incorporated may well be unknown at that point. The essence of a feature branch is that it exists as long as the feature is in development, but will eventually be merged back into develop (to definitely add the new feature to the upcoming release) or discarded (in case of a disappointing experiment). - When starting work on a new feature, branch off from the develop branch. git checkout -b myfeature develop - Finished features may be merged into the develop branch to definitely add them to the upcoming release. git checkout develop # Switched to branch 'develop' git merge --no-ff myfeature # The --no-ff flag causes the merge to always create a new commit object, even if the merge could be performed with a fast-forward. This avoids losing information about the historical existence of a feature branch and groups together all commits that together added the feature. git branch -d myfeature #Deleted branch myfeature (was 05e9557). git push origin develop Release branches May branch off from: develop . Must merge back into: develop and main . Branch naming convention: release- . - Release branches support preparation of a new production release. Furthermore, they allow for minor bug fixes and preparing meta-data for a release (version number, build dates, etc.). By doing all of this work on a release branch, the develop branch is cleared to receive features for the next big release. - The * key moment** to branch off a new release branch from develop is when develop (almost) reflects the desired state of the new release. At least all features that are targeted for the release-to-be-built must be merged in to develop at this point in time. All features targeted at future releases may not\u2014they must wait until after the release branch is branched off. git checkout -b release-1.2 develop - This new branch may exist there for a while, until the release may be rolled out definitely. During that time, bug fixes may be applied in this branch (rather than on the develop branch). Adding large new features here is strictly prohibited. They must be merged into develop, and therefore, wait for the next big release. - When the state of the release branch is ready to become a real release, some actions need to be carried out. First, the release branch is merged into main .Next, that commit on main must be tagged for easy future reference to this historical version. Finally, the changes made on the release branch need to be merged back into develop , so that future releases also contain these bug fixes. git checkout main # Switched to branch 'main' git merge --no-ff release-1.2 # Merge made by recursive. git tag -a 1 .2 # The release is now done, and tagged for future reference. # To keep the changes made in the release branch, we need to merge those back into develop. git checkout develop # Switched to branch 'develop' git merge --no-ff release-1.2 # Delete release branch git branch -d release-1.2 Hotfix Branches May branch off from: main . Must merge back into: develop and main . Branch naming convention: hotfix-* - Hotfix branches are very much like release branches in that they are also meant to prepare for a new production release, albeit unplanned. - They arise from the necessity to act immediately upon an undesired state of a live production version. When a critical bug in a production version must be resolved immediately, a hotfix branch may be branched off from the corresponding tag on the master branch that marks the production version. git checkout -b hotfix-1.2.1 master - When finished, the bugfix needs to be merged back into master , but also needs to be merged back into develop , in order to safeguard that the bugfix is included in the next release as well. git checkout master # Switched to branch 'master' git merge --no-ff hotfix-1.2.1 # Merge made by recursive. git tag -a 1 .2.1 # Next, include the bugfix in develop git checkout develop # Switched to branch 'develop' git merge --no-ff hotfix-1.2.1 # Delete hot-fix branch git branch -d hotfix-1.2.1 - The one exception to the rule here is that, when a release branch currently exists, the hotfix changes need to be merged into that release branch, instead of develop . Back-merging the bugfix into the release branch will eventually result in the bugfix being merged into develop too, when the release branch is finished. - If work in develop immediately requires this bugfix and cannot wait for the release branch to be finished, you may safely merge the bugfix into develop now already as well.","title":"Git-flow"},{"location":"devops/ci/#pull-request-strategy","text":"Smaller PRs are easier to review, therefore more bugs and code flaws could be caught, which leads to a better quality of the code. Spilting PR Writing good and clear commit messages","title":"Pull Request Strategy"},{"location":"devops/ci/#automated-testing","text":"Automated testing uses software to execute your code and compare the actual results with the results you expect. Compare this with exploratory or manual testing , where a human typically follows instructions in a test plan to verify that software functions as expected. - Documentation and the ability to more easily refactor your code are two added benefits of automated testing. - Manual testing has its benefits. But as your code base grows in size, testing all features manually (including edge cases) can become repetitive, tedious, and error prone. Automated testing can help eliminate some of this burden and enable manual testers to focus on what they do best : ensuring that your users will have a positive experience with your software . - Focus most of your effort on writing tests that verify the foundational levels of your software, such as functions, classes, and methods. - You focus progressively less effort as features are combined, such as at the user interface (UI) layer. - The idea is that if you can verify that each lower-level component works as expected in isolation, tests at the higher levels need only verify that multiple components work together to get the expected result. - Unit tests are a great way to automatically test for regression bugs. Continuous testing means tests are run early in the development process and as every change moves through the pipeline. Shifting left means considering software quality and testing earlier in the development process. Shifting left often requires testers to get involved in the design process, even before any code for the feature is written. Automated tests can serve as a type of documentation as to how software should behave and why certain features exist. Automated test code often uses a human-readable format. The set of inputs you provide represent values your users might enter. Each associated output specifies the result your users should expect. Many developers follow the test-driven development , or TDD, method by writing their test code before implementing a new feature. The idea is to write a set of tests, often called specs, that initially fail. Then, the developer incrementally writes code to implement the feature until all tests pass. Not only do the specs document the requirements, but the TDD process helps ensure that only the necessary amount of code is written to implement the feature. When you have a set of passing tests, you're better able to experiment and refactor your code. When you make a change, all you need to do is run your tests and verify that they continue to pass. After you've met your refactoring goals, you can submit your change to the build pipeline so that everyone can benefit, but with a lower risk of something breaking.","title":"Automated Testing"},{"location":"devops/ci/#types-of-testing","text":"Development testing refers to tests you can run before you deploy the application to a test or production environment. lint testing , a form of static code analysis, checks your source code to determine whether it conforms to your team's style guide. Unit testing verifies the most fundamental components of your program or library, such as an individual function or method. You specify one or more inputs along with the expected results. The test runner performs each test and checks to see whether the actual and expected results match. Code coverage testing computes the percentage of your code that's covered by your unit tests. Code coverage testing can include conditional branches in your code to ensure that a function is completely covered.","title":"Types of Testing"},{"location":"devops/ci/#what-makes-a-good-test","text":"Don't test for the sake of testing : Your tests should serve a purpose beyond being a checklist item to cross off. Write tests that verify that your critical code works as intended and doesn't break existing functionality. Keep your tests short : Tests should finish as quickly as possible, especially those that happen during the development and build phases. When tests are run as each change moves through the pipeline, you don't want them to be the bottleneck. Ensure that your tests are repeatable : Test runs should produce the same results each time, whether you run them on your computer, a coworker's computer, or in the build pipeline. Keep your tests focused : A common misconception is that tests are meant to cover code written by others. Ordinarily, your tests should cover only your code. For example, if you're using an open-source graphics library in your project, you don't need to test that library. Choose the right granularity : For example, if you're performing unit testing, an individual test shouldn't combine or test multiple functions or methods. Test each function separately and later write integration tests that verify that multiple components interact properly.","title":"What makes a good test?"},{"location":"devops/ci/#plan-build-dependencies-for-your-pipeline","text":"","title":"Plan build dependencies for your pipeline"},{"location":"devops/ci/#what-is-a-package","text":"A package contains reusable code that other developers can use in their own projects, even though they didn't write it. - For compiled languages, a package typically contains the compiled binary code, such as .dll files in .NET, or .class files in Java. - For languages that are interpreted instead of compiled, such as JavaScript or Python, a package might include source code. - Either way, packages are typically compressed to ZIP or a similar format. Package systems will often define a unique file extension, such as .nupkg or .jar, to make the package's use clear. Compression can help reduce download time, and also produces a single file to make management simpler. - Packages also often contain one or more files that provide metadata, or information, about the package. This metadata might describe what the package does, specify its license terms, the author's contact information, and the package's version.","title":"What is a package?"},{"location":"devops/ci/#why-should-i-build-a-package","text":"One reason to create a package instead of duplicating code is to prevent drift . When code is duplicated, each copy can quickly diverge to satisfy the requirements of a particular app. It becomes difficult to migrate changes from one copy to the others. In other words, you lose the ability to improve the code in ways that benefit everyone. Packages also group related functionality into one reusable component. Depending on the programming language, a package can provide apps with access to certain types and functions, while restricting access to their implementation details. Another reason to build a package is to provide a consistent way to build and test that package's functionality. When code is duplicated, each app might build and test that code in different ways. One set of tests might include checks that another set could benefit from. One tradeoff is that with a package, you have another codebase to test and maintain. You must also be careful when adding features.","title":"Why should I build a package?"},{"location":"devops/ci/#how-can-i-identify-dependencies","text":"If the goal is to reorganize your code into separate components, you need to identify those pieces of your app that can be removed, packaged to be reusable, stored in a central location, and versioned. You may even want to replace your own code with third-party components that are either open source or that you license. Here are some ways to identify dependencies: Duplicate code . If certain pieces of code appear in several places, that's a good indication that this code can be reused. Centralize these duplicate pieces of code and repackage them appropriately. High cohesion and low coupling . A second approach is to look for code elements that have a high cohesion to each other and low coupling with other parts of the code. In essence, high cohesion means keeping parts of a codebase that are related to each other in a single place. Low coupling, at the same time, is about separating unrelated parts of the code base as much as possible. Individual lifecycle . Look for parts of the code that have a similar lifecycle and can be deployed and released individually. If this code can be maintained by a separate team, it's a good indication that it can be packaged as a component outside of the solution. Stable parts . Some parts of your codebase might be stable and change infrequently. Check your code repository to find code with a low change frequency. Independent code and components . Whenever code and components are independent and unrelated to other parts of the system, they can potentially be isolated into separate dependencies.","title":"How can I identify dependencies?"},{"location":"devops/ci/#what-kinds-of-packages-are-there","text":"Each programming language or framework provides its own way to build packages. Popular package systems provide documentation about how the process works. NuGet : packages .NET libraries NPM : packages JavaScript libraries Maven : packages Java libraries Docker : packages software in isolated units called containers","title":"What kinds of packages are there?"},{"location":"devops/ci/#where-are-packages-hosted","text":"You can host packages on your own network, or you can use a hosting service. A hosting service is often called a package repository or package registry. Many of these services provide free hosting for open source projects. A package feed refers to your package repository server. This server can be on the internet or behind your firewall on your network. When you host packages behind the firewall, you can include feeds to your own packages. You can also cache packages that you trust on your network when your systems can't connect to the internet.","title":"Where are packages hosted?"},{"location":"devops/ci/#what-elements-make-up-a-good-dependency-management-strategy","text":"A good dependency management strategy depends on these three elements: Standardization . Standardizing how you declare and resolve dependencies will help your automated release process remain repeatable and predictable. Packaging formats and sources . Each dependency should be packaged using the applicable format and stored in a central location. Versioning . You need to keep track of the changes that occur over time in dependencies just as you do with your own code. This means that dependencies should be versioned.","title":"What elements make up a good dependency management strategy?"},{"location":"devops/ci/#how-are-packages-versioned","text":"Semantic Versioning is a popular versioning scheme. Here's the format: Major.Minor.Patch[-Suffix] A new Major version introduces breaking changes. Apps typically need to update how they use the package to work with a new major version. A new Minor version introduces new features, but is backward compatible with earlier versions. A new Patch introduces backward compatible bug fixes, but not new features. The -Suffix part is optional and identifies the package as a pre-release version. For example, 1.0.0-beta1 might identify the package as the first beta pre-release build for the 1.0.0 release.","title":"How are packages versioned?"},{"location":"devops/ci/#include-a-versioning-strategy-in-your-build-pipeline","text":"When you use a build pipeline, packages need versions before they can be consumed and tested. However, only after you've tested the package can you know its quality. Because package versions should never be changed, it becomes challenging to choose a certain version beforehand. A common use is to share package versions that have been tested, validated, or deployed but hold back packages still under development and not ready for public consumption. This approach works well with semantic versioning, which is useful for predicting the intent of a particular version. Essentially, allow a consumer to make a conscious decision to choose from released packages, or opt-in to prereleases of a certain quality level.","title":"Include a versioning strategy in your build pipeline"},{"location":"devops/ci/#package-security","text":"Ensuring the security of your packages is as important as ensuring the security of the rest of your code. One aspect of package security is securing access to the package feeds where a feed is where you store packages. - Setting permissions on the feed allows you to share your packages with as many or as few people as your scenario requires. Configure the pipeline to access security and license ratings There are several tools available from third parties to help you assess the security and license rating of the software packages you use. Some of these tools scan the packages as they are included in the build or CD pipeline. During the build process, the tool scans the packages and gives instantaneous feedback. During the CD process, the tool uses the build artifacts and performs scans. Two examples of such tools are WhiteSource Bolt and Black Duck .","title":"Package security"},{"location":"devops/ci/#benefits-of-continous-integration-in-the-build-process","text":"The time it takes to set up source control for new features. The team achieved this improvement by moving from centralized source control to Git, a form of distributed source control. By using distributed source control, they don't need to wait for files to be unlocked. The time it takes to deliver code to the tester. The team achieved this improvement by moving their build process to CI Pipelines. CI Pipelines automatically notifies the tester when a build is available. Developers no longer need to update spreadsheet to notify testers. The time it takes to test new features. The team achieved this improvement by unit-testing their code. They run unit tests each time a change moves through the build pipeline, so fewer bugs and regressions reach testers. The reduced workload means that testers can complete each manual test faster.","title":"Benefits of Continous Integration in the Build process"},{"location":"devops/devops-engineer/","text":"Developer Skills Roadmap Devops Skills Roadmap Devops Skillset Search for Autopilot and Technology to find for automation scripts or patterns A DevOps Engineer \u00b6 A DevOps Engineer works on automating the various processes and operations. Some of the responsibilities include 1) automating software code build, testing, and deployment 2) handle IT infrastructure, automate provisioning, upgrades, manage environments 3) monitor application and system performances. - It's not really a job role, but more of a mindset of how dev's and sysadmin's come together to solve problems. DevOps to me is like saying \"We Do Agile\". It speaks more to how you approach work and the team then your actual job tasking. Most DevOps job descriptions really just mean \"We need you to take the dev team's code, and create/manage an automated path of tools that test the code, build the runtime environments (servers and containers) and get the code onto them in a reliable and consistent way.\" So here's a longer job description : \"We need someone who understands code and dev tools, but can also get that code setup in CI/CD, knows git, and also knows how to manage and troubleshoot servers. They know AWS/Azure and how to start from zero and build out a whole stack of services, and then back it up, monitor it for health and performance, set up logging and alerting, and maybe knows some security like TLS, SSH, certificates and key generation, IP/Sec, VPN's, Firewall basics.\" - You must be a self-starter that is good at break/fix, problem analysis, and \"systems thinking\". - The three core qualities I think that set the best of any IT role so far in front of others are: 1. Empathy, the ability to hear what others are saying and put yourself in their shoes to better understand the problem or desired outcome. 1. The drive to help others, to even put them first. 1. A deep curiosity for understanding how things work. Code, servers, or networks. Always be learning. Journey \u00b6 Take Linux courses. Take AWS courses. Learn a ton about networking, the OSI Layers, how TCP packets are made up, how firewalls and NAT really work. Learn common sysadmin CLI tools like SSH, Bash, package managers, and how drivers and system services are configured. Force yourself to use network storage (NFS, iSCSI), load balancers, and do backups and restores of databases. Automate everything you can. Pick a system automation tool like Ansible or SaltStack or Puppet and start using it to control servers rather than manual SSH commands. Pick a monitoring and logging tool and get confident in them. Use them for even your smallest personal projects. Learn the basics of these things, then over time, go deeper in the areas you gravitate to. GitHub and Git Flow. Learn AWS basics. Not just the how, but also the why and when to use each tool. Skip 75% of their products and focus on the core tools everyone uses: 1. EC2, VPC's, Security Groups, Elastic IP's, ELB's, Route 53 2. Storage . EBS, EFS, S3 3. Lambda, CloudWatch, CodeDeploy 4. CloudFormation \u2190 key to AWS infrastructure automation and \"infrastructure as code\" but you need to know the services above first and why they exist before automating their creation. Learn TCP/IP networking, NAT firewalls, and the 7 OSI layer basics. Learn Jenkins and the CI/CD workflow. Learn Docker, and then how to use it in Jenkins to build, test, and deploy containers to servers. Learn K8s for creating a container cluster to deploy containers on many servers as easy as one. Learn Linux. Take an admin course online. Pick one distribution to know better than the others. Learn Ansible for automating sysadmin tasks across many servers as easy as one. Learn Terraform for creating servers in any cloud via \"infrastructure as code\". Learn Nginx or HAProxy for HTTP Reverse Proxy. cAdvisor, Prometheus, and Grafana for Monitoring. Use something like \"swarmprom\" to learn how the above tools work together to give you graphs and alerting of your apps and cluster. REX-Ray, used shared network storage to store your persistent data (databases). Common points to consider for application deployment Does the ecosystem for that language prefer file-based configs or can it also do envvar based (the best way for containers)? How should it be built for containers? Inside a language, there can be config standards that are specific to a toolkit, so there might be multiple ways depending on what the dev team is doing. What are the sysadmin-concerns about that language? Does it have memory management settings that usually need to be changed (Java)? Is it single threaded so that you'll need to manage solutions for multi-core servers (Node.js)? Does it have common caching or temp dir settings and permission issues (PHP)? Does it require special ways of installing on a server that are different then developers are used to locally (lots of them, but Ruby can be the worst)? Often you'll get a code repo from a developer that you need to change the way it deploys for testing and production, which means you need to understand how to install the language build/runtime environment, envvars, and dependencies. Usually, doing this on a server is different than how the developer did it on their computer. Devops Daily Routine \u00b6 Create a very basic hello world app in a language of their choice. Use git to commit that code to a remote git repository. Understand modern CVS workflows like pull-requests, bug tracking systems, code commits, code merges, etc. Create documentation a simple way for other devs to download that repo and get started coding on it as well. (working in a team and having empathy for others is important, remember) Use their choice of CI to test that app in basic ways. CI should run on every code commit. Provision cloud servers, storage, networking, firewalls, and load balancers for the app. Use CD to automate the deployment of code after successful CI to the servers. Do this for a test and prod set of servers. Store the configs of those two environments in a way that's easily manageable and has change tracking. Deploy basic backups, monitoring, alerting, and log collection. Leave the fancy stuff for Ops-focused people. Do all of this \"infrastructure as code\" style where they use configuration files to store the settings of all these systems. If they can do that all with the tools of their choice, then it shows they understand the full lifecycle of applications and how all the parts fit together. Understanding both the Dev and Ops roles in these basic ways helps you fully support both teams and bridge the gap.","title":"Skills"},{"location":"devops/devops-engineer/#a-devops-engineer","text":"A DevOps Engineer works on automating the various processes and operations. Some of the responsibilities include 1) automating software code build, testing, and deployment 2) handle IT infrastructure, automate provisioning, upgrades, manage environments 3) monitor application and system performances. - It's not really a job role, but more of a mindset of how dev's and sysadmin's come together to solve problems. DevOps to me is like saying \"We Do Agile\". It speaks more to how you approach work and the team then your actual job tasking. Most DevOps job descriptions really just mean \"We need you to take the dev team's code, and create/manage an automated path of tools that test the code, build the runtime environments (servers and containers) and get the code onto them in a reliable and consistent way.\" So here's a longer job description : \"We need someone who understands code and dev tools, but can also get that code setup in CI/CD, knows git, and also knows how to manage and troubleshoot servers. They know AWS/Azure and how to start from zero and build out a whole stack of services, and then back it up, monitor it for health and performance, set up logging and alerting, and maybe knows some security like TLS, SSH, certificates and key generation, IP/Sec, VPN's, Firewall basics.\" - You must be a self-starter that is good at break/fix, problem analysis, and \"systems thinking\". - The three core qualities I think that set the best of any IT role so far in front of others are: 1. Empathy, the ability to hear what others are saying and put yourself in their shoes to better understand the problem or desired outcome. 1. The drive to help others, to even put them first. 1. A deep curiosity for understanding how things work. Code, servers, or networks. Always be learning.","title":"A DevOps Engineer"},{"location":"devops/devops-engineer/#journey","text":"Take Linux courses. Take AWS courses. Learn a ton about networking, the OSI Layers, how TCP packets are made up, how firewalls and NAT really work. Learn common sysadmin CLI tools like SSH, Bash, package managers, and how drivers and system services are configured. Force yourself to use network storage (NFS, iSCSI), load balancers, and do backups and restores of databases. Automate everything you can. Pick a system automation tool like Ansible or SaltStack or Puppet and start using it to control servers rather than manual SSH commands. Pick a monitoring and logging tool and get confident in them. Use them for even your smallest personal projects. Learn the basics of these things, then over time, go deeper in the areas you gravitate to. GitHub and Git Flow. Learn AWS basics. Not just the how, but also the why and when to use each tool. Skip 75% of their products and focus on the core tools everyone uses: 1. EC2, VPC's, Security Groups, Elastic IP's, ELB's, Route 53 2. Storage . EBS, EFS, S3 3. Lambda, CloudWatch, CodeDeploy 4. CloudFormation \u2190 key to AWS infrastructure automation and \"infrastructure as code\" but you need to know the services above first and why they exist before automating their creation. Learn TCP/IP networking, NAT firewalls, and the 7 OSI layer basics. Learn Jenkins and the CI/CD workflow. Learn Docker, and then how to use it in Jenkins to build, test, and deploy containers to servers. Learn K8s for creating a container cluster to deploy containers on many servers as easy as one. Learn Linux. Take an admin course online. Pick one distribution to know better than the others. Learn Ansible for automating sysadmin tasks across many servers as easy as one. Learn Terraform for creating servers in any cloud via \"infrastructure as code\". Learn Nginx or HAProxy for HTTP Reverse Proxy. cAdvisor, Prometheus, and Grafana for Monitoring. Use something like \"swarmprom\" to learn how the above tools work together to give you graphs and alerting of your apps and cluster. REX-Ray, used shared network storage to store your persistent data (databases). Common points to consider for application deployment Does the ecosystem for that language prefer file-based configs or can it also do envvar based (the best way for containers)? How should it be built for containers? Inside a language, there can be config standards that are specific to a toolkit, so there might be multiple ways depending on what the dev team is doing. What are the sysadmin-concerns about that language? Does it have memory management settings that usually need to be changed (Java)? Is it single threaded so that you'll need to manage solutions for multi-core servers (Node.js)? Does it have common caching or temp dir settings and permission issues (PHP)? Does it require special ways of installing on a server that are different then developers are used to locally (lots of them, but Ruby can be the worst)? Often you'll get a code repo from a developer that you need to change the way it deploys for testing and production, which means you need to understand how to install the language build/runtime environment, envvars, and dependencies. Usually, doing this on a server is different than how the developer did it on their computer.","title":"Journey"},{"location":"devops/devops-engineer/#devops-daily-routine","text":"Create a very basic hello world app in a language of their choice. Use git to commit that code to a remote git repository. Understand modern CVS workflows like pull-requests, bug tracking systems, code commits, code merges, etc. Create documentation a simple way for other devs to download that repo and get started coding on it as well. (working in a team and having empathy for others is important, remember) Use their choice of CI to test that app in basic ways. CI should run on every code commit. Provision cloud servers, storage, networking, firewalls, and load balancers for the app. Use CD to automate the deployment of code after successful CI to the servers. Do this for a test and prod set of servers. Store the configs of those two environments in a way that's easily manageable and has change tracking. Deploy basic backups, monitoring, alerting, and log collection. Leave the fancy stuff for Ops-focused people. Do all of this \"infrastructure as code\" style where they use configuration files to store the settings of all these systems. If they can do that all with the tools of their choice, then it shows they understand the full lifecycle of applications and how all the parts fit together. Understanding both the Dev and Ops roles in these basic ways helps you fully support both teams and bridge the gap.","title":"Devops Daily Routine"},{"location":"devops/gitops/","text":"Introduction \u00b6 GitOps is the next phase of infrastructure management. In conjunction with DevOps and Kubernetes, your business can achieve a higher lever of stability, efficiency, and reliability in the software development lifecycle. GitOps ensures that the software and deployment lifecycle is more predictable and repeatable , which makes your business more profitable. What is GitOps? \u00b6 Operations that are driven thorugh Git is called GitOps . - GitOps is a paradigm that empowers developers to undertake tasks that might otherwise be handled by operations. - Operations are the processes and services that are overseen by a company\u2019s IT department. - This may include technology and infrastructure management (including software), quality assurance, network administration, and device management. - Traditionally, developers don\u2019t function under the operations umbrella. - This can place development and operations in their own silos. - GitOps aims to remove those silos, and enable operations to employ the same tools and methodologies that developers use for efficient collaboration. - GitOps, as its name implies, relies on Git as the only source of truth; even for code related to IT operations. - GitOps is possible due to Infrastructure as Code (IaC) tools that allow you to create and manage your infrastructure using declarative configuration files. The three basic components of GitOps are the following: Infrastructure as Code (IaC) , a methodology that stores all infrastructure configuration as code. Merge Requests (MRs) to serve as a change mechanism for infrastructure updates. Continuous Integration/Continuous Delivery (CI/CD) that automates building, testing, and deploying applications, and services. Gitops Principles \u00b6 1. Describe the state of the system declaratively. 2. Store the system's desired state in git. 3. Change the system desired state using git commit and automation. No manual changes are present in Gitops. 4. An Operator within the system ensures an drift is automatically corrected. GitOps vs. DevOps \u00b6 GitOps borrows best practices from DevOps and applies them to infrastructure automation. This includes version control, collaboration, compliance, and CI/CD. Tools like Kubernetes have helped automate the software development lifecycle. Because so many businesses use container deployment to scale applications and services, they often depend upon third-party, cloud-based services to host their infrastructure. This has led to the rise of infrastructure automation to achieve a level of elasticity not possible with traditional infrastructure. DevOps assists in the automation of the software development lifecycle, while GitOps contributes to the automation of infrastructure. There are a few key differences between GitOps and DevOps. First, GitOps uses Git to manage infrastructure provisioning and software deployment. DevOps, on the other hand, focuses primarily on CI/CD and does not focus on any one tool. The primary focus of GitOps is to ensure that DevOps is done correctly, whereas DevOps focuses less on correctness. GitOps is also less flexible than DevOps. It is also much easier to adopt GitOps in a business that already employs DevOps. GitOps and Kubernetes \u00b6 GitOps focuses on automating infrastructure, so it\u2019s a perfect workflow for businesses that employ Kubernetes. When you employ GitOps and Kubernetes: GitOps ensures everything operates as it was intended. Kubernetes ensures stability and availability. Kubernetes always makes sure a deployed application or service remains in a stable state and scales as needed . When GitOps is along for the ride, it ensures everything runs as it should, including the infrastructure necessary for the deployments. GitOps serves as the glue between application build/delivery (Kubernetes) and where the application is to run. Traditional vs GitOps \u00b6 Common pattern used by organizations is the CIOps . In this pattern, a change is made to the application in the source repository. This triggers the build system. The CI pipeline builds the software and executes tests against it and packages it into a container image and is stored in a container registry. CI pipeline then triggeres the CD pipeline which has permission to make calls against the K8s API. This the point where the security concern is raised. Once the cluster state is updated in ETCD, the cluster begins to materialize the desired state by pulling the built image from the container registry and deploying it against the nodes. If the application change is not correct, it is difficult to recover from a bad deployment as the state is updated and the cluster has become unstable. Disadvantages of CIOps There is little separation between CI and CD system as it could be the same tool. Its like forcing the CI tool to do continuous delivery which it was not supposed to do. CI tool needs priviledges to make K8s API calls to perform deployments. It violates the trust boundary and can allow an attacker to use a compromised pipeline to execute a malicious code. Its not stable as success or failure of deployments is unknown. This is true in case a deployment fails and leaves the cluster in an unstable state. Not a feasible option for multiple clusters or a single large cluster. Gitops pattern First store the desired state declaratively in git. Pull the desired state into the system using an operator. Use a control loop to keep all of this in sync. It keeps the CI pipeline intact from CIOps except for removing the trigger for the CD pipeline. Under Gitops, the sole responsiblity of CI is to produce an artifact, in this case its a container image that is placed within an artifact repository. The Operator takes over from CI. It monitors the desired state in git for any changes which are made to a separate Environment repo which are made using pull requests . Once operator detects a change to the K8s manifest in the repo and provides this to the K8s control plane using the API to update the new state of the cluster. This causes the cluster to pull the new image and deploys it across the nodes. Gitops Usecases \u00b6 1. Infrastructure Operations : Gitops expects resources can be rapidly deployed using Cloud services. Teams use tools like Trraform to declare the infrastructure in manifest to create K8s cluster in a particular region. 2. Application Operations : Flux is used to ensure applications are deployed in an automated fashion to a K8s cluster 3. Release Operations : GitOps practises can be applied to declaratively define complex release strategies like Canary or Blue green deployments. Tools like Flagger fall into this space. The GitOps Workflow \u00b6 The traditional application lifecycle resembles the following: Design --> Build --> Image --> Test --> Deploy When you add GitOps into the mix, that lifecycle looks as follows: Design --> Build --> Image --> Test --> Monitor --> Log changes/events --> Alert when a change has occurred --> Update With a Kubernetes workflow as your source of truth all necessary code is stored in a Git repository with the help of automation. Anyone with Kubernetes management rights can create pull requests, edit code, and issue merge requests to the repository. Once a merge request is complete, the automated GitOps operator detects the changes, another automator declares if the change is operational, and the change is automatically deployed to the cluster. Within the GitOps workflow you not only have a high level of automation, but there\u2019s also a much higher probability that every deployment works exactly as expected. Using Flux as the gitops operator within Kubernetes. Using Flux and Flagger together to bring in Progressive or Blue-Green Deployment strategies. An alternate workflow created by Gitlab Advantages of GitOps CI and CD are decoupled into separate concerns. CI pipeline is left intact. However the CD pipeline is shifted inside the cluster where an Operator is responsible for automating deployment of changes to ensure it matches the desired state found in git. This eliminates the potential security concerns associated with the traditional push model from CIOps. It also provides recoverabilty as the cluster can be quickly rebuilt from source code. GitOps Tools \u00b6 There are several tools useful to GitOps, some of those tools include: Git - a version control system. GitHub - a code repository for housing your code. Cloud Build - a service that executes the build step of your deployment lifecycle using pre-packaged docker containers that include all of the appropriate tooling. CircleCI - a SaaS-style build engine that simplifies the build steps and can serve as a CI/CD engine. Kubernetes - a container orchestration platform that can be seamlessly integrated with GitOps. Helm - a robust tool for configuring Kubernetes resources. Flux - the GitOps operator for Kubernetes which automatically adjusts the Kubernetes cluster configuration based on the configurations found in your Git repo. Flagger - automates the detection of errors in code and prevents those errors from being deployed. Prometheus - a powerful GitOps monitoring tool that can generate alerts that are detected by Flagger. Quay - an application registry and container image manager.","title":"GitOps"},{"location":"devops/gitops/#introduction","text":"GitOps is the next phase of infrastructure management. In conjunction with DevOps and Kubernetes, your business can achieve a higher lever of stability, efficiency, and reliability in the software development lifecycle. GitOps ensures that the software and deployment lifecycle is more predictable and repeatable , which makes your business more profitable.","title":"Introduction"},{"location":"devops/gitops/#what-is-gitops","text":"Operations that are driven thorugh Git is called GitOps . - GitOps is a paradigm that empowers developers to undertake tasks that might otherwise be handled by operations. - Operations are the processes and services that are overseen by a company\u2019s IT department. - This may include technology and infrastructure management (including software), quality assurance, network administration, and device management. - Traditionally, developers don\u2019t function under the operations umbrella. - This can place development and operations in their own silos. - GitOps aims to remove those silos, and enable operations to employ the same tools and methodologies that developers use for efficient collaboration. - GitOps, as its name implies, relies on Git as the only source of truth; even for code related to IT operations. - GitOps is possible due to Infrastructure as Code (IaC) tools that allow you to create and manage your infrastructure using declarative configuration files. The three basic components of GitOps are the following: Infrastructure as Code (IaC) , a methodology that stores all infrastructure configuration as code. Merge Requests (MRs) to serve as a change mechanism for infrastructure updates. Continuous Integration/Continuous Delivery (CI/CD) that automates building, testing, and deploying applications, and services.","title":"What is GitOps?"},{"location":"devops/gitops/#gitops-principles","text":"1. Describe the state of the system declaratively. 2. Store the system's desired state in git. 3. Change the system desired state using git commit and automation. No manual changes are present in Gitops. 4. An Operator within the system ensures an drift is automatically corrected.","title":"Gitops Principles"},{"location":"devops/gitops/#gitops-vs-devops","text":"GitOps borrows best practices from DevOps and applies them to infrastructure automation. This includes version control, collaboration, compliance, and CI/CD. Tools like Kubernetes have helped automate the software development lifecycle. Because so many businesses use container deployment to scale applications and services, they often depend upon third-party, cloud-based services to host their infrastructure. This has led to the rise of infrastructure automation to achieve a level of elasticity not possible with traditional infrastructure. DevOps assists in the automation of the software development lifecycle, while GitOps contributes to the automation of infrastructure. There are a few key differences between GitOps and DevOps. First, GitOps uses Git to manage infrastructure provisioning and software deployment. DevOps, on the other hand, focuses primarily on CI/CD and does not focus on any one tool. The primary focus of GitOps is to ensure that DevOps is done correctly, whereas DevOps focuses less on correctness. GitOps is also less flexible than DevOps. It is also much easier to adopt GitOps in a business that already employs DevOps.","title":"GitOps vs. DevOps"},{"location":"devops/gitops/#gitops-and-kubernetes","text":"GitOps focuses on automating infrastructure, so it\u2019s a perfect workflow for businesses that employ Kubernetes. When you employ GitOps and Kubernetes: GitOps ensures everything operates as it was intended. Kubernetes ensures stability and availability. Kubernetes always makes sure a deployed application or service remains in a stable state and scales as needed . When GitOps is along for the ride, it ensures everything runs as it should, including the infrastructure necessary for the deployments. GitOps serves as the glue between application build/delivery (Kubernetes) and where the application is to run.","title":"GitOps and Kubernetes"},{"location":"devops/gitops/#traditional-vs-gitops","text":"Common pattern used by organizations is the CIOps . In this pattern, a change is made to the application in the source repository. This triggers the build system. The CI pipeline builds the software and executes tests against it and packages it into a container image and is stored in a container registry. CI pipeline then triggeres the CD pipeline which has permission to make calls against the K8s API. This the point where the security concern is raised. Once the cluster state is updated in ETCD, the cluster begins to materialize the desired state by pulling the built image from the container registry and deploying it against the nodes. If the application change is not correct, it is difficult to recover from a bad deployment as the state is updated and the cluster has become unstable. Disadvantages of CIOps There is little separation between CI and CD system as it could be the same tool. Its like forcing the CI tool to do continuous delivery which it was not supposed to do. CI tool needs priviledges to make K8s API calls to perform deployments. It violates the trust boundary and can allow an attacker to use a compromised pipeline to execute a malicious code. Its not stable as success or failure of deployments is unknown. This is true in case a deployment fails and leaves the cluster in an unstable state. Not a feasible option for multiple clusters or a single large cluster. Gitops pattern First store the desired state declaratively in git. Pull the desired state into the system using an operator. Use a control loop to keep all of this in sync. It keeps the CI pipeline intact from CIOps except for removing the trigger for the CD pipeline. Under Gitops, the sole responsiblity of CI is to produce an artifact, in this case its a container image that is placed within an artifact repository. The Operator takes over from CI. It monitors the desired state in git for any changes which are made to a separate Environment repo which are made using pull requests . Once operator detects a change to the K8s manifest in the repo and provides this to the K8s control plane using the API to update the new state of the cluster. This causes the cluster to pull the new image and deploys it across the nodes.","title":"Traditional vs GitOps"},{"location":"devops/gitops/#gitops-usecases","text":"1. Infrastructure Operations : Gitops expects resources can be rapidly deployed using Cloud services. Teams use tools like Trraform to declare the infrastructure in manifest to create K8s cluster in a particular region. 2. Application Operations : Flux is used to ensure applications are deployed in an automated fashion to a K8s cluster 3. Release Operations : GitOps practises can be applied to declaratively define complex release strategies like Canary or Blue green deployments. Tools like Flagger fall into this space.","title":"Gitops Usecases"},{"location":"devops/gitops/#the-gitops-workflow","text":"The traditional application lifecycle resembles the following: Design --> Build --> Image --> Test --> Deploy When you add GitOps into the mix, that lifecycle looks as follows: Design --> Build --> Image --> Test --> Monitor --> Log changes/events --> Alert when a change has occurred --> Update With a Kubernetes workflow as your source of truth all necessary code is stored in a Git repository with the help of automation. Anyone with Kubernetes management rights can create pull requests, edit code, and issue merge requests to the repository. Once a merge request is complete, the automated GitOps operator detects the changes, another automator declares if the change is operational, and the change is automatically deployed to the cluster. Within the GitOps workflow you not only have a high level of automation, but there\u2019s also a much higher probability that every deployment works exactly as expected. Using Flux as the gitops operator within Kubernetes. Using Flux and Flagger together to bring in Progressive or Blue-Green Deployment strategies. An alternate workflow created by Gitlab Advantages of GitOps CI and CD are decoupled into separate concerns. CI pipeline is left intact. However the CD pipeline is shifted inside the cluster where an Operator is responsible for automating deployment of changes to ensure it matches the desired state found in git. This eliminates the potential security concerns associated with the traditional push model from CIOps. It also provides recoverabilty as the cluster can be quickly rebuilt from source code.","title":"The GitOps Workflow"},{"location":"devops/gitops/#gitops-tools","text":"There are several tools useful to GitOps, some of those tools include: Git - a version control system. GitHub - a code repository for housing your code. Cloud Build - a service that executes the build step of your deployment lifecycle using pre-packaged docker containers that include all of the appropriate tooling. CircleCI - a SaaS-style build engine that simplifies the build steps and can serve as a CI/CD engine. Kubernetes - a container orchestration platform that can be seamlessly integrated with GitOps. Helm - a robust tool for configuring Kubernetes resources. Flux - the GitOps operator for Kubernetes which automatically adjusts the Kubernetes cluster configuration based on the configurations found in your Git repo. Flagger - automates the detection of errors in code and prevents those errors from being deployed. Prometheus - a powerful GitOps monitoring tool that can generate alerts that are detected by Flagger. Quay - an application registry and container image manager.","title":"GitOps Tools"},{"location":"devops/iac/","text":"Cloud Computing Models \u00b6 IaaS \u00b6 IaaS is a mature computing model that first became popular about a decade ago. It is currently the most common cloud computing paradigm. IaaS cloud providers, offer IaaS services from their extensive pool of physical servers in their data centers. These vendors use a hypervisor , also known as a Virtual Machine Monitor (VMM) , to create the virtual service. A hypervisor is a type of emulator that runs on an actual hardware host, which is referred to as the host machine. It runs a Virtual Machine (VM) that mimics an actual server or network. Some common types of hypervisors include Xen, Oracle VirtualBox, Oracle VM, KVM, and VMware ESX. The most common way of creating an IaaS VM is by using cloud orchestration technologies. These programs choose a hypervisor to run the VM on and then create the virtual machine. They also frequently allocate storage and add firewalls, logging services, and networking essentials including IP addresses. Advanced services might include clustering, encryption, billing, load balancing, and more complicated Virtual Local Area Networks (VLAN). A Virtual Private Cloud (VPC) can assist in further isolating the cloud resources. Both Central Processing Unit (CPU) and Graphics Processing Unit (GPU) systems are typically available. Customers of IaaS access their virtualized infrastructure over the internet. They use a visual dashboard or Graphical User Interface (GUI) to quickly create or modify devices, often with the push of a button. The dashboard can also be used to monitor performance, collect data, troubleshoot, and track costs. All services are provided on a pay-as-you-go model. Services can also be provisioned programmatically using APIs. This technique is often used together with Infrastructure as Code (IaC) technologies, which deploys the infrastructure using scripts. IaC allows users to standardize common infrastructure tasks and test their deployments using automation. One important point about IaaS is the customer does not control the underlying physical hardware components and interconnections. These remain under the control of the cloud provider. Users of IaaS are typically responsible for the selection and installation of the operating system and all software applications, including databases and middleware . PaaS \u00b6 PaaS extends the IaaS model to include operating systems, web servers, tools, databases, and other managed services that is handled by the cloud providers. The PaaS user is still responsible for adding and managing applications on top of the vendor-provided platform . This makes deployment even easier at the cost of some flexibility. PaaS services are often used for software and application development . SaaS \u00b6 SaaS provides software services to customers on demand . Users access the software on the provider\u2019s server, typically through a web browser. SaaS clients are only responsible for configuring the application, maintaining an access control list, and the actual content . SaaS is usually geared towards a completely different audience. Most SaaS clients do not require a platform and do not need IaaS capabilities. Serverless \u00b6 Serverless computing eliminates the need to manage your infrastructure . It lies somewhere between PaaS and SaaS in term of the control/ease-of-use it provides, but is not exactly like either model. The name \u201cserverless\u201d is somewhat misleading. Servers are being used, but the end-user has no knowledge or visibility of them. Serverless computing provisions all resources on-demand, and automatically and dynamically scales them up and down in close to real-time. This makes more efficient use of computer resources because the provider allocates the memory, CPU, and networking resources based on the calculated demand. Serverless computing is well suited to a microservices model and is frequently used in software development. However, it can add more latency, and cold starts can affect the system\u2019s performance. Conatiners \u00b6 Containers, such as Docker, are another option for implementing virtualization, but they follow a completely different model. Containers do not use hypervisors. They run on a Linux partition directly on the hardware. Containers could be said to offer operating-system-level virtualization. Containers offer better performance than hypervisor-based servers at the cost of some additional complexity and have become increasingly popular. Reasons to Use IaaS \u00b6 The IaaS model is particularly well suited to the following scenarios. Backup and recovery : IaaS services are handy for backing up applications and data, and for rapid recovery systems for on-site networks. In this case, the IaaS network typically mirrors the configuration of the on-site servers. Testing and rapid development : IaaS allows for quick prototyping and efficient automated testing. Servers can be created and deleted as required. IaaS facilitates the testing of an application on different platforms and networks. It is also useful for temporary or experimental workflows. Legal and compliance requirements : IaaS systems are a good choice for data retention and other record-keeping requirements. High-performance and specialized computing : The cost of buying high-performance equipment capable of specialized tasks might otherwise be prohibitive for smaller businesses. IaaS enables smaller businesses to access advanced systems capable of handling data analysis, computer modeling, and 3-D graphics. Managing unpredictable demand : The ability to scale up and down means IaaS is a good choice for unpredictable scenarios when the demand is unknown or might vary dramatically. It allows companies to handle unexpected surges. Rapid migration to the cloud : IaaS APIs allow for the easy translation of the original network and configuration into IaaS specifications. Application and web development : IaaS is also frequently used for web hosting. Advantages and Disadvantages \u00b6 Advantages: It reduces maintenance, operating costs, and lab space. IaaS allows businesses to focus on their core activities instead of running a on-premises servers. It eliminates the need for capital expenditures on equipment. The pay-as-you-go operating expense of IaaS is easier to budget for than large capital expenditures. IaaS networks can react rapidly to changing business demands, by quickly expanding or contracting. New services are easily created, and customers only use and pay for what they need. IaaS elegantly handles system backups and redundancy and increases reliability. For example, cloud providers have multiple labs and are hardened against failures. Providers of IaaS offer different packages with different levels of memory and performance. It is easy for a customer to find the right package for their network. Customers can also upgrade or downgrade according to their current situation. IaaS providers have more expertise with hardware and networking technologies and can provide advice and support. Many IaaS vendors have geographically diverse locations. This makes it easier for organizations to position their resources closer to their end-users. It also provides an even greater level of redundancy and protection in the case of local outages or failures. IaaS can provide better security because vendors are more familiar with updated security protocols. Disadvantages: The biggest drawback is the relative lack of flexibility and low-level visibility compared to on-premises servers. Customers cannot deploy a system that their IaaS vendor does not offer, and they cannot control attributes such as IP address blocks and subnets. However, this is usually not a big concern for most deployments. Customers should also be aware that most hypervisors support multiple users, and their performance and throughput could be affected by other customers at times. Migration Strategies \u00b6 Several options are available to organizations that want to relocate their existing network to use an IaaS model. Staged : In a staged migration, certain components of the old network are moved over to IaaS before others. For instance, the database or the data could be moved over first. Meanwhile, the original on-site servers are still used to access the database. This strategy reduces the overall risk of the move. Re-hosting : This method is also known as the \u201clift and shift\u201d strategy. The existing configuration, data, and applications are migrated to an IaaS model without any significant modifications. The new IaaS servers are configured the same way the original physical servers were. Refactoring : Refactoring re-engineers the environment from scratch to take advantage of IaaS capabilities. This might involve a more detailed API roll-out using IaC products, closer attention to scalability, and more efficient use of cloud resources. During the move, candidate tasks for automation and streamlining can be identified. Hybrid : With this strategy, infrastructure items are moved to IaaS selectively. Some resources might remain on the old network for security, logistical, business, or legal reasons. IaaS Deployment Strategies \u00b6 Each IaaS deployment is unique, but the following high-level principles generally apply. Understand and be clear about the business requirements and the budget before proceeding with any deployments. Carefully review and understand the policies of the cloud provider and their plans, packages, and products. Be clear about the capabilities of the virtualized infrastructure, including the throughput, storage, and memory/performance of each item. Consider how any existing databases and servers should be migrated using one of the techniques in the Migration Strategies section. Attempt to reduce downtime. Schedule a maintenance window for the migration. Test the new network before live deployment. Consider how much storage and what storage types should be used. The main types of storage are object storage, file storage, and block storage. Object storage has become more popular recently because its distributed architecture fits well with the IaaS model. Consider the resiliency and reliability requirements for the network. If necessary, determine the level of support and the service package that is required. Decide what network metrics are important, and monitor these items during and after the initial deployment. Scrutinize the entire network as a single system and continue to regularly maintain, adjust, and optimize it. Terraform vs Ansible \u00b6 Both Ansible and Terraform are tools for implementing Infrastructure as Code, although they focus on different components. Ansible is geared towards configuration management whereas Terraform\u2019s strength lies in service and cloud orchestration. There could also be situations where the two tools are best used together. Terraform is a service orchestration tool which is optimized for cloud and multi-cloud networks. It ensures an environment is in its desired state, stores this state, and restores the system after it is reloaded. It does not focus on configuration management. Ansible is a ** configuration management tool**. It excels in provisioning software and devices, and deploying the applications that run on top of the infrastructure. It operates on a particular device in isolation from the network and ensures it is functioning normally. There is some overlap between the tools because Ansible can perform some service orchestration. Its playbooks can be extended to deploy applications in a cloud, and it features modules for most major cloud providers. But it is not as good at orchestrating services and interconnected, dependent applications . The Main Uses for Terraform \u00b6 Terraform is an open source IaC tool that is very straightforward to use. Its main purpose is to build and scale Cloud services and to manage the state of the network . Terraform does not specialize in software configuration, and does not install and manage software on existing devices. Instead, it is geared towards creating, modifying, and destroying servers and other Cloud resources. This means it is most commonly found in data centers and in software-defined networking (SDN) environments . It works effectively with both lower-level elements, including storage and networking devices, and higher-level Software as a Service (SaSS) entries. In terms of state management, it maps the actual resources back to the configuration, stores metadata, and improves network performance. Terraform can manage external service providers, including cloud networks, and in-house solutions. It is especially useful for multi-tier or N-tier applications , such as web servers that use a database layer. Because Terraform models the dependencies between applications and add-ons, it ensures the database layer is ready before any web servers are launched. Terraform is cloud agnostic, and can manage multiple clouds to increase fault tolerance. A single configuration file can oversee multiple providers and handle cross-cloud dependencies. Terraform is very efficient for demos or other disposable environments due to the ease of creating a network on a cloud provider. It helps manage parallel environments, so it is a good choice for testing, validating bug fixes, and formal acceptance. The Main Uses for Ansible \u00b6 The main purpose of Red Hat\u2019s Ansible is IT automation . Ansible automates software provisioning, configuration management, application deployment, and continuous integration (CI) pipelines. Ansible runs on most Linux distributions, and can provision both Linux and Windows-based devices. The design goals of Ansible are to be minimal, consistent, secure, reliable, and easy to learn. It is straightforward to install, and no special programming skills are necessarily required to use it. Ansible handles all types of infrastructure platforms, including bare metal, virtualized devices such as hypervisors, and cloud networks. It integrates well with legacy applications and existing automated scripts, and is designed to manage the complex, multi-faceted facilities found in large businesses. Ansible supports idempotent behavior, which means it can place the node into the same state every time. This is necessary for consistency and standardized behavior. IaC \u00b6 Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. IaC can be applied throughout the lifecycle, both on the initial build, as well as throughout the life of the infrastructure. Commonly, these are referred to as Day 0 and Day 1 activities. \u201cDay 0\u201d code provisions and configures your initial infrastructure. \u201cDay 0\u201d code provisions and configures your initial infrastructure. If your infrastructure never changes after the initial build (no OS updates, no patches, no app configurations, etc.) then you may not need tools that support subsequent updates, changes, and expansions. \u201cDay 1\u201d refers to OS and application configurations you apply after you\u2019ve initially built your infrastructure. IaC makes changes idempotent, consistent, repeatable, and predictable. With IaC, we can test the code and review the results before the code is applied to our target environments. Should a result not align to our expectations, we iterate on the code until the results pass our tests and align to our expectations. Following this pattern allows for the outcome to be predicted before the code is applied to a production environment. Once ready for use, we can then apply that code via automation, at scale, ensuring consistency and repeatability in how it is applied. Since code is checked into version control systems such as GitHub, GitLab, BitBucket, etc., it is possible to review how the infrastructure evolves over time. The idempotent characteristic provided by IaC tools ensures that, even if the same code is applied multiple times, the result remains the same. Steps to Define your Infrastructure Scope : Identify the Infrastructure for your project. Author : Write configuration to define your infrastructure. Initialize : Install the required Terraform providers. Plan : Preview the changes Terraform will make. Apply : Make the changes to your infrastructure. Using Terraform has several advantages over manually managing your infrastructure: Terraform can manage infrastructure on multiple cloud platforms. The human-readable configuration language helps you write infrastructure code quickly. Terraform's state allows you to track resource changes throughout your deployments. You can commit your configurations to version control to safely collaborate on infrastructure. The terraform {} block contains Terraform settings, including the required providers Terraform will use to provision your infrastructure. A provider is a plugin that Terraform uses to create and manage your resources. Use resource blocks to define components of your infrastructure. A resource might be a physical or virtual component such as an EC2 instance, or it can be a logical resource such as a Heroku application. Resource blocks have two strings before the block: the resource type and the resource name.","title":"IAC"},{"location":"devops/iac/#cloud-computing-models","text":"","title":"Cloud Computing Models"},{"location":"devops/iac/#iaas","text":"IaaS is a mature computing model that first became popular about a decade ago. It is currently the most common cloud computing paradigm. IaaS cloud providers, offer IaaS services from their extensive pool of physical servers in their data centers. These vendors use a hypervisor , also known as a Virtual Machine Monitor (VMM) , to create the virtual service. A hypervisor is a type of emulator that runs on an actual hardware host, which is referred to as the host machine. It runs a Virtual Machine (VM) that mimics an actual server or network. Some common types of hypervisors include Xen, Oracle VirtualBox, Oracle VM, KVM, and VMware ESX. The most common way of creating an IaaS VM is by using cloud orchestration technologies. These programs choose a hypervisor to run the VM on and then create the virtual machine. They also frequently allocate storage and add firewalls, logging services, and networking essentials including IP addresses. Advanced services might include clustering, encryption, billing, load balancing, and more complicated Virtual Local Area Networks (VLAN). A Virtual Private Cloud (VPC) can assist in further isolating the cloud resources. Both Central Processing Unit (CPU) and Graphics Processing Unit (GPU) systems are typically available. Customers of IaaS access their virtualized infrastructure over the internet. They use a visual dashboard or Graphical User Interface (GUI) to quickly create or modify devices, often with the push of a button. The dashboard can also be used to monitor performance, collect data, troubleshoot, and track costs. All services are provided on a pay-as-you-go model. Services can also be provisioned programmatically using APIs. This technique is often used together with Infrastructure as Code (IaC) technologies, which deploys the infrastructure using scripts. IaC allows users to standardize common infrastructure tasks and test their deployments using automation. One important point about IaaS is the customer does not control the underlying physical hardware components and interconnections. These remain under the control of the cloud provider. Users of IaaS are typically responsible for the selection and installation of the operating system and all software applications, including databases and middleware .","title":"IaaS"},{"location":"devops/iac/#paas","text":"PaaS extends the IaaS model to include operating systems, web servers, tools, databases, and other managed services that is handled by the cloud providers. The PaaS user is still responsible for adding and managing applications on top of the vendor-provided platform . This makes deployment even easier at the cost of some flexibility. PaaS services are often used for software and application development .","title":"PaaS"},{"location":"devops/iac/#saas","text":"SaaS provides software services to customers on demand . Users access the software on the provider\u2019s server, typically through a web browser. SaaS clients are only responsible for configuring the application, maintaining an access control list, and the actual content . SaaS is usually geared towards a completely different audience. Most SaaS clients do not require a platform and do not need IaaS capabilities.","title":"SaaS"},{"location":"devops/iac/#serverless","text":"Serverless computing eliminates the need to manage your infrastructure . It lies somewhere between PaaS and SaaS in term of the control/ease-of-use it provides, but is not exactly like either model. The name \u201cserverless\u201d is somewhat misleading. Servers are being used, but the end-user has no knowledge or visibility of them. Serverless computing provisions all resources on-demand, and automatically and dynamically scales them up and down in close to real-time. This makes more efficient use of computer resources because the provider allocates the memory, CPU, and networking resources based on the calculated demand. Serverless computing is well suited to a microservices model and is frequently used in software development. However, it can add more latency, and cold starts can affect the system\u2019s performance.","title":"Serverless"},{"location":"devops/iac/#conatiners","text":"Containers, such as Docker, are another option for implementing virtualization, but they follow a completely different model. Containers do not use hypervisors. They run on a Linux partition directly on the hardware. Containers could be said to offer operating-system-level virtualization. Containers offer better performance than hypervisor-based servers at the cost of some additional complexity and have become increasingly popular.","title":"Conatiners"},{"location":"devops/iac/#reasons-to-use-iaas","text":"The IaaS model is particularly well suited to the following scenarios. Backup and recovery : IaaS services are handy for backing up applications and data, and for rapid recovery systems for on-site networks. In this case, the IaaS network typically mirrors the configuration of the on-site servers. Testing and rapid development : IaaS allows for quick prototyping and efficient automated testing. Servers can be created and deleted as required. IaaS facilitates the testing of an application on different platforms and networks. It is also useful for temporary or experimental workflows. Legal and compliance requirements : IaaS systems are a good choice for data retention and other record-keeping requirements. High-performance and specialized computing : The cost of buying high-performance equipment capable of specialized tasks might otherwise be prohibitive for smaller businesses. IaaS enables smaller businesses to access advanced systems capable of handling data analysis, computer modeling, and 3-D graphics. Managing unpredictable demand : The ability to scale up and down means IaaS is a good choice for unpredictable scenarios when the demand is unknown or might vary dramatically. It allows companies to handle unexpected surges. Rapid migration to the cloud : IaaS APIs allow for the easy translation of the original network and configuration into IaaS specifications. Application and web development : IaaS is also frequently used for web hosting.","title":"Reasons to Use IaaS"},{"location":"devops/iac/#advantages-and-disadvantages","text":"Advantages: It reduces maintenance, operating costs, and lab space. IaaS allows businesses to focus on their core activities instead of running a on-premises servers. It eliminates the need for capital expenditures on equipment. The pay-as-you-go operating expense of IaaS is easier to budget for than large capital expenditures. IaaS networks can react rapidly to changing business demands, by quickly expanding or contracting. New services are easily created, and customers only use and pay for what they need. IaaS elegantly handles system backups and redundancy and increases reliability. For example, cloud providers have multiple labs and are hardened against failures. Providers of IaaS offer different packages with different levels of memory and performance. It is easy for a customer to find the right package for their network. Customers can also upgrade or downgrade according to their current situation. IaaS providers have more expertise with hardware and networking technologies and can provide advice and support. Many IaaS vendors have geographically diverse locations. This makes it easier for organizations to position their resources closer to their end-users. It also provides an even greater level of redundancy and protection in the case of local outages or failures. IaaS can provide better security because vendors are more familiar with updated security protocols. Disadvantages: The biggest drawback is the relative lack of flexibility and low-level visibility compared to on-premises servers. Customers cannot deploy a system that their IaaS vendor does not offer, and they cannot control attributes such as IP address blocks and subnets. However, this is usually not a big concern for most deployments. Customers should also be aware that most hypervisors support multiple users, and their performance and throughput could be affected by other customers at times.","title":"Advantages and Disadvantages"},{"location":"devops/iac/#migration-strategies","text":"Several options are available to organizations that want to relocate their existing network to use an IaaS model. Staged : In a staged migration, certain components of the old network are moved over to IaaS before others. For instance, the database or the data could be moved over first. Meanwhile, the original on-site servers are still used to access the database. This strategy reduces the overall risk of the move. Re-hosting : This method is also known as the \u201clift and shift\u201d strategy. The existing configuration, data, and applications are migrated to an IaaS model without any significant modifications. The new IaaS servers are configured the same way the original physical servers were. Refactoring : Refactoring re-engineers the environment from scratch to take advantage of IaaS capabilities. This might involve a more detailed API roll-out using IaC products, closer attention to scalability, and more efficient use of cloud resources. During the move, candidate tasks for automation and streamlining can be identified. Hybrid : With this strategy, infrastructure items are moved to IaaS selectively. Some resources might remain on the old network for security, logistical, business, or legal reasons.","title":"Migration Strategies"},{"location":"devops/iac/#iaas-deployment-strategies","text":"Each IaaS deployment is unique, but the following high-level principles generally apply. Understand and be clear about the business requirements and the budget before proceeding with any deployments. Carefully review and understand the policies of the cloud provider and their plans, packages, and products. Be clear about the capabilities of the virtualized infrastructure, including the throughput, storage, and memory/performance of each item. Consider how any existing databases and servers should be migrated using one of the techniques in the Migration Strategies section. Attempt to reduce downtime. Schedule a maintenance window for the migration. Test the new network before live deployment. Consider how much storage and what storage types should be used. The main types of storage are object storage, file storage, and block storage. Object storage has become more popular recently because its distributed architecture fits well with the IaaS model. Consider the resiliency and reliability requirements for the network. If necessary, determine the level of support and the service package that is required. Decide what network metrics are important, and monitor these items during and after the initial deployment. Scrutinize the entire network as a single system and continue to regularly maintain, adjust, and optimize it.","title":"IaaS Deployment Strategies"},{"location":"devops/iac/#terraform-vs-ansible","text":"Both Ansible and Terraform are tools for implementing Infrastructure as Code, although they focus on different components. Ansible is geared towards configuration management whereas Terraform\u2019s strength lies in service and cloud orchestration. There could also be situations where the two tools are best used together. Terraform is a service orchestration tool which is optimized for cloud and multi-cloud networks. It ensures an environment is in its desired state, stores this state, and restores the system after it is reloaded. It does not focus on configuration management. Ansible is a ** configuration management tool**. It excels in provisioning software and devices, and deploying the applications that run on top of the infrastructure. It operates on a particular device in isolation from the network and ensures it is functioning normally. There is some overlap between the tools because Ansible can perform some service orchestration. Its playbooks can be extended to deploy applications in a cloud, and it features modules for most major cloud providers. But it is not as good at orchestrating services and interconnected, dependent applications .","title":"Terraform vs Ansible"},{"location":"devops/iac/#the-main-uses-for-terraform","text":"Terraform is an open source IaC tool that is very straightforward to use. Its main purpose is to build and scale Cloud services and to manage the state of the network . Terraform does not specialize in software configuration, and does not install and manage software on existing devices. Instead, it is geared towards creating, modifying, and destroying servers and other Cloud resources. This means it is most commonly found in data centers and in software-defined networking (SDN) environments . It works effectively with both lower-level elements, including storage and networking devices, and higher-level Software as a Service (SaSS) entries. In terms of state management, it maps the actual resources back to the configuration, stores metadata, and improves network performance. Terraform can manage external service providers, including cloud networks, and in-house solutions. It is especially useful for multi-tier or N-tier applications , such as web servers that use a database layer. Because Terraform models the dependencies between applications and add-ons, it ensures the database layer is ready before any web servers are launched. Terraform is cloud agnostic, and can manage multiple clouds to increase fault tolerance. A single configuration file can oversee multiple providers and handle cross-cloud dependencies. Terraform is very efficient for demos or other disposable environments due to the ease of creating a network on a cloud provider. It helps manage parallel environments, so it is a good choice for testing, validating bug fixes, and formal acceptance.","title":"The Main Uses for Terraform"},{"location":"devops/iac/#the-main-uses-for-ansible","text":"The main purpose of Red Hat\u2019s Ansible is IT automation . Ansible automates software provisioning, configuration management, application deployment, and continuous integration (CI) pipelines. Ansible runs on most Linux distributions, and can provision both Linux and Windows-based devices. The design goals of Ansible are to be minimal, consistent, secure, reliable, and easy to learn. It is straightforward to install, and no special programming skills are necessarily required to use it. Ansible handles all types of infrastructure platforms, including bare metal, virtualized devices such as hypervisors, and cloud networks. It integrates well with legacy applications and existing automated scripts, and is designed to manage the complex, multi-faceted facilities found in large businesses. Ansible supports idempotent behavior, which means it can place the node into the same state every time. This is necessary for consistency and standardized behavior.","title":"The Main Uses for Ansible"},{"location":"devops/iac/#iac","text":"Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. IaC can be applied throughout the lifecycle, both on the initial build, as well as throughout the life of the infrastructure. Commonly, these are referred to as Day 0 and Day 1 activities. \u201cDay 0\u201d code provisions and configures your initial infrastructure. \u201cDay 0\u201d code provisions and configures your initial infrastructure. If your infrastructure never changes after the initial build (no OS updates, no patches, no app configurations, etc.) then you may not need tools that support subsequent updates, changes, and expansions. \u201cDay 1\u201d refers to OS and application configurations you apply after you\u2019ve initially built your infrastructure. IaC makes changes idempotent, consistent, repeatable, and predictable. With IaC, we can test the code and review the results before the code is applied to our target environments. Should a result not align to our expectations, we iterate on the code until the results pass our tests and align to our expectations. Following this pattern allows for the outcome to be predicted before the code is applied to a production environment. Once ready for use, we can then apply that code via automation, at scale, ensuring consistency and repeatability in how it is applied. Since code is checked into version control systems such as GitHub, GitLab, BitBucket, etc., it is possible to review how the infrastructure evolves over time. The idempotent characteristic provided by IaC tools ensures that, even if the same code is applied multiple times, the result remains the same. Steps to Define your Infrastructure Scope : Identify the Infrastructure for your project. Author : Write configuration to define your infrastructure. Initialize : Install the required Terraform providers. Plan : Preview the changes Terraform will make. Apply : Make the changes to your infrastructure. Using Terraform has several advantages over manually managing your infrastructure: Terraform can manage infrastructure on multiple cloud platforms. The human-readable configuration language helps you write infrastructure code quickly. Terraform's state allows you to track resource changes throughout your deployments. You can commit your configurations to version control to safely collaborate on infrastructure. The terraform {} block contains Terraform settings, including the required providers Terraform will use to provision your infrastructure. A provider is a plugin that Terraform uses to create and manage your resources. Use resource blocks to define components of your infrastructure. A resource might be a physical or virtual component such as an EC2 instance, or it can be a logical resource such as a Heroku application. Resource blocks have two strings before the block: the resource type and the resource name.","title":"IaC"},{"location":"devops/questions/","text":"Personal \u00b6 Tell us about your roles and responsibilties Any key learnings which you are proud off Devops \u00b6 How will you plan a new Devops process for a Microservices Architecture? What are the different phases in a Devops process and design it? How will you convince a customer and teams to onboard to Devops practise? What are Devops metrics? Plan and Design a CI process and how will you include Secops in it to make it Devsecops? Difference between CI and CD and explain that with a help of Gitflow strategy? Explain which branch is used in which process? Suppose a bug comes in PR, how is the branching strategy affected? How will you sync branches which are in Testing phases with the Bugfix? What are deployment strategies? (Answer Blue-Green, Canary, Rolling) What are the drawbacks and how to select an appropriate strategy in a microservices architecture? Difference between git fetch and git pull Linux \u00b6 How does TLS authentication happen between a client and a server? How does DNS resolution happen? Troubleshoot a Linux Server by checking various metrics? Trobleshoot an application that is running as a service? How will you find a file in the filesystem with a text \"Hello\" when location or filename is not known? Significance of EXIT command? How will catch EXIT in a shell script (hint Trap) Pipelines \u00b6 How is a pipeline triggered and describe the automation process? Jenkinsfile and how is a shared library used? Maven and build process plugins tht you have worked with? What are steps present in a CI pipeline? What base images did you use? How are various images for tooling maintained using pipelines? Central pipeline libraries and how will you trigger them from code repos which have the calling function? How will you build and deploy images that have been modified in a pipeline? Image maintenance and the way to update them in an automated manner if the base image is modified? Docker \u00b6 Dockerfile structure and its main components? Explian entire process of building and deploying a image to Dockerhub or a private registry? Difference between CMD and RUN? Difference between CMD and ENTRYPOINT? Remove unused docker containers from a machine without removing the volumes? How will you exec into a running container? Kubernetes \u00b6 Describe Kubernetes components? Describe K8s Networking viz Container to Container communication, Pod to Pod communication and Node to Node communication? Describe a Deployment resource structure and how is a service defined / mapped within it? How will you scale an appliciation managed by a Deployment object without modifying the YAML? Describe an Ingress resource structure? How to link a Ingress resource to a correct Ingress controller in a multi-controller environment? Troubleshoot an application in a given environment, describe each step from identifying the cluster to navigating to the namespace? Troubleshoot master plane when kubectl access is not present? Helm \u00b6 Basic Structure of a Helmchart and explain? What will happen when you deploy a helmchart and the resources already exists How will you sync resources in an environment which have been modified outside the helm upgrade process? How will you pass multiple values using files in command line? Terraform \u00b6 What are terraform modules? What are terraform providers? Terraform variables and how are they used? How will you override a value during execution (Hint: using tfvars)? Variables defined in vars.tf and inside TFE? Which one will be picked up? How will you override them during execution? How are different environments mapped in a single terraform modules","title":"Personal"},{"location":"devops/questions/#personal","text":"Tell us about your roles and responsibilties Any key learnings which you are proud off","title":"Personal"},{"location":"devops/questions/#devops","text":"How will you plan a new Devops process for a Microservices Architecture? What are the different phases in a Devops process and design it? How will you convince a customer and teams to onboard to Devops practise? What are Devops metrics? Plan and Design a CI process and how will you include Secops in it to make it Devsecops? Difference between CI and CD and explain that with a help of Gitflow strategy? Explain which branch is used in which process? Suppose a bug comes in PR, how is the branching strategy affected? How will you sync branches which are in Testing phases with the Bugfix? What are deployment strategies? (Answer Blue-Green, Canary, Rolling) What are the drawbacks and how to select an appropriate strategy in a microservices architecture? Difference between git fetch and git pull","title":"Devops"},{"location":"devops/questions/#linux","text":"How does TLS authentication happen between a client and a server? How does DNS resolution happen? Troubleshoot a Linux Server by checking various metrics? Trobleshoot an application that is running as a service? How will you find a file in the filesystem with a text \"Hello\" when location or filename is not known? Significance of EXIT command? How will catch EXIT in a shell script (hint Trap)","title":"Linux"},{"location":"devops/questions/#pipelines","text":"How is a pipeline triggered and describe the automation process? Jenkinsfile and how is a shared library used? Maven and build process plugins tht you have worked with? What are steps present in a CI pipeline? What base images did you use? How are various images for tooling maintained using pipelines? Central pipeline libraries and how will you trigger them from code repos which have the calling function? How will you build and deploy images that have been modified in a pipeline? Image maintenance and the way to update them in an automated manner if the base image is modified?","title":"Pipelines"},{"location":"devops/questions/#docker","text":"Dockerfile structure and its main components? Explian entire process of building and deploying a image to Dockerhub or a private registry? Difference between CMD and RUN? Difference between CMD and ENTRYPOINT? Remove unused docker containers from a machine without removing the volumes? How will you exec into a running container?","title":"Docker"},{"location":"devops/questions/#kubernetes","text":"Describe Kubernetes components? Describe K8s Networking viz Container to Container communication, Pod to Pod communication and Node to Node communication? Describe a Deployment resource structure and how is a service defined / mapped within it? How will you scale an appliciation managed by a Deployment object without modifying the YAML? Describe an Ingress resource structure? How to link a Ingress resource to a correct Ingress controller in a multi-controller environment? Troubleshoot an application in a given environment, describe each step from identifying the cluster to navigating to the namespace? Troubleshoot master plane when kubectl access is not present?","title":"Kubernetes"},{"location":"devops/questions/#helm","text":"Basic Structure of a Helmchart and explain? What will happen when you deploy a helmchart and the resources already exists How will you sync resources in an environment which have been modified outside the helm upgrade process? How will you pass multiple values using files in command line?","title":"Helm"},{"location":"devops/questions/#terraform","text":"What are terraform modules? What are terraform providers? Terraform variables and how are they used? How will you override a value during execution (Hint: using tfvars)? Variables defined in vars.tf and inside TFE? Which one will be picked up? How will you override them during execution? How are different environments mapped in a single terraform modules","title":"Terraform"},{"location":"ide/","text":"IDE Tips and Tricks \u00b6 VS Code \u00b6 Intellij \u00b6 How to open multiple modules in the same workspace? Open the first module / project. Select File -> Project Structure -> Modules. Select Copy (icon) and open Browse your local directory and select the project you would like to add. Module name will resolve automatically. When you want to reopen to project with multiple sub-projects, in order to avoid re-doing steps as described above, just go to File -> Open Recent -> 'Your Big Project'. IDE Shortcuts Option Description Ctrl + Space Code Completion Ctrl + Shift + / Comments selected lines of code Ctrl + N Navigate to any class by typing the name in the editor Ctrl + B Jump to declaration","title":"IDE Tips and Tricks"},{"location":"ide/#ide-tips-and-tricks","text":"","title":"IDE Tips and Tricks"},{"location":"ide/#vs-code","text":"","title":"VS Code"},{"location":"ide/#intellij","text":"How to open multiple modules in the same workspace? Open the first module / project. Select File -> Project Structure -> Modules. Select Copy (icon) and open Browse your local directory and select the project you would like to add. Module name will resolve automatically. When you want to reopen to project with multiple sub-projects, in order to avoid re-doing steps as described above, just go to File -> Open Recent -> 'Your Big Project'. IDE Shortcuts Option Description Ctrl + Space Code Completion Ctrl + Shift + / Comments selected lines of code Ctrl + N Navigate to any class by typing the name in the editor Ctrl + B Jump to declaration","title":"Intellij"},{"location":"ide/markdown/","text":"General Syntax \u00b6 MarkDown Cheat Sheet Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Horizontal Rule ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks Emphasis Bold **rendered as bold text** Italics _rendered as italicized text_ BlockQuote > Blockquotes Nested BlockQuote > First Level >> Second Level >>> Third Level List Unordered * or + or - Ordered 1. Code Block ` or [```html] (3 backticks and follwed by the language) Links Inline Links [Text](http://text.io) Link titles [Text](https://github.com/site/ \"Visit Site!\") Images ![Minion](http://octodex.github.com/images/minion.png) Using Mkdocs formatting \u00b6 Tabbed Data Inline Examples Output p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} , p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} . Markdown $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ , \\( p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } \\) . Adding Tips Inline Configuration This is an example of a tip. Make the paragragh tabbed inline with the heading Adding Danger Reminder This is a call to action Adding Note Note Adding notes Adding Summary Summary This is to sumarize the information New Information New 7.1 New Info Adding Note Collapsible tab Click Me! Thanks! Success Collapsible tab Success Content. Warning Collapsible tab Warning Content. Adding Settings Gear Basic Software Setup sudo apt install Adding multi-level collapisble tabs Details must contain a blank line before they start. Use ??? to start a details block or ???+ if you want to start a details block whose default state is 'open'. Follow the start of the block with an optional class or classes (separated with spaces) and the summary contained in quotes. Content is placed below the header and must be indented. Open styled details Nested details! And more content again. Adding checklist inside summary Tasklist eggs bread milk Adding strikethrough and subscript Tilde Tilde is syntactically built around the ~ character. It adds support for inserting sub scripts and adds an easy way to place text in a < del > tag. Showing Critic changes Critic Added CSS changes in extra.css to activate Critic change higlights. This is deleted This is added Showing Emojis Inline Code Highlighting InlineHilite utilizes the following syntax to insert inline highlighted code: `:::language mycode` or `#!language mycode` . Inline Highlighted Code Example Here is some code: import pymdownx ; pymdownx . __version__ . The mock shebang will be treated like text here: #!js var test = 0; . Marking words Mark Example mark me Preserve Tab spaces ============================================================ T Tp Sp D Dp S D7 T ------------------------------------------------------------ A F#m Bm E C#m D E7 A A# Gm Cm F Dm D# F7 A# B\u266d Gm Cm F Dm E\u266dm F7 B\u266d Showing Line Number in code import foo.bar import car - Highlighting specific line numbers \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz Highlighting line range and specific lines import foo import boo.baz import foo.bar.baz class Foo : def __init__ ( self ): self . foo = None self . bar = None self . baz = None","title":"Markdown"},{"location":"ide/markdown/#general-syntax","text":"MarkDown Cheat Sheet Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Horizontal Rule ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks Emphasis Bold **rendered as bold text** Italics _rendered as italicized text_ BlockQuote > Blockquotes Nested BlockQuote > First Level >> Second Level >>> Third Level List Unordered * or + or - Ordered 1. Code Block ` or [```html] (3 backticks and follwed by the language) Links Inline Links [Text](http://text.io) Link titles [Text](https://github.com/site/ \"Visit Site!\") Images ![Minion](http://octodex.github.com/images/minion.png)","title":"General Syntax"},{"location":"ide/markdown/#using-mkdocs-formatting","text":"Tabbed Data Inline Examples Output p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} , p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} . Markdown $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ , \\( p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } \\) . Adding Tips Inline Configuration This is an example of a tip. Make the paragragh tabbed inline with the heading Adding Danger Reminder This is a call to action Adding Note Note Adding notes Adding Summary Summary This is to sumarize the information New Information New 7.1 New Info Adding Note Collapsible tab Click Me! Thanks! Success Collapsible tab Success Content. Warning Collapsible tab Warning Content. Adding Settings Gear Basic Software Setup sudo apt install Adding multi-level collapisble tabs Details must contain a blank line before they start. Use ??? to start a details block or ???+ if you want to start a details block whose default state is 'open'. Follow the start of the block with an optional class or classes (separated with spaces) and the summary contained in quotes. Content is placed below the header and must be indented. Open styled details Nested details! And more content again. Adding checklist inside summary Tasklist eggs bread milk Adding strikethrough and subscript Tilde Tilde is syntactically built around the ~ character. It adds support for inserting sub scripts and adds an easy way to place text in a < del > tag. Showing Critic changes Critic Added CSS changes in extra.css to activate Critic change higlights. This is deleted This is added Showing Emojis Inline Code Highlighting InlineHilite utilizes the following syntax to insert inline highlighted code: `:::language mycode` or `#!language mycode` . Inline Highlighted Code Example Here is some code: import pymdownx ; pymdownx . __version__ . The mock shebang will be treated like text here: #!js var test = 0; . Marking words Mark Example mark me Preserve Tab spaces ============================================================ T Tp Sp D Dp S D7 T ------------------------------------------------------------ A F#m Bm E C#m D E7 A A# Gm Cm F Dm D# F7 A# B\u266d Gm Cm F Dm E\u266dm F7 B\u266d Showing Line Number in code import foo.bar import car - Highlighting specific line numbers \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz Highlighting line range and specific lines import foo import boo.baz import foo.bar.baz class Foo : def __init__ ( self ): self . foo = None self . bar = None self . baz = None","title":"Using Mkdocs formatting"},{"location":"k8s/","text":"Kubernetes \u00b6","title":"Kubernetes"},{"location":"k8s/#kubernetes","text":"","title":"Kubernetes"},{"location":"k8s/1-tips/","text":"CNCF Tips # For Windows: Ctrl+Insert to copy and Shift+Insert to paste. # In addition, you might find it helpful to use the Notepad (see top menu under 'Exam Controls') to manipulate text before pasting to the command line. # Only a single terminal console is available during the exam. Terminal multiplexers such as GNU Screen and tmux can be used to create virtual consoles. # You can switch the cluster/configuration context using a command such as the following: kubectl config use-context <cluster/context name> # Nodes making up each cluster can be reached via ssh, using a command such as the following: ssh <nodename> # You can assume elevated privileges on any node by issuing the following command: sudo -i # You can also use `sudo` to execute commands with elevated privileges at any time # You must return to the base node (`hostname node-1`) after completing each task. # When you want to get some data quickly from a node without getting a shell ssh <nodename> < command to execute> # This will give you the output instead of changing nodes # You can use kubectl and the appropriate context to work on any cluster from the base node. # When connected to a cluster member via ssh, you will only be able to work on that particular cluster via kubectl. # For your convenience, all environments, in other words, the base system and the cluster nodes, have the following additional command-line tools pre-installed and pre-configured: # - kubectl with k alias and Bash autocompletion # - jq for YAML/JSON processing # - tmux for terminal multiplexing # - curl and wget for testing web services # - man and man pages for further documentation # Where no explicit namespace is specified, the default namespace should be acted upon. # If you need to destroy/recreate a resource to perform a certain task, it is your responsibility to back up the resource definition appropriately prior to destroying the resource. # Dont waste time waiting for prompt to come back when deleting pods, use the --force flag <Careful> Shortcuts po Pod rs ReplciaSet deploy Deployment svc Service ns Namespace netpol Network Policy pv Persistent Volume pvc Persistent Volume Claims sa Service Account Implicit Commands # Do not copy paste from documentation into OS Editor. Copy into Notepad and then make changes. [Not recommended] to avoid wasting time in formatting. Type in Editor directly or use implicit commands # Use Nano as preferred editor KUBE_EDITOR = nano kubectl edit deploy nginx # Nano Editor Shortcuts # Set Context and Namespace kubectl config set-context <cluster> --namespace = myns # Copy this command in notepad and save time # Explain Commands # When is it useful: sometimes when editing/creating yaml files, it is not clear where exaclty rsource should be placed (indented) in the file. Using this command gives a quick overview of resources structure as well as helpful explanation. Sometimes this is faster then looking up in k8s docs. kubectl explain cronjob.spec.jobTemplate --recursive | less kubectl explain pods.spec.containers --recursive | less # Generators --restart --dry-run kubectl run nginx --image = nginx # (deployment) kubectl run nginx --image = nginx --restart = Never # (pod) kubectl run nginx --image = nginx --restart = OnFailure #(job) kubectl run nginx --image = nginx --restart = OnFailure \\ --schedule = \"* * * * *\" # (cronJob) kubectl run nginx -image = nginx \\ --restart = Never \\ --port = 80 \\ --namespace = myname \\ --command --serviceaccount = mysa1 \\ --env = HOSTNAME = local \\ --labels = bu = finance,env = dev \\ --requests = 'cpu=100m,memory=256Mi' \\ --limits = 'cpu=200m,memory=512Mi' \\ --dry-run -o yaml - /bin/sh -c 'echo hello world' > pod.yaml # If we specify two selectors separated by a comma, only the objects that satisfy both will be returned. This is a logical AND operation: kubectl get pods --selector = \"bu=finance,env=dev\" # We can also ask if a label is one of a set of values. Here we ask for all pods where the app label is set to alpacaor bandicoot (which will be all six pods): kubectl get pods --selector = \"env in (dev,test)\" # Check last 10 events on pod k describe pod <pod-name> | grep -i events -A 10 # Determine proper api_group/version for a resource # When is it useful: after creating/modyfing pod or during troubleshooting exercise check quickly if there are no errors in pod k api-resources | grep -i \"resource name\" k api-versions | grep -i \"api_group name\" # Example: k api-resources | grep -i deploy # -> produces apps in APIGROUPS collumn k api-versions | grep -i apps # -> produces apps/v1 # Quickly find kube api server setting # When is it useful: since on all the exams, kubernetes services are running as pods, it is faster to check settings with grep rather than move to folder and look at the file. ps -ef --forest | grep kube-apiserver | grep \"search string\" # Example: ps -ef --forest | grep kube-apiserver | grep admission-plugins # -> find admission plugins config kubectl run frontend --replicas = 2 --labels = run = load-balancer-example --image = busybox --port = 8080 kubectl expose deployment frontend --type = NodePort --name = frontend-service --port = 6262 --target-port = 8080 kubectl set serviceaccount deployment frontend myuser kubectl create service clusterip my-cs --tcp = 5678 :8080 --dry-run -o yaml # Use busybox for running utilities # When is it useful: this command will create temporary busybox pod. Full features of Busybox - https://busybox.net/downloads/BusyBox.html kubectl run -it --rm debug --image = busybox --restart = Never -- sh # Verify pod connectivity # When it is useful: when making changes to a pod, it is very important to veryify if it works. One of the best wayst to verify is to check pod connectivity. If succesfull this command will return a response. kubectl run -it --rm debug --image = radial/busyboxplus:curl --restart = Never -- curl http://servicename # There is no way to add environment variable from a Secret or ConfigMap imperatively from CLI. So use the `--env SOMETHING=this --dry-run -o yaml` to generate a quick template then vim edit it to match the desired configuration. This is very useful considering you cannot copy-paste a whole yaml from documentation to the exam terminal. # Create k8s resource on the fly from copied YAML # When is it useful: sometimes it's quicker to just grab YAML from k8s documentation page and create a resource much quicker than writting YAML yourself cat <<EOF | kubectl create -f - <YAML content goes here> EOF # Command alternative: alternatively use cat > filename.yaml [ enter ] [ Ctrl + Shift - to paste file content ] [ enter - adds one line to the file ] [ Ctrl + C - exit ] # after that use vim/nano to edit the file and create resource based on it # Save time on editing and re-creating running pods. # During the exams you are often asked to change existing pod spec. This usually requires: # 1. saving pod config as yaml file kubectl get po <pod name> <optional -n namespace> -o yaml > <filename>.yaml # check if file was correctly saves cat <filename>.yaml # 2. deleting existing pod kubectl delete po <pod name> <optional -n namespace> --wait = false # 3. editing the file and making required changes vim <or nano> <filename>.yaml # 4. creating new pod from the file kubectl create -f <filename>.yaml # The other template to remember is that of a Pod. It is especially useful for creating Static Pods on other Nodes. # Do not skip the part about jsonpath thinking that it is too easy to come in the exam. Remember that jsonpath is required for sorting output and custom columns. # *** Important - Store the YAML files in home folder wth question number, so in case of review, you can verify and apply it say in correct namespace # Unix Bash one-liners #if-else a = 10 ; b = 5 ; if [ $a -le $b ] ; then echo \"a is small\" ; else echo \"b is small\" ; fi # while x = 1 ; while [ $x -le 10 ] ; do echo \"welcome $x times\" ; x = $(( x+1 )) ; done # for PODS = $( kubectl get pods -o jsonpath -template = '{.items[*].metadata.name}' for x in $PODS ; do kubectl delete pods ${ x } sleep 60 done # Examples args: [ \"-c\" , \"while true;do date >> /var/log/app.txt;sleep 5;done\" ] args: [ /bin/sh, -c, 'i=0; while true; do echo \"$i:$(date)\";i=$((i+1))\";sleep 1;done' ] args: [ \"-c\" , \"mkdir -p collect;while true;do cat /var/data/* > /collect/data.txt;sleep 5;done\" ] # Use of grep when selector filter doesnt work kubectl describe pods | grep --context = 10 annotations: kubectl describe pods | grep --context = 10 Events: # This command will ensure that you set the namespace correctly for your current context. kubectl config view --minify | grep namespace # Aliases ######## Choose any style # Step 1: enabled auto-complete feature in the bash shell after setting the aliases source < ( kubectl completion bash ) echo \"source <(kubectl completion bash)\" >> ~/.bashrc complete -F __start_kubectl k alias k = kubectl alias kr = 'k run' alias krun = \"k run -h | grep '# ' -A2\" alias kg = 'k get' alias kd = 'k describe' alias kaf = 'k apply -f' alias kdf = 'k delete -f' alias kdp = 'k delete po' alias kgp = 'k get po' # if you\u2019re sure that the Pod that you\u2019re deleting has no Persistent Volumes attached to it (or other resources external to the Pod), then use below command alias kpd = 'k delete pod --force --grace-period=0' # Step 2: create short cut using env variable export dr = \"--dry-run=client -o yaml\" # Using k run mypod --image=nginx $dr > mypod.yaml # Step 3: To avoid typing namespaces in the imperative command, set it once and then type all commands k config get-contexts # Copy paste the set-context and use-context alias kns = \"kubectl config set-context --current --namespace\" # Set the alias as above # Set the namespace where you want to execute the commands, like in myns kns myns # Tip: always remember to switch back to default namespace for the next question kns default # Changing namespaces # Suppose your have namespaces test1, test2, test3 kubectl -n test1 get pods # Now run the last command but in different namespace like test2 ^test1^test2 # Type the above command and hit enter, Linux will substitute the namespace and rerun the get pods command ################# alias k = 'kubectl' alias kc = 'k config view --minify | grep name' # Many pods are created to save time when you go wrong during the exam alias kpd = 'k delete pod --force --grace-period=0' alias kdp = 'kubectl describe pod' alias krh = 'kubectl run --help | more' alias ugh = 'kubectl get --help | more' alias c = 'clear' alias kd = 'kubectl describe pod' alias ke = 'kubectl explain' alias kf = 'kubectl create -f' alias kg = 'kubectl get pods --show-labels' alias kr = 'kubectl replace -f' alias kh = 'kubectl --help | more' alias krh = 'kubectl run --help | more' alias ks = 'kubectl get namespaces' alias l = 'ls -lrt' alias ll = 'vi ls -rt | tail -1' alias kga = 'k get pod --all-namespaces' alias kgaa = 'kubectl get all --show-labels' # Create file with implicit commands kubectl run busybox --image = busybox --dry-run = client -o yaml --restart = Never > yamlfile.yaml kubectl create job my-job --dry-run = client -o yaml --image = busybox -- date > yamlfile.yaml kubectl get -o yaml deploy/nginx > 1 .yaml kubectl run wordpress --image = wordpress \u2013-expose \u2013-port = 8989 --restart = Never -o yaml kubectl run test --image = busybox --restart = Never --dry-run = client -o yaml -- bin/sh -c 'echo test;sleep 100' > yamlfile.yaml # (Notice that --bin comes at the end. This will create yaml file.) kubectl run busybox --image = busybox --dry-run = client -o yaml --restart = Never -- /bin/sh -c \"while true; do echo hello; echo hello again;done\" > yamlfile.yaml # Viewing resource utilization kubectl top node kubectl top pod watch kubectl top node -n 5 # Runs the command every 5 sec - Linux Hard Way Mummshad - Kubernetes the Hard Way Tools used \u00b6 vim kubectl kubeadm systemctl & journalctl nslookup Vim \u00b6 Achieving comfort with the editor helps you focus on solving the K8s exam challenges VIM Tutorial # Vim config to be set in the exam terminal echo \"set ts=2 sts=2 sw=2 et number ai\" >> ~/.vimrc source ~/.vimrc ################ # These stand for: # # ts - tabstop to indent using 2 spaces on pressing tab key # sts - softtabstop to move 2 cursor spaces on pressing tab key # sw - shiftwidth to shift by 2 spaces on pressing tab key # et - expandtab to insert space character instead of tab on pressing tab key # number for line numbers while editing # There is one additional useful config - ai to allow autoindent on pressing return key (but this messes when # copy pasting text) Shortcuts <linenumber>G # Go to line number :set paste # Tells vim to shutdown autoindent and makes it redy for paste # Say you copy paste in wrong position and want to tab all the lines shift + v + arrow up or down # selects all the lines for movement <number of places to indent>+> # 2> will indent by 2 places all the selected lines # Similarly to unindent 2< # Say when copying from html you also copy hidden Tab characters :set list # Shows all hidden Tab characters. ^I is for Tab :retab # Fix this issue by replacing ^I with spaces # Handle multiline values like certificate data which spans multiple lines in YAML without leaving the editor # Open the file and go to the line where you want to add this data. # Add a | character to tell YAML this is a multi line value like request: | and Press enter :read !base64 certificate.pem # This will invoke base64 command on data from .pem file and read it into the next line. # After the data is copied, indent 2> after selecting the entire contents. General Tips \u00b6 Flag Questions You Can\u2019t Immediately Answer The exam environment comes with built-in flagging functionality. If you read a question that you don\u2019t immediately know how to answer, flag it. Flagged questions will be highlighted in your question list, allowing you to return to them quickly once you\u2019ve finished answering the ones you\u2019re comfortable with. Keep Score Along with flagging capabilities, the exam provides you with a built-in notepad. Make use of the notepad to keep a running tally of questions you\u2019ve answered and their percentage value (this number is given to you with each question). Format of scoring Question Number %weight Total I like to keep a list of the questions I haven\u2019t answered and their associated percentages too. It\u2019s a good way to inform how you should prioritise your time, particularly towards the end of the exam. If you're cutting it fine with the 74% pass mark, your time is probably better spent on a 7% question, than a 2% one. Save YAML Files by Question Number If you have to create a YAML file when answering a question, for example a Pod config file, make sure you name the file according to the question number. I rely heavily on a solid muscle memory for all commands. The most atomic part of a cluster is a Pod so creating one through kubectl run --generator=run-pod/v1 nginx --image nginx should be out under 10 seconds or so. Even though the exam allows you to copy-paste content from the official documentation, it has a limit of 2-3 lines so I had to be prepared to \u2013dry-run -o yaml > q2.yaml each time and edit the file as per the question. Useful commands # list running processes $ ps -aux # search for string in the output $ ps -aux | grep -i 'string' # search for multiple expressions in the output (exp can be plain a string too) $ ps -aux | grep -e 'exp-one' -e 'exp-two' # get details about network interfaces $ ifconfig # list network interfaces and their IP address $ ip a # get the route details $ ip r # check service status and also show logs $ systemctl status kubelet # restart a service $ systemctl restart kubelet # reload the service daemon, if you changed the service file $ systemctl daemon reload # detailed logs of the service $ journalctl -u kubelet # list out ports, protocol and what processes are listening on those ports $ netstat -tunlp Troubleshooting Tips \u00b6 What is the scope of the issue? > Entire Custer > User > Pod > Service How Long the issue has been going on? What can be done to reproduce the issue? Establish a Probable cause. Develop a Hypothesis \u2192 > Clue 1 > + Clue 2 > + Kubernetes Knowledge ======================== Probable Cause Experiment - Test your Hypothesis","title":"1 tips"},{"location":"k8s/1-tips/#tools-used","text":"vim kubectl kubeadm systemctl & journalctl nslookup","title":"Tools used"},{"location":"k8s/1-tips/#vim","text":"Achieving comfort with the editor helps you focus on solving the K8s exam challenges VIM Tutorial # Vim config to be set in the exam terminal echo \"set ts=2 sts=2 sw=2 et number ai\" >> ~/.vimrc source ~/.vimrc ################ # These stand for: # # ts - tabstop to indent using 2 spaces on pressing tab key # sts - softtabstop to move 2 cursor spaces on pressing tab key # sw - shiftwidth to shift by 2 spaces on pressing tab key # et - expandtab to insert space character instead of tab on pressing tab key # number for line numbers while editing # There is one additional useful config - ai to allow autoindent on pressing return key (but this messes when # copy pasting text) Shortcuts <linenumber>G # Go to line number :set paste # Tells vim to shutdown autoindent and makes it redy for paste # Say you copy paste in wrong position and want to tab all the lines shift + v + arrow up or down # selects all the lines for movement <number of places to indent>+> # 2> will indent by 2 places all the selected lines # Similarly to unindent 2< # Say when copying from html you also copy hidden Tab characters :set list # Shows all hidden Tab characters. ^I is for Tab :retab # Fix this issue by replacing ^I with spaces # Handle multiline values like certificate data which spans multiple lines in YAML without leaving the editor # Open the file and go to the line where you want to add this data. # Add a | character to tell YAML this is a multi line value like request: | and Press enter :read !base64 certificate.pem # This will invoke base64 command on data from .pem file and read it into the next line. # After the data is copied, indent 2> after selecting the entire contents.","title":"Vim"},{"location":"k8s/1-tips/#general-tips","text":"Flag Questions You Can\u2019t Immediately Answer The exam environment comes with built-in flagging functionality. If you read a question that you don\u2019t immediately know how to answer, flag it. Flagged questions will be highlighted in your question list, allowing you to return to them quickly once you\u2019ve finished answering the ones you\u2019re comfortable with. Keep Score Along with flagging capabilities, the exam provides you with a built-in notepad. Make use of the notepad to keep a running tally of questions you\u2019ve answered and their percentage value (this number is given to you with each question). Format of scoring Question Number %weight Total I like to keep a list of the questions I haven\u2019t answered and their associated percentages too. It\u2019s a good way to inform how you should prioritise your time, particularly towards the end of the exam. If you're cutting it fine with the 74% pass mark, your time is probably better spent on a 7% question, than a 2% one. Save YAML Files by Question Number If you have to create a YAML file when answering a question, for example a Pod config file, make sure you name the file according to the question number. I rely heavily on a solid muscle memory for all commands. The most atomic part of a cluster is a Pod so creating one through kubectl run --generator=run-pod/v1 nginx --image nginx should be out under 10 seconds or so. Even though the exam allows you to copy-paste content from the official documentation, it has a limit of 2-3 lines so I had to be prepared to \u2013dry-run -o yaml > q2.yaml each time and edit the file as per the question. Useful commands # list running processes $ ps -aux # search for string in the output $ ps -aux | grep -i 'string' # search for multiple expressions in the output (exp can be plain a string too) $ ps -aux | grep -e 'exp-one' -e 'exp-two' # get details about network interfaces $ ifconfig # list network interfaces and their IP address $ ip a # get the route details $ ip r # check service status and also show logs $ systemctl status kubelet # restart a service $ systemctl restart kubelet # reload the service daemon, if you changed the service file $ systemctl daemon reload # detailed logs of the service $ journalctl -u kubelet # list out ports, protocol and what processes are listening on those ports $ netstat -tunlp","title":"General Tips"},{"location":"k8s/1-tips/#troubleshooting-tips","text":"What is the scope of the issue? > Entire Custer > User > Pod > Service How Long the issue has been going on? What can be done to reproduce the issue? Establish a Probable cause. Develop a Hypothesis \u2192 > Clue 1 > + Clue 2 > + Kubernetes Knowledge ======================== Probable Cause Experiment - Test your Hypothesis","title":"Troubleshooting Tips"},{"location":"k8s/2-exam/","text":"CKA exam testing \u00b6 Preparation \u00b6 Q: Create a Job that run 60 time with 2 jobs running in parallel https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ Q: Find which Pod is taking max CPU Use kubectl top to find CPU usage per pod Q: List all PersistentVolumes sorted by their name Use kubectl get pv --sort-by= <- this problem is buggy & also by default kubectl give the output sorted by name. Q: Create a NetworkPolicy to allow connect to port 8080 by busybox pod only https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ Make sure to use apiVersion: extensions/v1beta1 which works on both 1.6 and 1.7 Q: fixing broken nodes, see https://kubernetes.io/docs/concepts/architecture/nodes/ Q: etcd backup, see https://kubernetes.io/docs/getting-started-guides/ubuntu/backups/ https://www.mirantis.com/blog/everything-you-ever-wanted-to-know-about-using-etcd-with-kubernetes-v1-6-but-were-afraid-to-ask/ Q: TLS bootstrapping, see https://coreos.com/kubernetes/docs/latest/openssl.html https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/ cloudflare/cfssl Q: You have a Container with a volume mount. Add a init container that creates an empty file in the volume. (only trick is to mount the volume to init-container as well) https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo The app is running! && sleep 3600'] volumeMounts: - mountPath: /cache name: cache-volume initContainers: - name: init-touch-file image: busybox volumeMounts: - mountPath: /data name: cache-volume command: ['sh', '-c', 'echo \"\" > /data/harshal.txt'] volumes: - name: cache-volume emptyDir: {} ```` Q: When running a redis key-value store in your pre-production environments many deployments are incoming from CI and leaving behind a lot of stale cache data in redis which is causing test failures. The CI admin has requested that each time a redis key-value-store is deployed in staging that it not persist its data. Create a pod named non-persistent-redis that specifies a named-volume with name app-cache, and mount path /data/redis. It should launch in the staging namespace and the volume MUST NOT be persistent. Create a Pod with EmptyDir and in the YAML file add namespace: CI Q: Setting up K8s master components with a binaries/from tar balls: Also, convert CRT to PEM: openssl x509 -in abc.crt -out abc.pem - https://coreos.com/kubernetes/docs/latest/openssl.html - https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md - https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/08-bootstrapping-kubernetes-controllers.md - https://gist.github.com/mhausenblas/0e09c448517669ef5ece157fd4a5dc4b - https://kubernetes.io/docs/getting-started-guides/scratch/ - http://alexander.holbreich.org/kubernetes-on-ubuntu/ maybe dashboard? - https://kubernetes.io/docs/getting-started-guides/binary_release/ - http://kamalmarhubi.com/blog/2015/09/06/kubernetes-from-the-ground-up-the-api-server/ Q: Find the error message with the string \u201cSome-error message here\u201d. https://kubernetes.io/docs/concepts/cluster-administration/logging/ see kubectl logs and /var/log for system services Q 17: Create an Ingress resource, Ingress controller and a Service that resolves to cs.rocks.ch. First, create controller and default backend ```BASH kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress/master/controllers/nginx/examples/default-backend.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/nginx-ingress-controller.yaml Second, create service and expose ``` kubectl run ingress-pod \u2013image=nginx \u2013port 80 kubectl expose deployment ingress-pod \u2013port=80 \u2013target-port=80 \u2013type=NodePort Create the ingress ``` cat <<EOF >ingress-cka.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-service spec: rules: - host: \"cs.rocks.ch\" http: paths: - backend: serviceName: ingress-pod servicePort: 80 EOF To test, run a curl pod kubectl run -i --tty client --image=tutum/curl curl -I -L --resolve cs.rocks.ch:80:10.240.0.5 http://cs.rocks.ch/ Q: Run a Jenkins Pod on a specified node only. https://kubernetes.io/docs/tasks/administer-cluster/static-pod/ Create the Pod manifest at the specified location and then edit the systemd service file for kubelet(/etc/systemd/system/kubelet.service) to include --pod-manifest-path=/specified/path . Once done restart the service. Q: Use the utility nslookup to look up the DNS records of the service and pod. From this guide, https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ Look for \u201cQuick Diagnosis\u201d $ kubectl exec -ti busybox \u2013 nslookup mysvc.myns.svc.cluster.local Naming conventions for services and pods: For a regular service, this resolves to the port number and the CNAME: my-svc.my-namespace.svc.cluster.local. For a headless service, this resolves to multiple answers, one for each pod that is backing the service, and contains the port number and a CNAME of the pod of the form auto-generated-name.my-svc.my-namespace.svc.cluster.local When enabled, pods are assigned a DNS A record in the form of pod-ip-address.my-namespace.pod.cluster.local. For example, a pod with IP 1.2.3.4 in the namespace default with a DNS name of cluster.local would have an entry: 1-2-3-4.default.pod.cluster.local Q: Start a pod automatically by keeping manifest in /etc/kubernetes/manifests Refer to https://kubernetes.io/docs/tasks/administer-cluster/static-pod/ Edit kubelet.service on any worker node to contain this flag \u2013pod-manifest-path=/etc/kubernetes/manifests then place the pod manifest at /etc/kubernetes/manifests. Now restart kubelet. Some other Questions: Main container looks for a file and crashes if it doesnt find the file. Write an init container to create the file and make it available for the main container Install and Configure kubelet on a node to run pod on that node without contacting the api server Take backup of etcd cluster rotate TLS certificates 5.rolebinding 6.Troubleshooting - involved identifying failing nodes, pods , services and identifying cpu utilization of pods. General Questions \u00b6 Backup/restore etcd on specific location using certificates. Fix the broker node. You should check kubelet process. Mostly you have to start and enable for permanent change. Create network policy to allow incoming connection from specific namespace , port combination. They might ask from specific pods. Two questions related to jsonpath , you can output the resource in json format and then find the details. They ask to create ingress . Please note ingress controller was already there. One question related to sidecar container. Trick is you have to mount the volume of on the side container. Question related to which pod consume most cpu. You don\u2019t need to install metrics server.","title":"CKA exam testing"},{"location":"k8s/2-exam/#cka-exam-testing","text":"","title":"CKA exam testing"},{"location":"k8s/2-exam/#preparation","text":"Q: Create a Job that run 60 time with 2 jobs running in parallel https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ Q: Find which Pod is taking max CPU Use kubectl top to find CPU usage per pod Q: List all PersistentVolumes sorted by their name Use kubectl get pv --sort-by= <- this problem is buggy & also by default kubectl give the output sorted by name. Q: Create a NetworkPolicy to allow connect to port 8080 by busybox pod only https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ Make sure to use apiVersion: extensions/v1beta1 which works on both 1.6 and 1.7 Q: fixing broken nodes, see https://kubernetes.io/docs/concepts/architecture/nodes/ Q: etcd backup, see https://kubernetes.io/docs/getting-started-guides/ubuntu/backups/ https://www.mirantis.com/blog/everything-you-ever-wanted-to-know-about-using-etcd-with-kubernetes-v1-6-but-were-afraid-to-ask/ Q: TLS bootstrapping, see https://coreos.com/kubernetes/docs/latest/openssl.html https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/ cloudflare/cfssl Q: You have a Container with a volume mount. Add a init container that creates an empty file in the volume. (only trick is to mount the volume to init-container as well) https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo The app is running! && sleep 3600'] volumeMounts: - mountPath: /cache name: cache-volume initContainers: - name: init-touch-file image: busybox volumeMounts: - mountPath: /data name: cache-volume command: ['sh', '-c', 'echo \"\" > /data/harshal.txt'] volumes: - name: cache-volume emptyDir: {} ```` Q: When running a redis key-value store in your pre-production environments many deployments are incoming from CI and leaving behind a lot of stale cache data in redis which is causing test failures. The CI admin has requested that each time a redis key-value-store is deployed in staging that it not persist its data. Create a pod named non-persistent-redis that specifies a named-volume with name app-cache, and mount path /data/redis. It should launch in the staging namespace and the volume MUST NOT be persistent. Create a Pod with EmptyDir and in the YAML file add namespace: CI Q: Setting up K8s master components with a binaries/from tar balls: Also, convert CRT to PEM: openssl x509 -in abc.crt -out abc.pem - https://coreos.com/kubernetes/docs/latest/openssl.html - https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md - https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/08-bootstrapping-kubernetes-controllers.md - https://gist.github.com/mhausenblas/0e09c448517669ef5ece157fd4a5dc4b - https://kubernetes.io/docs/getting-started-guides/scratch/ - http://alexander.holbreich.org/kubernetes-on-ubuntu/ maybe dashboard? - https://kubernetes.io/docs/getting-started-guides/binary_release/ - http://kamalmarhubi.com/blog/2015/09/06/kubernetes-from-the-ground-up-the-api-server/ Q: Find the error message with the string \u201cSome-error message here\u201d. https://kubernetes.io/docs/concepts/cluster-administration/logging/ see kubectl logs and /var/log for system services Q 17: Create an Ingress resource, Ingress controller and a Service that resolves to cs.rocks.ch. First, create controller and default backend ```BASH kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress/master/controllers/nginx/examples/default-backend.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/nginx-ingress-controller.yaml Second, create service and expose ``` kubectl run ingress-pod \u2013image=nginx \u2013port 80 kubectl expose deployment ingress-pod \u2013port=80 \u2013target-port=80 \u2013type=NodePort Create the ingress ``` cat <<EOF >ingress-cka.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-service spec: rules: - host: \"cs.rocks.ch\" http: paths: - backend: serviceName: ingress-pod servicePort: 80 EOF To test, run a curl pod kubectl run -i --tty client --image=tutum/curl curl -I -L --resolve cs.rocks.ch:80:10.240.0.5 http://cs.rocks.ch/ Q: Run a Jenkins Pod on a specified node only. https://kubernetes.io/docs/tasks/administer-cluster/static-pod/ Create the Pod manifest at the specified location and then edit the systemd service file for kubelet(/etc/systemd/system/kubelet.service) to include --pod-manifest-path=/specified/path . Once done restart the service. Q: Use the utility nslookup to look up the DNS records of the service and pod. From this guide, https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ Look for \u201cQuick Diagnosis\u201d $ kubectl exec -ti busybox \u2013 nslookup mysvc.myns.svc.cluster.local Naming conventions for services and pods: For a regular service, this resolves to the port number and the CNAME: my-svc.my-namespace.svc.cluster.local. For a headless service, this resolves to multiple answers, one for each pod that is backing the service, and contains the port number and a CNAME of the pod of the form auto-generated-name.my-svc.my-namespace.svc.cluster.local When enabled, pods are assigned a DNS A record in the form of pod-ip-address.my-namespace.pod.cluster.local. For example, a pod with IP 1.2.3.4 in the namespace default with a DNS name of cluster.local would have an entry: 1-2-3-4.default.pod.cluster.local Q: Start a pod automatically by keeping manifest in /etc/kubernetes/manifests Refer to https://kubernetes.io/docs/tasks/administer-cluster/static-pod/ Edit kubelet.service on any worker node to contain this flag \u2013pod-manifest-path=/etc/kubernetes/manifests then place the pod manifest at /etc/kubernetes/manifests. Now restart kubelet. Some other Questions: Main container looks for a file and crashes if it doesnt find the file. Write an init container to create the file and make it available for the main container Install and Configure kubelet on a node to run pod on that node without contacting the api server Take backup of etcd cluster rotate TLS certificates 5.rolebinding 6.Troubleshooting - involved identifying failing nodes, pods , services and identifying cpu utilization of pods.","title":"Preparation"},{"location":"k8s/2-exam/#general-questions","text":"Backup/restore etcd on specific location using certificates. Fix the broker node. You should check kubelet process. Mostly you have to start and enable for permanent change. Create network policy to allow incoming connection from specific namespace , port combination. They might ask from specific pods. Two questions related to jsonpath , you can output the resource in json format and then find the details. They ask to create ingress . Please note ingress controller was already there. One question related to sidecar container. Trick is you have to mount the volume of on the side container. Question related to which pod consume most cpu. You don\u2019t need to install metrics server.","title":"General Questions"},{"location":"k8s/3-commands/","text":"Useful subcommand \u00b6 kubectl set # k set --help will tell you what parameters can be set kubectl label kubectl scale kubectl edit kns default # alias to set context to default kubectl create deployment examplehttpapp --image = katacoda/docker-http-server --replicas = 2 kubectl get deployments kubectl get pods -o wide kubectl get pods -L labels # L adds a custom column called labels in tablular output # Set # Changing images on both the pods at the same time kubectl set image deploy examplehttpapp * = nginx:1.19 # Check if image has been updated kubectl describe po <podname> | grep -i image # -i for insensitive search kubectl describe po <podname> | grep -i image -A 2 # -A for showing next 2 lines 'A'fter match kubectl create ns testns kns testns # alias to set context to testns kubectl create deployment namespacedeg -n testns --image = katacoda/docker-http-server kubectl get pods -n testns # Scaling kns default # alias to set context to default kubectl scale deployment examplehttpapp --replicas = 5 --record # record is important for scaling kubectl --record = true set image deployment examplehttpapp docker-http-server = katacoda/docker-http-server:v2 kubectl rollout status deployment examplehttpapp # It will show 2 change cause records kubectl rollout undo deployment examplehttpapp # Incase you want to undo the last change # Labels # Edit kubectl edit deploy examplehttpapp # Change the image, another way to do instead of set kubectl expose deployment examplehttpapp --port 80 kubectl get svc -o wide kubectl describe svc examplehttpapp # Incase you just want to see the labels on the svc, instead of describe kubectl get svc --show-labels kubectl get services -l app = examplehttpapp -o go-template = '{{(index .items 0).spec.clusterIP}}' curl $( kubectl get services -l app = examplehttpapp -o go-template = '{{(index .items 0).spec.clusterIP}}' ) kubectl logs $( kubectl get pods -l app = examplehttpapp -o go-template = '{{(index .items 0).metadata.name}}' ) Create a temporary Pod and execute the wget command inside of its container using the IP address and the container port. kubectl run busybox --image = busybox --rm -it --restart = Never -n ckad -- wget 10 .244.1.2:80 All Commands \u00b6 ################# # How to navigate through your cluster. ################# # To see only the Kubernetes kubectl client version and not the Kubernetes version kubectl version --client = true # You might need to gather information about the endpoints of the master and services in the cluster. This information will come in handy if you have to troubleshoot your cluster. kubectl cluster-info # Command autocompletion help and options kubectl completion -h # setup is using the bash shell source < ( kubectl completion bash ) # check the status of the Nodes kubectl get no # various Namespaces that are critical to Kubernetes operations kubectl get ns # look at the Pods in the cluster kubectl get po kubectl get po -n kube-system # see the Pods in all the Namespaces kubectl get pods -A # see deployments kubectl get deploy # Shows key information about the Deployment such as: # Labels, Number of Replicas, Annotations, Deployment Strategy Type, Events kubectl describe deploy nginx # Get additional information on the Pods kubectl get pod -o wide # YAML output format kubectl get pod -o yaml # sort the output of queries, you can use the --sort-by flag # sort by various data points for pod specs like # Pod Ip, Pod name, Pod nodeName, Pod hostname, Pod volumes kubectl get pod -o wide --sort-by = .status.podIP kubectl get pod -o wide --sort-by = .spec.podName # Get the documentation for Kubernetes resources such as Pods or Services: kubectl explain deployment # View all the supported resource types: kubectl api-resources # View all the resources kubectl get all ################# # How to switch between contexts and namespaces # How to find resources and format their output # How to update, patch, delete, and scale resources ################# # View kubectl configuration kubectl view config # list of all our configured clusters kubectl config get-clusters # From above 2 commands, you can see for example - there is currently one cluster named kubernetes and one user named kubernetes-admin. # Same can be seen in one line with below command. In this case, the context is kubernetes-admin@kubernetes. kubectl config current-context # create a new kubectl context using the existing kubernetes-admin user. kubectl config set-context dev-context --cluster kubernetes --user = kubernetes-admin kubectl config get-contexts # Notice that the current active context is set to kubernetes-admin@kubernetes # To switch to the dev-context context kubectl config use-context dev-context # Let's switch back to the kubernetes-admin@kubernetes context kubectl config use-context kubernetes-admin@kubernetes # As you can see, switching back and forth between contexts with the above kubectl command can be very tedious especially when dealing with multiple contexts. # The alternative would be to install kubectx. # When the installion is done, type the kubectx command to list all the contexts: kubectx # Notice that the current active context kubernetes-admin@kubernetes is highlighted in yellow # To switch between contexts, you can now type kubectx CONTEXT NAME. kubectx dev-context # Go ahead and switch back to the kubernetes-admin@kubernetes context: kubectx kubernetes-admin@kubernetes # to create the namespace: kubectl create ns frontend # Declarative method kubectl create namespace backend -o yaml --dry-run = client > ns-backend.yaml kubectl apply -f ns-backend.yaml # deploy a single redis container kubectl run redis --image = quay.io/quay/redis -n backend kubectl run nginx --image = quay.io/bitnami/nginx -n frontend kubectl get pods -n frontend # Switch to the appropriate namespace context where the resources live without having to specify ns kubectl config set-context --current --namespace = frontend # If you are constantly switching between namespaces and want to avoid using the long kubectl command above, then the kubens plugin becomes handy. # Let's list all the namespaces using the kubens command. kubens # Notice again, the current active namespace default is highlighted in yellow. # let's switch to the frontend namespace kubens frontend # switch back to the default namespace: kubens default # You can use the --selector flag to filter and find resources based on their assigned labels. # Use the deployment yaml mentioned below kubectl create -f ~/label-deploy.yaml kubectl get pods -n frontend --show-labels # find all the pods that have the label app: web in the frontend namespace: kubectl get pods -n frontend --selector = app = web # You can also use the -l flag, which represents label and is equivalent to the --selector flag. kubectl get pods -n frontend -l app = haproxy # let's find nodes within our cluster that do NOT have the taint label: node-role.kubernetes.io/master kubectl get nodes --selector = '!node-role.kubernetes.io/master' # As you can see, the --selector or -l flags could come in very handy when identifying thousands of kubernetes resources with differing labels. # Use jsonpath to find/filter resources # The -o=jsonpath flag with the kubectl command allows you to filter resources and display them in the way you desire. # Let's say we want to find the names of all the kubernetes nodes along with their CPU resources. kubectl get nodes -o = jsonpath = '{.items[*].metadata.name} {.items[*].status.capacity.cpu}' # As you may notice, the output does not look pretty. What if we add a \\n (newline character) between the two JSONPath pairs as: kubectl get nodes -o = jsonpath = '{.items[*].metadata.name}{\"\\n\"}{.items[*].status.capacity.cpu}{\"\\n\"}' # we wanted to get an output that is formatted as the output shown below: # master 2 # node01 4 # To achieve this, we would use the range JSONPath operator to iterate through each item (nodes in this case) and use tabulation \\t as well as new line \\n characters to achieve the desired output. # To do this in JSONPath, we would use the range and end operators kubectl get nodes -o = jsonpath = '{range .items[*]}{.metadata.name}{\"\\t\"}{.status.capacity.cpu}{\"\\n\"}{end}' # Format output with custom-columns # we want to get all nodes within our cluster and nicely format the output with a column header called NAME. kubectl get nodes -o = custom-columns = NAME:.metadata.name # You can add additional columns to the above command by adding JSONPath pairs (COLUMN HEADER:.metadata) separated by a comma. kubectl get nodes -o = custom-columns = NAME:.metadata.name,CPU:.status.capacity.cpu # let's find all the pods that were deployed and output them in a tabulated format with column headers POD_NAME and IMAGE_VER: kubectl get pods -n frontend -o custom-columns = POD_NAME:.metadata.name,IMAGE_VER:.spec.containers [ * ] .image # Scale resources kubectl create deployment nginx-deployment --image = quay.io/bitnami/nginx:1.20 kubectl scale deploy nginx-deployment --replicas = 5 # scale the deployment down to 1 replica kubectl scale deploy/nginx-deployment --replicas = 1 # Update resources # we are going to update the nginx image from nginx:1.20 to nginx:1.21 with no downtime. kubectl set image deployment/nginx-deployment nginx = quay.io/bitnami/nginx:1.21 --record # watch the status of the nginx-deployment deployment's rollingUpdate changes until completion. kubectl rollout status -w deployment/nginx-deployment # output of the rollout history kubectl rollout history deployment/nginx-deployment # To undo the update kubectl rollout undo deployment/nginx-deployment # Let's change the image version back to nginx:1.21 kubectl rollout undo deployment/nginx-deployment --to-revision = 2 # Patch and label resources # Patching can be used to partially update any kubernetes resources such as nodes, pods, deployments, etc. # we are going to deploy an nginx pod with a label of env: prod kubectl run nginx --image = quay.io/bitnami/nginx --labels = env = prod kubectl get pod nginx --show-labels # let's update the label to env=dev using the patch command: kubectl patch pod nginx -p '{\"metadata\":{\"labels\":{\"env\":\"dev\"}}}' kubectl get pod nginx --show-labels # We can also use the kubectl label command to add a label, update an existing label, or delete a label. kubectl label pod nginx env = prod --overwrite # Note: the --overwrite flag is used when the label already exists. # To delete the label, append the - to env , which is the value of the label's key. # Alternatively, use the kubectl edit pod nginx command and manually edit the .metadata.label.env and save your changes. # Delete resources kubectl delete pod nginx ################# # Advanced kubeclt commands that can be used in the field as a cluster operator/administrator. # > krew a kubectl plugin manager # > Interaction with pods # kubectl logs # kubectl cp # kubectl exec # > Interacting with nodes: # kubectl taint # Pod's Tolerations # kubectl cordon/uncordon # kubectl drain # kubectl top ################# # krew is a plugin manager for kubectl. # We will be using the following plugins: # access-matrix - shows an RBAC (role based access control) access matrix for server resources # ns - view or change switch namespace contexts # ctx - switch between Kubernetes cluster contexts # Let's discover some of these plugins: kubectl krew search # Install plugins via krew cat > ~/plugins <<EOF access-matrix ca-cert ctx get-all iexec images ns pod-dive pod-logs whoami who-can EOF # Install the plugins for plugin in $( cat ~/plugins ) ; do echo -en $( kubectl krew install $plugin ) ; done # Verify and list the installed plugins: kubectl krew list # You can also list the installed plugins: kubectl plugin list # We can begin by listing who the current authenticated user is: kubectl whoami # Let's also look at the who-can plugin, which is equivalent to the kubectl auth can-i VERB [TYPE/NAME]: kubectl who-can create nodes kubectl who-can '*' pods # list the namespaces: kubectl ns # Let's get the name of the first pod, assign it to a variable and run the pod-dive plugin: POD = $( kubectl get pods -o = jsonpath = '{.items[0].metadata.name}' ) && echo $POD kubectl pod-dive $POD # The above output shows a nice pod resource tree (node, namespace, type of resource, etc.). # display all the images in all namespaces: kubectl images -A # access-matrix plug-in, which is handy when looking for a RBAC Access matrix for Kubernetes resources: kubectl access-matrix # Interacting with pods # Let's switch to the kube-system namespace and access some logs: kubectl ns kube-system # Use the pod-logs plugin to get the weave pods logs: kubectl pod-logs # The pod-logs plug-in does not allow output redirection. Therefore, if you want to redirect the output use kubectl logs as such: kubectl logs POD -c CONTAINER > logsfile # kubectl exec /iexec # Let's create a single container pod called test with an nginx image: kubectl run test --image = quay.io/bitnami/nginx # Let's get the output of the date command from the running test container without logging into it: kubectl exec test -- date # Using the iexec plug-in, let's get the content of the /etc/resolv.conf/ file from the running test container: kubectl iexec test cat /etc/resolv.conf # To login and interact with the container's shell kubectl iexec test # Alternatively, you can use the below command: kubectl exec test -it -- /bin/sh # kubectl cp # The cp command can be used to copy files and directories to and from containers within a pod. # let's copy the content of the krew-install directory to the test container's /tmp directory kubectl cp ~/krew-install test:/tmp # Let's verify whether the directory has been copied. kubectl iexec test ls /tmp/krew-install # Now, let's copy the welcome.txt file from the test container to the master server's /tmp directory: kubectl cp test:/tmp/welcome.txt /tmp/welcome.txt # kubectl taint # A taint consist of a key, value, and effect. As an argument, it is expressed as key=value:effect. # The effect should be one these values: NoShedule, PreferNoSchedule, or NoExecute # Here is how it is used with the kubectl command: kubectl taint NODE NAME KEY_1 = VAL_1:TAINT_EFFECT # Let's taint node01 as dedicated to the devops-group only kubectl taint node node01 dedicated = devops-group:NoSchedule # Verify that node01 is tainted: kubectl describe node node01 | grep -i taints # You can also check the taints on all nodes: kubectl get nodes -o custom-columns = NAME:.metadata.name,TAINTS:.spec.taints [ * ] .key # Let's now try to deploy a single pod: kubectl run my-app --image = quay.io/bitnami/nginx # The newly deployed pod will be in a pending state, because it will not tolerate the taints applied to both nodes. Therefore, it will not be scheduled. # To see the error, type the below command and check under the events section: kubectl describe pod my-app # Alternatively, you can run the below command: kubectl get events # There are 2 ways to solve this issue. We can add a toleration matching the taint that was applied to the nodes, or remove the taint from the nodes. For now, let's remove the taint on node01: kubectl taint node node01 dedicated- # Note: to remove a taint, append the - to the value of the key. # And by default, the control node is tainted with node-role.kubernetes.io/master, therefore, any pod that does not have a toleration matching the node's taint cannot be deployed onto the control node. # kubectl cordon # Let's now try to get one of the pods that are deployed on node01 and assign its name to a variable: APOD = $( kubectl get pods -ojsonpath = '{.items[?(@.spec.nodeName == \"node01\")].metadata.name}' ) && echo $APOD # let's run the pod-dive plugin: kubectl pod-dive $APOD # Before we drain the node, we will cordon it first. cordon means ensuring that no pod can be scheduled on the particular node. kubectl cordon node01 # If you list the nodes now, you will find the status of node01 set to Ready,SchedulingDisabled kubectl get nodes # kubectl drain # Draining a node means removing all running pods from the node, typically performed for maintenance activities. # Open a second terminal and run the below command to watch the output in Terminal 2: watch -d kubectl get pods -o wide # Run the below command to drain node01: kubectl drain node01 --ignore-daemonsets # you will observe, how the pods in node01 are being terminated and re-deployed on the controlplane node. # Now, let's uncordon node01: kubectl uncordon node01 # In Terminal 2, you will notice that the pods have not been moved back to node01. These Pods will not be rescheduled automatically to the new nodes. # let's try to scale up the deployment to 8 replicas. kubectl scale deployment/nginx-deployment --replicas = 8 # Note: The --ignore-daemonsets flag in the kubectl drain command is required because DaemonSet pods are required to run on each node when deployed. This allows pods that are not part of a DaemonSet to be re-deployed on another available node # kubectl top # The kubectl top allows you to see the resource consumption for nodes or pods. However, in order to use the top command, we have to install a metrics server. git clone https://github.com/mbahvw/kubernetes-metrics-server.git kubectl apply -f kubernetes-metrics-server/ # Let's verify that we are getting a response from the metric server API: kubectl get --raw /apis/metrics.k8s.io/ # Let's get the CPU and memory utilization for all nodes in the cluster: kubectl top nodes # let's try to get the memory and CPU utilization of pods in all namespaces: kubectl top pods --all-namespaces # We can also gather the metrics of all the pods in the kube-system namespace: kubectl top pods -n kube-system # cat ~/label-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : frontend-deployment labels : app : web tier : frontend namespace : frontend spec : replicas : 3 selector : matchLabels : app : web template : metadata : labels : app : web spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : haproxy-deployment labels : app : haproxy tier : frontend namespace : frontend spec : replicas : 3 selector : matchLabels : app : haproxy template : metadata : labels : app : haproxy spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80 Install kubectx \u00b6 cat ~/kubectx.sh #!/bin/bash cd ~/ echo \"You are on the $PWD directory\" sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens Install Krew \u00b6 # cat ~/krew-install/install-krew.sh #!/bin/bash #Downloading krew from repo\" echo -en \"Downloading and installing krew\\n\" set -x ; cd \" $( mktemp -d ) \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/download/v0.3.4/krew.{tar.gz,yaml}\" && tar zxvf krew.tar.gz && KREW = ./krew- \" $( uname | tr '[:upper:]' '[:lower:]' ) _amd64\" && \" $KREW \" install --manifest = krew.yaml --archive = krew.tar.gz && \" $KREW \" update #Adding it to home user ~/.bashrc file echo 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' >>~/.bashrc #source the bashrc and restart bash source ~/.bashrc exec bash # Automation ideas # cat kc_step3.sh #!/bin/bash kubectl config set-context test-context --cluster kubernetes --user = kubernetes-admin for x in developers admins dbadmins ; do kubectl create namespace $x ; done cd ~/deployment kubectl create -f explore-deploy.yaml # cat kc_step4.sh #!/bin/bash kubectl ns default cd ~/deployment kubectl delete -f explore-deploy.yaml kubectl delete namespace developers kubectl delete namespace dbadmins kubectl delete namespace admins # cat explore-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : frontend-deployment labels : app : webapp tier : devops namespace : developers spec : replicas : 3 selector : matchLabels : app : webapp template : metadata : labels : app : webapp spec : containers : - name : nginx image : quay.io/bitnami/nginx:latest ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : haproxy-deployment labels : app : haproxy tier : admins namespace : admins spec : replicas : 3 selector : matchLabels : app : haproxy template : metadata : labels : app : haproxy spec : containers : - name : nginx image : quay.io/bitnami/nginx:latest ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : db-deployment labels : app : redis tier : dbadmins namespace : dbadmins spec : replicas : 3 selector : matchLabels : app : redis template : metadata : labels : app : redis spec : containers : - name : redis image : quay.io/quay/redis # Deploy pod on master node as pod taints match master node # cat nginx-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : web tier : frontend namespace : default spec : replicas : 4 selector : matchLabels : app : web template : metadata : labels : app : web spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80 tolerations : - key : \"node-role.kubernetes.io/master\" operator : Exists effect : NoSchedule","title":"Useful subcommand"},{"location":"k8s/3-commands/#useful-subcommand","text":"kubectl set # k set --help will tell you what parameters can be set kubectl label kubectl scale kubectl edit kns default # alias to set context to default kubectl create deployment examplehttpapp --image = katacoda/docker-http-server --replicas = 2 kubectl get deployments kubectl get pods -o wide kubectl get pods -L labels # L adds a custom column called labels in tablular output # Set # Changing images on both the pods at the same time kubectl set image deploy examplehttpapp * = nginx:1.19 # Check if image has been updated kubectl describe po <podname> | grep -i image # -i for insensitive search kubectl describe po <podname> | grep -i image -A 2 # -A for showing next 2 lines 'A'fter match kubectl create ns testns kns testns # alias to set context to testns kubectl create deployment namespacedeg -n testns --image = katacoda/docker-http-server kubectl get pods -n testns # Scaling kns default # alias to set context to default kubectl scale deployment examplehttpapp --replicas = 5 --record # record is important for scaling kubectl --record = true set image deployment examplehttpapp docker-http-server = katacoda/docker-http-server:v2 kubectl rollout status deployment examplehttpapp # It will show 2 change cause records kubectl rollout undo deployment examplehttpapp # Incase you want to undo the last change # Labels # Edit kubectl edit deploy examplehttpapp # Change the image, another way to do instead of set kubectl expose deployment examplehttpapp --port 80 kubectl get svc -o wide kubectl describe svc examplehttpapp # Incase you just want to see the labels on the svc, instead of describe kubectl get svc --show-labels kubectl get services -l app = examplehttpapp -o go-template = '{{(index .items 0).spec.clusterIP}}' curl $( kubectl get services -l app = examplehttpapp -o go-template = '{{(index .items 0).spec.clusterIP}}' ) kubectl logs $( kubectl get pods -l app = examplehttpapp -o go-template = '{{(index .items 0).metadata.name}}' ) Create a temporary Pod and execute the wget command inside of its container using the IP address and the container port. kubectl run busybox --image = busybox --rm -it --restart = Never -n ckad -- wget 10 .244.1.2:80","title":"Useful subcommand"},{"location":"k8s/3-commands/#all-commands","text":"################# # How to navigate through your cluster. ################# # To see only the Kubernetes kubectl client version and not the Kubernetes version kubectl version --client = true # You might need to gather information about the endpoints of the master and services in the cluster. This information will come in handy if you have to troubleshoot your cluster. kubectl cluster-info # Command autocompletion help and options kubectl completion -h # setup is using the bash shell source < ( kubectl completion bash ) # check the status of the Nodes kubectl get no # various Namespaces that are critical to Kubernetes operations kubectl get ns # look at the Pods in the cluster kubectl get po kubectl get po -n kube-system # see the Pods in all the Namespaces kubectl get pods -A # see deployments kubectl get deploy # Shows key information about the Deployment such as: # Labels, Number of Replicas, Annotations, Deployment Strategy Type, Events kubectl describe deploy nginx # Get additional information on the Pods kubectl get pod -o wide # YAML output format kubectl get pod -o yaml # sort the output of queries, you can use the --sort-by flag # sort by various data points for pod specs like # Pod Ip, Pod name, Pod nodeName, Pod hostname, Pod volumes kubectl get pod -o wide --sort-by = .status.podIP kubectl get pod -o wide --sort-by = .spec.podName # Get the documentation for Kubernetes resources such as Pods or Services: kubectl explain deployment # View all the supported resource types: kubectl api-resources # View all the resources kubectl get all ################# # How to switch between contexts and namespaces # How to find resources and format their output # How to update, patch, delete, and scale resources ################# # View kubectl configuration kubectl view config # list of all our configured clusters kubectl config get-clusters # From above 2 commands, you can see for example - there is currently one cluster named kubernetes and one user named kubernetes-admin. # Same can be seen in one line with below command. In this case, the context is kubernetes-admin@kubernetes. kubectl config current-context # create a new kubectl context using the existing kubernetes-admin user. kubectl config set-context dev-context --cluster kubernetes --user = kubernetes-admin kubectl config get-contexts # Notice that the current active context is set to kubernetes-admin@kubernetes # To switch to the dev-context context kubectl config use-context dev-context # Let's switch back to the kubernetes-admin@kubernetes context kubectl config use-context kubernetes-admin@kubernetes # As you can see, switching back and forth between contexts with the above kubectl command can be very tedious especially when dealing with multiple contexts. # The alternative would be to install kubectx. # When the installion is done, type the kubectx command to list all the contexts: kubectx # Notice that the current active context kubernetes-admin@kubernetes is highlighted in yellow # To switch between contexts, you can now type kubectx CONTEXT NAME. kubectx dev-context # Go ahead and switch back to the kubernetes-admin@kubernetes context: kubectx kubernetes-admin@kubernetes # to create the namespace: kubectl create ns frontend # Declarative method kubectl create namespace backend -o yaml --dry-run = client > ns-backend.yaml kubectl apply -f ns-backend.yaml # deploy a single redis container kubectl run redis --image = quay.io/quay/redis -n backend kubectl run nginx --image = quay.io/bitnami/nginx -n frontend kubectl get pods -n frontend # Switch to the appropriate namespace context where the resources live without having to specify ns kubectl config set-context --current --namespace = frontend # If you are constantly switching between namespaces and want to avoid using the long kubectl command above, then the kubens plugin becomes handy. # Let's list all the namespaces using the kubens command. kubens # Notice again, the current active namespace default is highlighted in yellow. # let's switch to the frontend namespace kubens frontend # switch back to the default namespace: kubens default # You can use the --selector flag to filter and find resources based on their assigned labels. # Use the deployment yaml mentioned below kubectl create -f ~/label-deploy.yaml kubectl get pods -n frontend --show-labels # find all the pods that have the label app: web in the frontend namespace: kubectl get pods -n frontend --selector = app = web # You can also use the -l flag, which represents label and is equivalent to the --selector flag. kubectl get pods -n frontend -l app = haproxy # let's find nodes within our cluster that do NOT have the taint label: node-role.kubernetes.io/master kubectl get nodes --selector = '!node-role.kubernetes.io/master' # As you can see, the --selector or -l flags could come in very handy when identifying thousands of kubernetes resources with differing labels. # Use jsonpath to find/filter resources # The -o=jsonpath flag with the kubectl command allows you to filter resources and display them in the way you desire. # Let's say we want to find the names of all the kubernetes nodes along with their CPU resources. kubectl get nodes -o = jsonpath = '{.items[*].metadata.name} {.items[*].status.capacity.cpu}' # As you may notice, the output does not look pretty. What if we add a \\n (newline character) between the two JSONPath pairs as: kubectl get nodes -o = jsonpath = '{.items[*].metadata.name}{\"\\n\"}{.items[*].status.capacity.cpu}{\"\\n\"}' # we wanted to get an output that is formatted as the output shown below: # master 2 # node01 4 # To achieve this, we would use the range JSONPath operator to iterate through each item (nodes in this case) and use tabulation \\t as well as new line \\n characters to achieve the desired output. # To do this in JSONPath, we would use the range and end operators kubectl get nodes -o = jsonpath = '{range .items[*]}{.metadata.name}{\"\\t\"}{.status.capacity.cpu}{\"\\n\"}{end}' # Format output with custom-columns # we want to get all nodes within our cluster and nicely format the output with a column header called NAME. kubectl get nodes -o = custom-columns = NAME:.metadata.name # You can add additional columns to the above command by adding JSONPath pairs (COLUMN HEADER:.metadata) separated by a comma. kubectl get nodes -o = custom-columns = NAME:.metadata.name,CPU:.status.capacity.cpu # let's find all the pods that were deployed and output them in a tabulated format with column headers POD_NAME and IMAGE_VER: kubectl get pods -n frontend -o custom-columns = POD_NAME:.metadata.name,IMAGE_VER:.spec.containers [ * ] .image # Scale resources kubectl create deployment nginx-deployment --image = quay.io/bitnami/nginx:1.20 kubectl scale deploy nginx-deployment --replicas = 5 # scale the deployment down to 1 replica kubectl scale deploy/nginx-deployment --replicas = 1 # Update resources # we are going to update the nginx image from nginx:1.20 to nginx:1.21 with no downtime. kubectl set image deployment/nginx-deployment nginx = quay.io/bitnami/nginx:1.21 --record # watch the status of the nginx-deployment deployment's rollingUpdate changes until completion. kubectl rollout status -w deployment/nginx-deployment # output of the rollout history kubectl rollout history deployment/nginx-deployment # To undo the update kubectl rollout undo deployment/nginx-deployment # Let's change the image version back to nginx:1.21 kubectl rollout undo deployment/nginx-deployment --to-revision = 2 # Patch and label resources # Patching can be used to partially update any kubernetes resources such as nodes, pods, deployments, etc. # we are going to deploy an nginx pod with a label of env: prod kubectl run nginx --image = quay.io/bitnami/nginx --labels = env = prod kubectl get pod nginx --show-labels # let's update the label to env=dev using the patch command: kubectl patch pod nginx -p '{\"metadata\":{\"labels\":{\"env\":\"dev\"}}}' kubectl get pod nginx --show-labels # We can also use the kubectl label command to add a label, update an existing label, or delete a label. kubectl label pod nginx env = prod --overwrite # Note: the --overwrite flag is used when the label already exists. # To delete the label, append the - to env , which is the value of the label's key. # Alternatively, use the kubectl edit pod nginx command and manually edit the .metadata.label.env and save your changes. # Delete resources kubectl delete pod nginx ################# # Advanced kubeclt commands that can be used in the field as a cluster operator/administrator. # > krew a kubectl plugin manager # > Interaction with pods # kubectl logs # kubectl cp # kubectl exec # > Interacting with nodes: # kubectl taint # Pod's Tolerations # kubectl cordon/uncordon # kubectl drain # kubectl top ################# # krew is a plugin manager for kubectl. # We will be using the following plugins: # access-matrix - shows an RBAC (role based access control) access matrix for server resources # ns - view or change switch namespace contexts # ctx - switch between Kubernetes cluster contexts # Let's discover some of these plugins: kubectl krew search # Install plugins via krew cat > ~/plugins <<EOF access-matrix ca-cert ctx get-all iexec images ns pod-dive pod-logs whoami who-can EOF # Install the plugins for plugin in $( cat ~/plugins ) ; do echo -en $( kubectl krew install $plugin ) ; done # Verify and list the installed plugins: kubectl krew list # You can also list the installed plugins: kubectl plugin list # We can begin by listing who the current authenticated user is: kubectl whoami # Let's also look at the who-can plugin, which is equivalent to the kubectl auth can-i VERB [TYPE/NAME]: kubectl who-can create nodes kubectl who-can '*' pods # list the namespaces: kubectl ns # Let's get the name of the first pod, assign it to a variable and run the pod-dive plugin: POD = $( kubectl get pods -o = jsonpath = '{.items[0].metadata.name}' ) && echo $POD kubectl pod-dive $POD # The above output shows a nice pod resource tree (node, namespace, type of resource, etc.). # display all the images in all namespaces: kubectl images -A # access-matrix plug-in, which is handy when looking for a RBAC Access matrix for Kubernetes resources: kubectl access-matrix # Interacting with pods # Let's switch to the kube-system namespace and access some logs: kubectl ns kube-system # Use the pod-logs plugin to get the weave pods logs: kubectl pod-logs # The pod-logs plug-in does not allow output redirection. Therefore, if you want to redirect the output use kubectl logs as such: kubectl logs POD -c CONTAINER > logsfile # kubectl exec /iexec # Let's create a single container pod called test with an nginx image: kubectl run test --image = quay.io/bitnami/nginx # Let's get the output of the date command from the running test container without logging into it: kubectl exec test -- date # Using the iexec plug-in, let's get the content of the /etc/resolv.conf/ file from the running test container: kubectl iexec test cat /etc/resolv.conf # To login and interact with the container's shell kubectl iexec test # Alternatively, you can use the below command: kubectl exec test -it -- /bin/sh # kubectl cp # The cp command can be used to copy files and directories to and from containers within a pod. # let's copy the content of the krew-install directory to the test container's /tmp directory kubectl cp ~/krew-install test:/tmp # Let's verify whether the directory has been copied. kubectl iexec test ls /tmp/krew-install # Now, let's copy the welcome.txt file from the test container to the master server's /tmp directory: kubectl cp test:/tmp/welcome.txt /tmp/welcome.txt # kubectl taint # A taint consist of a key, value, and effect. As an argument, it is expressed as key=value:effect. # The effect should be one these values: NoShedule, PreferNoSchedule, or NoExecute # Here is how it is used with the kubectl command: kubectl taint NODE NAME KEY_1 = VAL_1:TAINT_EFFECT # Let's taint node01 as dedicated to the devops-group only kubectl taint node node01 dedicated = devops-group:NoSchedule # Verify that node01 is tainted: kubectl describe node node01 | grep -i taints # You can also check the taints on all nodes: kubectl get nodes -o custom-columns = NAME:.metadata.name,TAINTS:.spec.taints [ * ] .key # Let's now try to deploy a single pod: kubectl run my-app --image = quay.io/bitnami/nginx # The newly deployed pod will be in a pending state, because it will not tolerate the taints applied to both nodes. Therefore, it will not be scheduled. # To see the error, type the below command and check under the events section: kubectl describe pod my-app # Alternatively, you can run the below command: kubectl get events # There are 2 ways to solve this issue. We can add a toleration matching the taint that was applied to the nodes, or remove the taint from the nodes. For now, let's remove the taint on node01: kubectl taint node node01 dedicated- # Note: to remove a taint, append the - to the value of the key. # And by default, the control node is tainted with node-role.kubernetes.io/master, therefore, any pod that does not have a toleration matching the node's taint cannot be deployed onto the control node. # kubectl cordon # Let's now try to get one of the pods that are deployed on node01 and assign its name to a variable: APOD = $( kubectl get pods -ojsonpath = '{.items[?(@.spec.nodeName == \"node01\")].metadata.name}' ) && echo $APOD # let's run the pod-dive plugin: kubectl pod-dive $APOD # Before we drain the node, we will cordon it first. cordon means ensuring that no pod can be scheduled on the particular node. kubectl cordon node01 # If you list the nodes now, you will find the status of node01 set to Ready,SchedulingDisabled kubectl get nodes # kubectl drain # Draining a node means removing all running pods from the node, typically performed for maintenance activities. # Open a second terminal and run the below command to watch the output in Terminal 2: watch -d kubectl get pods -o wide # Run the below command to drain node01: kubectl drain node01 --ignore-daemonsets # you will observe, how the pods in node01 are being terminated and re-deployed on the controlplane node. # Now, let's uncordon node01: kubectl uncordon node01 # In Terminal 2, you will notice that the pods have not been moved back to node01. These Pods will not be rescheduled automatically to the new nodes. # let's try to scale up the deployment to 8 replicas. kubectl scale deployment/nginx-deployment --replicas = 8 # Note: The --ignore-daemonsets flag in the kubectl drain command is required because DaemonSet pods are required to run on each node when deployed. This allows pods that are not part of a DaemonSet to be re-deployed on another available node # kubectl top # The kubectl top allows you to see the resource consumption for nodes or pods. However, in order to use the top command, we have to install a metrics server. git clone https://github.com/mbahvw/kubernetes-metrics-server.git kubectl apply -f kubernetes-metrics-server/ # Let's verify that we are getting a response from the metric server API: kubectl get --raw /apis/metrics.k8s.io/ # Let's get the CPU and memory utilization for all nodes in the cluster: kubectl top nodes # let's try to get the memory and CPU utilization of pods in all namespaces: kubectl top pods --all-namespaces # We can also gather the metrics of all the pods in the kube-system namespace: kubectl top pods -n kube-system # cat ~/label-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : frontend-deployment labels : app : web tier : frontend namespace : frontend spec : replicas : 3 selector : matchLabels : app : web template : metadata : labels : app : web spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : haproxy-deployment labels : app : haproxy tier : frontend namespace : frontend spec : replicas : 3 selector : matchLabels : app : haproxy template : metadata : labels : app : haproxy spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80","title":"All Commands"},{"location":"k8s/3-commands/#install-kubectx","text":"cat ~/kubectx.sh #!/bin/bash cd ~/ echo \"You are on the $PWD directory\" sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens","title":"Install kubectx"},{"location":"k8s/3-commands/#install-krew","text":"# cat ~/krew-install/install-krew.sh #!/bin/bash #Downloading krew from repo\" echo -en \"Downloading and installing krew\\n\" set -x ; cd \" $( mktemp -d ) \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/download/v0.3.4/krew.{tar.gz,yaml}\" && tar zxvf krew.tar.gz && KREW = ./krew- \" $( uname | tr '[:upper:]' '[:lower:]' ) _amd64\" && \" $KREW \" install --manifest = krew.yaml --archive = krew.tar.gz && \" $KREW \" update #Adding it to home user ~/.bashrc file echo 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' >>~/.bashrc #source the bashrc and restart bash source ~/.bashrc exec bash # Automation ideas # cat kc_step3.sh #!/bin/bash kubectl config set-context test-context --cluster kubernetes --user = kubernetes-admin for x in developers admins dbadmins ; do kubectl create namespace $x ; done cd ~/deployment kubectl create -f explore-deploy.yaml # cat kc_step4.sh #!/bin/bash kubectl ns default cd ~/deployment kubectl delete -f explore-deploy.yaml kubectl delete namespace developers kubectl delete namespace dbadmins kubectl delete namespace admins # cat explore-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : frontend-deployment labels : app : webapp tier : devops namespace : developers spec : replicas : 3 selector : matchLabels : app : webapp template : metadata : labels : app : webapp spec : containers : - name : nginx image : quay.io/bitnami/nginx:latest ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : haproxy-deployment labels : app : haproxy tier : admins namespace : admins spec : replicas : 3 selector : matchLabels : app : haproxy template : metadata : labels : app : haproxy spec : containers : - name : nginx image : quay.io/bitnami/nginx:latest ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : db-deployment labels : app : redis tier : dbadmins namespace : dbadmins spec : replicas : 3 selector : matchLabels : app : redis template : metadata : labels : app : redis spec : containers : - name : redis image : quay.io/quay/redis # Deploy pod on master node as pod taints match master node # cat nginx-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : web tier : frontend namespace : default spec : replicas : 4 selector : matchLabels : app : web template : metadata : labels : app : web spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80 tolerations : - key : \"node-role.kubernetes.io/master\" operator : Exists effect : NoSchedule","title":"Install Krew"},{"location":"k8s/4-api/","text":"Example Apps to run on K8s \u00b6 Kuard Kubectl Book Kubernetes API \u00b6 The Kubernetes API is a RESTful API based on HTTP and JSON and provided by an API server. All of the components in Kubernetes communicate through the API. Basic Objects : Pods, ReplicaSets, and Services STORAGE : PERSISTENT VOLUMES, CONFIGMAPS, AND SECRETS Organizing Your Cluster with Namespaces, Labels, and Annotations Advanced Concepts : Deployments, Ingress, and StatefulSets API via Command Line \u00b6 # reveal all the API resources kubectl get --raw / # At the top of this list is v1 and under that is namespaces, so request the namespaces kubectl get --raw /api/v1/namespaces # One of the namespaces is called default, so request details on the default namespace kubectl get --raw /api/v1/namespaces/default # jq is like sed for JSON data. Using jq can make the JSON output from kubectl much easier to read with syntax highlighting. kubectl get --raw /api/v1/namespaces/default | jq . # There is also a Python json.tool kubectl get -v = 9 --raw /api/v1/namespaces/default | python -m json.tool - These are all the versions behind the API root path /apis/. In the version list, most of the lines are composed of two parts separated with a slash (/). The left token is the API Group and the right side is the version in that group. - Such as: batch/v1 and batch/v1beta Proxy \u00b6 There is a proxy command that will allow you to access the cluster via localhost. This proxy will run in the background. kubectl proxy 8001 > /dev/null & # Hit Enter to ensure you get the shell prompt back. # With this proxy you can access the Kubernetes API locally at the specified port. curl localhost:8001 curl localhost:8001/api/v1/namespaces/default | jq . # if you want to stop the proxy, use the command fg to move the proxy to the foregound and then exit the proxy - The easiest way to \u201caccess a terminal\u201d within a namespace is to launch a pod with an interactive terminal inside the desired namespace. kubectl run curl --namespace $SESSION_NAMESPACE --image = radial/busyboxplus:curl -i --tty --rm --overrides = '{\"spec\": { \"securityContext\": { \"runAsUser\": 1000 }}}' API-Resources \u00b6 # Get a list of api-resources kubectl api-resources # Most resources are associated with Namespaces, however, some cluster scope resources do not make sense to be associated with a Namespace. # List the cluster scoped resources kubectl api-resources --namespaced = false # As you can see, resources like PersistentVolumes are scoped at the cluster level and not associated with Namespaces. # Most of the api-resources are grouped. # For instance, the two job types are grouped in the batch group. kubectl api-resources --api-group = batch # Check Permissions on the User kubectl auth can-i --list Explaining Resources \u00b6 The Explain command is a great way to understand the defined structure of a resource or kind. kubectl explain ns # To get the full structure of this kind, use the --recursive flag kubectl explain ns --recursive | less # Notice the status field phase. Let's display that as an output kubectl get ns -o custom-columns = NAME:.metadata.name,PHASE:.status.phase Describe \u00b6 Don't confuse the Explain command with the Describe command. While Explain reports on the type of the resource, Describe reports the details of the instance of a resource. kubectl describe namespace kube-system Cluster Components \u00b6 # Kubernetes proxy is responsible for routing network traffic to load-balanced services in the Kubernetes cluster. To do its job, the proxy must be present on every node in the cluster. kubectl get daemonSets --namespace = kube-system kube-proxy # Kubernetes also runs a DNS server, which provides naming and discovery for the services that are defined in the cluster. Can be replaced by coredns kubectl get deployments --namespace = kube-system kube-dns [ coredns ] # Kubernetes service that performs load-balancing for the DNS server. kubectl get services --namespace = kube-system kube-dns Contexts \u00b6 # creates a new context, but it doesn\u2019t actually start using it yet. kubectl config set-context my-context --namespace = mystuff # use this newly created context kubectl config use-context my-context # Contexts can also be used to manage different clusters or different users for authenticating to those clusters using the --users or --clusters flags with the set-context command. Labeling and Annotating Objects \u00b6 kubectl label pods bar color = red,env = dev # Add Label # By default, label and annotate will not let you overwrite an existing label. To do this, you need to add the `--overwrite` flag # Remove a label, you can use the -<label-name> syntax kubectl label pods bar color- kubectl get pods --show-labels kubectl get pods -L labels # L adds a custom column called labels in tablular output kubectl get pods --selector = \"env=dev\" # If we specify two selectors separated by a comma, only the objects that satisfy both will be returned. This is a logical AND operation: kubectl get pods --selector = \"color=red,env=dev\" kubectl label deployments alpaca-test \"canary=true\" kubectl get deployments --show-labels \u00b6 Operator Description \u00b6 key=value key is set to value key!=value key is not set to value key in (value1, value2) key is one of value1 or value2 key notin (value1, value2) key is not one of value1 or value2 key key is set !key key is not set Filter output using jq kubectl get pods -n kube-system calico -ojson | jq .metadata.labels Debugging Commands \u00b6 kubectl logs <pod-name> # If you have multiple containers in your pod you can choose the container to view using the -c flag. # If you instead want to continuously stream the logs back to the terminal without exiting, you can add the -f (follow) command-line flag. # Adding the --previous flag will get logs from a previous instance of the container. This is useful, for example, if your containers are continuously restarting due to a problem at container startup. # Use the exec command to execute a command in a running container kubectl exec -it <pod-name> -- bash # copy files to and from a container using the cp command kubectl cp <pod-name>:/path/to/remote/file /path/to/local/file # You can also specify directories, or reverse the syntax to copy a file from your local machine back out into the container. # A secure tunnel is created from your local machine, through the Kubernetes master, to the instance of the Pod running on one of the worker nodes. kubectl port-forward nginx 80 :8080 Replacing Objects \u00b6 Download Deployment or any object into a YAML file and then use the replace command Adding --save-config adds an annotation so that, when applying changes in the future, kubectl will know what the last applied configuration was for smarter merging of configs. kubectl get deployments nginx --export -o yaml > nginx-deployment.yaml kubectl replace -f nginx-deployment.yaml --save-config Pods \u00b6 In general, the right question to ask yourself when designing Pods is, \u201cWill these containers work correctly if they land on different machines?\u201d If the answer is \u201cno,\u201d a Pod is the correct grouping for the containers. If the answer is \u201cyes,\u201d multiple Pods is probably the correct solution. Declarative configuration means that you write down the desired state of the world in a configuration and then submit that configuration to a service that takes actions to ensure the desired state becomes the actual state. Imperative configuration , where you simply take a series of actions (e.g., apt-get install foo) to modify the world. Years of production experience have taught us that maintaining a written record of the system\u2019s desired state leads to a more manageable, reliable system. Pod manifests can be written using YAML or JSON, but YAML is generally preferred because it is slightly more human-editable and has the ability to add comments. All Pods have a termination grace period. By default, this is 30 seconds. Liveness health checks run application-specific logic (e.g., loading a web page) to verify that the application is not just still running, but is functioning properly. Readiness describes when a container is ready to serve user requests. Containers that fail readiness checks are removed from service load balancers. What probe type (liveness/readiness) should be used for each and what handler should be used (TCP, HTTP, EXEC)? Figure out which ones to use? Port Check - Liveness using TCP handler DB Query - Readiness using an EXEC handler executing a SQL query \u201crequest\u201d specifies a minimum. It does not specify a maximum cap on the resources a Pod may use. If a container is over its memory request, the OS can\u2019t just remove memory from the process, because it\u2019s been allocated. Consequently, when the system runs out of memory, the kubelet terminates containers whose memory usage is greater than their requested memory. These containers are automatically restarted, but with less available memory on the machine for the container to consume. \"limits\" specifies a maximum cap on the resources a Pod may use. When you establish limits on a container, the kernel is configured to ensure that consumption cannot exceed these limits. A container with a CPU limit of 0.5 cores will only ever get 0.5 cores, even if the CPU is otherwise idle. A container with a memory limit of 256 MB will not be allowed additional memory. Labels selectors are used to filter Kubernetes objects based on a set of labels. Annotations provide a place to store additional metadata for Kubernetes objects with the sole purpose of assisting tools and libraries. Annotations can be used for the tool itself or to pass configuration information between external systems. There is overlap with labels, and it is a matter of taste as to when to use an annotation or a label. When in doubt, add information to an object as an annotation and promote it to a label if you find yourself wanting to use it in a selector. During rolling deployments, annotations are used to track rollout status and provide the necessary information required to roll back a deployment to a previous state. Service \u00b6 Real service discovery in Kubernetes starts with a Service object. kubectl run alpaca-prod \\ --image = gcr.io/kuar-demo/kuard-amd64:1 \\ --replicas = 3 \\ --port = 8080 \\ --labels = \"ver=1,app=alpaca,env=prod\" kubectl expose deployment alpaca-prod kubectl run bandicoot-prod \\ --image = gcr.io/kuar-demo/kuard-amd64:2 \\ --replicas = 2 \\ --port = 8080 \\ --labels = \"ver=2,app=bandicoot,env=prod\" kubectl expose deployment bandicoot-prod kubectl get services -o wide After running these commands, we have three services. The ones we just created are alpaca-prod and bandicoot-prod. The kubernetes service is automatically created for you so that you can find and talk to the Kubernetes API from within the app. Endpoints are a lower-level way of finding what a service is sending traffic to. kubectl get endpoints alpaca-prod --watch At some point, we have to allow new traffic in! The most portable way to do this is to use a feature called NodePorts . You use the NodePort without knowing where any of the Pods for that service are running. kubectl expose deployment alpaca-prod --type = NodePort kubectl describe svc alpaca-prod # Assume Port 32711 is assigned # If your cluster is in the cloud someplace, you can use SSH tunneling with something like this: ssh <node> -L 8080 :localhost:32711 # Now if you open your browser to http://localhost:8080 you will be connected to that service. ReplicaSets \u00b6 A ReplicaSet acts as a cluster-wide Pod manager, ensuring that the right types and number of Pods are running at all times. When we define a ReplicaSet, we define a specification for the Pods we want to create (the \u201ccookie cutter\u201d), and a desired number of replicas. Additionally, we need to define a way of finding Pods that the ReplicaSet should control. The actual act of managing the replicated Pods is an example of a reconciliation loop . -Though ReplicaSets create and manage Pods, they do not own the Pods they create. ReplicaSets use label queries to identify the set of Pods they should be managing. Because ReplicaSets are decoupled from the Pods they manage, you can simply create a ReplicaSet that will \u201cadopt\u201d the existing Pod, and scale out additional copies of those containers. In this way you can seamlessly move from a single imperative Pod to a replicated set of Pods managed by a ReplicaSet. A Pod can be misbehaving but still be part of the replicated set. You can modify the set of labels on the sick Pod. Doing so will disassociate it from the ReplicaSet (and service) so that you can debug the Pod. The ReplicaSet controller will notice that a Pod is missing and create a new copy, but because the Pod is still running, it is available to developers for interactive debugging, which is significantly more valuable than debugging from logs. ReplicaSets are designed to represent a single, scalable microservice inside your architecture. ReplicaSets are designed for stateless (or nearly stateless) services. Finding a ReplicaSet from a Pod \u00b6 Sometimes you may wonder if a Pod is being managed by a ReplicaSet, and, if it is, which ReplicaSet. To enable this kind of discovery, the ReplicaSet controller adds an annotation to every Pod that it creates. - The key for the annotation is kubernetes.io/created-by . If you run the following, look for the kubernetes.io/created-by entry in the annotations section: kubectl get pods <pod-name> -o yaml Note that such annotations are best-effort; they are only created when the Pod is created by the ReplicaSet, and can be removed by a Kubernetes user at any time. Finding a Set of Pods for a ReplicaSet \u00b6 First, you can get the set of labels using the kubectl describe command. To find the Pods that match this selector, use the --selector flag or the shorthand -l : kubectl get pods -l app = kuard,version = 2 This is exactly the same query that the ReplicaSet executes to determine the current number of Pods. Scaling ReplicaSets \u00b6 kubectl scale replicasets kuard --replicas = 4 Horizontal Pod Autoscaling (HPA) \u00b6 HPA requires the presence of the heapster Pod on your cluster. heapster keeps track of metrics and provides an API for consuming metrics HPA uses when making scaling decisions. # Creates an autoscaler that scales between two and five replicas with a CPU threshold of 80%. kubectl autoscale rs kuard --min = 2 --max = 5 --cpu-percent = 80 kubectl get hpa Deleting ReplicaSets \u00b6 # This also deletes the Pods that are managed by the ReplicaSet kubectl delete rs kuard # If you don\u2019t want to delete the Pods that are being managed by the ReplicaSet you can set the --cascade flag to false kubectl delete rs kuard --cascade = false DaemonSets \u00b6 A DaemonSet ensures a copy of a Pod is running across a set of nodes in a Kubernetes cluster. DaemonSets are used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node. However, there are some cases where you want to deploy a Pod to only a subset of nodes. In cases like these node labels can be used to tag specific nodes that meet workload requirements. # Using a label selector we can filter nodes based on labels. kubectl get nodes --selector ssd = true Node selectors can be used to limit what nodes a Pod can run on in a given Kubernetes cluster. Node selectors are defined as part of the Pod spec when creating a DaemonSet. The inverse is also true: if a required label is removed from a node, the Pod will be removed by the DaemonSet controller. Deleting a DaemonSet will also delete all the Pods being managed by that DaemonSet. Set the --cascade flag to false to ensure only the DaemonSet is deleted and not the Pods. Jobs \u00b6 Jobs are designed to manage batch-like workloads where work items are processed by one or more Pods. By default each Job runs a single Pod once until successful termination. --restart=OnFailure is the option that tells kubectl to create a Job object. Because Jobs have a finite beginning and ending, it is common for users to create many of them. This makes picking unique labels more difficult and more critical. For this reason, the Job object will automatically pick a unique label and use it to identify the pods it creates. kubectl run -i oneshot \\ --image = gcr.io/kuar-demo/kuard-amd64:1 \\ --restart = OnFailure \\ -- --keygen-enable \\ --keygen-exit-on-complete \\ --keygen-num-to-gen 10 # The -i option to kubectl indicates that this is an interactive command. kubectl will wait until the Job is running and then show the log output from the first (and in this case only) pod in the Job. # All of the options after -- are command-line arguments to the container image. These instruct our test server (kuard) to generate 10 4,096-bit SSH keys and then exit. kubectl get pod -l job-name = oneshot -a # Without -a flag kubectl hides completed Jobs. kubectl delete jobs oneshot ConfigMaps \u00b6 Configmaps can be used as a set of variables that can be used when defining the environment or command line for your containers. The key thing is that the ConfigMap is combined with the Pod right before it is run. This means that the container image and the pod definition itself can be reused across many apps by just changing the ConfigMap that is used. There are three main ways to use a ConfigMap: Filesystem : You can mount a ConfigMap into a Pod. A file is created for each entry based on the key name. The contents of that file are set to the value. Environment variable : A ConfigMap can be used to dynamically set the value of an environment variable. Command-line argument : Kubernetes supports dynamically creating the command line for a container based on ConfigMap values. cat my-config.txt parameter1 = value1 parameter2 = value2 kubectl create configmap my-config \\ --from-file = my-config.txt \\ --from-literal = extra-param = extra-value \\ --from-literal = another-param = another-value Secrets \u00b6 There is certain data that is extra-sensitive. This can include passwords, security tokens, or other types of private keys. Collectively, we call this type of data \u201csecrets.\u201d Secret data can be exposed to pods using the secrets volume type. Secrets volumes are managed by the kubelet and are created at pod creation time. Secrets are stored on tmpfs volumes (aka RAM disks) and, as such, are not written to disk on nodes. Each data element of a secret is stored in a separate file under the target mount point specified in the volume mount. # The TLS key and certificate for the kuard application can be downloaded by running the following commands. curl -o kuard.crt https://storage.googleapis.com/kuar-demo/kuard.crt curl -o kuard.key https://storage.googleapis.com/kuar-demo/kuard.key # With the kuard.crt and kuard.key files stored locally, we are ready to create a secret. kubectl create secret generic kuard-tls \\ --from-file = kuard.crt \\ --from-file = kuard.key # Replacing secrets from file kubectl create secret generic kuard-tls \\ --from-file = kuard.crt --from-file = kuard.key \\ --dry-run -o yaml | kubectl replace -f - # This command line first creates a new secret with the same name as our existing secret. If we just stopped there, the Kubernetes API server would return an error complaining that we are trying to create a secret that already exists. Instead, we tell kubectl not to actually send the data to the server but instead to dump the YAML that it would have sent to the API server to stdout. We then pipe that to kubectl replace and use -f - to tell it to read from stdin. In this way we can update a secret from files on disk without having to manually base64-encode data. Extracting secrets into a file kubectl get secret demo-secret -o json | jq -r .data.value | base64 --decode > ./demo-secret # Output the value of secret as JSON, run JQ to parse the output # As the secret data is base64 encoded, decode it before writing the data to the client machine Deployments \u00b6 The Deployment object exists to manage the release of new versions. -In the output of describe there is a great deal of important information. Two of the most important pieces of information in the output are OldReplicaSets and NewReplicaSet . These fields point to the ReplicaSet objects this Deployment is currently managing. If a Deployment is in the middle of a rollout, both fields will be set to a value. If a rollout is complete, OldReplicaSets will be set to . You can use kubectl rollout history to obtain the history of rollouts associated with a particular Deployment. If you have a current Deployment in progress, then you can use kubectl rollout status to obtain the current status of a rollout. You can undo both partially completed and fully completed rollouts. An undo of a rollout is actually simply a rollout in reverse (e.g., from v2 to v1, instead of from v1 to v2), and all of the same policies that control the rollout strategy apply to the undo strategy as well. Specifying a revision of 0 is a shorthand way of specifying the previous revision. Or kubectl rollout undo If you ever want to manage that ReplicaSet directly, you need to delete the Deployment (remember to set --cascade to false, or else it will delete the ReplicaSet and Pods as well!). # You can see the label selector kubectl get deployments nginx \\ -o jsonpath --template { .spec.selector.matchLabels } # From this you can see that the Deployment is managing a ReplicaSet with the labels run=nginx. kubectl get replicasets --selector = run = nginx # If you are in the middle of a rollout and you want to temporarily pause it for some reason (e.g., if you start seeing weird behavior in your system and you want to investigate), you can use the pause command: kubectl rollout pause deployments nginx # If, after investigation, you believe the rollout can safely proceed, you can use the resume command to start up where you left off: kubectl rollout resume deployments nginx # You can see the deployment history by running: kubectl rollout history deployment nginx # If you are interested in more details about a particular revision, you can add the --revision flag to view details about that specific revision: kubectl rollout history deployment nginx --revision = 2 # You can roll back to a specific revision in the history using the --to-revision flag: kubectl rollout undo deployments nginx --to-revision = 3 NOTE: When you do a kubectl rollout undo you are updating the production state in a way that isn\u2019t reflected in your source control. An alternate (and perhaps preferred) way to undo a rollout is to revert your YAML file and kubectl apply the previous version. In this way your \u201cchange tracked configuration\u201d more closely tracks what is really running in your cluster. Deployment strategies \u00b6 Deployment strategies for rollingUpdate using the maxUnavailable parameter or the maxSurge parameter. The maxUnavailable parameter sets the maximum number of Pods that can be unavailable during a rolling update. It can either be set to an absolute number (e.g., 3 meaning a maximum of three Pods can be unavailable) or to a percentage (e.g., 20% meaning a maximum of 20% of the desired number of replicas can be unavailable). Generally speaking, using a percentage is a good approach for most services, since the value is correctly applicable regardless of the desired number of replicas in the Deployment. If you have four replicas and have set maxUnavailable to 50%, it will scale it down to two replicas. The rolling update will then replace the removed pods by scaling the new ReplicaSet up to two replicas, for a total of four replicas (two old, two new). It will then scale the old ReplicaSet down to zero replicas, for a total size of two new replicas. Finally, it will scale the new ReplicaSet up to four replicas, completing the rollout. Thus, with maxUnavailableset to 50%, our rollout completes in four steps, but with only 50% of our service capacity at times. However, there are situations where you don\u2019t want to fall below 100% capacity, but you are willing to temporarily use additional resources in order to perform a rollout. In these situations, you can set the maxUnavailable parameter to 0% , and instead control the rollout using the maxSurge parameter. The maxSurge parameter controls how many extra resources can be created to achieve a rollout. To illustrate how this works, imagine we have a service with 10 replicas. We set maxUnavailable to 0 and maxSurge to 20%. The first thing the rollout will do is scale the new ReplicaSet up to 2 replicas, for a total of 12 (120%) in the service. It will then scale the old ReplicaSet down to 8 replicas, for a total of 10 (8 old, 2 new) in the service. This process proceeds until the rollout is complete. At any time, the capacity of the service is guaranteed to be at least 100% and the maximum extra resources used for the rollout are limited to an additional 20% of all resources. Setting maxSurge to 100% is equivalent to a blue/green deployment . The Deployment controller first scales the new version up to 100% of the old version. Once the new version is healthy, it immediately scales the old version down to 0%. Setting minReadySeconds to 60 indicates that the Deployment must wait for 60 seconds after seeing a Pod become healthy before moving on to updating the next Pod. To set the timeout period, the Deployment parameter progressDeadlineSeconds is set to 600 . This sets the progress deadline to 10 minutes. If any particular stage in the rollout fails to progress in 10 minutes, then the Deployment is marked as failed, and all attempts to move the Deployment forward are halted. Ingress \u00b6 Service objects provide a great way to do simple TCP-level load balancing, they don\u2019t provide an application-level way to do load balancing and routing. The truth is that most of the applications that users deploy using containers and Kubernetes are HTTP web-based applications. These are better served by a load balancer that understands HTTP. To address these needs, the Ingress API was added to Kubernetes. Ingress represents a path and host-based HTTP load balancer and router. When you create an Ingress object, it receives a virtual IP address just like a Service, but instead of the one-to-one relationship between a Service IP address and a set of Pods, an Ingress can use the content of an HTTP request to route requests to different Services. StatefulSets \u00b6 Some applications, especially stateful storage workloads or sharded applications, require more differentiation between the replicas in the application. To resolve this, Kubernetes has recently introduced StatefulSets as a complement to ReplicaSets, but for more stateful workloads. Like ReplicaSets, StatefulSets create multiple instances of the same container image running in a Kubernetes cluster, but the manner in which containers are created and destroyed is more deterministic , as are the names of each container. With StatefulSets, each replica receives a monotonically increasing index (e.g., backed-0, backend-1, and so on). StatefulSets guarantee that replica zero will be created and become healthy before replica one is created and so forth. StatefulSets receive DNS names so that each replica can be accessed directly. This allows clients to easily target specific shards in a sharded service.","title":"Example Apps to run on K8s"},{"location":"k8s/4-api/#example-apps-to-run-on-k8s","text":"Kuard Kubectl Book","title":"Example Apps to run on K8s"},{"location":"k8s/4-api/#kubernetes-api","text":"The Kubernetes API is a RESTful API based on HTTP and JSON and provided by an API server. All of the components in Kubernetes communicate through the API. Basic Objects : Pods, ReplicaSets, and Services STORAGE : PERSISTENT VOLUMES, CONFIGMAPS, AND SECRETS Organizing Your Cluster with Namespaces, Labels, and Annotations Advanced Concepts : Deployments, Ingress, and StatefulSets","title":"Kubernetes API"},{"location":"k8s/4-api/#api-via-command-line","text":"# reveal all the API resources kubectl get --raw / # At the top of this list is v1 and under that is namespaces, so request the namespaces kubectl get --raw /api/v1/namespaces # One of the namespaces is called default, so request details on the default namespace kubectl get --raw /api/v1/namespaces/default # jq is like sed for JSON data. Using jq can make the JSON output from kubectl much easier to read with syntax highlighting. kubectl get --raw /api/v1/namespaces/default | jq . # There is also a Python json.tool kubectl get -v = 9 --raw /api/v1/namespaces/default | python -m json.tool - These are all the versions behind the API root path /apis/. In the version list, most of the lines are composed of two parts separated with a slash (/). The left token is the API Group and the right side is the version in that group. - Such as: batch/v1 and batch/v1beta","title":"API via Command Line"},{"location":"k8s/4-api/#proxy","text":"There is a proxy command that will allow you to access the cluster via localhost. This proxy will run in the background. kubectl proxy 8001 > /dev/null & # Hit Enter to ensure you get the shell prompt back. # With this proxy you can access the Kubernetes API locally at the specified port. curl localhost:8001 curl localhost:8001/api/v1/namespaces/default | jq . # if you want to stop the proxy, use the command fg to move the proxy to the foregound and then exit the proxy - The easiest way to \u201caccess a terminal\u201d within a namespace is to launch a pod with an interactive terminal inside the desired namespace. kubectl run curl --namespace $SESSION_NAMESPACE --image = radial/busyboxplus:curl -i --tty --rm --overrides = '{\"spec\": { \"securityContext\": { \"runAsUser\": 1000 }}}'","title":"Proxy"},{"location":"k8s/4-api/#api-resources","text":"# Get a list of api-resources kubectl api-resources # Most resources are associated with Namespaces, however, some cluster scope resources do not make sense to be associated with a Namespace. # List the cluster scoped resources kubectl api-resources --namespaced = false # As you can see, resources like PersistentVolumes are scoped at the cluster level and not associated with Namespaces. # Most of the api-resources are grouped. # For instance, the two job types are grouped in the batch group. kubectl api-resources --api-group = batch # Check Permissions on the User kubectl auth can-i --list","title":"API-Resources"},{"location":"k8s/4-api/#explaining-resources","text":"The Explain command is a great way to understand the defined structure of a resource or kind. kubectl explain ns # To get the full structure of this kind, use the --recursive flag kubectl explain ns --recursive | less # Notice the status field phase. Let's display that as an output kubectl get ns -o custom-columns = NAME:.metadata.name,PHASE:.status.phase","title":"Explaining Resources"},{"location":"k8s/4-api/#describe","text":"Don't confuse the Explain command with the Describe command. While Explain reports on the type of the resource, Describe reports the details of the instance of a resource. kubectl describe namespace kube-system","title":"Describe"},{"location":"k8s/4-api/#cluster-components","text":"# Kubernetes proxy is responsible for routing network traffic to load-balanced services in the Kubernetes cluster. To do its job, the proxy must be present on every node in the cluster. kubectl get daemonSets --namespace = kube-system kube-proxy # Kubernetes also runs a DNS server, which provides naming and discovery for the services that are defined in the cluster. Can be replaced by coredns kubectl get deployments --namespace = kube-system kube-dns [ coredns ] # Kubernetes service that performs load-balancing for the DNS server. kubectl get services --namespace = kube-system kube-dns","title":"Cluster Components"},{"location":"k8s/4-api/#contexts","text":"# creates a new context, but it doesn\u2019t actually start using it yet. kubectl config set-context my-context --namespace = mystuff # use this newly created context kubectl config use-context my-context # Contexts can also be used to manage different clusters or different users for authenticating to those clusters using the --users or --clusters flags with the set-context command.","title":"Contexts"},{"location":"k8s/4-api/#labeling-and-annotating-objects","text":"","title":"Labeling and Annotating Objects"},{"location":"k8s/4-api/#kubectl-label-pods-bar-colorredenvdev---------add-label-by-default-label-and-annotate-will-not-let-you-overwrite-an-existing-label-to-do-this-you-need-to-add-the---overwrite-flag-remove-a-label-you-can-use-the--label-name-syntaxkubectl-label-pods-bar-color-kubectl-get-pods---show-labelskubectl-get-pods--l-labels-------------l-adds-a-custom-column-called-labels-in-tablular-outputkubectl-get-pods---selectorenvdev-if-we-specify-two-selectors-separated-by-a-comma-only-the-objects-that-satisfy-both-will-be-returned-this-is-a-logical-and-operationkubectl-get-pods---selectorcolorredenvdevkubectl-label-deployments-alpaca-test-canarytruekubectl-get-deployments---show-labels","text":"","title":"kubectl label pods bar color=red,env=dev        # Add Label\n# By default, label and annotate will not let you overwrite an existing label. To do this, you need to add the `--overwrite` flag\n# Remove a label, you can use the -&lt;label-name&gt; syntax\nkubectl label pods bar color-\nkubectl get pods --show-labels\nkubectl get pods -L labels            # L adds a custom column called labels in tablular output\nkubectl get pods --selector=&quot;env=dev&quot;\n# If we specify two selectors separated by a comma, only the objects that satisfy both will be returned. This is a logical AND operation:\nkubectl get pods --selector=&quot;color=red,env=dev&quot;\nkubectl label deployments alpaca-test &quot;canary=true&quot;\nkubectl get deployments --show-labels\n"},{"location":"k8s/4-api/#operator--------------------description","text":"key=value key is set to value key!=value key is not set to value key in (value1, value2) key is one of value1 or value2 key notin (value1, value2) key is not one of value1 or value2 key key is set !key key is not set Filter output using jq kubectl get pods -n kube-system calico -ojson | jq .metadata.labels","title":"Operator                    Description"},{"location":"k8s/4-api/#debugging-commands","text":"kubectl logs <pod-name> # If you have multiple containers in your pod you can choose the container to view using the -c flag. # If you instead want to continuously stream the logs back to the terminal without exiting, you can add the -f (follow) command-line flag. # Adding the --previous flag will get logs from a previous instance of the container. This is useful, for example, if your containers are continuously restarting due to a problem at container startup. # Use the exec command to execute a command in a running container kubectl exec -it <pod-name> -- bash # copy files to and from a container using the cp command kubectl cp <pod-name>:/path/to/remote/file /path/to/local/file # You can also specify directories, or reverse the syntax to copy a file from your local machine back out into the container. # A secure tunnel is created from your local machine, through the Kubernetes master, to the instance of the Pod running on one of the worker nodes. kubectl port-forward nginx 80 :8080","title":"Debugging Commands"},{"location":"k8s/4-api/#replacing-objects","text":"Download Deployment or any object into a YAML file and then use the replace command Adding --save-config adds an annotation so that, when applying changes in the future, kubectl will know what the last applied configuration was for smarter merging of configs. kubectl get deployments nginx --export -o yaml > nginx-deployment.yaml kubectl replace -f nginx-deployment.yaml --save-config","title":"Replacing Objects"},{"location":"k8s/4-api/#pods","text":"In general, the right question to ask yourself when designing Pods is, \u201cWill these containers work correctly if they land on different machines?\u201d If the answer is \u201cno,\u201d a Pod is the correct grouping for the containers. If the answer is \u201cyes,\u201d multiple Pods is probably the correct solution. Declarative configuration means that you write down the desired state of the world in a configuration and then submit that configuration to a service that takes actions to ensure the desired state becomes the actual state. Imperative configuration , where you simply take a series of actions (e.g., apt-get install foo) to modify the world. Years of production experience have taught us that maintaining a written record of the system\u2019s desired state leads to a more manageable, reliable system. Pod manifests can be written using YAML or JSON, but YAML is generally preferred because it is slightly more human-editable and has the ability to add comments. All Pods have a termination grace period. By default, this is 30 seconds. Liveness health checks run application-specific logic (e.g., loading a web page) to verify that the application is not just still running, but is functioning properly. Readiness describes when a container is ready to serve user requests. Containers that fail readiness checks are removed from service load balancers. What probe type (liveness/readiness) should be used for each and what handler should be used (TCP, HTTP, EXEC)? Figure out which ones to use? Port Check - Liveness using TCP handler DB Query - Readiness using an EXEC handler executing a SQL query \u201crequest\u201d specifies a minimum. It does not specify a maximum cap on the resources a Pod may use. If a container is over its memory request, the OS can\u2019t just remove memory from the process, because it\u2019s been allocated. Consequently, when the system runs out of memory, the kubelet terminates containers whose memory usage is greater than their requested memory. These containers are automatically restarted, but with less available memory on the machine for the container to consume. \"limits\" specifies a maximum cap on the resources a Pod may use. When you establish limits on a container, the kernel is configured to ensure that consumption cannot exceed these limits. A container with a CPU limit of 0.5 cores will only ever get 0.5 cores, even if the CPU is otherwise idle. A container with a memory limit of 256 MB will not be allowed additional memory. Labels selectors are used to filter Kubernetes objects based on a set of labels. Annotations provide a place to store additional metadata for Kubernetes objects with the sole purpose of assisting tools and libraries. Annotations can be used for the tool itself or to pass configuration information between external systems. There is overlap with labels, and it is a matter of taste as to when to use an annotation or a label. When in doubt, add information to an object as an annotation and promote it to a label if you find yourself wanting to use it in a selector. During rolling deployments, annotations are used to track rollout status and provide the necessary information required to roll back a deployment to a previous state.","title":"Pods"},{"location":"k8s/4-api/#service","text":"Real service discovery in Kubernetes starts with a Service object. kubectl run alpaca-prod \\ --image = gcr.io/kuar-demo/kuard-amd64:1 \\ --replicas = 3 \\ --port = 8080 \\ --labels = \"ver=1,app=alpaca,env=prod\" kubectl expose deployment alpaca-prod kubectl run bandicoot-prod \\ --image = gcr.io/kuar-demo/kuard-amd64:2 \\ --replicas = 2 \\ --port = 8080 \\ --labels = \"ver=2,app=bandicoot,env=prod\" kubectl expose deployment bandicoot-prod kubectl get services -o wide After running these commands, we have three services. The ones we just created are alpaca-prod and bandicoot-prod. The kubernetes service is automatically created for you so that you can find and talk to the Kubernetes API from within the app. Endpoints are a lower-level way of finding what a service is sending traffic to. kubectl get endpoints alpaca-prod --watch At some point, we have to allow new traffic in! The most portable way to do this is to use a feature called NodePorts . You use the NodePort without knowing where any of the Pods for that service are running. kubectl expose deployment alpaca-prod --type = NodePort kubectl describe svc alpaca-prod # Assume Port 32711 is assigned # If your cluster is in the cloud someplace, you can use SSH tunneling with something like this: ssh <node> -L 8080 :localhost:32711 # Now if you open your browser to http://localhost:8080 you will be connected to that service.","title":"Service"},{"location":"k8s/4-api/#replicasets","text":"A ReplicaSet acts as a cluster-wide Pod manager, ensuring that the right types and number of Pods are running at all times. When we define a ReplicaSet, we define a specification for the Pods we want to create (the \u201ccookie cutter\u201d), and a desired number of replicas. Additionally, we need to define a way of finding Pods that the ReplicaSet should control. The actual act of managing the replicated Pods is an example of a reconciliation loop . -Though ReplicaSets create and manage Pods, they do not own the Pods they create. ReplicaSets use label queries to identify the set of Pods they should be managing. Because ReplicaSets are decoupled from the Pods they manage, you can simply create a ReplicaSet that will \u201cadopt\u201d the existing Pod, and scale out additional copies of those containers. In this way you can seamlessly move from a single imperative Pod to a replicated set of Pods managed by a ReplicaSet. A Pod can be misbehaving but still be part of the replicated set. You can modify the set of labels on the sick Pod. Doing so will disassociate it from the ReplicaSet (and service) so that you can debug the Pod. The ReplicaSet controller will notice that a Pod is missing and create a new copy, but because the Pod is still running, it is available to developers for interactive debugging, which is significantly more valuable than debugging from logs. ReplicaSets are designed to represent a single, scalable microservice inside your architecture. ReplicaSets are designed for stateless (or nearly stateless) services.","title":"ReplicaSets"},{"location":"k8s/4-api/#finding-a-replicaset-from-a-pod","text":"Sometimes you may wonder if a Pod is being managed by a ReplicaSet, and, if it is, which ReplicaSet. To enable this kind of discovery, the ReplicaSet controller adds an annotation to every Pod that it creates. - The key for the annotation is kubernetes.io/created-by . If you run the following, look for the kubernetes.io/created-by entry in the annotations section: kubectl get pods <pod-name> -o yaml Note that such annotations are best-effort; they are only created when the Pod is created by the ReplicaSet, and can be removed by a Kubernetes user at any time.","title":"Finding a ReplicaSet from a Pod"},{"location":"k8s/4-api/#finding-a-set-of-pods-for-a-replicaset","text":"First, you can get the set of labels using the kubectl describe command. To find the Pods that match this selector, use the --selector flag or the shorthand -l : kubectl get pods -l app = kuard,version = 2 This is exactly the same query that the ReplicaSet executes to determine the current number of Pods.","title":"Finding a Set of Pods for a ReplicaSet"},{"location":"k8s/4-api/#scaling-replicasets","text":"kubectl scale replicasets kuard --replicas = 4","title":"Scaling ReplicaSets"},{"location":"k8s/4-api/#horizontal-pod-autoscaling-hpa","text":"HPA requires the presence of the heapster Pod on your cluster. heapster keeps track of metrics and provides an API for consuming metrics HPA uses when making scaling decisions. # Creates an autoscaler that scales between two and five replicas with a CPU threshold of 80%. kubectl autoscale rs kuard --min = 2 --max = 5 --cpu-percent = 80 kubectl get hpa","title":"Horizontal Pod Autoscaling (HPA)"},{"location":"k8s/4-api/#deleting-replicasets","text":"# This also deletes the Pods that are managed by the ReplicaSet kubectl delete rs kuard # If you don\u2019t want to delete the Pods that are being managed by the ReplicaSet you can set the --cascade flag to false kubectl delete rs kuard --cascade = false","title":"Deleting ReplicaSets"},{"location":"k8s/4-api/#daemonsets","text":"A DaemonSet ensures a copy of a Pod is running across a set of nodes in a Kubernetes cluster. DaemonSets are used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node. However, there are some cases where you want to deploy a Pod to only a subset of nodes. In cases like these node labels can be used to tag specific nodes that meet workload requirements. # Using a label selector we can filter nodes based on labels. kubectl get nodes --selector ssd = true Node selectors can be used to limit what nodes a Pod can run on in a given Kubernetes cluster. Node selectors are defined as part of the Pod spec when creating a DaemonSet. The inverse is also true: if a required label is removed from a node, the Pod will be removed by the DaemonSet controller. Deleting a DaemonSet will also delete all the Pods being managed by that DaemonSet. Set the --cascade flag to false to ensure only the DaemonSet is deleted and not the Pods.","title":"DaemonSets"},{"location":"k8s/4-api/#jobs","text":"Jobs are designed to manage batch-like workloads where work items are processed by one or more Pods. By default each Job runs a single Pod once until successful termination. --restart=OnFailure is the option that tells kubectl to create a Job object. Because Jobs have a finite beginning and ending, it is common for users to create many of them. This makes picking unique labels more difficult and more critical. For this reason, the Job object will automatically pick a unique label and use it to identify the pods it creates. kubectl run -i oneshot \\ --image = gcr.io/kuar-demo/kuard-amd64:1 \\ --restart = OnFailure \\ -- --keygen-enable \\ --keygen-exit-on-complete \\ --keygen-num-to-gen 10 # The -i option to kubectl indicates that this is an interactive command. kubectl will wait until the Job is running and then show the log output from the first (and in this case only) pod in the Job. # All of the options after -- are command-line arguments to the container image. These instruct our test server (kuard) to generate 10 4,096-bit SSH keys and then exit. kubectl get pod -l job-name = oneshot -a # Without -a flag kubectl hides completed Jobs. kubectl delete jobs oneshot","title":"Jobs"},{"location":"k8s/4-api/#configmaps","text":"Configmaps can be used as a set of variables that can be used when defining the environment or command line for your containers. The key thing is that the ConfigMap is combined with the Pod right before it is run. This means that the container image and the pod definition itself can be reused across many apps by just changing the ConfigMap that is used. There are three main ways to use a ConfigMap: Filesystem : You can mount a ConfigMap into a Pod. A file is created for each entry based on the key name. The contents of that file are set to the value. Environment variable : A ConfigMap can be used to dynamically set the value of an environment variable. Command-line argument : Kubernetes supports dynamically creating the command line for a container based on ConfigMap values. cat my-config.txt parameter1 = value1 parameter2 = value2 kubectl create configmap my-config \\ --from-file = my-config.txt \\ --from-literal = extra-param = extra-value \\ --from-literal = another-param = another-value","title":"ConfigMaps"},{"location":"k8s/4-api/#secrets","text":"There is certain data that is extra-sensitive. This can include passwords, security tokens, or other types of private keys. Collectively, we call this type of data \u201csecrets.\u201d Secret data can be exposed to pods using the secrets volume type. Secrets volumes are managed by the kubelet and are created at pod creation time. Secrets are stored on tmpfs volumes (aka RAM disks) and, as such, are not written to disk on nodes. Each data element of a secret is stored in a separate file under the target mount point specified in the volume mount. # The TLS key and certificate for the kuard application can be downloaded by running the following commands. curl -o kuard.crt https://storage.googleapis.com/kuar-demo/kuard.crt curl -o kuard.key https://storage.googleapis.com/kuar-demo/kuard.key # With the kuard.crt and kuard.key files stored locally, we are ready to create a secret. kubectl create secret generic kuard-tls \\ --from-file = kuard.crt \\ --from-file = kuard.key # Replacing secrets from file kubectl create secret generic kuard-tls \\ --from-file = kuard.crt --from-file = kuard.key \\ --dry-run -o yaml | kubectl replace -f - # This command line first creates a new secret with the same name as our existing secret. If we just stopped there, the Kubernetes API server would return an error complaining that we are trying to create a secret that already exists. Instead, we tell kubectl not to actually send the data to the server but instead to dump the YAML that it would have sent to the API server to stdout. We then pipe that to kubectl replace and use -f - to tell it to read from stdin. In this way we can update a secret from files on disk without having to manually base64-encode data. Extracting secrets into a file kubectl get secret demo-secret -o json | jq -r .data.value | base64 --decode > ./demo-secret # Output the value of secret as JSON, run JQ to parse the output # As the secret data is base64 encoded, decode it before writing the data to the client machine","title":"Secrets"},{"location":"k8s/4-api/#deployments","text":"The Deployment object exists to manage the release of new versions. -In the output of describe there is a great deal of important information. Two of the most important pieces of information in the output are OldReplicaSets and NewReplicaSet . These fields point to the ReplicaSet objects this Deployment is currently managing. If a Deployment is in the middle of a rollout, both fields will be set to a value. If a rollout is complete, OldReplicaSets will be set to . You can use kubectl rollout history to obtain the history of rollouts associated with a particular Deployment. If you have a current Deployment in progress, then you can use kubectl rollout status to obtain the current status of a rollout. You can undo both partially completed and fully completed rollouts. An undo of a rollout is actually simply a rollout in reverse (e.g., from v2 to v1, instead of from v1 to v2), and all of the same policies that control the rollout strategy apply to the undo strategy as well. Specifying a revision of 0 is a shorthand way of specifying the previous revision. Or kubectl rollout undo If you ever want to manage that ReplicaSet directly, you need to delete the Deployment (remember to set --cascade to false, or else it will delete the ReplicaSet and Pods as well!). # You can see the label selector kubectl get deployments nginx \\ -o jsonpath --template { .spec.selector.matchLabels } # From this you can see that the Deployment is managing a ReplicaSet with the labels run=nginx. kubectl get replicasets --selector = run = nginx # If you are in the middle of a rollout and you want to temporarily pause it for some reason (e.g., if you start seeing weird behavior in your system and you want to investigate), you can use the pause command: kubectl rollout pause deployments nginx # If, after investigation, you believe the rollout can safely proceed, you can use the resume command to start up where you left off: kubectl rollout resume deployments nginx # You can see the deployment history by running: kubectl rollout history deployment nginx # If you are interested in more details about a particular revision, you can add the --revision flag to view details about that specific revision: kubectl rollout history deployment nginx --revision = 2 # You can roll back to a specific revision in the history using the --to-revision flag: kubectl rollout undo deployments nginx --to-revision = 3 NOTE: When you do a kubectl rollout undo you are updating the production state in a way that isn\u2019t reflected in your source control. An alternate (and perhaps preferred) way to undo a rollout is to revert your YAML file and kubectl apply the previous version. In this way your \u201cchange tracked configuration\u201d more closely tracks what is really running in your cluster.","title":"Deployments"},{"location":"k8s/4-api/#deployment-strategies","text":"Deployment strategies for rollingUpdate using the maxUnavailable parameter or the maxSurge parameter. The maxUnavailable parameter sets the maximum number of Pods that can be unavailable during a rolling update. It can either be set to an absolute number (e.g., 3 meaning a maximum of three Pods can be unavailable) or to a percentage (e.g., 20% meaning a maximum of 20% of the desired number of replicas can be unavailable). Generally speaking, using a percentage is a good approach for most services, since the value is correctly applicable regardless of the desired number of replicas in the Deployment. If you have four replicas and have set maxUnavailable to 50%, it will scale it down to two replicas. The rolling update will then replace the removed pods by scaling the new ReplicaSet up to two replicas, for a total of four replicas (two old, two new). It will then scale the old ReplicaSet down to zero replicas, for a total size of two new replicas. Finally, it will scale the new ReplicaSet up to four replicas, completing the rollout. Thus, with maxUnavailableset to 50%, our rollout completes in four steps, but with only 50% of our service capacity at times. However, there are situations where you don\u2019t want to fall below 100% capacity, but you are willing to temporarily use additional resources in order to perform a rollout. In these situations, you can set the maxUnavailable parameter to 0% , and instead control the rollout using the maxSurge parameter. The maxSurge parameter controls how many extra resources can be created to achieve a rollout. To illustrate how this works, imagine we have a service with 10 replicas. We set maxUnavailable to 0 and maxSurge to 20%. The first thing the rollout will do is scale the new ReplicaSet up to 2 replicas, for a total of 12 (120%) in the service. It will then scale the old ReplicaSet down to 8 replicas, for a total of 10 (8 old, 2 new) in the service. This process proceeds until the rollout is complete. At any time, the capacity of the service is guaranteed to be at least 100% and the maximum extra resources used for the rollout are limited to an additional 20% of all resources. Setting maxSurge to 100% is equivalent to a blue/green deployment . The Deployment controller first scales the new version up to 100% of the old version. Once the new version is healthy, it immediately scales the old version down to 0%. Setting minReadySeconds to 60 indicates that the Deployment must wait for 60 seconds after seeing a Pod become healthy before moving on to updating the next Pod. To set the timeout period, the Deployment parameter progressDeadlineSeconds is set to 600 . This sets the progress deadline to 10 minutes. If any particular stage in the rollout fails to progress in 10 minutes, then the Deployment is marked as failed, and all attempts to move the Deployment forward are halted.","title":"Deployment strategies"},{"location":"k8s/4-api/#ingress","text":"Service objects provide a great way to do simple TCP-level load balancing, they don\u2019t provide an application-level way to do load balancing and routing. The truth is that most of the applications that users deploy using containers and Kubernetes are HTTP web-based applications. These are better served by a load balancer that understands HTTP. To address these needs, the Ingress API was added to Kubernetes. Ingress represents a path and host-based HTTP load balancer and router. When you create an Ingress object, it receives a virtual IP address just like a Service, but instead of the one-to-one relationship between a Service IP address and a set of Pods, an Ingress can use the content of an HTTP request to route requests to different Services.","title":"Ingress"},{"location":"k8s/4-api/#statefulsets","text":"Some applications, especially stateful storage workloads or sharded applications, require more differentiation between the replicas in the application. To resolve this, Kubernetes has recently introduced StatefulSets as a complement to ReplicaSets, but for more stateful workloads. Like ReplicaSets, StatefulSets create multiple instances of the same container image running in a Kubernetes cluster, but the manner in which containers are created and destroyed is more deterministic , as are the names of each container. With StatefulSets, each replica receives a monotonically increasing index (e.g., backed-0, backend-1, and so on). StatefulSets guarantee that replica zero will be created and become healthy before replica one is created and so forth. StatefulSets receive DNS names so that each replica can be accessed directly. This allows clients to easily target specific shards in a sharded service.","title":"StatefulSets"},{"location":"k8s/chaos/","text":"Chaos Enginnering \u00b6 Generating Random number d = $(( ( RANDOM % 10 ) + 1 )) Example Cluster with examples Pure Chaos \u00b6 Install Registry # It's helpful to have a container registry during the build, push, and deploy phases. There is no need to shuttle private images over the internet. helm repo add twuni https://helm.twun.io helm install registry twuni/docker-registry \\ --version 1 .10.0 \\ --namespace kube-system \\ --set service.type = NodePort \\ --set service.nodePort = 31500 kubectl get service --namespace kube-system # Assign an environment variable to the common registry location export REGISTRY = 2886795330 -31500-kira01.environments.katacoda.com # It will be a few moments before the registry deployment reports it's Available kubectl get deployments registry-docker-registry --namespace kube-system # Once the registry is serving, inspect the contents of the empty registry curl $REGISTRY /v2/_catalog | jq -c Install Sample Application # Let's create a small collection of applications. # you will create a deployment of applications that log random messages. kubectl create namespace learning-place # Run the random-logger container in a Pod to start generating continuously random logging events kubectl create deployment random-logger --image = chentex/random-logger -n learning-place kubectl scale deployment/random-logger --replicas = 10 -n learning-place kubectl get pods -n learning-place Random Logger Snowflake Melter # The most common chaos for Kubernetes is to periodically and randomly terminate Pods. # To define the terminator, all we need is a container that has some logic in it to find the application Pods you just started and terminate them. The Kubernetes API offers all the control we need to find and remove Pods. # We'll choose Python as we can import a helpful Kubernetes API and the script can be loaded into a Python container. # cat snowflake_melter.py from kubernetes import client , config import random # Access Kubernetes config . load_incluster_config () v1 = client . CoreV1Api () # List Namespaces all_namespaces = v1 . list_namespace () # Get Pods from namespaces annotated with chaos marker pod_candidates = [] for namespace in all_namespaces . items : if ( namespace . metadata . annotations is not None and namespace . metadata . annotations . get ( \"chaos\" , None ) == 'yes' ): pods = v1 . list_namespaced_pod ( namespace . metadata . name ) pod_candidates . extend ( pods . items ) # Determine how many Pods to remove removal_count = random . randint ( 0 , len ( pod_candidates )) if len ( pod_candidates ) > 0 : print ( \"Found\" , len ( pod_candidates ), \"pods and melting\" , removal_count , \"of them.\" ) else : print ( \"No eligible Pods found with annotation chaos=yes.\" ) # Remove a few Pods for _ in range ( removal_count ): pod = random . choice ( pod_candidates ) pod_candidates . remove ( pod ) print ( \"Removing pod\" , pod . metadata . name , \"from namespace\" , pod . metadata . namespace , \".\" ) body = client . V1DeleteOptions () v1 . delete_namespaced_pod ( pod . metadata . name , pod . metadata . namespace , body = body ) Dockerfile # cat Dockerfile # ARGS at this level referenced only by FROMs ARG BASE_IMAGE = python:3.8.5-alpine3.12 # -------------------------------------- # Build dependencies in build stage # -------------------------------------- FROM ${ BASE_IMAGE } as builder WORKDIR /app # Cache installed dependencies between builds COPY ./requirements.txt ./requirements.txt RUN pip install -r ./requirements.txt --user # -------------------------------------- # Create final container loaded with app # -------------------------------------- FROM ${ BASE_IMAGE } LABEL scenario = pure-chaos ENV USER = docker GROUP = docker \\ UID = 12345 GID = 23456 \\ HOME = /app PYTHONUNBUFFERED = 1 # Create user/group RUN addgroup --gid \" ${ GID } \" \" ${ GROUP } \" \\ && adduser \\ --disabled-password \\ --gecos \"\" \\ --home \" $( pwd ) \" \\ --ingroup \" ${ GROUP } \" \\ --no-create-home \\ --uid \" ${ UID } \" \\ \" ${ USER } \" WORKDIR ${ HOME } # TODO, will switching user work? # USER ${USER} COPY --from = builder /root/.local /usr/local COPY --chown = ${ USER } : ${ GROUP } . . CMD [ \"python\" , \"snowflake_melter.py\" ] # cat requirements.txt kubernetes == 11 .0.0 Build and Push Image export IMAGE = $REGISTRY /snowflake_melter:0.1.0 docker build -t $IMAGE . docker push $IMAGE curl $REGISTRY /v2/_catalog | jq Invoke Chaos # Run your newly created application as a Kubernetes CronJob kubectl create cronjob snowflake-melter --image = $IMAGE --schedule = '*/1 * * * *' # The chaos CronJob is will now be running once a minute. More flexible chaos systems would randomize this period. kubectl get cronjobs # At the beginning of the next minute on the clock, the CronJob will create a new Pod. kubectl get pods # Every minute a new Pod will create and run the chaos logic. Kubernetes automatically purges the older Job Pods. Getting the logs from all the Jobs is a bit tricky, but there is a common client tool called Stern that collates and displays logs from related Pods. stern snowflake-melter --container-state terminated --since 2m --timestamps # You will discover in the logs that the code is reporting that it's not finding Pods that are eligible for deleting. Target the Chaos # The current logic for the chaotic Pod deletion requires a namespace to be annotated with chaos=yes. Assign the random-logger Pods as chaos targets by annotating the learning-place namespace. kubectl annotate namespace learning-place chaos = yes kubectl describe namespace learning-place # The next time chaos Job runs it will see this annotation and the interesting work will be reported. watch kubectl get pods -n learning-place For real applications, if scaled correctly, all this chaos and resilience will be happening behind the scenes in the cluster while your users experience no downtime or delays. You could modify the Python code a bit more and go crazy with other Kubernetes API calls to create clever forms of havoc. Chaos Mesh \u00b6 Chaos Mesh is a cloud native Chaos Engineering platform that orchestrates chaos on Kubernetes environments. At the current stage, it has the following components: - Chaos Operator : the core component for chaos orchestration; fully open source. - Chaos Dashboard : a Web UI for managing, designing, and monitoring Chaos Experiments; under development. Choas Mesh is one of the better chaos engines for Kubernetes because: 1. In a short amount of time there has been heavy community support and it's a CNCF sandbox project. 2. It's a native experience to Kubernetes leveraging the Operator Pattern and CRDs permitting IaC with your pipelines. 3. If you have followed the best practices by applying plenty of labels and annotations to your Deployments, then there is no need to make modifications to your apps for your chaos experiments. 4. There are a wide variety of experiment types, not just Pod killing. 5. Installs with a Helm chart and you have complete control over the engine with CRDs. - Install Chaos Mesh kubectl create namespace chaos-mesh # Add the chart repository for the Helm chart to be installed helm search repo chaos-mesh -l helm repo add chaos-mesh https://charts.chaos-mesh.org helm install chaos-mesh chaos-mesh/chaos-mesh \\ --version v2.0.0 \\ --namespace chaos-mesh \\ --set chaosDaemon.runtime = containerd \\ --set chaosDaemon.socketPath = /run/containerd/containerd.sock # Verify kubectl get deployments,pods,services --namespace chaos-mesh The control plane components for the Chaos Mesh are: 1. chaos=controller-manager : This is used to schedule and manage the lifecycle of chaos experiments. (This is a misnomer. This should be just named controller, not controller-manager, as it's the controller based on the Operator Pattern. The controller-manager is the Kubernetes control plane component that manages all the controllers like this one). 2. chaos-daemon : These are the Pods that control the chaos mesh. The Pods run on every cluster Node and are wrapped in a DaemonSet. These DaemonSets have privileged system permissions to access each Node's network, cgroups, chroot, and other resources that are accessed based on your experiments. 3. chaos-dashboard : An optional web interface providing you an alternate means to administer the engine and experiments. Its use is for convenience and any production use of the engine should be through the YAML resources for the Chaos Mesh CRDs. - Chaos Mesh Dashboard The chaos dashboard is accessible via a NodePort. For this scenario we need the nodePort at a specific value, rather than its current random port number. Set the nodePort to a specific port: kubectl patch service chaos-dashboard -n chaos-mesh --type = 'json' --patch = '[{\"op\": \"replace\", \"path\": \"/spec/ports/0/nodePort\", \"value\":31111}]' # With the correct port value set, the web interface for Chaos Mesh dashboard can be seen from the tab Chaos Mesh above the command-line area or this link: https://2886795275-31111-kira01.environments.katacoda.com/. - There are no experiments yet, but take a few moments to explore the general layout of the dashboard. There is a way through the user interface to create, update, and delete experiments. - Chaos Mesh Experiment Types ============================================================ Category Type Experiment Description Pod Lifecycle Pod Failure Killing pods. Pod Lifecycle Pod Kill Pods becoming unavailable. Pod Lifecycle Container Kill Killing containers in pods. Network Partition Separate Pods into independent subnets by blocking communication between them. Network Loss Inject network communication loss. Network Delay Inject network communication latency. Network Duplication Inject packet duplications. Network Corrupt Inject network communication corruption. Network Bandwidth Limit the network bandwidth. I/O Delay Inject delay during I/O. I/O Errno Inject error during I/O. I/O Delay and Errno Inject both delays and errors with I/O. Linux Kernel Inject kernel errors into pods. Clock Offset Inject clock skew into pods. Stress CPU Simulate pod CPU stress. Stress Memory Simulate pod memory stress. Stress CPU & Memory Simulate both CPU and memory stress on Pods. # cat web-show-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : web-show labels : app : web-show spec : replicas : 1 selector : matchLabels : app : web-show template : metadata : labels : app : web-show spec : containers : - name : web-show image : pingcap/web-show imagePullPolicy : Always command : - /usr/local/bin/web-show - --target-ip=$(TARGET_IP) env : - name : TARGET_IP valueFrom : configMapKeyRef : name : web-show-context key : target.ip ports : - name : web-port containerPort : 8081 hostPort : 8081 # cat web-show-service.yaml apiVersion : v1 kind : Service metadata : name : web-show labels : app : web-show spec : selector : app : web-show type : NodePort ports : - port : 8081 protocol : TCP targetPort : 8081 nodePort : 30081 # cat nginx.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx annotations : deployment.kubernetes.io/revision : \"1\" labels : app : nginx spec : replicas : 8 selector : matchLabels : app : nginx template : metadata : labels : app : nginx chaos : blast-here spec : containers : - image : nginx name : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : shine-on-you-crazy-diamond spec : replicas : 2 selector : matchLabels : app : cant-touch-dis template : metadata : labels : app : cant-touch-dis spec : containers : - image : nginx name : nginx # cat network-delay-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : NetworkChaos metadata : name : web-show-network-delay spec : action : delay mode : one selector : namespaces : - default labelSelectors : app : web-show delay : latency : 10ms # cat scheduled-network-delay-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : Schedule metadata : name : web-show-scheduled-network-delay spec : schedule : '@every 60s' type : NetworkChaos historyLimit : 5 concurrencyPolicy : Forbid networkChaos : action : delay mode : one selector : namespaces : - default labelSelectors : app : web-show delay : latency : 10ms duration : 30s # cat pod-removal-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : Schedule metadata : name : pod-kill-example namespace : chaos-mesh spec : schedule : '@every 15s' type : PodChaos historyLimit : 5 concurrencyPolicy : Forbid podChaos : action : pod-kill mode : one selector : namespaces : - chaos-sandbox labelSelectors : chaos : blast-here - Network Delay Experiment # Install an example application as a target for the experiment. This application is designed by the Chaos Mesh project as a hello world example for your first experiment. # The application needs an environment variable for the TARGET_IP, which is the cluster IP, so this context you provide as a ConfigMap. That ConfigMap variable is referenced in the Deployment YAML. TARGET_IP = $( kubectl get pod -n kube-system -o wide | grep kube-controller | head -n 1 | awk '{print $6}' ) kubectl create configmap web-show-context --from-literal = target.ip = ${ TARGET_IP } kubectl apply -f web-show-deployment.yaml kubectl apply -f web-show-service.yaml # With the web-show application running, its web interface can be accessed from the \"Web Show\" above the command-line area or this link: https://2886795275-30081-kira01.environments.katacoda.com/ # Define Experiment kubectl get crds # The Chaos Mesh has installed several custom resources # You can reference these resources to create declarative YAML manifests that define your experiment. # For your first experiment, you will impose a network delay. The delay is defined in the NetworkChaos manifest # The experiment declares that a 10ms network delay should be injected. The delay will only be applied to the target service labeled \"app\": \"web-show\". # This is the **blast radius**. kubectl get deployments,pods -l app = 'web-show' # Apply Experiment kubectl apply -f network-delay-experiment.yaml # The experiment is now running. kubectl get NetworkChaos # The application has a built-in graph that will show the latency it's experiencing. With the experiment applied you will see the 10ms delay. # Update Experiment # At any time you can change the YAML declaration and apply further experiment updates. kubectl apply -f network-delay-experiment.yaml # The experiment can be paused kubectl annotate networkchaos web-show-network-delay experiment.chaos-mesh.org/pause = true # and resumed kubectl annotate networkchaos web-show-network-delay experiment.chaos-mesh.org/pause- # Since the NetworkChaos is like any other Kubernetes resource, the experiment can be easily removed. kubectl delete -f network-delay-experiment.yaml - Scheduled Experiment # This experiment will inject network chaos periodically: 10ms network delay should be injected every minute that lasts for 30 seconds # Apply Scheduled Experiment kubectl apply -f scheduled-network-delay-experiment.yaml # The schedule experiment is now running. Scheduled experiment will not create NetworkChaos object immediately, intead it creates an Schedule object called web-show-scheduled-network-delay kubectl get Schedule # NetworkChaos is very similar with what between CronJob and Job: Schedule will spawn NetworkChaos when trigger by @every 60s. kubectl get NetworkChaos -w # The experiment can be paused kubectl annotate schedule web-show-scheduled-network-delay experiment.chaos-mesh.org/pause = true # and resumed: kubectl annotate schedule web-show-scheduled-network-delay experiment.chaos-mesh.org/pause- - Pod Removal Experiment # Install an example application as a target for the experiment. It's just a deployment of the common Nginx web server with Pod replications. Apply the Deployment to the chaos-sandbox namespace. kubectl create namespace chaos-sandbox kubectl apply -f nginx.yaml -n chaos-sandbox # The experiment declares that the specific pod should be killed every 15s. The removal will only be applied to the target pod labeled \"chaos\": \"blast here\", which is the blast radius. # Apply Experiment kubectl apply -f pod-removal-experiment.yaml kubectl get Schedule -n chaos-mesh # Based on the cron time in the experiment, watch the Pods randomly terminate and new ones start. watch kubectl get -n chaos-sandbox deployments,pods,services # Notice the blast radius is targeting only the nginx Pods, while the shine-on-you-crazy-diamond Pods remain undisturbed. Litmus \u00b6 Litmus is a toolset to do cloud native chaos engineering. Litmus provides tools to orchestrate chaos on Kubernetes to help SREs find weaknesses in their deployments. SREs use Litmus to run chaos experiments initially in the staging environment and eventually in production to find bugs and vulnerabilities. Fixing the weaknesses leads to increased resilience of the system. - Litmus offers you these compelling features: 1. Kubernetes native CRDs to manage chaos. Using chaos API, orchestration, scheduling, and complex workflow management can be orchestrated declaratively. 2. Most of the generic chaos experiments are readily available for you to get started with your initial chaos engineering needs. 3. An SDK is available in GO, Python, and Ansible. A basic experiment structure is created quickly using SDK and developers and SREs just need to add the chaos logic to make a new experiment. 4. It's simple to complex chaos workflows are easy to construct. Use GitOps and the chaos workflows to scale your chaos engineering efforts and increase the resilience of your Kubernetes platform. # For this scenario, we'll install the standard NGINX application and make it a target. Install NGINX into the default namespace. kubectl create deploy nginx --image = nginx kubectl get deployments,pods --show-labels - Install Litmus Operator # The recommended way to start Litmus is by installing the Litmus Operator. kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v1.8.0.yaml kubectl get namespaces # In the list, you see litmus as a new namespace. # An operator is a custom Kubernetes controller that uses custom resources (CR) to manage applications and their components. The Litmus Operator is comprised of a few controllers maintaining the CRs. kubectl get crds | grep litmus # Check the Litmus API resources are available kubectl api-resources | grep litmus kubectl get all -n litmus - The key components and object associated with Litmus are: 1. RBAC for chaotic administration access targeted objects on your cluster. 2. The Litmus controller that manages the custom resources and the following apps: - ChaosEngine : A resource to link a Kubernetes application or Kubernetes node to a ChaosExperiment. ChaosEngine is watched by Litmus' Chaos-Operator which then invokes Chaos-Experiments. - ChaosExperiment : A resource to group the configuration parameters of a chaos experiment. ChaosExperiment CRs are created by the operator when experiments are invoked by ChaosEngine. - ChaosResult : A resource to hold the results of a chaos-experiment. The Chaos-exporter reads the results and exports the metrics into a configured Prometheus server. - Install Chaos Experiments - These experiments are installed on your cluster as Litmus resources declarations in the form of the Kubernetes CRDs. Because the chaos experiments are just Kubernetes YAML manifests, these experiments are published on Chaos Hub . - generic/pod-delete kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/experiment.yaml # Verify the pod-delete experiment has been installed kubectl get chaosexperiments # Setup RBAC with Service Account # A service account should be created to allow ChaosEngine to run experiments in your application namespace. kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/rbac.yaml # Verify the ServiceAccount RBAC rules have been applied for pod-delete-sa kubectl get serviceaccount,role,rolebinding # Annotate Application # In this case, we'll annotate the NGINX deployment with litmuschaos.io/chaos=\"true\" kubectl annotate deploy/nginx litmuschaos.io/chaos = \"true\" # Verify the annotation has been applied kubectl get deployment nginx -o = custom-columns = 'ANNOTATIONS:metadata.annotations' # Run the Experiment kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/engine.yaml # Start watching the Pods in the default namespace watch -n 1 kubectl get pods # In a moment an nginx-chaos-runner Pod will start. This Pod is created by the Litmus engine based on the experiment criteria. # In a moment, the chaos-runner will create a new Pod called pod-delete-<hash>. This Pod is responsible for the actual Pod deletion. # Shortly after the pod-delete-<hash> Pod starts, you'll notice the NGINX Pod is killed. # Observe and Verify Experiments kubectl describe chaosresult nginx-chaos-pod-delete # The status.verdict is set to Awaited when the experiment is in progress, eventually changing to either Pass or Fail. Chaoskube \u00b6 Chaoskube periodically kills random Pods in your Kubernetes cluster, which allows you to test how your system behaves under arbitrary Pod failures. Helm Input kubectl version --short && \\ kubectl get componentstatus && \\ kubectl get nodes && \\ kubectl cluster-info helm version --short kubectl create namespace chaoskube helm repo add chaoskube https://linki.github.io/chaoskube # Install the chart # The interval parameter instructs Chaoskube to kill Pods every 20 seconds. # The targeted Pods are any with the label app-purpose=chaos, and the kube-system namespace has to be explicitly excluded (!) from the list of namespaces to look for Pods to kill. helm install chaoskube chaoskube/chaoskube \\ --version = 0 .1.0 \\ --namespace chaoskube \\ --set image.tag = v0.21.0 \\ --set dryRun = false \\ --set 'namespaces=!kube-system' \\ --set labels = app-purpose = chaos \\ --set interval = 20s kubectl get -n chaoskube deployments kubectl rollout -n chaoskube status deployment chaoskube # You can periodically check the Chaoskube log to see its Pod killing activity. POD = $( kubectl -n chaoskube get pods -l = 'app.kubernetes.io/instance=chaoskube' --output = jsonpath = '{.items[0].metadata.name}' ) kubectl -n chaoskube logs -f $POD # nginx.yaml apiVersion : apps/v1 kind : Deployment metadata : annotations : deployment.kubernetes.io/revision : \"1\" labels : app : nginx app-purpose : chaos name : nginx spec : replicas : 8 selector : matchLabels : app : nginx template : metadata : labels : app : nginx app-purpose : chaos spec : containers : - image : nginx name : nginx # cat ghost.yaml apiVersion : apps/v1 kind : Deployment metadata : annotations : deployment.kubernetes.io/revision : \"1\" labels : app : ghost app-purpose : chaos name : ghost spec : replicas : 3 selector : matchLabels : app : ghost template : metadata : labels : app : ghost app-purpose : chaos spec : containers : - image : ghost:3.11.0-alpine name : ghost The Deployments and Pods are labeled to mark these Pods as potential victim targets of the Chaoskube Pod killer. The Deployment and Pod template have the label app-purpose: chaos that makes the Pod an eligible target for Chaoskube. The label is provided as a configuration value during the Helm chart installation. kubectl apply -f nginx.yaml kubectl create namespace more-apps kubectl create --namespace more-apps kubectl apply -f ghost.yaml Observe the Chaos Notice as Pods are deleted every 20 secs, the Kubernetes resilience feature is making sure they are restored. watch kubectl get deployments,pods --all-namespaces -l app-purpose = chaos In a real chaos testing platform, you should complement this Pod killing activity with automated tests to ensure these disruptions are either unnoticed or acceptable for your business processes.","title":"Chaos Enginnering"},{"location":"k8s/chaos/#chaos-enginnering","text":"Generating Random number d = $(( ( RANDOM % 10 ) + 1 )) Example Cluster with examples","title":"Chaos Enginnering"},{"location":"k8s/chaos/#pure-chaos","text":"Install Registry # It's helpful to have a container registry during the build, push, and deploy phases. There is no need to shuttle private images over the internet. helm repo add twuni https://helm.twun.io helm install registry twuni/docker-registry \\ --version 1 .10.0 \\ --namespace kube-system \\ --set service.type = NodePort \\ --set service.nodePort = 31500 kubectl get service --namespace kube-system # Assign an environment variable to the common registry location export REGISTRY = 2886795330 -31500-kira01.environments.katacoda.com # It will be a few moments before the registry deployment reports it's Available kubectl get deployments registry-docker-registry --namespace kube-system # Once the registry is serving, inspect the contents of the empty registry curl $REGISTRY /v2/_catalog | jq -c Install Sample Application # Let's create a small collection of applications. # you will create a deployment of applications that log random messages. kubectl create namespace learning-place # Run the random-logger container in a Pod to start generating continuously random logging events kubectl create deployment random-logger --image = chentex/random-logger -n learning-place kubectl scale deployment/random-logger --replicas = 10 -n learning-place kubectl get pods -n learning-place Random Logger Snowflake Melter # The most common chaos for Kubernetes is to periodically and randomly terminate Pods. # To define the terminator, all we need is a container that has some logic in it to find the application Pods you just started and terminate them. The Kubernetes API offers all the control we need to find and remove Pods. # We'll choose Python as we can import a helpful Kubernetes API and the script can be loaded into a Python container. # cat snowflake_melter.py from kubernetes import client , config import random # Access Kubernetes config . load_incluster_config () v1 = client . CoreV1Api () # List Namespaces all_namespaces = v1 . list_namespace () # Get Pods from namespaces annotated with chaos marker pod_candidates = [] for namespace in all_namespaces . items : if ( namespace . metadata . annotations is not None and namespace . metadata . annotations . get ( \"chaos\" , None ) == 'yes' ): pods = v1 . list_namespaced_pod ( namespace . metadata . name ) pod_candidates . extend ( pods . items ) # Determine how many Pods to remove removal_count = random . randint ( 0 , len ( pod_candidates )) if len ( pod_candidates ) > 0 : print ( \"Found\" , len ( pod_candidates ), \"pods and melting\" , removal_count , \"of them.\" ) else : print ( \"No eligible Pods found with annotation chaos=yes.\" ) # Remove a few Pods for _ in range ( removal_count ): pod = random . choice ( pod_candidates ) pod_candidates . remove ( pod ) print ( \"Removing pod\" , pod . metadata . name , \"from namespace\" , pod . metadata . namespace , \".\" ) body = client . V1DeleteOptions () v1 . delete_namespaced_pod ( pod . metadata . name , pod . metadata . namespace , body = body ) Dockerfile # cat Dockerfile # ARGS at this level referenced only by FROMs ARG BASE_IMAGE = python:3.8.5-alpine3.12 # -------------------------------------- # Build dependencies in build stage # -------------------------------------- FROM ${ BASE_IMAGE } as builder WORKDIR /app # Cache installed dependencies between builds COPY ./requirements.txt ./requirements.txt RUN pip install -r ./requirements.txt --user # -------------------------------------- # Create final container loaded with app # -------------------------------------- FROM ${ BASE_IMAGE } LABEL scenario = pure-chaos ENV USER = docker GROUP = docker \\ UID = 12345 GID = 23456 \\ HOME = /app PYTHONUNBUFFERED = 1 # Create user/group RUN addgroup --gid \" ${ GID } \" \" ${ GROUP } \" \\ && adduser \\ --disabled-password \\ --gecos \"\" \\ --home \" $( pwd ) \" \\ --ingroup \" ${ GROUP } \" \\ --no-create-home \\ --uid \" ${ UID } \" \\ \" ${ USER } \" WORKDIR ${ HOME } # TODO, will switching user work? # USER ${USER} COPY --from = builder /root/.local /usr/local COPY --chown = ${ USER } : ${ GROUP } . . CMD [ \"python\" , \"snowflake_melter.py\" ] # cat requirements.txt kubernetes == 11 .0.0 Build and Push Image export IMAGE = $REGISTRY /snowflake_melter:0.1.0 docker build -t $IMAGE . docker push $IMAGE curl $REGISTRY /v2/_catalog | jq Invoke Chaos # Run your newly created application as a Kubernetes CronJob kubectl create cronjob snowflake-melter --image = $IMAGE --schedule = '*/1 * * * *' # The chaos CronJob is will now be running once a minute. More flexible chaos systems would randomize this period. kubectl get cronjobs # At the beginning of the next minute on the clock, the CronJob will create a new Pod. kubectl get pods # Every minute a new Pod will create and run the chaos logic. Kubernetes automatically purges the older Job Pods. Getting the logs from all the Jobs is a bit tricky, but there is a common client tool called Stern that collates and displays logs from related Pods. stern snowflake-melter --container-state terminated --since 2m --timestamps # You will discover in the logs that the code is reporting that it's not finding Pods that are eligible for deleting. Target the Chaos # The current logic for the chaotic Pod deletion requires a namespace to be annotated with chaos=yes. Assign the random-logger Pods as chaos targets by annotating the learning-place namespace. kubectl annotate namespace learning-place chaos = yes kubectl describe namespace learning-place # The next time chaos Job runs it will see this annotation and the interesting work will be reported. watch kubectl get pods -n learning-place For real applications, if scaled correctly, all this chaos and resilience will be happening behind the scenes in the cluster while your users experience no downtime or delays. You could modify the Python code a bit more and go crazy with other Kubernetes API calls to create clever forms of havoc.","title":"Pure Chaos"},{"location":"k8s/chaos/#chaos-mesh","text":"Chaos Mesh is a cloud native Chaos Engineering platform that orchestrates chaos on Kubernetes environments. At the current stage, it has the following components: - Chaos Operator : the core component for chaos orchestration; fully open source. - Chaos Dashboard : a Web UI for managing, designing, and monitoring Chaos Experiments; under development. Choas Mesh is one of the better chaos engines for Kubernetes because: 1. In a short amount of time there has been heavy community support and it's a CNCF sandbox project. 2. It's a native experience to Kubernetes leveraging the Operator Pattern and CRDs permitting IaC with your pipelines. 3. If you have followed the best practices by applying plenty of labels and annotations to your Deployments, then there is no need to make modifications to your apps for your chaos experiments. 4. There are a wide variety of experiment types, not just Pod killing. 5. Installs with a Helm chart and you have complete control over the engine with CRDs. - Install Chaos Mesh kubectl create namespace chaos-mesh # Add the chart repository for the Helm chart to be installed helm search repo chaos-mesh -l helm repo add chaos-mesh https://charts.chaos-mesh.org helm install chaos-mesh chaos-mesh/chaos-mesh \\ --version v2.0.0 \\ --namespace chaos-mesh \\ --set chaosDaemon.runtime = containerd \\ --set chaosDaemon.socketPath = /run/containerd/containerd.sock # Verify kubectl get deployments,pods,services --namespace chaos-mesh The control plane components for the Chaos Mesh are: 1. chaos=controller-manager : This is used to schedule and manage the lifecycle of chaos experiments. (This is a misnomer. This should be just named controller, not controller-manager, as it's the controller based on the Operator Pattern. The controller-manager is the Kubernetes control plane component that manages all the controllers like this one). 2. chaos-daemon : These are the Pods that control the chaos mesh. The Pods run on every cluster Node and are wrapped in a DaemonSet. These DaemonSets have privileged system permissions to access each Node's network, cgroups, chroot, and other resources that are accessed based on your experiments. 3. chaos-dashboard : An optional web interface providing you an alternate means to administer the engine and experiments. Its use is for convenience and any production use of the engine should be through the YAML resources for the Chaos Mesh CRDs. - Chaos Mesh Dashboard The chaos dashboard is accessible via a NodePort. For this scenario we need the nodePort at a specific value, rather than its current random port number. Set the nodePort to a specific port: kubectl patch service chaos-dashboard -n chaos-mesh --type = 'json' --patch = '[{\"op\": \"replace\", \"path\": \"/spec/ports/0/nodePort\", \"value\":31111}]' # With the correct port value set, the web interface for Chaos Mesh dashboard can be seen from the tab Chaos Mesh above the command-line area or this link: https://2886795275-31111-kira01.environments.katacoda.com/. - There are no experiments yet, but take a few moments to explore the general layout of the dashboard. There is a way through the user interface to create, update, and delete experiments. - Chaos Mesh Experiment Types ============================================================ Category Type Experiment Description Pod Lifecycle Pod Failure Killing pods. Pod Lifecycle Pod Kill Pods becoming unavailable. Pod Lifecycle Container Kill Killing containers in pods. Network Partition Separate Pods into independent subnets by blocking communication between them. Network Loss Inject network communication loss. Network Delay Inject network communication latency. Network Duplication Inject packet duplications. Network Corrupt Inject network communication corruption. Network Bandwidth Limit the network bandwidth. I/O Delay Inject delay during I/O. I/O Errno Inject error during I/O. I/O Delay and Errno Inject both delays and errors with I/O. Linux Kernel Inject kernel errors into pods. Clock Offset Inject clock skew into pods. Stress CPU Simulate pod CPU stress. Stress Memory Simulate pod memory stress. Stress CPU & Memory Simulate both CPU and memory stress on Pods. # cat web-show-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : web-show labels : app : web-show spec : replicas : 1 selector : matchLabels : app : web-show template : metadata : labels : app : web-show spec : containers : - name : web-show image : pingcap/web-show imagePullPolicy : Always command : - /usr/local/bin/web-show - --target-ip=$(TARGET_IP) env : - name : TARGET_IP valueFrom : configMapKeyRef : name : web-show-context key : target.ip ports : - name : web-port containerPort : 8081 hostPort : 8081 # cat web-show-service.yaml apiVersion : v1 kind : Service metadata : name : web-show labels : app : web-show spec : selector : app : web-show type : NodePort ports : - port : 8081 protocol : TCP targetPort : 8081 nodePort : 30081 # cat nginx.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx annotations : deployment.kubernetes.io/revision : \"1\" labels : app : nginx spec : replicas : 8 selector : matchLabels : app : nginx template : metadata : labels : app : nginx chaos : blast-here spec : containers : - image : nginx name : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : shine-on-you-crazy-diamond spec : replicas : 2 selector : matchLabels : app : cant-touch-dis template : metadata : labels : app : cant-touch-dis spec : containers : - image : nginx name : nginx # cat network-delay-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : NetworkChaos metadata : name : web-show-network-delay spec : action : delay mode : one selector : namespaces : - default labelSelectors : app : web-show delay : latency : 10ms # cat scheduled-network-delay-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : Schedule metadata : name : web-show-scheduled-network-delay spec : schedule : '@every 60s' type : NetworkChaos historyLimit : 5 concurrencyPolicy : Forbid networkChaos : action : delay mode : one selector : namespaces : - default labelSelectors : app : web-show delay : latency : 10ms duration : 30s # cat pod-removal-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : Schedule metadata : name : pod-kill-example namespace : chaos-mesh spec : schedule : '@every 15s' type : PodChaos historyLimit : 5 concurrencyPolicy : Forbid podChaos : action : pod-kill mode : one selector : namespaces : - chaos-sandbox labelSelectors : chaos : blast-here - Network Delay Experiment # Install an example application as a target for the experiment. This application is designed by the Chaos Mesh project as a hello world example for your first experiment. # The application needs an environment variable for the TARGET_IP, which is the cluster IP, so this context you provide as a ConfigMap. That ConfigMap variable is referenced in the Deployment YAML. TARGET_IP = $( kubectl get pod -n kube-system -o wide | grep kube-controller | head -n 1 | awk '{print $6}' ) kubectl create configmap web-show-context --from-literal = target.ip = ${ TARGET_IP } kubectl apply -f web-show-deployment.yaml kubectl apply -f web-show-service.yaml # With the web-show application running, its web interface can be accessed from the \"Web Show\" above the command-line area or this link: https://2886795275-30081-kira01.environments.katacoda.com/ # Define Experiment kubectl get crds # The Chaos Mesh has installed several custom resources # You can reference these resources to create declarative YAML manifests that define your experiment. # For your first experiment, you will impose a network delay. The delay is defined in the NetworkChaos manifest # The experiment declares that a 10ms network delay should be injected. The delay will only be applied to the target service labeled \"app\": \"web-show\". # This is the **blast radius**. kubectl get deployments,pods -l app = 'web-show' # Apply Experiment kubectl apply -f network-delay-experiment.yaml # The experiment is now running. kubectl get NetworkChaos # The application has a built-in graph that will show the latency it's experiencing. With the experiment applied you will see the 10ms delay. # Update Experiment # At any time you can change the YAML declaration and apply further experiment updates. kubectl apply -f network-delay-experiment.yaml # The experiment can be paused kubectl annotate networkchaos web-show-network-delay experiment.chaos-mesh.org/pause = true # and resumed kubectl annotate networkchaos web-show-network-delay experiment.chaos-mesh.org/pause- # Since the NetworkChaos is like any other Kubernetes resource, the experiment can be easily removed. kubectl delete -f network-delay-experiment.yaml - Scheduled Experiment # This experiment will inject network chaos periodically: 10ms network delay should be injected every minute that lasts for 30 seconds # Apply Scheduled Experiment kubectl apply -f scheduled-network-delay-experiment.yaml # The schedule experiment is now running. Scheduled experiment will not create NetworkChaos object immediately, intead it creates an Schedule object called web-show-scheduled-network-delay kubectl get Schedule # NetworkChaos is very similar with what between CronJob and Job: Schedule will spawn NetworkChaos when trigger by @every 60s. kubectl get NetworkChaos -w # The experiment can be paused kubectl annotate schedule web-show-scheduled-network-delay experiment.chaos-mesh.org/pause = true # and resumed: kubectl annotate schedule web-show-scheduled-network-delay experiment.chaos-mesh.org/pause- - Pod Removal Experiment # Install an example application as a target for the experiment. It's just a deployment of the common Nginx web server with Pod replications. Apply the Deployment to the chaos-sandbox namespace. kubectl create namespace chaos-sandbox kubectl apply -f nginx.yaml -n chaos-sandbox # The experiment declares that the specific pod should be killed every 15s. The removal will only be applied to the target pod labeled \"chaos\": \"blast here\", which is the blast radius. # Apply Experiment kubectl apply -f pod-removal-experiment.yaml kubectl get Schedule -n chaos-mesh # Based on the cron time in the experiment, watch the Pods randomly terminate and new ones start. watch kubectl get -n chaos-sandbox deployments,pods,services # Notice the blast radius is targeting only the nginx Pods, while the shine-on-you-crazy-diamond Pods remain undisturbed.","title":"Chaos Mesh"},{"location":"k8s/chaos/#litmus","text":"Litmus is a toolset to do cloud native chaos engineering. Litmus provides tools to orchestrate chaos on Kubernetes to help SREs find weaknesses in their deployments. SREs use Litmus to run chaos experiments initially in the staging environment and eventually in production to find bugs and vulnerabilities. Fixing the weaknesses leads to increased resilience of the system. - Litmus offers you these compelling features: 1. Kubernetes native CRDs to manage chaos. Using chaos API, orchestration, scheduling, and complex workflow management can be orchestrated declaratively. 2. Most of the generic chaos experiments are readily available for you to get started with your initial chaos engineering needs. 3. An SDK is available in GO, Python, and Ansible. A basic experiment structure is created quickly using SDK and developers and SREs just need to add the chaos logic to make a new experiment. 4. It's simple to complex chaos workflows are easy to construct. Use GitOps and the chaos workflows to scale your chaos engineering efforts and increase the resilience of your Kubernetes platform. # For this scenario, we'll install the standard NGINX application and make it a target. Install NGINX into the default namespace. kubectl create deploy nginx --image = nginx kubectl get deployments,pods --show-labels - Install Litmus Operator # The recommended way to start Litmus is by installing the Litmus Operator. kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v1.8.0.yaml kubectl get namespaces # In the list, you see litmus as a new namespace. # An operator is a custom Kubernetes controller that uses custom resources (CR) to manage applications and their components. The Litmus Operator is comprised of a few controllers maintaining the CRs. kubectl get crds | grep litmus # Check the Litmus API resources are available kubectl api-resources | grep litmus kubectl get all -n litmus - The key components and object associated with Litmus are: 1. RBAC for chaotic administration access targeted objects on your cluster. 2. The Litmus controller that manages the custom resources and the following apps: - ChaosEngine : A resource to link a Kubernetes application or Kubernetes node to a ChaosExperiment. ChaosEngine is watched by Litmus' Chaos-Operator which then invokes Chaos-Experiments. - ChaosExperiment : A resource to group the configuration parameters of a chaos experiment. ChaosExperiment CRs are created by the operator when experiments are invoked by ChaosEngine. - ChaosResult : A resource to hold the results of a chaos-experiment. The Chaos-exporter reads the results and exports the metrics into a configured Prometheus server. - Install Chaos Experiments - These experiments are installed on your cluster as Litmus resources declarations in the form of the Kubernetes CRDs. Because the chaos experiments are just Kubernetes YAML manifests, these experiments are published on Chaos Hub . - generic/pod-delete kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/experiment.yaml # Verify the pod-delete experiment has been installed kubectl get chaosexperiments # Setup RBAC with Service Account # A service account should be created to allow ChaosEngine to run experiments in your application namespace. kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/rbac.yaml # Verify the ServiceAccount RBAC rules have been applied for pod-delete-sa kubectl get serviceaccount,role,rolebinding # Annotate Application # In this case, we'll annotate the NGINX deployment with litmuschaos.io/chaos=\"true\" kubectl annotate deploy/nginx litmuschaos.io/chaos = \"true\" # Verify the annotation has been applied kubectl get deployment nginx -o = custom-columns = 'ANNOTATIONS:metadata.annotations' # Run the Experiment kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/engine.yaml # Start watching the Pods in the default namespace watch -n 1 kubectl get pods # In a moment an nginx-chaos-runner Pod will start. This Pod is created by the Litmus engine based on the experiment criteria. # In a moment, the chaos-runner will create a new Pod called pod-delete-<hash>. This Pod is responsible for the actual Pod deletion. # Shortly after the pod-delete-<hash> Pod starts, you'll notice the NGINX Pod is killed. # Observe and Verify Experiments kubectl describe chaosresult nginx-chaos-pod-delete # The status.verdict is set to Awaited when the experiment is in progress, eventually changing to either Pass or Fail.","title":"Litmus"},{"location":"k8s/chaos/#chaoskube","text":"Chaoskube periodically kills random Pods in your Kubernetes cluster, which allows you to test how your system behaves under arbitrary Pod failures. Helm Input kubectl version --short && \\ kubectl get componentstatus && \\ kubectl get nodes && \\ kubectl cluster-info helm version --short kubectl create namespace chaoskube helm repo add chaoskube https://linki.github.io/chaoskube # Install the chart # The interval parameter instructs Chaoskube to kill Pods every 20 seconds. # The targeted Pods are any with the label app-purpose=chaos, and the kube-system namespace has to be explicitly excluded (!) from the list of namespaces to look for Pods to kill. helm install chaoskube chaoskube/chaoskube \\ --version = 0 .1.0 \\ --namespace chaoskube \\ --set image.tag = v0.21.0 \\ --set dryRun = false \\ --set 'namespaces=!kube-system' \\ --set labels = app-purpose = chaos \\ --set interval = 20s kubectl get -n chaoskube deployments kubectl rollout -n chaoskube status deployment chaoskube # You can periodically check the Chaoskube log to see its Pod killing activity. POD = $( kubectl -n chaoskube get pods -l = 'app.kubernetes.io/instance=chaoskube' --output = jsonpath = '{.items[0].metadata.name}' ) kubectl -n chaoskube logs -f $POD # nginx.yaml apiVersion : apps/v1 kind : Deployment metadata : annotations : deployment.kubernetes.io/revision : \"1\" labels : app : nginx app-purpose : chaos name : nginx spec : replicas : 8 selector : matchLabels : app : nginx template : metadata : labels : app : nginx app-purpose : chaos spec : containers : - image : nginx name : nginx # cat ghost.yaml apiVersion : apps/v1 kind : Deployment metadata : annotations : deployment.kubernetes.io/revision : \"1\" labels : app : ghost app-purpose : chaos name : ghost spec : replicas : 3 selector : matchLabels : app : ghost template : metadata : labels : app : ghost app-purpose : chaos spec : containers : - image : ghost:3.11.0-alpine name : ghost The Deployments and Pods are labeled to mark these Pods as potential victim targets of the Chaoskube Pod killer. The Deployment and Pod template have the label app-purpose: chaos that makes the Pod an eligible target for Chaoskube. The label is provided as a configuration value during the Helm chart installation. kubectl apply -f nginx.yaml kubectl create namespace more-apps kubectl create --namespace more-apps kubectl apply -f ghost.yaml Observe the Chaos Notice as Pods are deleted every 20 secs, the Kubernetes resilience feature is making sure they are restored. watch kubectl get deployments,pods --all-namespaces -l app-purpose = chaos In a real chaos testing platform, you should complement this Pod killing activity with automated tests to ensure these disruptions are either unnoticed or acceptable for your business processes.","title":"Chaoskube"},{"location":"k8s/install/","text":"Installing K8s \u00b6 Vagrant Setup \u00b6 Install Dev Setup Publish a web app Deployment Pattern Terraform App Deployment Kubeadm \u00b6 Kops \u00b6 Verification \u00b6 Networking vagrant ssh k8s-m-1 # Check Routing within the master node sudo apt-get install net-tools route # Displays the routing network # Check syslog errors tail -f /var/log/syslog # Ctrl + Z to exit # Copy file from Master Node to Host machine mkdir -p ~/.kube vagrant port k8s-m-1 # Find the SSH port of the k8s-m-1 server # Copy the file using scp (ssh password is vagrant) scp -P 2222 vagrant@127.0.0.1:/home/vagrant/.kube/config ~/.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config # Get Cluster Information kubectl cluster-info # Get Master Node Component health kubectl get componentstatus # In case Scheduler or Controller Manager is showing as Unhealthy or Connection refused. # Modify the following files on all master nodes: sudo vi /etc/kubernetes/manifests/kube-scheduler.yaml # Clear the line (spec->containers->command) containing this phrase: - --port=0 sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml # Clear the line (spec->containers->command) containing this phrase: - --port=0 sudo systemctl restart kubelet.service","title":"Installation"},{"location":"k8s/install/#installing-k8s","text":"","title":"Installing K8s"},{"location":"k8s/install/#vagrant-setup","text":"Install Dev Setup Publish a web app Deployment Pattern Terraform App Deployment","title":"Vagrant Setup"},{"location":"k8s/install/#kubeadm","text":"","title":"Kubeadm"},{"location":"k8s/install/#kops","text":"","title":"Kops"},{"location":"k8s/install/#verification","text":"Networking vagrant ssh k8s-m-1 # Check Routing within the master node sudo apt-get install net-tools route # Displays the routing network # Check syslog errors tail -f /var/log/syslog # Ctrl + Z to exit # Copy file from Master Node to Host machine mkdir -p ~/.kube vagrant port k8s-m-1 # Find the SSH port of the k8s-m-1 server # Copy the file using scp (ssh password is vagrant) scp -P 2222 vagrant@127.0.0.1:/home/vagrant/.kube/config ~/.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config # Get Cluster Information kubectl cluster-info # Get Master Node Component health kubectl get componentstatus # In case Scheduler or Controller Manager is showing as Unhealthy or Connection refused. # Modify the following files on all master nodes: sudo vi /etc/kubernetes/manifests/kube-scheduler.yaml # Clear the line (spec->containers->command) containing this phrase: - --port=0 sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml # Clear the line (spec->containers->command) containing this phrase: - --port=0 sudo systemctl restart kubelet.service","title":"Verification"},{"location":"k8s/task/","text":"Cluster - Use kubeadm to install a basic cluster - Kubernetes the Hard way - Perform a version upgrade (Minor version upgrade) - Implement etcd backup and restore (Always check where the data directory is on the host path mentioned in Volume section) - Provision underlying infrastructure to deploy a cluster - Manage a HA cluster RBAC - Manage RBAC Troubleshoot Worker Node (30%) - Kubelet Service status and Journald Log parsing - Docker Service status - Consult container logs directly from Docker - Consult resources pod, ds, cm in kube-system namespaces Challenges \u00b6 Use etcdctl to get data of a pod Use etcdctl for changes by watching output docker commands to check logs, exec into running containers openssl to check expiry of certs # On Master cd /etc/kubernetes/pki openssl x509 -text -noout -in ./apiserver.crt # Commit to memory openssl x509 -text -noout -in ./apiserver.crt | grep Validity -A 2 apt-get to check package information, cache, available versions","title":"Task"},{"location":"k8s/task/#challenges","text":"Use etcdctl to get data of a pod Use etcdctl for changes by watching output docker commands to check logs, exec into running containers openssl to check expiry of certs # On Master cd /etc/kubernetes/pki openssl x509 -text -noout -in ./apiserver.crt # Commit to memory openssl x509 -text -noout -in ./apiserver.crt | grep Validity -A 2 apt-get to check package information, cache, available versions","title":"Challenges"},{"location":"k8s/testing/","text":"Consumer-driven contracts (CDC) \u00b6 CDC is a concept and testing approach that embraces the perspective of multiple consumers that communicate with providers. Typically, testing tends to define API contracts from the provider's perspective. Through a registry of contracts, multiple consumers now have a voice to provide producers their expectations on how data should be exchanged between the consumers and producers. 1. Set up a Pact Broker on Kubernetes 1. Write a consumer that defines and publishes Pact contracts 1. Deploy and run a few Spring Boot microservices on Kubernetes 1. Connect microservices to a database and public data source 1. Verify the consumer pacts against a producer 1. Find API defects and fix them What is consumer-driven contract testing? \u00b6 The \"consumer-driven\" prefix simply states an additional philosophical position that advocates for better internal microservices design by putting the consumers of such APIs at the heart of the design process. Provider-driven APIs tend to be biased towards the data that is being exposed and the system that is exposing it. - PACT # Adding the Private Repo helmchart (Refer Chaos) helm repo add twuni https://helm.twun.io helm install registry twuni/docker-registry \\ --version 1 .10.0 \\ --namespace kube-system \\ --set service.type = NodePort \\ --set service.nodePort = 31500 export REGISTRY = 2886795362 -31500-kira01.environments.katacoda.com curl $REGISTRY /v2/_catalog | jq # Registry Proxy for localhost:5000 # Docker tags require the address of the registry to be in the tag. # If we push and pull from the registry, the registry name tag must be the same when we are on the client or within Kubernetes. Within your cluster, the registry is available at localhost:5000. Use a port-forwarding command to make this client's localhost:5000 to be the same registry. kubectl port-forward -n kube-system service/registry-docker-registry 5000 :5000 > /dev/null & # This port forwarding will run in the background for the remainder of this scenario. # Now you can access the registry via localhost. curl http://localhost:5000/v2/_catalog | jq . # This means when you push to localhost:5000, your container images will be routed to the private registry running as a service on Kubernetes. # But what happens in the Pod specification when you want to pull the image using the tag localhost:5000? # We can add a proxy that runs as a DaemonSet that will resolve localhost:5000 to the registry whenever a Pod requests a container from localhost:5000. Install the proxy. helm repo add incubator https://charts.helm.sh/incubator # With the added repo, install the proxy daemons. helm install registry-proxy incubator/kube-registry-proxy \\ --version 0 .3.2 \\ --namespace kube-system \\ --set registry.host = registry-docker-registry.kube-system \\ --set registry.port = 5000 \\ --set hostPort = 5000 # For mature environments, you would have an official host name with a load balancer and an ingress that would resolve to a hardened registry service, albeit still running on Kubernetes. # You will use docker push commands and YAML container references both using localhost:5000. About the Application \u00b6 # Clone Source Code git clone https://github.com/javajon/cdc-with-k8s cd ~/cdc-with-k8s tree -d -L 2 # In summary, the aggregator serves data combining the daily COVID-19 metrics with the world population. Add Pact Broker \u00b6 The Pact broker is the key that enables separation between the consumers and producers. The pact broker is a registry, or a library, that serves the collections of contacts generated by the consumers during mock testing and referenced by the producers during verification. cd ~/cdc-with-k8s/cluster # There are two Kubernetes manifest files that declare the Deployments for the Pact Broker and its associated Postgres database. kubectl apply -f pact-broker-postgres.yaml kubectl apply -f pact-broker.yaml # The database is used for storing the Pacts and is attached to a Persistent Volume (PV) reserved in this cluster. # Open the Pact Broker web interface and observe its contents. # The broker is essentially empty. It does have an Example App, but this is just a sample. Generate Pact from Consumer \u00b6 With the Pact framework, it's the consumers that create the Pacts. Independent of the producers or any service, the consumers write testing code that creates conversations with local mocked services. The Mocks are the consumer's perspectives of how the producers should react to the consumer's requests. This particular consumer is written in Node.js. It also uses Jest - a delightful JavaScript Testing Framework with a focus on simplicity. CDC with Jest and Pact cd ~/cdc-with-k8s/pact # Generate Pact Contracts npm install # Produce the two Pact files: npm run test:consumer-a npm run test:consumer-b # Once complete, new Pact JSON files are in the pacts directory. Inspect one of the contract files: cat pacts/consumer_a-aggregator.json | jq -C . # Publish Pacts to Pact Broker # Define access to the Pact Broker: export PACT_BROKER_URL = https://2886795362-30111-kira01.environments.katacoda.com/ export BROKER_USERNAME = pactbroker export BROKER_PASSWORD = pactbroker # Publish the pact to the broker: npm run publish:pact # Verify the pact has been published to the Pact Broker. Run H2 Database A low-level container in this application is a small database that contains world population data. You will use it as a read-only datastore providing the populations for the countries of the world, as well as the populations and locations of major cities. Provided in this example is a SQL script that will seed a relational database, so we need to create an \"initContainer\" that will run next to the H2 container and seed it with the country and city population data when it starts. The InitContainer pattern is very common for ensuring Pods are in the correct state when started. cd ~/cdc-with-k8s/h2-seeder # Take a look at the Dockerfile to see how when it runs is uses an H2 RunScript utility to inject the world.sql into the H2 database that is assumed to be local, in the same Pod. docker build -t localhost:5000/ $( basename $PWD ) :0.0.1 . # During the image build you can safely ignore the TLS certificate validation not implemented docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Apply this manifest declaration to set up a Pod and Service for H2. kubectl apply -f ../cluster/h2-world.yaml # The H2 database serves a convenient web interface for you to interact with the database. When you are presented with the connection information just put in jdbc:h2:/h2-data/world for the JDBC driver URL and leave the username and password blank. # Use the Connect button to enter the SQL explorer. Enter select * from country Run World Population Microservice This is a Spring Boot based Microservice that simply reads world population data from the H2 database using SQL select calls. It offers REST endpoints to get the populations from /countries and /cities . Data is provided in the JSON format. cd ~/cdc-with-k8s/world-pop # Build Microservice Container Image # Spring Boot with Gradle (or Maven) has a convenient task called bootBuildImage. Without having to write a Dockerfile this task will bundle the Java application into an optimized container image. Build and tag the microservice container image ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml # The microservice will be running in a moment. # Verify Microservice curl https://2886795296-30101-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30101-kira01.environments.katacoda.com/countries | jq . curl https://2886795296-30101-kira01.environments.katacoda.com/cities | jq . Run COVID-19 Microservice This is a Spring Boot based Microservice that reads COVID-19 metrics from a public CSV file. This microservice offers REST endpoints to get the data from the /metrics. Data is provided in the JSON format. cd ~/cdc-with-k8s/covid-19 ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml kubectl get pods,deployments,services -l app = $( basename $PWD ) # Verify Microservice curl https://2886795296-30102-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30102-kira01.environments.katacoda.com/metrics | jq . Run Aggregator Microservice This microservice is called the Aggregator as it follows the common architecture pattern of an aggregator. It provides a single API gateway to access the other two microservices: world-pop and covid-19. The data from the two other microservices are merged into responses where COVID-19 data is merged with population data. With population data, you can get visibility in infection rates based on per capita. cd ~/cdc-with-k8s/aggregator ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml # Verify Microservice curl https://2886795296-30103-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30103-kira01.environments.katacoda.com/countries | jq . # Get a single country. curl https://2886795296-30103-kira01.environments.katacoda.com/countries/ind | jq . # Get the top countries with the highest infections per capita: curl https://2886795296-30103-kira01.environments.katacoda.com/countries/percapita | jq . Verify Application (Failed?) Now that everything is started let's verify the contracts against the actual service. cd ~/cdc-with-k8s/aggregator # Verify the pacts on the producer side: ./gradlew pactVerify # You will see that the verification failed. This shows us that a consumer disagrees with the producer. This disagreement is fantastic because it's unveiling a defect before it rolls further to production. # If you inspect all the percentCases fields from the producer, they are all zero: curl https://2886795296-30103-kira01.environments.katacoda.com/countries/percapita | jq . | grep percentCases # The consumer contracts all expect the percentage of infection values to be greater than zero. The consumer code is written with these rules. ` expect ( Number ( response [ 0 ] .percentCases )) .toBeGreaterThan ( 0 .0 ) ; percentCases: term ({ generate: \"0.3333\" , matcher: \"^([0-9]*[1-9][0-9]*(\\.[0-9]+)?|[0]+\\.[0-9]*[1-9][0-9]*) $ \" }) , ` # So for some reason, the producer is producing only zeros. A typical defect on any normal day. Make Correction The problem is in the producer (aggregator) code cd ~/cdc-with-k8s/aggregator # Look at the code in models/Country.java: sed -n '{;=;p}' src/main/java/com/dijure/aggregator/models/Country.java | sed \"N;s/\\n/ /g\" | sed -n '65,70p;!d' # There's the bugger... Someone left in some testing code with an experimentation comment! # Find and remove the offending line: OFFENDING_LINE = $( sed -n '\\|population = 0;|=' src/main/java/com/dijure/aggregator/models/Country.java ) sed -i \" ${ OFFENDING_LINE } d\" src/main/java/com/dijure/aggregator/models/Country.java # Verify the moth has been removed from the relay Verify Application Rebuild the new container with an increased SemVer number. We bump the patch value, since it's a bug fix ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.2 # Push the corrected container image to the private registry on your Kubernetes cluster docker push localhost:5000/ $( basename $PWD ) :0.0.2 # Patch the current deployment to use the new container version kubectl patch deployment aggregator -p \\ '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"aggregator\",\"image\":\"localhost:5000/aggregator:0.0.2\"}]}}}}' # Verify kubectl get pods,deployments,services -l app = $( basename $PWD ) # Finally, run the pactVerify task again, and let's see if that bug has been squashed: ./gradlew pactVerify This means that your producer agrees with the contracts independently generated by all its consumers.","title":"Consumer-driven contracts (CDC)"},{"location":"k8s/testing/#consumer-driven-contracts-cdc","text":"CDC is a concept and testing approach that embraces the perspective of multiple consumers that communicate with providers. Typically, testing tends to define API contracts from the provider's perspective. Through a registry of contracts, multiple consumers now have a voice to provide producers their expectations on how data should be exchanged between the consumers and producers. 1. Set up a Pact Broker on Kubernetes 1. Write a consumer that defines and publishes Pact contracts 1. Deploy and run a few Spring Boot microservices on Kubernetes 1. Connect microservices to a database and public data source 1. Verify the consumer pacts against a producer 1. Find API defects and fix them","title":"Consumer-driven contracts (CDC)"},{"location":"k8s/testing/#what-is-consumer-driven-contract-testing","text":"The \"consumer-driven\" prefix simply states an additional philosophical position that advocates for better internal microservices design by putting the consumers of such APIs at the heart of the design process. Provider-driven APIs tend to be biased towards the data that is being exposed and the system that is exposing it. - PACT # Adding the Private Repo helmchart (Refer Chaos) helm repo add twuni https://helm.twun.io helm install registry twuni/docker-registry \\ --version 1 .10.0 \\ --namespace kube-system \\ --set service.type = NodePort \\ --set service.nodePort = 31500 export REGISTRY = 2886795362 -31500-kira01.environments.katacoda.com curl $REGISTRY /v2/_catalog | jq # Registry Proxy for localhost:5000 # Docker tags require the address of the registry to be in the tag. # If we push and pull from the registry, the registry name tag must be the same when we are on the client or within Kubernetes. Within your cluster, the registry is available at localhost:5000. Use a port-forwarding command to make this client's localhost:5000 to be the same registry. kubectl port-forward -n kube-system service/registry-docker-registry 5000 :5000 > /dev/null & # This port forwarding will run in the background for the remainder of this scenario. # Now you can access the registry via localhost. curl http://localhost:5000/v2/_catalog | jq . # This means when you push to localhost:5000, your container images will be routed to the private registry running as a service on Kubernetes. # But what happens in the Pod specification when you want to pull the image using the tag localhost:5000? # We can add a proxy that runs as a DaemonSet that will resolve localhost:5000 to the registry whenever a Pod requests a container from localhost:5000. Install the proxy. helm repo add incubator https://charts.helm.sh/incubator # With the added repo, install the proxy daemons. helm install registry-proxy incubator/kube-registry-proxy \\ --version 0 .3.2 \\ --namespace kube-system \\ --set registry.host = registry-docker-registry.kube-system \\ --set registry.port = 5000 \\ --set hostPort = 5000 # For mature environments, you would have an official host name with a load balancer and an ingress that would resolve to a hardened registry service, albeit still running on Kubernetes. # You will use docker push commands and YAML container references both using localhost:5000.","title":"What is consumer-driven contract testing?"},{"location":"k8s/testing/#about-the-application","text":"# Clone Source Code git clone https://github.com/javajon/cdc-with-k8s cd ~/cdc-with-k8s tree -d -L 2 # In summary, the aggregator serves data combining the daily COVID-19 metrics with the world population.","title":"About the Application"},{"location":"k8s/testing/#add-pact-broker","text":"The Pact broker is the key that enables separation between the consumers and producers. The pact broker is a registry, or a library, that serves the collections of contacts generated by the consumers during mock testing and referenced by the producers during verification. cd ~/cdc-with-k8s/cluster # There are two Kubernetes manifest files that declare the Deployments for the Pact Broker and its associated Postgres database. kubectl apply -f pact-broker-postgres.yaml kubectl apply -f pact-broker.yaml # The database is used for storing the Pacts and is attached to a Persistent Volume (PV) reserved in this cluster. # Open the Pact Broker web interface and observe its contents. # The broker is essentially empty. It does have an Example App, but this is just a sample.","title":"Add Pact Broker"},{"location":"k8s/testing/#generate-pact-from-consumer","text":"With the Pact framework, it's the consumers that create the Pacts. Independent of the producers or any service, the consumers write testing code that creates conversations with local mocked services. The Mocks are the consumer's perspectives of how the producers should react to the consumer's requests. This particular consumer is written in Node.js. It also uses Jest - a delightful JavaScript Testing Framework with a focus on simplicity. CDC with Jest and Pact cd ~/cdc-with-k8s/pact # Generate Pact Contracts npm install # Produce the two Pact files: npm run test:consumer-a npm run test:consumer-b # Once complete, new Pact JSON files are in the pacts directory. Inspect one of the contract files: cat pacts/consumer_a-aggregator.json | jq -C . # Publish Pacts to Pact Broker # Define access to the Pact Broker: export PACT_BROKER_URL = https://2886795362-30111-kira01.environments.katacoda.com/ export BROKER_USERNAME = pactbroker export BROKER_PASSWORD = pactbroker # Publish the pact to the broker: npm run publish:pact # Verify the pact has been published to the Pact Broker. Run H2 Database A low-level container in this application is a small database that contains world population data. You will use it as a read-only datastore providing the populations for the countries of the world, as well as the populations and locations of major cities. Provided in this example is a SQL script that will seed a relational database, so we need to create an \"initContainer\" that will run next to the H2 container and seed it with the country and city population data when it starts. The InitContainer pattern is very common for ensuring Pods are in the correct state when started. cd ~/cdc-with-k8s/h2-seeder # Take a look at the Dockerfile to see how when it runs is uses an H2 RunScript utility to inject the world.sql into the H2 database that is assumed to be local, in the same Pod. docker build -t localhost:5000/ $( basename $PWD ) :0.0.1 . # During the image build you can safely ignore the TLS certificate validation not implemented docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Apply this manifest declaration to set up a Pod and Service for H2. kubectl apply -f ../cluster/h2-world.yaml # The H2 database serves a convenient web interface for you to interact with the database. When you are presented with the connection information just put in jdbc:h2:/h2-data/world for the JDBC driver URL and leave the username and password blank. # Use the Connect button to enter the SQL explorer. Enter select * from country Run World Population Microservice This is a Spring Boot based Microservice that simply reads world population data from the H2 database using SQL select calls. It offers REST endpoints to get the populations from /countries and /cities . Data is provided in the JSON format. cd ~/cdc-with-k8s/world-pop # Build Microservice Container Image # Spring Boot with Gradle (or Maven) has a convenient task called bootBuildImage. Without having to write a Dockerfile this task will bundle the Java application into an optimized container image. Build and tag the microservice container image ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml # The microservice will be running in a moment. # Verify Microservice curl https://2886795296-30101-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30101-kira01.environments.katacoda.com/countries | jq . curl https://2886795296-30101-kira01.environments.katacoda.com/cities | jq . Run COVID-19 Microservice This is a Spring Boot based Microservice that reads COVID-19 metrics from a public CSV file. This microservice offers REST endpoints to get the data from the /metrics. Data is provided in the JSON format. cd ~/cdc-with-k8s/covid-19 ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml kubectl get pods,deployments,services -l app = $( basename $PWD ) # Verify Microservice curl https://2886795296-30102-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30102-kira01.environments.katacoda.com/metrics | jq . Run Aggregator Microservice This microservice is called the Aggregator as it follows the common architecture pattern of an aggregator. It provides a single API gateway to access the other two microservices: world-pop and covid-19. The data from the two other microservices are merged into responses where COVID-19 data is merged with population data. With population data, you can get visibility in infection rates based on per capita. cd ~/cdc-with-k8s/aggregator ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml # Verify Microservice curl https://2886795296-30103-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30103-kira01.environments.katacoda.com/countries | jq . # Get a single country. curl https://2886795296-30103-kira01.environments.katacoda.com/countries/ind | jq . # Get the top countries with the highest infections per capita: curl https://2886795296-30103-kira01.environments.katacoda.com/countries/percapita | jq . Verify Application (Failed?) Now that everything is started let's verify the contracts against the actual service. cd ~/cdc-with-k8s/aggregator # Verify the pacts on the producer side: ./gradlew pactVerify # You will see that the verification failed. This shows us that a consumer disagrees with the producer. This disagreement is fantastic because it's unveiling a defect before it rolls further to production. # If you inspect all the percentCases fields from the producer, they are all zero: curl https://2886795296-30103-kira01.environments.katacoda.com/countries/percapita | jq . | grep percentCases # The consumer contracts all expect the percentage of infection values to be greater than zero. The consumer code is written with these rules. ` expect ( Number ( response [ 0 ] .percentCases )) .toBeGreaterThan ( 0 .0 ) ; percentCases: term ({ generate: \"0.3333\" , matcher: \"^([0-9]*[1-9][0-9]*(\\.[0-9]+)?|[0]+\\.[0-9]*[1-9][0-9]*) $ \" }) , ` # So for some reason, the producer is producing only zeros. A typical defect on any normal day. Make Correction The problem is in the producer (aggregator) code cd ~/cdc-with-k8s/aggregator # Look at the code in models/Country.java: sed -n '{;=;p}' src/main/java/com/dijure/aggregator/models/Country.java | sed \"N;s/\\n/ /g\" | sed -n '65,70p;!d' # There's the bugger... Someone left in some testing code with an experimentation comment! # Find and remove the offending line: OFFENDING_LINE = $( sed -n '\\|population = 0;|=' src/main/java/com/dijure/aggregator/models/Country.java ) sed -i \" ${ OFFENDING_LINE } d\" src/main/java/com/dijure/aggregator/models/Country.java # Verify the moth has been removed from the relay Verify Application Rebuild the new container with an increased SemVer number. We bump the patch value, since it's a bug fix ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.2 # Push the corrected container image to the private registry on your Kubernetes cluster docker push localhost:5000/ $( basename $PWD ) :0.0.2 # Patch the current deployment to use the new container version kubectl patch deployment aggregator -p \\ '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"aggregator\",\"image\":\"localhost:5000/aggregator:0.0.2\"}]}}}}' # Verify kubectl get pods,deployments,services -l app = $( basename $PWD ) # Finally, run the pactVerify task again, and let's see if that bug has been squashed: ./gradlew pactVerify This means that your producer agrees with the contracts independently generated by all its consumers.","title":"Generate Pact from Consumer"},{"location":"learning/docker/","text":"Running Docker and passing shell commands \u00b6 docker run -e TERM -e COLORTERM -it \u2013rm alpine sh -uec ' apk update apk add git zsh nano vim git clone \u2013depth=1 romkatv/powerlevel10k.git ~/powerlevel10k echo \"source ~/powerlevel10k/powerlevel10k.zsh-theme\" >>~/.zshrc cd ~/powerlevel10k exec zsh'","title":"Running Docker and passing shell commands"},{"location":"learning/docker/#running-docker-and-passing-shell-commands","text":"docker run -e TERM -e COLORTERM -it \u2013rm alpine sh -uec ' apk update apk add git zsh nano vim git clone \u2013depth=1 romkatv/powerlevel10k.git ~/powerlevel10k echo \"source ~/powerlevel10k/powerlevel10k.zsh-theme\" >>~/.zshrc cd ~/powerlevel10k exec zsh'","title":"Running Docker and passing shell commands"},{"location":"learning/git/","text":"Git Book Setting up multiple Github users have different ssh keys \u00b6 https://gist.github.com/oanhnn/80a89405ab9023894df7 It has a solution to test ssh configuration Create a new repository on the command line \u00b6 git init git add README.md git commit -m \"first commit\" git branch -M master git remote add origin leslieclif/dotfiles.git git push -u origin master Git Commands \u00b6 cd into git folder \u2192 ls -la \u2192 cd .git \u2192 ls -la Git help \u00b6 git help To quit help \u2192 q \u00b6 Best practise: Always do a pull before a push to merge changes from remote \u00b6 git pull origin master To git add and git commit for tracked files in a single comand use -a \u00b6 git commit -am \"Commit message\" Amend Commit message \u00b6 git commit \u2013amend \"New commit message\" Check for tracked files in git \u00b6 git ls-files Back out changes that have been commited, but not pushed to remote. Once unstaged, you can remove the changes using checkout \u00b6 git reset HEAD git checkout \u2013 Rename file-name. It also automatically stages the changes, so need to do git add \u00b6 git mv level3\u2013file.txt level3.txt If file is renamed outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A \u00b6 git add -A Moving files and staging the changes \u00b6 git mv level2.txt new-folder If file is moved outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A \u00b6 mv level2.txt .. git add -A file renamed in OS. But say, git has identifed unwanted files during git status and you dont want to add those files, then don't use -A, use -u \u00b6 Individually add the new renamed file first then update git \u00b6 git add level1.txt git add -u Delete files tracked by git \u00b6 git rm doomed.txt If file is delete outside git, it will delete and is not staged. To add and stage the deleted file use -A \u00b6 git add -A Git History \u00b6 git log To quit help \u2192 q \u00b6 Git history in one line \u00b6 git log \u2013oneline \u2013graph \u2013decorate Git history using duration \u00b6 git log \u2013since=\"3 days ago\" Show all user actions \u00b6 git reflog Show commit history \u2192 do git log get commit id \u00b6 git show #TODO: Get a git diff tool Show git config \u00b6 git config \u2013global \u2013list Compare with staging and current changes \u00b6 git diff Compare between current changes and remote last commit \u00b6 git diff HEAD Compare between staging and remote last commit \u00b6 git diff \u2013staged HEAD Compare file changes with staging and current changes \u00b6 git diff \u2013 Compare between commits (do git log to get commits) \u00b6 git diff Compare local and remote branches \u00b6 git diff master origin/master Compare local branches \u00b6 git diff master test-branch Branching \u00b6 List local and remote branches \u00b6 git branch -a Create new branch \u00b6 git branch Rename local branch \u00b6 git branch -m Delete a branch. Note: You have to be on another bracnh before you can delete the target branch \u00b6 git branch -d Create new branch and switch to it in single command \u00b6 git checkout -b Fash forward Merges \u2192 First switch to the target branches, do a git diff to review the changes. \u00b6 git merge Disable fast forward merge \u2192 Give tracing of merge by giving a custom merge message and also the commit history of the branch \u00b6 git merge \u2013no-ff Automatic merge \u00b6 git merge -m \" \" Merge Conflict and Resolution \u00b6 Inside the merging workspace incase of conflict, open the conflicting file in editor or the merge-diff tool. Resolve conflict and close the file. \u00b6 Rebase feature branch from master \u00b6 git checkout feature-branch git rebase master Abort rebase \u00b6 git rebase \u2013abort Rebase conflict resolution \u2192 Use merging tool to fix conflict, save and quit. Add file to git staging, then continue rebase \u00b6 git rebase \u2013continue Pull with Rebase (Rebase local master with remote master) \u00b6 git fetch origin master (non destructive merge which only updates references) git pull \u2013rebase origin master Stash \u00b6 git stash Stash + saving untracked files of git as well \u00b6 git stash -u Get the stash back to local \u00b6 git stash apply List the stash \u00b6 git stash list Drop the stash \u00b6 git stash drop Combination of apply and drop in one command. Brings the last saved state \u00b6 git stash pop Multiple Stashes \u00b6 git stash save \" \" Show any arbitary stash changes whithout popping. Do stash list first to get the stash ID \u00b6 git stash show stash@{1} Apply any arbitary stash changes. Do stash list first to get the stash ID \u00b6 git stash apply stash@{1} Drop any arbitary stash changes that was applied or not needed. \u00b6 git stash drop stash@{1} Stashing changes into a new branch. First see if you have any untracked files that also needs to be saved \u00b6 git stash -u git stash branch newbranchName Tagging \u00b6 Create Lightweight tag \u00b6 git tag mytag List existing tags \u00b6 git tag \u2013list Delete tag \u00b6 git tag \u2013delete mytag Create Annotated tags (It has additional information like release notes) \u00b6 git tag -a v1.0.0 -m \"Release 1.0.0\" Comparing tags \u00b6 git diff v1.0.0 v1.0.1 Tagging a specific commit ID \u00b6 git tag -a v0.0.9 -m \"Release 0.0.9\" Updating an existing tag with new commit id \u00b6 git tag -a v0.0.9 -f -m \"Correct Release 0.0.9\" Pushing tags to remote \u00b6 git push origin v1.0.0 Pushing all local tags to remote \u00b6 git push origin master \u2013tags Deleting tags in remote (puting :before tag name will delete it from remote). Does not delete tag from local \u00b6 git push origin :v0.0.9 Reset HEAD position \u00b6 git reset Using Stash and Branch combination \u00b6 First stash the changes (WIP) in one brabch, then checkout a new test branch and then pop the changes into this test branch \u00b6 git stash git checkout -b test git stash pop Cherry Pick (Hot Fix scenario) \u00b6 git cherry-pick","title":"Git"},{"location":"learning/git/#setting-up-multiple-github-users-have-different-ssh-keys","text":"https://gist.github.com/oanhnn/80a89405ab9023894df7 It has a solution to test ssh configuration","title":"Setting up multiple Github users have different ssh keys"},{"location":"learning/git/#create-a-new-repository-on-the-command-line","text":"git init git add README.md git commit -m \"first commit\" git branch -M master git remote add origin leslieclif/dotfiles.git git push -u origin master","title":"Create a new repository on the command line"},{"location":"learning/git/#git-commands","text":"cd into git folder \u2192 ls -la \u2192 cd .git \u2192 ls -la","title":"Git Commands"},{"location":"learning/git/#git-help","text":"git help","title":"Git help"},{"location":"learning/git/#to-quit-help----q","text":"","title":"To quit help --&gt; q"},{"location":"learning/git/#best-practise-always-do-a-pull-before-a-push-to-merge-changes-from-remote","text":"git pull origin master","title":"Best practise: Always do a pull before a push to merge changes from remote"},{"location":"learning/git/#to-git-add-and-git-commit-for-tracked-files-in-a-single-comand-use--a","text":"git commit -am \"Commit message\"","title":"To git add and git commit for tracked files in a single comand use -a"},{"location":"learning/git/#amend-commit-message","text":"git commit \u2013amend \"New commit message\"","title":"Amend Commit message"},{"location":"learning/git/#check-for-tracked-files-in-git","text":"git ls-files","title":"Check for tracked files in git"},{"location":"learning/git/#back-out-changes-that-have-been-commited-but-not-pushed-to-remote-once-unstaged-you-can-remove-the-changes-using-checkout","text":"git reset HEAD git checkout \u2013","title":"Back out changes that have been commited, but not pushed to remote. Once unstaged, you can remove the changes using checkout"},{"location":"learning/git/#rename-file-name-it-also-automatically-stages-the-changes-so-need-to-do-git-add","text":"git mv level3\u2013file.txt level3.txt","title":"Rename file-name. It also automatically stages the changes, so need to do git add"},{"location":"learning/git/#if-file-is-renamed-outside-git-it-will-delete-and-add-the-files-in-git-history-and-is-not-staged-to-add-new-file-and-stage-the-renamed-files-use--a","text":"git add -A","title":"If file is renamed outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A"},{"location":"learning/git/#moving-files-and-staging-the-changes","text":"git mv level2.txt new-folder","title":"Moving files and staging the changes"},{"location":"learning/git/#if-file-is-moved-outside-git-it-will-delete-and-add-the-files-in-git-history-and-is-not-staged-to-add-new-file-and-stage-the-renamed-files-use--a","text":"mv level2.txt .. git add -A","title":"If file is moved outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A"},{"location":"learning/git/#file-renamed-in-os-but-say-git-has-identifed-unwanted-files-during-git-status-and-you-dont-want-to-add-those-files-then-dont-use--a-use--u","text":"","title":"file renamed in OS. But say, git has identifed unwanted files during git status and you dont want to add those files, then don't use -A, use -u"},{"location":"learning/git/#individually-add-the-new-renamed-file-first-then-update-git","text":"git add level1.txt git add -u","title":"Individually add the new renamed file first then update git"},{"location":"learning/git/#delete-files-tracked-by-git","text":"git rm doomed.txt","title":"Delete files tracked by git"},{"location":"learning/git/#if-file-is-delete-outside-git-it-will-delete-and-is-not-staged-to-add-and-stage-the-deleted-file--use--a","text":"git add -A","title":"If file is delete outside git, it will delete and is not staged. To add and stage the deleted file  use -A"},{"location":"learning/git/#git-history","text":"git log","title":"Git History"},{"location":"learning/git/#to-quit-help----q_1","text":"","title":"To quit help --&gt; q"},{"location":"learning/git/#git-history-in-one-line","text":"git log \u2013oneline \u2013graph \u2013decorate","title":"Git history in one line"},{"location":"learning/git/#git-history-using-duration","text":"git log \u2013since=\"3 days ago\"","title":"Git history using duration"},{"location":"learning/git/#show-all-user-actions","text":"git reflog","title":"Show all user actions"},{"location":"learning/git/#show-commit-history----do-git-log-get-commit-id","text":"git show #TODO: Get a git diff tool","title":"Show commit history --&gt; do git log get commit id"},{"location":"learning/git/#show-git-config","text":"git config \u2013global \u2013list","title":"Show git config"},{"location":"learning/git/#compare-with-staging-and-current-changes","text":"git diff","title":"Compare with staging and current changes"},{"location":"learning/git/#compare-between-current-changes-and-remote-last-commit","text":"git diff HEAD","title":"Compare between current changes and remote last commit"},{"location":"learning/git/#compare-between-staging-and-remote-last-commit","text":"git diff \u2013staged HEAD","title":"Compare between staging and remote last commit"},{"location":"learning/git/#compare-file-changes-with-staging-and-current-changes","text":"git diff \u2013","title":"Compare file changes with staging and current changes"},{"location":"learning/git/#compare-between-commits-do-git-log-to-get-commits","text":"git diff","title":"Compare between commits (do git log to get commits)"},{"location":"learning/git/#compare-local-and-remote-branches","text":"git diff master origin/master","title":"Compare local and remote branches"},{"location":"learning/git/#compare-local-branches","text":"git diff master test-branch","title":"Compare local branches"},{"location":"learning/git/#branching","text":"","title":"Branching"},{"location":"learning/git/#list-local-and-remote-branches","text":"git branch -a","title":"List local and remote branches"},{"location":"learning/git/#create-new-branch","text":"git branch","title":"Create new branch"},{"location":"learning/git/#rename-local-branch","text":"git branch -m","title":"Rename local branch"},{"location":"learning/git/#delete-a-branch-note-you-have-to-be-on-another-bracnh-before-you-can-delete-the-target-branch","text":"git branch -d","title":"Delete a branch. Note: You have to be on another bracnh before you can delete the target branch"},{"location":"learning/git/#create-new-branch-and-switch-to-it-in-single-command","text":"git checkout -b","title":"Create new branch and switch to it in single command"},{"location":"learning/git/#fash-forward-merges-----first-switch-to-the-target-branches-do-a-git-diff-to-review-the-changes","text":"git merge","title":"Fash forward Merges  --&gt; First switch to the target branches, do a git diff to review the changes."},{"location":"learning/git/#disable-fast-forward-merge----give-tracing-of-merge-by-giving-a-custom-merge-message-and-also-the-commit-history-of-the-branch","text":"git merge \u2013no-ff","title":"Disable fast forward merge --&gt; Give tracing of merge by giving a custom merge message and also the commit history of the branch"},{"location":"learning/git/#automatic-merge","text":"git merge -m \" \"","title":"Automatic merge"},{"location":"learning/git/#merge-conflict-and-resolution","text":"","title":"Merge Conflict and Resolution"},{"location":"learning/git/#inside-the-merging-workspace-incase-of-conflict-open-the-conflicting-file-in-editor-or-the-merge-diff-tool-resolve-conflict-and-close-the-file","text":"","title":"Inside the merging workspace incase of conflict, open the conflicting file in editor or the merge-diff tool. Resolve conflict and close the file."},{"location":"learning/git/#rebase-feature-branch-from-master","text":"git checkout feature-branch git rebase master","title":"Rebase feature branch from master"},{"location":"learning/git/#abort-rebase","text":"git rebase \u2013abort","title":"Abort rebase"},{"location":"learning/git/#rebase-conflict-resolution----use-merging-tool-to-fix-conflict-save-and-quit-add-file-to-git-staging-then-continue-rebase","text":"git rebase \u2013continue","title":"Rebase conflict resolution --&gt; Use merging tool to fix conflict, save and quit. Add file to git staging, then continue rebase"},{"location":"learning/git/#pull-with-rebase-rebase-local-master-with-remote-master","text":"git fetch origin master (non destructive merge which only updates references) git pull \u2013rebase origin master","title":"Pull with Rebase (Rebase local master with remote master)"},{"location":"learning/git/#stash","text":"git stash","title":"Stash"},{"location":"learning/git/#stash--saving-untracked-files-of-git-as-well","text":"git stash -u","title":"Stash + saving untracked files of git as well"},{"location":"learning/git/#get-the-stash-back-to-local","text":"git stash apply","title":"Get the stash back to local"},{"location":"learning/git/#list-the-stash","text":"git stash list","title":"List the stash"},{"location":"learning/git/#drop-the-stash","text":"git stash drop","title":"Drop the stash"},{"location":"learning/git/#combination-of-apply-and-drop-in-one-command-brings-the-last-saved-state","text":"git stash pop","title":"Combination of apply and drop in one command. Brings the last saved state"},{"location":"learning/git/#multiple-stashes","text":"git stash save \" \"","title":"Multiple Stashes"},{"location":"learning/git/#show-any-arbitary-stash-changes-whithout-popping-do-stash-list-first-to-get-the-stash-id","text":"git stash show stash@{1}","title":"Show any arbitary stash changes whithout popping. Do stash list first to get the stash ID"},{"location":"learning/git/#apply-any-arbitary-stash-changes-do-stash-list-first-to-get-the-stash-id","text":"git stash apply stash@{1}","title":"Apply any arbitary stash changes. Do stash list first to get the stash ID"},{"location":"learning/git/#drop-any-arbitary-stash-changes-that-was-applied-or-not-needed","text":"git stash drop stash@{1}","title":"Drop any arbitary stash changes that was applied or not needed."},{"location":"learning/git/#stashing-changes-into-a-new-branch-first-see-if-you-have-any-untracked-files-that-also-needs-to-be-saved","text":"git stash -u git stash branch newbranchName","title":"Stashing changes into a new branch. First see if you have any untracked files that also needs to be saved"},{"location":"learning/git/#tagging","text":"","title":"Tagging"},{"location":"learning/git/#create-lightweight-tag","text":"git tag mytag","title":"Create Lightweight tag"},{"location":"learning/git/#list-existing-tags","text":"git tag \u2013list","title":"List existing tags"},{"location":"learning/git/#delete-tag","text":"git tag \u2013delete mytag","title":"Delete tag"},{"location":"learning/git/#create-annotated-tags-it-has-additional-information-like-release-notes","text":"git tag -a v1.0.0 -m \"Release 1.0.0\"","title":"Create Annotated tags (It has additional information like release notes)"},{"location":"learning/git/#comparing-tags","text":"git diff v1.0.0 v1.0.1","title":"Comparing tags"},{"location":"learning/git/#tagging-a-specific-commit-id","text":"git tag -a v0.0.9 -m \"Release 0.0.9\"","title":"Tagging a specific commit ID"},{"location":"learning/git/#updating-an-existing-tag-with-new-commit-id","text":"git tag -a v0.0.9 -f -m \"Correct Release 0.0.9\"","title":"Updating an existing tag with new commit id"},{"location":"learning/git/#pushing-tags-to-remote","text":"git push origin v1.0.0","title":"Pushing tags to remote"},{"location":"learning/git/#pushing-all-local-tags-to-remote","text":"git push origin master \u2013tags","title":"Pushing all local tags to remote"},{"location":"learning/git/#deleting-tags-in-remote-puting-before-tag-name-will-delete-it-from-remote-does-not-delete-tag-from-local","text":"git push origin :v0.0.9","title":"Deleting tags in remote (puting :before tag name will delete it from remote). Does not delete tag from local"},{"location":"learning/git/#reset-head-position","text":"git reset","title":"Reset HEAD position"},{"location":"learning/git/#using-stash-and-branch-combination","text":"","title":"Using Stash and Branch combination"},{"location":"learning/git/#first-stash-the-changes-wip-in-one-brabch-then-checkout-a-new-test-branch-and-then-pop-the-changes-into-this-test-branch","text":"git stash git checkout -b test git stash pop","title":"First stash the changes (WIP) in one brabch, then checkout a new test branch and then pop the changes into this test branch"},{"location":"learning/git/#cherry-pick-hot-fix-scenario","text":"git cherry-pick","title":"Cherry Pick (Hot Fix scenario)"},{"location":"learning/python/","text":"# Range and For for index in range ( 6 ): print ( index ) # Range function is used generate a sequence of integers index = range ( 10 , - 1 , - 1 ) # start, stop and step, stops at 0 not including -1 # set class provides a mapping of unique immutable elements # One use of set is to remove duplicate elements dup_list = ( 'c' , 'd' , 'c' , 'e' ) beta = set ( dup_list ) uniq_list = list ( beta ) # dict class is an associative array of keys and values. keys must be unique immutable objects dict_syn = { 'k1' : 'v1' , 'k2' : 'v2' } dict_syn = dict ( k1 = 'v1' , k2 = 'v2' ) dict_syn [ 'k3' ] = 'v3' # adding new key value del ( dict_syn [ 'k3' ]) # delete key value print ( dict_syn . keys ()) # prints all keys print ( dict_syn . values ()) # prints all values # User Input name = input ( 'Name :' ) # Functions * A function is a piece of code , capable of performing a similar task repeatedly . * It is defined using ** def ** keyword in python . def < function_name > ( < parameter1 > , < parameter2 > , ... ): 'Function documentation' function_body return < value > * Parameters , return expression and documentation string are optional . def square ( n ): \"Returns Square of a given number\" return n ** 2 print ( square . __doc__ ) // prints the function documentation string * 4 types of arguments * Required Arguments : non - keyword arguments def showname ( name , age ) showname ( \"Jack\" , 40 ) // name = \"Jack\" , age = 40 showname ( 40 , \"Jack\" ) // name = 40 , age = \"Jack\" * Keyword Arguments : identified by paramater names def showname ( name , age ) showname ( age = 40 , name = \"Jack\" ) * Default Arguments : Assumes a default argument , if an arg is not passsed . def showname ( name , age = 50 ) showname ( \"Jack\" ) // name = \"Jack\" , age = 50 showname ( age = 40 , \"Jack\" ) // name = \"Jack\" , age = 40 showname ( name = \"Jack\" , age = 40 ) // name = \"Jack\" , age = 40 showname ( name = \"Jack\" , 40 ) // Python does not allow passing non - keyword after keyword arg . This will fail . * Variable Length Arguments : Function preocessed with more arguments than specified while defining the function def showname ( name , * vartuple , ** vardict ) # *vartuple = Variable non keyword argument which will be a tuple. Denoted by * # **vardict = Variable keyword argument which will be a dictionary. Denoted by ** showname ( \"Jack\" ) // name = \"Jack\" showname ( \"Jack\" , 35 , 'M' , 'Kansas' ) // name = \"Jack\" , * vartuple = ( 35 , 'M' , 'Kansas' ) showname ( \"Jack\" , 35 , city = 'Kansas' , sex = 'M' ) // name = \"Jack\" , * vartuple = ( 35 ), ** vardict = { city = 'Kansas' , sex = 'M' } # An Iterator is an object, which allows a programmer to traverse through all the elements of a collection, regardless of its specific implementation. x = [ 6 , 3 , 1 ] s = iter ( x ) print ( next ( s )) # -> 6 # List Comprehensions -> Alternative to for loops. * More concise , readable , efficient and mimic functional programming style . * Used to : Apply a method to all or specific elements of a list , and Filter elements of a list satisfying specific criteria . x = [ 6 , 3 , 1 ] y = [ i ** 2 for i in x ] # List Comprehension expression print ( y ) # -> [36, 9, 1] * Filter positive numbers ( using for and if ) vec = [ - 4 , - 2 , 0 , 2 , 4 ] pos_elm = [ x for x in vec if x >= 0 ] # Can be read as for every elem x in vec, filter x if x is greater than or equal to 0 print ( pos_elm ) # -> [0, 2, 4] * Applying a method to a list def add10 ( x ): return x + 10 n = [ 34 , 56 , 75 , 3 ] mod_n = [ add10 ( num ) for num in n ] print ( mod_n ) # A Generator is a function that produces a sequence of results instead of a single value def arithmatic_series ( a , r ): while a < 50 : yield a # yield is used in place of return which suspends processing a += r s = arithmatic_series ( 3 , 10 ) # Execution of further 'arithmetic series' can be resumed only by calling nextfunction again on generator 's' print ( s ) // Generator #output=3 print ( next ( s )) // Generator starts execution # output=13 print ( next ( s )) // resumed # output=23 # A Generator expresions are generator versions of list comprehensions. They return a generator instead of a list. x = [ 6 , 3 , 1 ] g = ( i ** 2 for i in x ) # generator expression print ( next ( g )) # -> 36 # Dictionary Comprehensions -> takes the form {key: value for (key, value) in iterable} myDict = { x : x ** 2 for x in [ 1 , 2 , 3 , 4 , 5 ]} print ( myDict ) # Output {1: 1, 2: 4, 3: 9, 4: 16, 5: 25} # Calculate the frequency of each identified unique word in the list words = [ 'Hello' , 'Hi' , 'Hello' ] freq = { w : words . count ( w ) for w in words } print ( freq ) # Output {'Hello': 2, 'Hi': 1} Create the dictionary frequent_words , which filter words having frequency greater than one words = [ 'Hello' , 'Hi' , 'Hello' ] freq = { w : words . count ( w ) for w in words if words . count ( w ) > 1 } print ( freq ) # Output {'Hello': 2} # Defining Classes * Syntax class < ClassName > ( < parent1 > , ... ): class_body # Creating Objects * An object is created by calling the class name followed by a pair of parenthesis . class Person : pass p1 = Person () # Creating the object 'p1' print ( p1 ) # -> '<__main__.Person object at 0x0A...>' # tells you what class it belongs to and hints on memory address it is referenced to. # initializer method -> __init__ * defined inside the class and called by default , during an object creation . * It also takes self as the first argument , which refers to the current object . class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname p1 = Person ( 'George' , 'Smith' ) print ( p1 . fname , '-' , p1 . lname ) # -> 'George - Smith' # Documenting a Class * Each class or a method definition can have an optional first line , known as docstring . class Person : 'Represents a person.' # Inheritance * Inheritance describes is a kind of relationship between two or more classes , abstracting common details into super class and storing specific ones in the subclass . * To create a child class , specify the parent class name inside the pair of parenthesis , followed by it 's name. class Child ( Parent ): pass * Every child class inherits all the behaviours exhibited by their parent class . * In Python , every class uses inheritance and is inherited from ** object ** by default . class MySubClass ( object ): # object is known as parent or super class. pass # Inheritance in Action class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname class Employee ( Person ): all_employees = [] def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) # Employee class utilizes __init __ method of the parent class Person to create its object. self . empid = empid Employee . all_employees . append ( self ) e1 = Employee ( 'Jack' , 'simmons' , 456342 ) print ( e1 . fname , '-' , e1 . empid ) # Output -> Jack - 456342 # Polymorphism * Polymorphism allows a subclass to override or change a specific behavior , exhibited by the parent class class Employee ( Person ): all_employees = EmployeesList () def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) self . empid = empid Employee . all_employees . append ( self ) def getSalary ( self ): return 'You get Monthly salary.' def getBonus ( self ): return 'You are eligible for Bonus.' * Definition of ContractEmployee class derived from Employee. It overrides functionality of getSalary and getBonus methods found in it 's parent class Employee. class ContractEmployee ( Employee ): def getSalary ( self ): return 'You will not get Salary from Organization.' def getBonus ( self ): return 'You are not eligible for Bonus.' e1 = Employee ( 'Jack' , 'simmons' , 456342 ) e2 = ContractEmployee ( 'John' , 'williams' , 123656 ) print ( e1 . getBonus ()) # Output - You are eligible for Bonus. print ( e2 . getBonus ()) # Output - You are not eligible for Bonus. # Abstraction * Abstraction means working with something you know how to use without knowing how it works internally . * It is hiding the defaults and sharing only necessary information . # Encapsulation * Encapsulation allows binding data and associated methods together in a unit i . e class . * Bringing related data and methods inside a class to avoid misuse outside . * These principles together allows a programmer to define an interface for applications , i . e . to define all tasks the program is capable to execute and their respective input and output data . * A good example is a television set . We don \u2019 t need to know the inner workings of a TV , in order to use it . All we need to know is how to use the remote control ( i . e the interface for the user to interact with the TV ) . # Abstracting Data * Direct access to data can be restricted by making required attributes or methods private , ** just by prefixing it 's name with one or two underscores.** * An attribute or a method starting with : + ** no underscores ** is a ** public ** one . + ** a single underscore ** is ** private ** , however , still accessible from outside. + ** double underscores ** is ** strongly private ** and not accessible from outside. # Abstraction and Encapsulation Example * ** empid ** attribute of Employee class is made private and is accessible outside the class only using the method ** getEmpid **. class Employee ( Person ): all_employees = EmployeesList () def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) self . __empid = empid Employee . all_employees . append ( self ) def getEmpid ( self ): return self . __empid e1 = Employee ( 'Jack' , 'simmons' , 456342 ) print ( e1 . fname , e1 . lname ) # Output -> Jack simmons print ( e1 . getEmpid ()) # Output -> 456342 print ( e1 . __empid ) # Output -> AttributeError: Employee instance has no attribute '__empid' # Exceptions * Python allows a programmer to handle such exceptions using ** try ... except ** clauses , thus avoiding the program to crash . * Some of the python expressions , though written correctly in syntax , result in error during execution . ** Such scenarios have to be handled .** * In Python , every error message has two parts . The first part tells what type of exception it is and second part explains the details of error . # Handling Exception * A try block is followed by one or more except clauses . * The code to be handled is written inside try clause and the code to be executed when an exception occurs is written inside except clause . try : a = pow ( 2 , 4 ) print ( \"Value of 'a' :\" , a ) b = pow ( 2 , 'hello' ) # results in exception print ( \"Value of 'b' :\" , b ) except TypeError as e : print ( 'oops!!!' ) print ( 'Out of try ... except.' ) Output -> Value of 'a' : 16 --> oops !!! --> Out of try ... except . # Raising Exceptions * ** raise ** keyword is used when a programmer wants a specific exception to occur . try : a = 2 ; b = 'hello' if not ( isinstance ( a , int ) and isinstance ( b , int )): raise TypeError ( 'Two inputs must be integers.' ) c = a ** b except TypeError as e : print ( e ) # User Defined Exception Functions * Python also allows a programmer to create custom exceptions , derived from base Exception class . class CustomError ( Exception ): def __init__ ( self , value ): self . value = value def __str__ ( self ): return str ( self . value ) try : a = 2 ; b = 'hello' if not ( isinstance ( a , int ) and isinstance ( b , int )): raise CustomError ( 'Two inputs must be integers.' ) # CustomError is raised in above example, instead of TypeError. c = a ** b except CustomError as e : print ( e ) # Using 'finally' clause * ** finally ** clause is an optional one that can be used with try ... except clauses . * All the statements under finally clause are executed irrespective of exception occurrence . def divide ( a , b ): try : result = a / b return result except ZeroDivisionError : print ( \"Dividing by Zero.\" ) finally : print ( \"In finally clause.\" ) # Statements inside finally clause are ALWAYS executed before the return back # Using 'else' clause * ** else ** clause is also an optional clause with try ... except clauses . * Statements under else clause are executed ** only when no exception occurs in try clause **. try : a = 14 / 7 except ZeroDivisionError : print ( 'oops!!!' ) else : print ( 'First ELSE' ) try : a = 14 / 0 except ZeroDivisionError : print ( 'oops!!!' ) else : print ( 'Second ELSE' ) Output : First ELSE --> oops !!! # Module * Any file containing logically organized Python code can be used as a module . * A module generally contains ** any of the defined functions , classes and variables **. A module can also include executable code . * Any Python source file can be used as a module by using an import statement in some other Python source file . # Packages * A package is a collection of modules present in a folder . * The name of the package is the name of the folder itself . * A package generally contains an empty file named ** __init__ . py ** in the same folder , which is required to treat the folder as a package . # Import Modules import math # Recommended method of importing a module import math as m from math import pi , tan from math import pi as pie , tan as tangent # Working with Files * Data from an opened file can be read using any of the methods : ** read , readline and readlines **. * Data can be written to a file using either ** write ** or ** writelines ** method . * A file ** must be opened ** , before it is used for reading or writing . fp = open ( 'temp.txt' , 'r' ) # opening ( operations 'r' & 'w') content = fp . read () # reading fp . close () # closing # read() -> Reads the entire contents of a file as bytes. # readline() -> Reads a single line at a time. # readlines() -> Reads a all the line & each line is stored as an element of a list. # write() -> Writes a single string to output file. # writelines() -> Writes multiple lines to output file & each string is stored as an element of a list. * Reading contents of file and storing as a dictionary fp = open ( 'emp_data.txt' , 'r' ) emps = fp . readlines () # Preprocessing data emps = [ emp . strip ( ' \\n ' ) for emp in emps ] emps = [ emp . split ( ';' ) for emp in emps ] header = emps . pop # remove header record separately emps = [ dict ( zip ( header , emp ) for emp in emps ] # header record is used to combine with data to form a dictionary print ( emps [: 2 ]) # prints first 2 records * Filtering data based on criteria fil_emps = [ emp [ 'Emp_name' ] for emp in emps if emp [ 'Emp_work_location' ] == 'HYD' ] * Filtering data based on pattern import re pattern = re . compile ( r 'oracle' , re . IGNORECASE ) # Regular Expression oracle_emps = [ emp [ 'Emp_name' ] for emp in emps if pattern . search ( emp [ 'Emp_skillset' ])] * Filter and Sort data in ascending order fil_emps = [ emp for emp in emps if emp [ 'Emp_designation' ] == 'ASE' ] fil_emps = sorted ( fil_emps , key = lambda k : k [ 'Emp_name' ]) print ( emp [ 'Emp_name' ] for emp in fil_emps ) * Sorting all employees based on custom sorting criteria order = { 'ASE' : 1 , 'ITA' : 2 , 'AST' : 3 } sorted_emp = sorted ( emp , key = lambda k : order [ k [ 'designation' ]]) * Filter data and write into files fil_emps = [ emp for emp in emps if emp [ 'Emp_Designation' ] == 'ITA' ] ofp = open ( outputtext . txt , 'w' ) keys = fil_emps [ 0 ] . keys () # Remove header from key name for key in keys : ofp . write ( key + \" \\t \" ) ofp . write ( \" \\n \" ) for emp in fil_emps : for key in keys : ofp . write ( emp [ key ] + \" \\t \" ) ofp . write ( \" \\n \" ) ofp . close () # Regular Expressions * Regex are useful to construct patterns that helps in filtering the text possessing the pattern . * ** re module ** is used to deal with regex . * ** search ** method takes pattern and text to scan and returns a Match object . Return None if not found . * Match object holds info on the nature of the match like ** original input string , Regular expression used , location within the original string ** match = re . search ( pattern , text ) start_index = match . start () # start location of match end_index = match . end () regex = match . re . pattern () print ( 'Found \" {} \" pattern in \" {} \" from {} to {} ' . format ( st , text , start_index , end_index )) # Compiling Expressions * In Python , its more efficient t compile the patterns that are frequently used . * ** compile ** function of re module converts an expression string into a ** RegexObject **. patterns = [ 'this' , 'that' ] regexes = [ re . compile ( p ) for p in patterns ] for regex in regexes : if regex . search ( text ): # pattern is not required print ( 'Match found' ) * search method only returns the first matching occurrence . # Finding Multiple Matches * findall method returns all the substrings of the pattern without overlapping pattern = 'ab' for match in re . findall ( pattern , text ): print ( 'match found - {} ' . format ( match )) # Grouping Matches * Adding groups to a pattern enables us to isolate parts of the matching text , expanding those capabilities to create a parser . * Groups are defined by enclosing patterns within parenthesis text = 'This is some text -- with punctuations.' for pattern in [ r '^(\\w+)' , # word at the start of the string r '(\\w+)\\S*$' , # word at the end of the string with punctuation r '(\\bt\\w+)\\W+(\\w+)' , # word staring with 't' and the next word r '(\\w+t)\\b' ]: # word ending with t regex = re . compile ( pattern ) match = regex . search ( text ) print ( match . groups ()) # Output -> ('This',) ('punctuations',) ('text','with') ('text',) # Naming Grouped Matches * Accessing the groups with defined names text = 'This is some text -- with punctuations.' for pattern in [ r '^(?P<first_word>\\w+)' , # word at the start of the string r '(?P<last_word>\\w+)\\S*$' , # word at the end of the string with punctuation r '(?P<t_word>\\bt\\w+)\\W+(?P<other_word>\\w+)' , # word staring with 't' and the next word r '(?P<ends_with_t>\\w+t)\\b' ]: # word ending with t regex = re . compile ( pattern ) match = regex . search ( text ) print ( \"Groups: \" , match . groups ()) # Output -> ('This',) ('punctuations',) ('text','with') ('text',) print ( \"Group Dictionary: \" , match . groupdict ()) # Output -> {'first_word':'This'} {'last_word': 'punctuations'} {'t_word':'text', 'other_word':'with'} {'ends_with_t':'text'} # Data Handling # Handling XML files * ** lxml ** 3 rd party module is a highly feature rich with ElementTree API and supports querying wthe xml content using XPATH . * In the ElementTree API , an element acts like a list . The items of the list are the elements children . * XML search is faster in lxml . < ? xml > < employee > < skill name = \"Python\" /> </ employee > from lxml import etree tree = etree . parse ( 'sample.xml' ) root = tree . getroot () # gets doc root <?xml> skills = tree . findall ( '//skill' ) # gets all skill tags for skill in skills : print ( \"Skills: \" , skill . attrib [ 'name' ]) # Adding new skill in the xml skill = etree . SubElement ( root , 'skill' , attrib = { 'name' : 'PHP' }) # Handling HTML files * ** lxml ** 3 rd party module is used for parsing HTML files as well . import urllib.request from lxml import etree def readURL ( url ): urlfile = urllib . request . urlopen ( url ) if urlfile . getcode () == 200 : contents = urlfile . read () return contents if __name__ == '__main__' : url = 'http://xkcd.com' html = readURL ( url ) # Data Serialization * Process of converting ** data types / objects ** into ** Transmittable / Storable ** format is called Data Serialization . * In python , ** pickle and json ** modules are used for Data Serialization . * Serialized data can then be written to file / Socket / Pipe . From these it can be de - serialized and stored into a new Object . json . dump ( data , file , indent = 2 ) # serialized data is written to file with indentation using dump method data_new = json . load ( file ) # de-serialized data is written to new object using load method # Database Connectivity * ** Python Database API ( DB - API ) ** is a standard interface to interact with various databases . * Different DB API \u2019 s are used for accessing different databases . Hence a programmer has to install DB API corresponding to the database one is working with . * Working with a database includes the following steps : + Importing the corresponding DB - API module . + Acquiring a connection with the database . + Executing SQL statements and stored procedures . + Closing the connection import sqlite3 # establishing a database connection con = sqlite3 . connect ( 'D: \\\\ TEST.db' ) # preparing a cursor object cursor = con . cursor () # preparing sql statements sql1 = 'DROP TABLE IF EXISTS EMPLOYEE' # closing the database connection con . close () # Inserting Data * Single rows are inserted using ** execute ** and multiple rows using ** executeMany ** method of created cursor object . # preparing sql statement rec = ( 456789 , 'Frodo' , 45 , 'M' , 100000.00 ) sql = ''' INSERT INTO EMPLOYEE VALUES ( ?, ?, ?, ?, ?) ''' # executing sql statement using try ... except blocks try : cursor . execute ( sql , rec ) con . commit () except Exception as e : print ( \"Error Message :\" , str ( e )) con . rollback () # Fetching Data * ** fetchone ** : It retrieves one record at a time in the form of a tuple . * ** fetchall ** : It retrieves all fetched records at a point in the form of tuple of tuples . # fetching the records records = cursor . fetchall () # Displaying the records for record in records : print ( record ) # Object Relational Mappers * An object - relational mapper ( ORM ) is a library that automates the transfer of data stored in relational database tables into objects that are adopted in application code . * ORMs offer a high - level abstraction upon a relational database , which permits a developer to write Python code rather than SQL to create , read , update and delete data and schemas in their database . * Such an ability to write Python code instead of SQL speeds up web application development . # Higher Order Functions * A ** Higher Order function ** is a function , which is capable of doing any one of the following things : + It can be functioned as a ** data ** and be assigned to a variable . + It can accept any other ** function as an argument **. + It can return a ** function as its result **. * The ability to build Higher order functions , ** allows a programmer to create Closures , which in turn are used to create Decorators **. # Function as a Data def greet (): return 'Hello Everyone!' print ( greet ()) wish = greet # 'greet' function assigned to variable 'wish' print ( type ( wish )) # Output -> <type 'function'> print ( wish ()) # Output -> Hello Everyone! # Function as an Argument def add ( x , y ): return x + y def sub ( x , y ): return x - y def prod ( x , y ): return x * y def do ( func , x , y ): return func ( x , y ) print ( do ( add , 12 , 4 )) # 'add' as arg # Output -> 16 print ( do ( sub , 12 , 4 )) # 'sub' as arg # Output -> 8 print ( do ( prod , 12 , 4 )) # 'prod' as arg # Output -> 48 # Returning a Function def outer (): def inner (): s = 'Hello world!' return s return inner () print ( outer ()) # Output -> Hello world! * You can observe from the output that the ** return value of 'outer' function is the return value of 'inner' function ** i . e 'Hello world!' . def outer (): def inner (): s = 'Hello world!' return s return inner # Removed '()' to return 'inner' function itself print ( outer ()) #returns 'inner' function # Output -> <function inner at 0xxxxxx> func = outer () print ( type ( func )) # Output -> <type 'function'> print ( func ()) # calling 'inner' function # Output -> Hello world! * Parenthesis after the ** inner ** function are removed so that the ** outer ** function returns ** inner function **. # Closures * A Closure is a ** function returned by a higher order function ** , whose return value depends on the data associated with the higher order function . def multiple_of ( x ): def multiple ( y ): return x * y return multiple c1 = multiple_of ( 5 ) # 'c1' is a closure c2 = multiple_of ( 6 ) # 'c2' is a closure print ( c1 ( 4 )) # Output -> 5 * 4 = 20 print ( c2 ( 4 )) # Output -> 6 * 4 = 24 * The first closure function , c1 binds the value 5 to argument x and when called with an argument 4 , it executes the body of multiple function and returns the product of 5 and 4. * Similarly c2 binds the value 6 to argument x and when called with argument 4 returns 24. # Decorators * Decorators are evolved from the concept of closures . * A decorator function is a higher order function that takes a function as an argument and returns the inner function . * A decorator is capable of adding extra functionality to an existing function , without altering it . * The decorator function is prefixed with **@ symbol ** and written above the function definition . + Shows the creation of closure function wish using the higher order function outer . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner def greet (): print ( 'Hello!' ) wish = outer ( greet ) # Output -> Accessing : greet wish () # Output -> Hello! - wish is the closure function obtained by calling an outer function with the argument greet . When wish function is called , inner function gets executed . + The second one shows the creation of decorator function outer , which is used to decorate function greet . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner def greet (): return 'Hello!' greet = outer ( greet ) # decorating 'greet' # Output -> No Output as return is used instead of print greet () # calling new 'greet' # Output -> Accessing : greet - The function returned by outer is assigned to greet i . e the function name passed as argument to outer . This makes outer a decorator to greet . + Third one displays decorating the greet function with decorator function , outer , using @ symbol . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner @outer # This is same as **greet = outer(greet)** def greet (): return 'Hello!' greet () # Output -> Accessing : greet # Descriptors * Python descriptors allow a programmer to create managed attributes . * In other object - oriented languages , you will find ** getter and setter ** methods to manage attributes . * However , Python allows a programmer to manage the attributes simply with the attribute name , without losing their protection . * This is achieved by defining a ** descriptor class ** , that implements any of ** __get__ , __set__ , __delete__ ** methods . class EmpNameDescriptor : def __get__ ( self , obj , owner ): return self . __empname def __set__ ( self , obj , value ): if not isinstance ( value , str ): raise TypeError ( \"'empname' must be a string.\" ) self . __empname = value * The descriptor , EmpNameDescriptor is defined to manage empname attribute . It checks if the value of empname attribute is a string or not . class EmpIdDescriptor : def __get__ ( self , obj , owner ): return self . __empid def __set__ ( self , obj , value ): if hasattr ( obj , 'empid' ): raise ValueError ( \"'empid' is read only attribute\" ) if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value * The descriptor , EmpIdDescriptor is defined to manage empid attribute . class Employee : empid = EmpIdDescriptor () empname = EmpNameDescriptor () def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name * Employee class is defined such that , it creates empid and empname attributes from descriptors EmpIdDescriptor and EmpNameDescriptor . e1 = Employee ( 123456 , 'John' ) print ( e1 . empid , '-' , e1 . empname ) # Output -> '123456 - John' e1 . empid = 76347322 # Output -> ValueError: 'empid' is read only attribute # Properties * Descriptors can also be created using property () type . + Syntax : property ( fget = None , fset = None , fdel = None , doc = None ) - where , fget : attribute get method fset : attribute set method fdel \u2013 attribute delete method doc \u2013 docstring class Employee : def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name def getEmpID ( self ): return self . __empid def setEmpID ( self , value ): if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value empid = property ( getEmpID , setEmpID ) # Property Decorators * Descriptors can also be created with property decorators . * While using property decorators , an attribute 's get method will be same as its name and will be decorated with property. * In a case of defining any set or delete methods , they will be decorated with respective setter and deleter methods . class Employee : def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name @property def empid ( self ): return self . __empid @empid . setter def empid ( self , value ): if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value e1 = Employee ( 123456 , 'John' ) print ( e1 . empid , '-' , e1 . empname ) # Output -> '123456 - John' # Introduction to Class and Static Methods Based on the ** scope ** , functions / methods are of two types . They are : * Class methods * Static methods # Class Methods * A method defined inside a class is bound to its object , by default . * However , if the method is bound to a Class , then it is known as ** classmethod **. class Circle ( object ): no_of_circles = 0 def __init__ ( self , radius ): self . __radius = radius Circle . no_of_circles += 1 def getCirclesCount ( self ): return Circle . no_of_circles c1 = Circle ( 3.5 ) c2 = Circle ( 5.2 ) c3 = Circle ( 4.8 ) print ( c1 . getCirclesCount ()) # -> 3 print ( Circle . getCirclesCount ( c3 )) # -> 3 print ( Circle . getCirclesCount ()) # -> TypeError: getCirclesCount() missing 1 required positional argument: 'self' class Circle ( object ): no_of_circles = 0 def __init__ ( self , radius ): self . __radius = radius Circle . no_of_circles += 1 @classmethod def getCirclesCount ( self ): return Circle . no_of_circles c1 = Circle ( 3.5 ) c2 = Circle ( 5.2 ) c3 = Circle ( 4.8 ) print ( c1 . getCirclesCount ()) # -> 3 print ( Circle . getCirclesCount ()) # -> 3 # Static Method * A method defined inside a class and not bound to either a class or an object is known as ** Static ** Method . * Decorating a method using ** @staticmethod ** decorator makes it a static method . def square ( x ): return x ** 2 class Circle ( object ): def __init__ ( self , radius ): self . __radius = radius def area ( self ): return 3.14 * square ( self . __radius ) c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 print ( square ( 10 )) # -> 100 * square function is not packaged properly and does not appear as integral part of class Circle . class Circle ( object ): def __init__ ( self , radius ): self . __radius = radius @staticmethod def square ( x ): return x ** 2 def area ( self ): return 3.14 * self . square ( self . __radius ) c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 print ( square ( 10 )) # -> NameError: name 'square' is not defined * square method is no longer accessible from outside the class Circle . * However , it is possible to access the static method using Class or the Object as shown below . print ( Circle . square ( 10 )) # -> 100 print ( c1 . square ( 10 )) # -> 100 # Abstract Base Classes * An ** Abstract Base Class ** or ** ABC ** mandates the derived classes to implement specific methods from the base class . * It is not possible to create an object from a defined ABC class . * Creating objects of derived classes is possible only when derived classes override existing functionality of all abstract methods defined in an ABC class . * In Python , an Abstract Base Class can be created using module abc . from abc import ABC , abstractmethod class Shape ( ABC ): @abstractmethod def area ( self ): pass @abstractmethod def perimeter ( self ): pass * Abstract base class Shape is defined with two abstract methods area and perimeter . class Circle ( Shape ): def __init__ ( self , radius ): self . __radius = radius @staticmethod def square ( x ): return x ** 2 def area ( self ): return 3.14 * self . square ( self . __radius ) def perimeter ( self ): return 2 * 3.14 * self . __radius c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 # Context Manager * A Context Manager allows a programmer to perform required activities , automatically , while entering or exiting a Context . * For example , opening a file , doing few file operations , and closing the file is manged using Context Manager as shown below . with open ( 'sample.txt' , 'w' ) as fp : content = fp . read () * The keyword ** with ** is used in Python to enable a context manager . It automatically takes care of closing the file . import sqlite3 class DbConnect ( object ): def __init__ ( self , dbname ): self . dbname = dbname def __enter__ ( self ): self . dbConnection = sqlite3 . connect ( self . dbname ) return self . dbConnection def __exit__ ( self , exc_type , exc_val , exc_tb ): self . dbConnection . close () with DbConnect ( 'TEST.db' ) as db : cursor = db . cursor () ''' Few db operations ... ''' * Example from contextlib import contextmanager @contextmanager def context (): print ( 'Entering Context' ) yield print ( \"Exiting Context\" ) with context (): print ( 'In Context' ) # Output -> Entering Context -> In Context -> Exiting Context # Coroutines * A Coroutine is ** generator ** which is capable of constantly receiving input data , process input data and may or may not return any output . * Coroutines are majorly used to build better ** Data Processing Pipelines **. * Similar to a generator , execution of a coroutine stops when it reaches ** yield ** statement . * A Coroutine uses ** send ** method to send any input value , which is captured by yield expression . def TokenIssuer (): tokenId = 0 while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) t = TokenIssuer () next ( t ) t . send ( 'George' ) # -> Token number of George: 1 t . send ( 'Rosy' ) # -> Token number of Rosy: 2 * ** TokenIssuer ** is a coroutine function , which uses yield to accept name as input . * Execution of coroutine function begins only when next is called on coroutine t . * This results in the execution of all the statements till a yield statement is encountered . * Further execution of function resumes when an input is passed using send , and processes all statements till next yield statement . def TokenIssuer ( tokenId = 0 ): try : while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) except GeneratorExit : print ( 'Last issued Token is :' , tokenId ) t = TokenIssuer ( 100 ) next ( t ) t . send ( 'George' ) # Token number of George: 101 t . send ( 'Rosy' ) # Token number of Rosy: 102 t . send ( 'Smith' ) # Token number of Smith: 103 t . close () # Last issued Token is: 103 * The coroutine function TokenIssuer takes an argument , which is used to set a starting number for tokens . * When coroutine t is closed , statements under GeneratorExit block are executed . * Many programmers may forget that passing input to coroutine is possible only after the first next function call , which results in error . * Such a scenario can be avoided using a decorator . def coroutine_decorator ( func ): def wrapper ( * args , ** kwdargs ): c = func ( * args , ** kwdargs ) next ( c ) return c return wrapper @coroutine_decorator def TokenIssuer ( tokenId = 0 ): try : while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) except GeneratorExit : print ( 'Last issued Token is :' , tokenId ) t = TokenIssuer ( 100 ) t . send ( 'George' ) t . send ( 'Rosy' ) t . send ( 'Smith' ) t . close () * coroutine_decorator takes care of calling next on the created coroutine t . def nameFeeder (): while True : fname = yield print ( 'First Name:' , fname ) lname = yield print ( 'Last Name:' , lname ) n = nameFeeder () next ( n ) n . send ( 'George' ) n . send ( 'Williams' ) n . send ( 'John' ) First Name : George Last Name : Williams First Name : John","title":"Python"},{"location":"learning/terraform/","text":"Example using Docker images \u00b6 // Main . tf resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image . nginx . latest name = \"webserver\" ports { internal = 80 external = 8050 } } Infrastructure as Code \u00b6 IaC is an important DevOps cornerstone that enables you to define, automatically manage, and provision infrastructure through source code. Infrastructure is managed as a software system . IaC is frequently referred to as Programmable Infrastructure . Benefits of Iac \u00b6 In IaC, A tool will monitor the state of the infrastructure . A script will be sent automatically to fix the issue . A script can be written to add new instances, and it can be reused . Faster process , no/less human involvement. IaC Implemetation Approaches \u00b6 Declarative \u00b6 Focuses on the desired end state of infrastructure (Functional) . Tools perform the necessary actions to reach that state . Automatically takes care of the order and executes it . Examples are Terraform and CloudFormation. Imperative \u00b6 Focuses on how to achieve the desired state (Procedural) . Follows the order of tasks and it executes in that order . Examples are Chef and Ansible. Configuration Management \u00b6 It is a system that keeps the track of organization's hardware, software, and other related information. It includes the software updates and versions present in the system. Configuration management is also called as configuration control . E.g.: Chef and Ansible Orchestration: \u00b6 It automates the coordination, management, and configuration of systems and hardware. E.g.: Terraform and Nomad Installing Terraform \u00b6 sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip unzip terraform_0.11.11_linux_amd64.zip sudo mv terraform /usr/local/bin/ terraform \u2013version Terraform Lifecycle \u00b6 Terraform init - Initializes the working directory having Terraform configuration files. Terraform plan - Creates an execution plan and it mentions the changes it is going to make. Terraform apply - When you are not amazed by seeing the results of the terraform plan, you can go ahead and run terraform apply to apply the changes. The main difference between plan and apply is - apply asks for a prompt like Do you want to perform these actions? and then implement the changes, whereas, plan shows the possible end state of your infrastructure without actually implementing them. Terraform destroy - Destroys the created infrastructure. Terraform Configuration \u00b6 A set of files that describe the infrastructure in Terraform is called as Terraform Configuration. The entire configuration file is saved with .tf extension. Make sure to have all the configurations maintained in a single .tf file instead of many. Terraform loads all the .tf files present in the working directory. Creating Virtual Network \u00b6 create a virtual network without using the Azure portal and azure CLI commands. resource \"azurerm_virtual_network\"\"myterraformnetwork\" { name = \"myvnet\" address_space = [\"10.0.0.0/16\"] location=\"East US\" resource_group_name=\"er-tyjy\" } Resource blocks define the infrastructure (resource \"azurerm_virtual_network\" \"myterraformnetwork\"). It contains the two strings - type of the resource(azurerm_virtualnetwork) and name of the resource(myterraformnetwork). Terraform uses plugin based architecture to support the various services providers and infrastructure available. When you run Terraform init command, it downloads and installs provider binary for the corresponding providers. In this case, it is Azure. Now, you can run the terraform plan command. It shows all the changes it is going to make. If the output is as you expected and there are no errors, then you can run Terraform apply. Terraform Validate \u00b6 You can run this command whenever you like to check whether the code is correct. Once you run this command, if there is no output, it means that there are no errors in the code. Variables \u00b6 Terraform provides the following variable types: Strings - The default data type is a string . List - A list is like simple arrays Map - A map value is nothing but a lookup table for string keys to get the string values. In terraform, to reduce the complexity of code and for easiness , all the variables are created in the variables.tf file. variables.tf variable \"rg\" { default = \"terraform-lab2\" } variable \"locations\" { default = [\"ap-southeast-1\" , \"ap-southeast-2\"] } variable \"tags\" { type = \"map\" default = { environment = \"training\" source=\"citadel\" } } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-east-2 = \"image-4321\" } } For example, you have defined resource_group name in the variable.tf file. you can call them in main.tf file like this \"${var.resource_group}\" Sensitive Parameters \u00b6 There may be sensitive parameters which should be shown only when running the terraform plan but not terraform apply. output \"sensitiveoutput\" { sensitive = true value = VALUE } When you run terraform apply, the output is labeled as sensitive. If there is any sensitive information, then you can protect it by using a sensitive parameter. terraform.tfvars File \u00b6 In terraform.tfvars file, we mention the \"key/value\" pair. The main.tf file takes the input from a terraform. tfvars file. If it is not present, it gets the input from variables.tf file. resource_group = \"user-ybje\" location = \"East US\" terraform fmt \u00b6 It rewrites the confguration files to canonical style and format. State File - terraform.tfstate \u00b6 It is a JSON file that maps the real world resources to the configuration files. In terraform, there are three states. Desired state - Represents how the infrastructure to be present. Actual state - Represents the current state of infrastructure. Known state - To bridge desired state and actual state, there is a known state. You can get the details of known state from a tfstate file. When you run terraform plan command , terraform performs a quick refresh and lists the actions needed to achieve the desired state. When terraform apply command is run, it applies the changes and updates the actual state. Modules \u00b6 A module is like a reusable blueprint of infrastructure . A module is nothing but a folder of Terraform files. In terraform the modules are categorized into two types: Root modules - The current directory of Terraform files on which you are running the commands. Child modules - The modules sourced by the root module. To create a child module, add resources to it. Module syntax: module \"child\" { source = \"../child\" } The difference between adding the resources and module is for adding the resource (Eg: resource \"azurerm_virtual_network\" 'test\") you need type and name but for adding a module (Eg: module \"child\") only the name is enough. The name of the module should be unique within configurations because it is used as a reference to the module and outputs. main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } Updates \u00b6 terraform get - This command is used to download and install the modules for that configuration. terraform get -update - It checks the downloaded modules and checks for the recent versions if any. Module Outputs \u00b6 If you need output from any module, you can get it by passing the required parameter. For example, the output of virtual network location can be found by following the below syntax. You can add this syntax to .tf file(main.tf or variables.tf) . output \"virtual_network_id\" { value = \"${azurerm_virtual_network.test.location}\" } Where virtual_network_id is a unique name and \"${azurerm_virtual_network.test.location}\" is the location of the virtual network using string interpolation. Benefits of Modules \u00b6 Code reuse : When there is a need to provision the group of resources on another resource at the same time, a module can be used instead of copying the same code. It helps in resolving the bugs easily. If you want to make changes, changing the code in one place is enough. Abstraction layer : It makes complex configurations easier to conceptualize. For example, if you like to add vault(Another hashicorp's tool for managing secrets) cluster to the environment, it requires dozens of components. Instead of worrying about individual components, it gives ready-to-use vault cluster. Black box : To create a vault, you only need some configuration parameters without worrying about the complexity. You don't need any knowledge on how to install vault, configuring vault cluster and working of the vault. You can create a working cluster in a few minutes. Best practices in organization : When a module is created, you can give it to the other teams. It helps in easily developing the infrastructure. **Versioned artifacts: They are immutable artifacts and can be promoted from one environment to others. Introduction to Meta Parameters \u00b6 There are some difficulties with the declarative language. For example, since it is a declarative language, there is no concept of for loops. How do you repeat a piece of code without copy paste? * Terraform provides primitives like meta parameter called as count, life cycle blocks like create_before_destroy, ternary operator and a large number of interpolation functions . This helps in performing certain types of loops, if statements and zero downtime deployments. Count \u00b6 Count - It defines how many parameters you like to create. Modify main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } To increase Vnets to 5, update count = 5 . To decrease Vnets to 2, update count = 2 , last 3 Vnets will get deleted. Elements \u00b6 Suppose you like to create the three virtual networks with names vnet-A, vnet-B and vnet-C, you can do this easily with the help of list . Mention how many vnets you are going to create with the help of list and define it in variables.tf file variable \"name\" { type= \"list\" default = [\"A\",\"B\",\"C\"] } You can call this variable in the main.tf file in the following way count = \"${length(var.name)}\" - It returns number of elements present in the list. you should store it in meta parameter count. \"${element(var.name,count.index)}\" - It acts as a loop, it takes the input from list and repeats until there are no elements in list. variables.tf ** variable \"resource_group\" { default = \"user-nuqo\" } variable \"location\" { default = \"East US\" } variable \"name\" { type = \"list\" default = [\"A\",\"B\",\"C\"] } **main.tf resource \"azurerm_virtual_network\"\"multiple\" { name = \"vnet-${element(var.name,count.index)}\" resource_group_name = \"${var.resource_group}\" location = \"${var.location}\" address_space=[\"10.0.0.0/16\"] count=\"${length(var.name)}\" } Conditions \u00b6 For example, a vnet has to be created only, if the variable number of vnets is 3 or else no. You can do this by using the ternary operator. variables.tf variable \"no_of_vnets\" { default = 3 } main.tf count = \"${var.no_of_vnets == 3 ? 1 : 0}\" * First, it will execute the ternary operator. If it is true, it takes the output as 1 or else 0. * Now, in the above case, the output is 1, the count becomes one, and the vnet is created. Inheriting Variables \u00b6 Inheriting the variables between modules keeps your Terraform code DRY. Don't Repeat Yourself (DRY) It aims at reducing repeating patterns and code. There is no need to write the duplicate code for each environment you are deploying. You can write the minimum code and achieve the goal by having maximum re-usability. Module File Structure \u00b6 You will follow the below file structure to inherit the variables between modules. modules | |___mod | |__mod.tf | |___vnet |__vnet.tf |__vars.tf There is one vnet and you can configure the vnet by passing the parameters from module. The variables in the vnet are overriden by the values provided in mod.tf file. Create a folder with name Modules Create two subfolders of names vnet and mod in the nested_modules folder. In the vnet folder, create a virtual network using Terraform configuration. Maintain two files one for main configuration(vnet.tf) and other for declaring variables(vars.tf) . Now, create main.tf file like this resource \"azurerm_virtual_network\"\"vnet\" { name = \"${var.name}\" address_space = [\"10.0.0.0/16\"] resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" } Now, vars.tf file looks like this variable \"name\" { description = \"name of the vnet\" } variable \"resourcegroup\" { description = \"name of the resource group\" } variable \"location\" { description =\"name of the location\" } variable \"address\" { type =\"list\" description = \"specify the address\" } Create a folder with name mod in the modules directory. Create a file with name mod.tf and write the below code in it. We are passing the variables from the root module (mod) to the child module (vnet) using source path . module \"child\" { source = \"../vnet\" name = \"modvnet\" resourcegroup = \"user-vcom\" location = \"eastus\" address = \"10.0.1.0/16\" } Run terraform plan to visualize how the directories will be created. You can see the name of the vnet is taken from the module file instead of getting it from vars.tf Nested Modules \u00b6 For any module, root module should exist. Root module is basically a directory that holds the terraform configuration files for the desired infrastructure. They also provide the entry point to the nested modules you are going to utilize. For any module main.tf, vars.tf, and outputs.tf naming conventions are recommended. If you are using nested modules, create them under the subdirectory with name modules . checking_modules |__modules | |____main.tf | |______virtual_network | |_main.tf | |_vars.tf | |______storage_account |_main.tf |_vars.tf You will create three folders with names modules, virtual_network, and storage account. Make sure that you are following the terraform standards like writing the terraform configuration in the main file and defining the variables alone in the variables(vars.tf) file. stoacctmain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"${var.name}\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"${var.account_replication}\" account_tier = \"${var.account_tier}\" } stoacctvars.tf variable \"rg\" { default = \"user-wmle\" } variable \"loc\" { default = \"eastus\" } variable \"name\" { default =\"storagegh\" } variable \"account_replication\" { default = \"LRS\" } variable \"account_tier\" { default = \"Basic\" } Root Module (main.tf) file under checking_modules module \"vnet\" { source = \"../virtual_network\" name = \"mymodvnet\" } module \"stoacct\" { source = \"../storage_account\" name = \"mymodstorageaccount\" } Not only the parameters mentioned above, but you can also pass any parameter from root module to the child modules. You can pass the parameters from the module to avoid the duplicity between them. Remote Backends \u00b6 It is better to store the code in a central repository . However, is having some disadvantages of doing so: You have to maintain push, pull configurations . If one pushes the wrong configuration and commits it, it becomes a problem. State file consists of sensitive information . So state files are non-editable. Backend in terraform explains how the state file is loaded and operations are executed. Backend can get initialize only after running terraform init command. So, terraform init is required to be run every time when backend is configured when any changes made to the backend when the backend configuration is removed completely Terraform cannot perform auto-initialize because it may require additional info from the user, to perform state migrations, etc.. Below are the steps which you will be maintaining for creating remote backend in Azure. Create a storage account Create a storage container Create a backend Get the storage account access key and resource group name, give it as a parameter to the backend. Below is the complete file structure for sample backend. Backend |____stoacct.tf |____stocontainer.tf |____backend.tf |____vars.tf |____output.tf stoacct.tf resource \"azurerm_storage_account\" \"storageaccount\" { name = \"storageaccountname\" resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" account_tier = \"${var.accounttier}\" account_replication_type = \"GRS\" tags { environment = \"staging\" } } stocontainer.tf resource \"azurerm_storage_container\" \"storagecontainer\" { name = \"vhds\" resource_group_name = \"${var.resourcegroup}\" storage_account_name = \"${azurerm_storage_account.storageaccount.name}\" container_access_type = \"private\" } vars.tf variable \"resourcegroup\" { default = \"user-abcd\" } variable \"location\" { default = \"eastus\" } variable \"accounttier\" { default = \"Basic\" } output.tf output \"storageacctname\" { value = \"${azurerm_storage_account.storageaccount.name}\" } output \"storageacctcontainer\" { value = \"${azurerm_storage_account.storagecontainer.name}\" } output \"access_key\" { value = \"${azurerm_storage_account.storageaccount.primary_access_key\" } Before creating backend, run the command to get the storage account keys list( az storage account keys list \u2013account-name storageacctname ) and copy one of the key somewhere. It is useful for configuring the backend. backend.tf terraform{ backend \"azurerm\" { storage_account_name = \"storageacct21\" container_name = \"mycontainer\" key = \"terraform.tfstate.change\" resource_group_name = \"user-okvt\" access_key = \"aJlf+XjZhxwRp4gsU4hkGrQJzO7xBzz7B9rSzMLB/ozwcM6k/1N.......==\" } } where Parameters: storage_account_name - name of the storage account. container_name - name of the container. key- name of the tfstate blob. resource_group_name - name of the resource group. access_key - Storage account access key(any one). Points to Remember \u00b6 You cannot use interpolation syntax to configure backends. After creating backend run terraform init, it will be in the locked state. By running terraform apply command automatically, the lease status is changed to locked. After it is applied, it will come to an unlocked state. This backend supports consistency checking and state locking using the native capabilities of the Azure storage account. Terragrunt \u00b6 Terragrunt is referred to as a thin wrapper for Terraform which provides extra tools to keep the terraform configurations DRY, manage remote state and to work with multiple terraform modules. Terragrunt Commands \u00b6 terragrunt get terragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt supports the following use cases: Keeps terraform code DRY. Remote state configuration DRY. CLI flags DRY. Executing terraform commands on multiple modules at a time. Advantages of using terragrunt: Provides locking mechanism. Allows you to use remote state always. Build-In Functions \u00b6 lookup \u00b6 This helps in performing a dynamic lookup into a map variable. The syntax for lookup variable is lookup(map,key, [default]) . map: The map parameter is a variable like var.name. key: The key parameter includes the key from where it should find the environment. If the key doesn't exist in the map, interpolation will fail, if it doesn't find a third argument (default). The lookup will not work on nested lists or maps. It only works on flat maps. Local Values \u00b6 Local values help in assigning names to the expressions , which can be used multiple times within a module without rewriting it. Local values are defined in the local blocks. It is recommended to group the logically related local values together as a single block, if there is a dependency between them. locals { service_name = \"Fresco\" owner = \"Team\" } Example locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } If a single value or a result is used in many places and it is likely to be changed in the future, then you can go with locals. It is recommended to not use many local values because it makes the read configurations hard to the future maintainers. Data Source \u00b6 Data source allows the data to be computed or fetched for use in Terraform configuration. Data sources allow the terraform configurations to build on the information defined outside the Terraform or by another Terraform configuration. Providers play a major role in implementing and defining data sources. Data source helps in two major ways: It provides a read-only view for the pre-existing data. It can compute new values on the fly. Every data source in the Terraform is mapped to the provider depending on the longest-prefix-matching . data \"azurerm_resource_group\" \"passed\" { name = \"${var.resource_group_name}\" } Concat and Contains \u00b6 concat(list1,list2) * It combines two or more lists into a single list. contains(list, element) * It returns true if the element is present in the list or else false. E.g. contains(var.list_of_strings, \"an_element\") Workspaces \u00b6 Every terraform configurations has associate backends which defines how the operations are executed and where the persistent data like terraform state are stored. Persistent data stored in backend belongs to a workspace. Previously, the backend has only one workspace which is called as default and there will be only one state file associated with the configuration. Some of the backends support multiple workspaces, i.e. It allows multiple state files to be stored within a single configuration. The configuration is still having only one backend, and the multiple distinct instances can be deployed without configuring to a new backend or by changing authentication credentials. default workspace is special because you can't delete the default workspace. You cannot delete your active workspace (the workspace in which you are currently working). If you haven't created the workspace before, then you are working on the default workspace. Workspace Commands \u00b6 terraform workspace new - It creates the workspace with specified name and switches to it. terraform workspace list - It lists the workspaces available. terraform workspace select - It switches to the specified workspace. terraform workspace delete - It deletes the mentioned workspace. Configuring Multiple Providers \u00b6 Below is the file structure that you will maintain for working with multiple providers. multiple_providers | |___awsmain.tf | |___azuremain.tf | |___vars.tf | |___out.tf Create an S3 bucket in Azure which helps in storing the file in AWS. awsmain.tf resource \"aws_s3_bucket\" \"b\" { bucket = \"my-tf-test-bucket-21\" acl = \"private\" tags = { Name = \"My bucket test\" Environment = \"Dev\" } } Resource has two parameters: name(aws_s3_bucket) and type(\"b\"). Name of the resource may vary from provider to provider. Type of the resource depends on the user. Bucket indicates the name of the bucket. If it is not assigned, terraform assigns a random unique name. acl (access control list) - It helps to manage access to the buckets and objects. tag - It is a label which is assigned to the AWS resource by you or AWS. Environment - On which environment do you like to deploy S3 bucket (Dev, Prod or Stage). azuremain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"storageacct21\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"LRS\" account_tier = \"Standard\" } Parameters name - It indicates the name of the storage account. resource_group_name and location- name of the resource group and location. account_replication_type - Type of the account replication. account_tier - Account tier type out.tf output \"account_tier\" { value = \"azurerm_storage_Account.storagecct.account_tier\" }","title":"Terraform"},{"location":"learning/terraform/#example-using-docker-images","text":"// Main . tf resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image . nginx . latest name = \"webserver\" ports { internal = 80 external = 8050 } }","title":"Example using Docker images"},{"location":"learning/terraform/#infrastructure-as-code","text":"IaC is an important DevOps cornerstone that enables you to define, automatically manage, and provision infrastructure through source code. Infrastructure is managed as a software system . IaC is frequently referred to as Programmable Infrastructure .","title":"Infrastructure as Code"},{"location":"learning/terraform/#benefits-of-iac","text":"In IaC, A tool will monitor the state of the infrastructure . A script will be sent automatically to fix the issue . A script can be written to add new instances, and it can be reused . Faster process , no/less human involvement.","title":"Benefits of Iac"},{"location":"learning/terraform/#iac-implemetation-approaches","text":"","title":"IaC Implemetation Approaches"},{"location":"learning/terraform/#declarative","text":"Focuses on the desired end state of infrastructure (Functional) . Tools perform the necessary actions to reach that state . Automatically takes care of the order and executes it . Examples are Terraform and CloudFormation.","title":"Declarative"},{"location":"learning/terraform/#imperative","text":"Focuses on how to achieve the desired state (Procedural) . Follows the order of tasks and it executes in that order . Examples are Chef and Ansible.","title":"Imperative"},{"location":"learning/terraform/#configuration-management","text":"It is a system that keeps the track of organization's hardware, software, and other related information. It includes the software updates and versions present in the system. Configuration management is also called as configuration control . E.g.: Chef and Ansible","title":"Configuration Management"},{"location":"learning/terraform/#orchestration","text":"It automates the coordination, management, and configuration of systems and hardware. E.g.: Terraform and Nomad","title":"Orchestration:"},{"location":"learning/terraform/#installing-terraform","text":"sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip unzip terraform_0.11.11_linux_amd64.zip sudo mv terraform /usr/local/bin/ terraform \u2013version","title":"Installing Terraform"},{"location":"learning/terraform/#terraform-lifecycle","text":"Terraform init - Initializes the working directory having Terraform configuration files. Terraform plan - Creates an execution plan and it mentions the changes it is going to make. Terraform apply - When you are not amazed by seeing the results of the terraform plan, you can go ahead and run terraform apply to apply the changes. The main difference between plan and apply is - apply asks for a prompt like Do you want to perform these actions? and then implement the changes, whereas, plan shows the possible end state of your infrastructure without actually implementing them. Terraform destroy - Destroys the created infrastructure.","title":"Terraform Lifecycle"},{"location":"learning/terraform/#terraform-configuration","text":"A set of files that describe the infrastructure in Terraform is called as Terraform Configuration. The entire configuration file is saved with .tf extension. Make sure to have all the configurations maintained in a single .tf file instead of many. Terraform loads all the .tf files present in the working directory.","title":"Terraform Configuration"},{"location":"learning/terraform/#creating-virtual-network","text":"create a virtual network without using the Azure portal and azure CLI commands. resource \"azurerm_virtual_network\"\"myterraformnetwork\" { name = \"myvnet\" address_space = [\"10.0.0.0/16\"] location=\"East US\" resource_group_name=\"er-tyjy\" } Resource blocks define the infrastructure (resource \"azurerm_virtual_network\" \"myterraformnetwork\"). It contains the two strings - type of the resource(azurerm_virtualnetwork) and name of the resource(myterraformnetwork). Terraform uses plugin based architecture to support the various services providers and infrastructure available. When you run Terraform init command, it downloads and installs provider binary for the corresponding providers. In this case, it is Azure. Now, you can run the terraform plan command. It shows all the changes it is going to make. If the output is as you expected and there are no errors, then you can run Terraform apply.","title":"Creating Virtual Network"},{"location":"learning/terraform/#terraform-validate","text":"You can run this command whenever you like to check whether the code is correct. Once you run this command, if there is no output, it means that there are no errors in the code.","title":"Terraform Validate"},{"location":"learning/terraform/#variables","text":"Terraform provides the following variable types: Strings - The default data type is a string . List - A list is like simple arrays Map - A map value is nothing but a lookup table for string keys to get the string values. In terraform, to reduce the complexity of code and for easiness , all the variables are created in the variables.tf file. variables.tf variable \"rg\" { default = \"terraform-lab2\" } variable \"locations\" { default = [\"ap-southeast-1\" , \"ap-southeast-2\"] } variable \"tags\" { type = \"map\" default = { environment = \"training\" source=\"citadel\" } } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-east-2 = \"image-4321\" } } For example, you have defined resource_group name in the variable.tf file. you can call them in main.tf file like this \"${var.resource_group}\"","title":"Variables"},{"location":"learning/terraform/#sensitive-parameters","text":"There may be sensitive parameters which should be shown only when running the terraform plan but not terraform apply. output \"sensitiveoutput\" { sensitive = true value = VALUE } When you run terraform apply, the output is labeled as sensitive. If there is any sensitive information, then you can protect it by using a sensitive parameter.","title":"Sensitive Parameters"},{"location":"learning/terraform/#terraformtfvars-file","text":"In terraform.tfvars file, we mention the \"key/value\" pair. The main.tf file takes the input from a terraform. tfvars file. If it is not present, it gets the input from variables.tf file. resource_group = \"user-ybje\" location = \"East US\"","title":"terraform.tfvars File"},{"location":"learning/terraform/#terraform-fmt","text":"It rewrites the confguration files to canonical style and format.","title":"terraform fmt"},{"location":"learning/terraform/#state-file---terraformtfstate","text":"It is a JSON file that maps the real world resources to the configuration files. In terraform, there are three states. Desired state - Represents how the infrastructure to be present. Actual state - Represents the current state of infrastructure. Known state - To bridge desired state and actual state, there is a known state. You can get the details of known state from a tfstate file. When you run terraform plan command , terraform performs a quick refresh and lists the actions needed to achieve the desired state. When terraform apply command is run, it applies the changes and updates the actual state.","title":"State File - terraform.tfstate"},{"location":"learning/terraform/#modules","text":"A module is like a reusable blueprint of infrastructure . A module is nothing but a folder of Terraform files. In terraform the modules are categorized into two types: Root modules - The current directory of Terraform files on which you are running the commands. Child modules - The modules sourced by the root module. To create a child module, add resources to it. Module syntax: module \"child\" { source = \"../child\" } The difference between adding the resources and module is for adding the resource (Eg: resource \"azurerm_virtual_network\" 'test\") you need type and name but for adding a module (Eg: module \"child\") only the name is enough. The name of the module should be unique within configurations because it is used as a reference to the module and outputs. main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 }","title":"Modules"},{"location":"learning/terraform/#updates","text":"terraform get - This command is used to download and install the modules for that configuration. terraform get -update - It checks the downloaded modules and checks for the recent versions if any.","title":"Updates"},{"location":"learning/terraform/#module-outputs","text":"If you need output from any module, you can get it by passing the required parameter. For example, the output of virtual network location can be found by following the below syntax. You can add this syntax to .tf file(main.tf or variables.tf) . output \"virtual_network_id\" { value = \"${azurerm_virtual_network.test.location}\" } Where virtual_network_id is a unique name and \"${azurerm_virtual_network.test.location}\" is the location of the virtual network using string interpolation.","title":"Module Outputs"},{"location":"learning/terraform/#benefits-of-modules","text":"Code reuse : When there is a need to provision the group of resources on another resource at the same time, a module can be used instead of copying the same code. It helps in resolving the bugs easily. If you want to make changes, changing the code in one place is enough. Abstraction layer : It makes complex configurations easier to conceptualize. For example, if you like to add vault(Another hashicorp's tool for managing secrets) cluster to the environment, it requires dozens of components. Instead of worrying about individual components, it gives ready-to-use vault cluster. Black box : To create a vault, you only need some configuration parameters without worrying about the complexity. You don't need any knowledge on how to install vault, configuring vault cluster and working of the vault. You can create a working cluster in a few minutes. Best practices in organization : When a module is created, you can give it to the other teams. It helps in easily developing the infrastructure. **Versioned artifacts: They are immutable artifacts and can be promoted from one environment to others.","title":"Benefits of Modules"},{"location":"learning/terraform/#introduction-to-meta-parameters","text":"There are some difficulties with the declarative language. For example, since it is a declarative language, there is no concept of for loops. How do you repeat a piece of code without copy paste? * Terraform provides primitives like meta parameter called as count, life cycle blocks like create_before_destroy, ternary operator and a large number of interpolation functions . This helps in performing certain types of loops, if statements and zero downtime deployments.","title":"Introduction to Meta Parameters"},{"location":"learning/terraform/#count","text":"Count - It defines how many parameters you like to create. Modify main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } To increase Vnets to 5, update count = 5 . To decrease Vnets to 2, update count = 2 , last 3 Vnets will get deleted.","title":"Count"},{"location":"learning/terraform/#elements","text":"Suppose you like to create the three virtual networks with names vnet-A, vnet-B and vnet-C, you can do this easily with the help of list . Mention how many vnets you are going to create with the help of list and define it in variables.tf file variable \"name\" { type= \"list\" default = [\"A\",\"B\",\"C\"] } You can call this variable in the main.tf file in the following way count = \"${length(var.name)}\" - It returns number of elements present in the list. you should store it in meta parameter count. \"${element(var.name,count.index)}\" - It acts as a loop, it takes the input from list and repeats until there are no elements in list. variables.tf ** variable \"resource_group\" { default = \"user-nuqo\" } variable \"location\" { default = \"East US\" } variable \"name\" { type = \"list\" default = [\"A\",\"B\",\"C\"] } **main.tf resource \"azurerm_virtual_network\"\"multiple\" { name = \"vnet-${element(var.name,count.index)}\" resource_group_name = \"${var.resource_group}\" location = \"${var.location}\" address_space=[\"10.0.0.0/16\"] count=\"${length(var.name)}\" }","title":"Elements"},{"location":"learning/terraform/#conditions","text":"For example, a vnet has to be created only, if the variable number of vnets is 3 or else no. You can do this by using the ternary operator. variables.tf variable \"no_of_vnets\" { default = 3 } main.tf count = \"${var.no_of_vnets == 3 ? 1 : 0}\" * First, it will execute the ternary operator. If it is true, it takes the output as 1 or else 0. * Now, in the above case, the output is 1, the count becomes one, and the vnet is created.","title":"Conditions"},{"location":"learning/terraform/#inheriting-variables","text":"Inheriting the variables between modules keeps your Terraform code DRY. Don't Repeat Yourself (DRY) It aims at reducing repeating patterns and code. There is no need to write the duplicate code for each environment you are deploying. You can write the minimum code and achieve the goal by having maximum re-usability.","title":"Inheriting Variables"},{"location":"learning/terraform/#module-file-structure","text":"You will follow the below file structure to inherit the variables between modules. modules | |___mod | |__mod.tf | |___vnet |__vnet.tf |__vars.tf There is one vnet and you can configure the vnet by passing the parameters from module. The variables in the vnet are overriden by the values provided in mod.tf file. Create a folder with name Modules Create two subfolders of names vnet and mod in the nested_modules folder. In the vnet folder, create a virtual network using Terraform configuration. Maintain two files one for main configuration(vnet.tf) and other for declaring variables(vars.tf) . Now, create main.tf file like this resource \"azurerm_virtual_network\"\"vnet\" { name = \"${var.name}\" address_space = [\"10.0.0.0/16\"] resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" } Now, vars.tf file looks like this variable \"name\" { description = \"name of the vnet\" } variable \"resourcegroup\" { description = \"name of the resource group\" } variable \"location\" { description =\"name of the location\" } variable \"address\" { type =\"list\" description = \"specify the address\" } Create a folder with name mod in the modules directory. Create a file with name mod.tf and write the below code in it. We are passing the variables from the root module (mod) to the child module (vnet) using source path . module \"child\" { source = \"../vnet\" name = \"modvnet\" resourcegroup = \"user-vcom\" location = \"eastus\" address = \"10.0.1.0/16\" } Run terraform plan to visualize how the directories will be created. You can see the name of the vnet is taken from the module file instead of getting it from vars.tf","title":"Module File Structure"},{"location":"learning/terraform/#nested-modules","text":"For any module, root module should exist. Root module is basically a directory that holds the terraform configuration files for the desired infrastructure. They also provide the entry point to the nested modules you are going to utilize. For any module main.tf, vars.tf, and outputs.tf naming conventions are recommended. If you are using nested modules, create them under the subdirectory with name modules . checking_modules |__modules | |____main.tf | |______virtual_network | |_main.tf | |_vars.tf | |______storage_account |_main.tf |_vars.tf You will create three folders with names modules, virtual_network, and storage account. Make sure that you are following the terraform standards like writing the terraform configuration in the main file and defining the variables alone in the variables(vars.tf) file. stoacctmain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"${var.name}\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"${var.account_replication}\" account_tier = \"${var.account_tier}\" } stoacctvars.tf variable \"rg\" { default = \"user-wmle\" } variable \"loc\" { default = \"eastus\" } variable \"name\" { default =\"storagegh\" } variable \"account_replication\" { default = \"LRS\" } variable \"account_tier\" { default = \"Basic\" } Root Module (main.tf) file under checking_modules module \"vnet\" { source = \"../virtual_network\" name = \"mymodvnet\" } module \"stoacct\" { source = \"../storage_account\" name = \"mymodstorageaccount\" } Not only the parameters mentioned above, but you can also pass any parameter from root module to the child modules. You can pass the parameters from the module to avoid the duplicity between them.","title":"Nested Modules"},{"location":"learning/terraform/#remote-backends","text":"It is better to store the code in a central repository . However, is having some disadvantages of doing so: You have to maintain push, pull configurations . If one pushes the wrong configuration and commits it, it becomes a problem. State file consists of sensitive information . So state files are non-editable. Backend in terraform explains how the state file is loaded and operations are executed. Backend can get initialize only after running terraform init command. So, terraform init is required to be run every time when backend is configured when any changes made to the backend when the backend configuration is removed completely Terraform cannot perform auto-initialize because it may require additional info from the user, to perform state migrations, etc.. Below are the steps which you will be maintaining for creating remote backend in Azure. Create a storage account Create a storage container Create a backend Get the storage account access key and resource group name, give it as a parameter to the backend. Below is the complete file structure for sample backend. Backend |____stoacct.tf |____stocontainer.tf |____backend.tf |____vars.tf |____output.tf stoacct.tf resource \"azurerm_storage_account\" \"storageaccount\" { name = \"storageaccountname\" resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" account_tier = \"${var.accounttier}\" account_replication_type = \"GRS\" tags { environment = \"staging\" } } stocontainer.tf resource \"azurerm_storage_container\" \"storagecontainer\" { name = \"vhds\" resource_group_name = \"${var.resourcegroup}\" storage_account_name = \"${azurerm_storage_account.storageaccount.name}\" container_access_type = \"private\" } vars.tf variable \"resourcegroup\" { default = \"user-abcd\" } variable \"location\" { default = \"eastus\" } variable \"accounttier\" { default = \"Basic\" } output.tf output \"storageacctname\" { value = \"${azurerm_storage_account.storageaccount.name}\" } output \"storageacctcontainer\" { value = \"${azurerm_storage_account.storagecontainer.name}\" } output \"access_key\" { value = \"${azurerm_storage_account.storageaccount.primary_access_key\" } Before creating backend, run the command to get the storage account keys list( az storage account keys list \u2013account-name storageacctname ) and copy one of the key somewhere. It is useful for configuring the backend. backend.tf terraform{ backend \"azurerm\" { storage_account_name = \"storageacct21\" container_name = \"mycontainer\" key = \"terraform.tfstate.change\" resource_group_name = \"user-okvt\" access_key = \"aJlf+XjZhxwRp4gsU4hkGrQJzO7xBzz7B9rSzMLB/ozwcM6k/1N.......==\" } } where Parameters: storage_account_name - name of the storage account. container_name - name of the container. key- name of the tfstate blob. resource_group_name - name of the resource group. access_key - Storage account access key(any one).","title":"Remote Backends"},{"location":"learning/terraform/#points-to-remember","text":"You cannot use interpolation syntax to configure backends. After creating backend run terraform init, it will be in the locked state. By running terraform apply command automatically, the lease status is changed to locked. After it is applied, it will come to an unlocked state. This backend supports consistency checking and state locking using the native capabilities of the Azure storage account.","title":"Points to Remember"},{"location":"learning/terraform/#terragrunt","text":"Terragrunt is referred to as a thin wrapper for Terraform which provides extra tools to keep the terraform configurations DRY, manage remote state and to work with multiple terraform modules.","title":"Terragrunt"},{"location":"learning/terraform/#terragrunt-commands","text":"terragrunt get terragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt supports the following use cases: Keeps terraform code DRY. Remote state configuration DRY. CLI flags DRY. Executing terraform commands on multiple modules at a time. Advantages of using terragrunt: Provides locking mechanism. Allows you to use remote state always.","title":"Terragrunt Commands"},{"location":"learning/terraform/#build-in-functions","text":"","title":"Build-In Functions"},{"location":"learning/terraform/#lookup","text":"This helps in performing a dynamic lookup into a map variable. The syntax for lookup variable is lookup(map,key, [default]) . map: The map parameter is a variable like var.name. key: The key parameter includes the key from where it should find the environment. If the key doesn't exist in the map, interpolation will fail, if it doesn't find a third argument (default). The lookup will not work on nested lists or maps. It only works on flat maps.","title":"lookup"},{"location":"learning/terraform/#local-values","text":"Local values help in assigning names to the expressions , which can be used multiple times within a module without rewriting it. Local values are defined in the local blocks. It is recommended to group the logically related local values together as a single block, if there is a dependency between them. locals { service_name = \"Fresco\" owner = \"Team\" } Example locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } If a single value or a result is used in many places and it is likely to be changed in the future, then you can go with locals. It is recommended to not use many local values because it makes the read configurations hard to the future maintainers.","title":"Local Values"},{"location":"learning/terraform/#data-source","text":"Data source allows the data to be computed or fetched for use in Terraform configuration. Data sources allow the terraform configurations to build on the information defined outside the Terraform or by another Terraform configuration. Providers play a major role in implementing and defining data sources. Data source helps in two major ways: It provides a read-only view for the pre-existing data. It can compute new values on the fly. Every data source in the Terraform is mapped to the provider depending on the longest-prefix-matching . data \"azurerm_resource_group\" \"passed\" { name = \"${var.resource_group_name}\" }","title":"Data Source"},{"location":"learning/terraform/#concat-and-contains","text":"concat(list1,list2) * It combines two or more lists into a single list. contains(list, element) * It returns true if the element is present in the list or else false. E.g. contains(var.list_of_strings, \"an_element\")","title":"Concat and Contains"},{"location":"learning/terraform/#workspaces","text":"Every terraform configurations has associate backends which defines how the operations are executed and where the persistent data like terraform state are stored. Persistent data stored in backend belongs to a workspace. Previously, the backend has only one workspace which is called as default and there will be only one state file associated with the configuration. Some of the backends support multiple workspaces, i.e. It allows multiple state files to be stored within a single configuration. The configuration is still having only one backend, and the multiple distinct instances can be deployed without configuring to a new backend or by changing authentication credentials. default workspace is special because you can't delete the default workspace. You cannot delete your active workspace (the workspace in which you are currently working). If you haven't created the workspace before, then you are working on the default workspace.","title":"Workspaces"},{"location":"learning/terraform/#workspace-commands","text":"terraform workspace new - It creates the workspace with specified name and switches to it. terraform workspace list - It lists the workspaces available. terraform workspace select - It switches to the specified workspace. terraform workspace delete - It deletes the mentioned workspace.","title":"Workspace Commands"},{"location":"learning/terraform/#configuring-multiple-providers","text":"Below is the file structure that you will maintain for working with multiple providers. multiple_providers | |___awsmain.tf | |___azuremain.tf | |___vars.tf | |___out.tf Create an S3 bucket in Azure which helps in storing the file in AWS. awsmain.tf resource \"aws_s3_bucket\" \"b\" { bucket = \"my-tf-test-bucket-21\" acl = \"private\" tags = { Name = \"My bucket test\" Environment = \"Dev\" } } Resource has two parameters: name(aws_s3_bucket) and type(\"b\"). Name of the resource may vary from provider to provider. Type of the resource depends on the user. Bucket indicates the name of the bucket. If it is not assigned, terraform assigns a random unique name. acl (access control list) - It helps to manage access to the buckets and objects. tag - It is a label which is assigned to the AWS resource by you or AWS. Environment - On which environment do you like to deploy S3 bucket (Dev, Prod or Stage). azuremain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"storageacct21\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"LRS\" account_tier = \"Standard\" } Parameters name - It indicates the name of the storage account. resource_group_name and location- name of the resource group and location. account_replication_type - Type of the account replication. account_tier - Account tier type out.tf output \"account_tier\" { value = \"azurerm_storage_Account.storagecct.account_tier\" }","title":"Configuring Multiple Providers"},{"location":"learning/tmux/","text":"tmux ls # List all sessions tmux attach -t 0 # -t #n indicates the session number from ls output tmux new -s work # Create a new session with a name tmux rename-session -t 0 work # Rename existing session tmux attach -t work # Attach to existing session after login tmux switch -t session_name # Switches to an existing session named session_name tmux list-sessions # Lists existing tmux sessions tmux detach # (prefix + d) detach the currently attached session Workflow Recommendations \u00b6 Use a single client \u2014 Although it is possible to run multiple tmux clients by opening multiple terminal tabs or windows, I find it better to focus on one client in a single terminal window. This provides a single high-level abstraction, which is easier to reason about and interact with. One project per session \u2014 I will open each project, roughly mapping to a git repository in its own session. Typically I will have Vim in the first window along with a pane for running tests and any background processes like the rails server running in additional windows. One Vim instance per session \u2014 In order to avoid conflicts with temp files and buffers getting out of sync, I will only use a single Vim instance per tmux session. Since each session maps to a specific project, this tends to keep me safe from conflicting edits and similar complications.","title":"Tmux"},{"location":"learning/tmux/#workflow-recommendations","text":"Use a single client \u2014 Although it is possible to run multiple tmux clients by opening multiple terminal tabs or windows, I find it better to focus on one client in a single terminal window. This provides a single high-level abstraction, which is easier to reason about and interact with. One project per session \u2014 I will open each project, roughly mapping to a git repository in its own session. Typically I will have Vim in the first window along with a pane for running tests and any background processes like the rails server running in additional windows. One Vim instance per session \u2014 In order to avoid conflicts with temp files and buffers getting out of sync, I will only use a single Vim instance per tmux session. Since each session maps to a specific project, this tends to keep me safe from conflicting edits and similar complications.","title":"Workflow Recommendations"},{"location":"learning/vagrant/","text":"Vagrant is not able to run on Windows 10 + WLS2 Windows setup \u00b6 vagrant plugin install vagrant-vbguest Image should have guest additions or it will not share folder from windows inside the VM Internal Network \u00b6 Go to VirtualBox \u2192 File \u2192 Host Network Manager \u2192 Check the enabled network DHCP address Windows Features Turn On Off \u00b6 Disable \"virtual machine platform\" and \"windows hypervisor platform Installation \u00b6 Install same version of Vagrant in Windows and WSL Verify vagrant --version in both to match In windows try downloading a box and start vagrant up \u2013provider=virtualbox Vagrant \u00b6 Init with a image in vagrant cloud \u00b6 vagrant init hashicorp/precise64 Start the vm \u00b6 vagrant up SSH into the vm \u00b6 vagrant ssh Hibernate the vm \u00b6 vagrant suspend Check the status of vagrant vm \u00b6 vagrant status Stop the vm \u00b6 vagrant halt Clean up the vm \u00b6 vagrant destroy Get Status of Vagrant Machines on host vagrant global-status Get SSH Settings vagrant ssh-config Reload Virtual Machine vagrant reload Make sure the ssh key you created is stored parallel to your Vagrantfile before you execute the vagrant up command. Vagrant commands Managing Vagrant boxes \u00b6 Download a box to a machine \u00b6 vagrant box add ubuntu/trusty64 vagrant box add centos/8 List boxes on machine \u00b6 vagrant box list Update an existing box on a machine \u00b6 vagrant box outdated vagrant box update Run a downloaded box \u2192 cd into a folder \u00b6 vagrant init ubuntu/trusty64 vagrant up Remove a downloaded box from a machine \u00b6 vagrant box remove ubuntu/trusty64 Finding boxes \u00b6 vagrantboxes.es & vagrantcloud \u2192 find a box and copy the url vagrant box add vagrant init vagrant up Using Plugins \u00b6 List existing plugins \u00b6 vagrant plugin list Install Plugins \u00b6 vagrant plugin install vagrant-vbguest Update Plugin version \u00b6 vagrant plugin update vagrant-vbguest Update all plugins \u00b6 vagrant plugin update Remove Plugins \u00b6 vagrant plugin uninstall vagrant-vbguest Adding services to startup boot \u00b6 sudo chkconfig \u2013add httpd sudo chkconfig httpd on sudo service httpd stop Create symbolic link which will serve file from local on vagrant machine, ensure index html file is there in local root \u00b6 cd /var/www/html cd .. && rm -rf html sudo ln -s /vagrant /var/www/html sudo service httpd start Packaging Vagrant after baking \u00b6 Imp that VM is running, check status \u00b6 vagrant status vagrant package \u2013output .box vagrant box add .box Custom base box packaging after customization / hardening \u00b6 vagrant package \u2013base Switching of guest additions checks if the plugin is available in local \u00b6 Add line in config \u00b6 config.vbguest.auto_update = false Adding a file from local machine not in the project folder to the vm \u00b6 config.vm.provision \"file\", source: \"~/vagrant/files/git-files\", destination: \"~/.gitconfig\" If VM is running when above provisioning is done, it is not reflected \u00b6 vagrant provision Adding software at provisioning \u00b6 config.vm.provision \"shell\", inline: \"yum install -y git nano\" Adding custom scripts not in the project folder to the vm \u00b6 config.vm.provision \"shell\", path: \"~/vagrant/scripts/provision.sh\" To restart vm \u00b6 sudo shutdown -r now Restart service \u00b6 sudo systemctl restart sshd.service Update centos kernal \u00b6 sudo yum update kernel* Check and delete old kernels \u00b6 rpm -qa kernel sudo package-cleanup \u2013old-kernels \u2013count=2 Debugging Vagrant \u00b6 During Vagrant Up your Windows system tries to connect to SSH. If you type on your command line: \u00b6 set VAGRANT_LOG=INFO Debug SSH \u00b6 set VAGRANT_PREFER_SYSTEM_BIN=0 vagrant ssh \u2013debug - Running this command will show which identity file is being used vagrant ssh-config | grep IdentityFile","title":"Vagrant"},{"location":"learning/vagrant/#windows-setup","text":"vagrant plugin install vagrant-vbguest Image should have guest additions or it will not share folder from windows inside the VM","title":"Windows setup"},{"location":"learning/vagrant/#internal-network","text":"Go to VirtualBox \u2192 File \u2192 Host Network Manager \u2192 Check the enabled network DHCP address","title":"Internal Network"},{"location":"learning/vagrant/#windows-features-turn-on-off","text":"Disable \"virtual machine platform\" and \"windows hypervisor platform","title":"Windows Features Turn On Off"},{"location":"learning/vagrant/#installation","text":"Install same version of Vagrant in Windows and WSL Verify vagrant --version in both to match In windows try downloading a box and start vagrant up \u2013provider=virtualbox","title":"Installation"},{"location":"learning/vagrant/#vagrant","text":"","title":"Vagrant"},{"location":"learning/vagrant/#init-with-a-image-in-vagrant-cloud","text":"vagrant init hashicorp/precise64","title":"Init with a image in vagrant cloud"},{"location":"learning/vagrant/#start-the-vm","text":"vagrant up","title":"Start the vm"},{"location":"learning/vagrant/#ssh-into-the-vm","text":"vagrant ssh","title":"SSH into the vm"},{"location":"learning/vagrant/#hibernate-the-vm","text":"vagrant suspend","title":"Hibernate the vm"},{"location":"learning/vagrant/#check-the-status-of-vagrant-vm","text":"vagrant status","title":"Check the status of vagrant vm"},{"location":"learning/vagrant/#stop-the-vm","text":"vagrant halt","title":"Stop the vm"},{"location":"learning/vagrant/#clean-up-the-vm","text":"vagrant destroy Get Status of Vagrant Machines on host vagrant global-status Get SSH Settings vagrant ssh-config Reload Virtual Machine vagrant reload Make sure the ssh key you created is stored parallel to your Vagrantfile before you execute the vagrant up command. Vagrant commands","title":"Clean up the vm"},{"location":"learning/vagrant/#managing-vagrant-boxes","text":"","title":"Managing Vagrant boxes"},{"location":"learning/vagrant/#download-a-box-to-a-machine","text":"vagrant box add ubuntu/trusty64 vagrant box add centos/8","title":"Download a box to a machine"},{"location":"learning/vagrant/#list-boxes-on-machine","text":"vagrant box list","title":"List boxes on machine"},{"location":"learning/vagrant/#update-an-existing-box-on-a-machine","text":"vagrant box outdated vagrant box update","title":"Update an existing box on a machine"},{"location":"learning/vagrant/#run-a-downloaded-box----cd-into-a-folder","text":"vagrant init ubuntu/trusty64 vagrant up","title":"Run a downloaded box --&gt; cd into a folder"},{"location":"learning/vagrant/#remove-a-downloaded-box--from-a-machine","text":"vagrant box remove ubuntu/trusty64","title":"Remove a downloaded box  from a machine"},{"location":"learning/vagrant/#finding-boxes","text":"vagrantboxes.es & vagrantcloud \u2192 find a box and copy the url vagrant box add vagrant init vagrant up","title":"Finding boxes"},{"location":"learning/vagrant/#using-plugins","text":"","title":"Using Plugins"},{"location":"learning/vagrant/#list-existing-plugins","text":"vagrant plugin list","title":"List existing plugins"},{"location":"learning/vagrant/#install-plugins","text":"vagrant plugin install vagrant-vbguest","title":"Install Plugins"},{"location":"learning/vagrant/#update-plugin-version","text":"vagrant plugin update vagrant-vbguest","title":"Update Plugin version"},{"location":"learning/vagrant/#update-all-plugins","text":"vagrant plugin update","title":"Update all plugins"},{"location":"learning/vagrant/#remove-plugins","text":"vagrant plugin uninstall vagrant-vbguest","title":"Remove Plugins"},{"location":"learning/vagrant/#adding-services-to-startup-boot","text":"sudo chkconfig \u2013add httpd sudo chkconfig httpd on sudo service httpd stop","title":"Adding services to startup boot"},{"location":"learning/vagrant/#create-symbolic-link-which-will-serve-file-from-local-on-vagrant-machine-ensure-index-html-file-is-there-in-local-root","text":"cd /var/www/html cd .. && rm -rf html sudo ln -s /vagrant /var/www/html sudo service httpd start","title":"Create symbolic link which will serve file from local on vagrant machine, ensure index html file is there in local root"},{"location":"learning/vagrant/#packaging-vagrant-after-baking","text":"","title":"Packaging Vagrant after baking"},{"location":"learning/vagrant/#imp-that-vm-is-running-check-status","text":"vagrant status vagrant package \u2013output .box vagrant box add .box","title":"Imp that VM is running, check status"},{"location":"learning/vagrant/#custom-base-box-packaging-after-customization--hardening","text":"vagrant package \u2013base","title":"Custom base box packaging after customization / hardening"},{"location":"learning/vagrant/#switching-of-guest-additions-checks-if-the-plugin-is-available-in-local","text":"","title":"Switching of guest additions checks if the plugin is available in local"},{"location":"learning/vagrant/#add-line-in-config","text":"config.vbguest.auto_update = false","title":"Add line in config"},{"location":"learning/vagrant/#adding-a-file-from-local-machine-not-in-the-project-folder-to-the-vm","text":"config.vm.provision \"file\", source: \"~/vagrant/files/git-files\", destination: \"~/.gitconfig\"","title":"Adding a file from local machine not in the project folder to the vm"},{"location":"learning/vagrant/#if-vm-is-running-when-above-provisioning-is-done-it-is-not-reflected","text":"vagrant provision","title":"If VM is running when above provisioning is done, it is not reflected"},{"location":"learning/vagrant/#adding-software-at-provisioning","text":"config.vm.provision \"shell\", inline: \"yum install -y git nano\"","title":"Adding software at provisioning"},{"location":"learning/vagrant/#adding-custom-scripts-not-in-the-project-folder-to-the-vm","text":"config.vm.provision \"shell\", path: \"~/vagrant/scripts/provision.sh\"","title":"Adding custom scripts not in the project folder to the vm"},{"location":"learning/vagrant/#to-restart-vm","text":"sudo shutdown -r now","title":"To restart vm"},{"location":"learning/vagrant/#restart-service","text":"sudo systemctl restart sshd.service","title":"Restart service"},{"location":"learning/vagrant/#update-centos-kernal","text":"sudo yum update kernel*","title":"Update centos kernal"},{"location":"learning/vagrant/#check-and-delete-old-kernels","text":"rpm -qa kernel sudo package-cleanup \u2013old-kernels \u2013count=2","title":"Check and delete old kernels"},{"location":"learning/vagrant/#debugging-vagrant","text":"","title":"Debugging Vagrant"},{"location":"learning/vagrant/#during-vagrant-up-your-windows-system-tries-to-connect-to-ssh-if-you-type-on-your-command-line","text":"set VAGRANT_LOG=INFO","title":"During Vagrant Up your Windows system tries to connect to SSH. If you type on your command line:"},{"location":"learning/vagrant/#debug-ssh","text":"set VAGRANT_PREFER_SYSTEM_BIN=0 vagrant ssh \u2013debug - Running this command will show which identity file is being used vagrant ssh-config | grep IdentityFile","title":"Debug SSH"},{"location":"learning/ansible/ansible/","text":"Introduction \u00b6 Note The rule of thumb is if you can script it, you can create a playbook for it. Ansible 101 Ansible Cheat Sheet App Security Ansible Tips and Tricks DO Practise examples & Explaination & Tutorials YAML Spec Ref Card Full Application on Cloud Maintaining Playbooks - Pitfalls Documentation on cmdline \u00b6 # System outputs the man page for debug module ansible-doc debug ansible-doc -l | grep aws # List all plugins and then grep for aws plugins ansible-doc -s shell # Shows snippets on how to use the plugins Setup Server \u00b6 Default Configuration \u00b6 ansible.cfg and hosts files are present inside /etc/ansible Testing ansible on Ubuntu WSL ansible localhost -m ping Enabling SSH on the VM \u00b6 If you need SSH enabled on the system, follow the below steps: Ensure the /etc/apt/sources.list file has been updated as per above Run the command: apt-get update Run the command: apt-get install openssh-server Run the command: service sshd start ssh-keygen -t rsa -C \"ansible\" #OR # Generate an SSH key pair for future connections to the VM instances (run the command exactly as it is): ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user -P \"\" #Add the SSH private key to the ssh-agent: ssh-add ~/.ssh/ansible-user #Verify that the key was added to the ssh-agent: $ ssh-add -l Access VM over SSH \u00b6 ssh vagrant@127.0.0.1 -p 2222 -i ~/.ssh/insecure_private_key Copy files recursively from local desktop to remote server \u00b6 scp -r ./scripts vagrant@127.0.0.1:/home/vagrant -p 2222 -i ~/.ssh/insecure_private_key Target Docker containers for Ansible controller \u00b6 The Docker file used to create the ubuntu-ssh-enabled Docker image is located here. Issues installing Ansible and its dependencies \u00b6 Once the Debian VM is up and running make the following changes to the /etc/apt/sources.list file to get the Ansible installation working right. deb http://security.debian.org/ jessie/updates main contrib deb-src http://security.debian.org/ jessie/updates main contrib deb http://ftp.debian.org/debian/ jessie-updates main contrib deb-src http://ftp.debian.org/debian/ jessie-updates main contrib deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main deb http://ftp.de.debian.org/debian sid main Ansible Directory Structure as per Best Practises \u00b6 This is the directory layout of this repository with explanation. production.ini # inventory file for production stage development.ini # inventory file for development stage test.ini # inventory file for test stage vpass # ansible-vault password file # This file should not be committed into the repository # therefore file is ignored by git ########################## This segration can be done if there are changes in variable values between environments \u251c\u2500\u2500 inventories \u2502 \u251c\u2500\u2500 development \u2502 \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u2502 \u2514\u2500\u2500 app.yml \u2502 \u2502 \u251c\u2500\u2500 hosts \u2502 \u2502 \u2514\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 production \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u2514\u2500\u2500 app.yml \u2502 \u251c\u2500\u2500 hosts \u2502 \u2514\u2500\u2500 host_vars ########################## group_vars/ all/ # variables under this directory belongs all the groups apt.yml # ansible-apt role variable file for all groups webservers/ # here we assign variables to webservers groups apt.yml # Each file will correspond to a role i.e. apt.yml nginx.yml # \"\" postgresql/ # here we assign variables to postgresql groups postgresql.yml # Each file will correspond to a role i.e. postgresql postgresql-password.yml # Encrypted password file plays/ ansible.cfg # Ansible.cfg file that holds all ansible config webservers.yml # playbook for webserver tier postgresql.yml # playbook for postgresql tier roles/ roles_requirements.yml# All the information about the roles external/ # All the roles that are in git or ansible galaxy # Roles that are in roles_requirements.yml file will be downloaded into this directory internal/ # All the roles that are not public common/ # common role tasks/ # main.yml # installing basic tasks scripts/ setup/ # All the setup files for updating roles and ansible dependencies Ansible Inventory \u00b6 Creating an inventory file and adding hosts \u00b6 Ansible supports two types of inventory\u2014static and dynamic Static inventories are by their very nature static; they are unchanging unless a human being goes and manually edits them. Even in small, closed environments, static inventories are a great way to manage your environment, especially when changes to the infrastructure are infrequent. # Sample inventory file in INI format target1.example.com ansible_host=192.168.81.142 ansible_port=3333 target2.example.com ansible_port=3333 ansible_user=danieloh target3.example.com ansible_host=192.168.81.143 ansible_port=5555 ansible_host: If the inventory hostname cannot be accessed directly\u2014perhaps because it is not in DNS, for example, this variable contains the hostname or IP address that Ansible will connect to instead. ansible_port: By default, Ansible attempts all communication over port 22 for SSH\u2014if you have an SSH daemon running on another port, you can tell Ansible about it using this variable. ansible_user: By default, Ansible will attempt to connect to the remote host using the current user account you are running the Ansible command from\u2014you can override this in several ways, of which this is one. Hence, the preceding three hosts can be summarized as follows: The target1.example.com host should be connected to using the 192.168.81.142 IP address, on port 3333. The target2.example.com host should be connected to on port 3333 also, but this time using the danieloh user rather than the account running the Ansible command. The target3.example.com host should be connected to using the 192.168.81.143 IP address, on port 5555. Using host groups \u00b6 Let's assume you have a simple three-tier web architecture, with multiple hosts in each tier for high availability and/or load balancing. The three tiers in this architecture might be the following: Frontend servers Application servers Database servers To keep the examples clear and concise, we'll assume that you can access all servers using their Fully Qualified Domain Names (FQDNs) , and hence won't add any host variables into these inventory files. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com We have created three groups called frontends, apps, and databases. Note that, in INI-formatted inventories, group names go inside square braces. Under each group name goes the server names that belong in each group, so the preceding example shows two servers in each group. Notice the outlier at the top, loadbalancer.example.com \u2014 this host isn't in any group. All ungrouped hosts must go at the very top of an INI-formatted file. The preceding inventory stands in its own right, but what if our frontend servers are built on Ubuntu, and the app and database servers are built on CentOS? There will be some fundamental differences in the ways we handle these hosts \u2014 for example, we might use the apt module to manage packages on Ubuntu and the yum module on CentOS. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends With the use of the children keyword in the group definition (inside the square braces), we can create groups of groups; hence, we can perform clever groupings to help our playbook design without having to specify each host more than once. ansible -i hostgroups-yml centos -m shell -a 'echo hello-yaml' -f 5 This is a powerful way of managing your inventory and making it easy to run commands on just the hosts you want to. The possibility of creating multiple groups makes life simple and easy, especially when you want to run different tasks on different groups of servers. Let's assume you have 100 app servers, all named sequentially, as follows: app01 to app100 [ apps ] app[01:100].prod.com The following inventory snippet actually produces an inventory with the same 100 app servers that we could create manually. Adding host and group variables to your inventory \u00b6 -Suppose that we need to set two variables for each of our two frontend servers. These are not special Ansible variables, but instead are variables entirely of our own choosing. - https_port, which defines the port that the frontend proxy should listen on - lb_vip, which defines the FQDN of the load-balancer in front of the frontend servers - You can assign variables to a host group as well as to hosts individually. [ frontends ] frt01.example.com frt02.example.com [frontends:vars] https_port=8443 lb_vip=lb.example.com - There will be times when you want to work with host variables for individual hosts, and times when group variables are more relevant. - It is also worth noting that host variables override group variables , so if we need to change the connection port to 8444 on the frt01.example.com one, we could do this as follows [ frontends ] frt01.example.com https_port=8444 frt02.example.com [frontends:vars] https_port=8443 lb_vip=lb.example.com - Right now, our examples are small and compact and only contain a handful of groups and variables; however, when you scale this up to a full infrastructure of servers, using a single flat inventory file could, once again, become unmanageable. - Luckily, Ansible also provides a solution to this. Two specially-named directories, host_vars and group_vars , are automatically searched for appropriate variable content if they exist within the playbook directory. - Under the host_vars directory, we'll create a file with the name of our host that needs the proxy setting, with .yml appended to it (that is, frt01.example.com.yml). --- https_port : 8444 - Under the group_vars directory, create a YAML file named after the group to which we want to assign variables (that is, frontends.yml) --- https_port : 8443 lb_vip : lb.example.com - Finally, we will create our inventory file as before, except that it contains no variables. # Final directory structure should look like this \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 frontends.yml \u251c\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 frt01.example.com.yml \u2514\u2500\u2500 inventory Note If you define the same variable at both a group level and a child group level, the variable at the child group level takes precedence. Consider our earlier inventory where we used child groups to differentiate between CentOS and Ubuntu hosts \u2014 if we add a variable with the same name to both the ubuntu child group and the frontends group (which is a child of the ubuntu group) as follows, what will the outcome be? loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [frontends:vars] testvar=childgroup [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends [ubuntu:vars] testvar=group Debugging variable at host level ansible -i hostgroups-children-vars-ini ubuntu -m debug -a \"var=testvar\" # Output frt01.example.com | SUCCESS = > { \"testvar\" : \"childgroup\" } frt02.example.com | SUCCESS = > { \"testvar\" : \"childgroup\" } It's important to note that the frontends group is a child of the ubuntu group in this inventory (hence, the group definition is [ubuntu:children]), and so the variable value we set at the frontends group level wins as this is the child group in this scenario. Special host management using patterns \u00b6 Let's look at how Ansible can work with patterns to figure out which hosts a command (or playbook) should be run against. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends We shall use the \u2013list-hosts switch with the ansible command to see which hosts Ansible would operate on. ansible -i hostgroups-children-ini all --list-hosts # The asterisk character has the same effect as all, but needs to be quoted in single quotes for the shell to interpret the command properly ansible -i hostgroups-children-ini '*' --list-hosts # Use : to specify a logical OR, meaning \"apply to hosts either in this group or that group,\" ansible -i hostgroups-children-ini frontends:apps --list-hosts # Use ! to exclude a specific group\u2014you can combine this with other characters such as : to show all hosts except those in the apps group. # Again, ! is a special character in the shell and so you must quote your pattern string in single quotes for it to work. ansible -i hostgroups-children-ini 'all:!apps' --list-hosts # Use :& to specify a logical AND between two groups, for example, if we want all hosts that are in the centos group and the apps group . ansible -i hostgroups-children-ini 'centos:&apps' --list-hosts # Use * wildcards ansible -i hostgroups-children-ini 'db*.example.com' --list-hosts # Another way you can limit which hosts a command is run on is to use the --limit switch with Ansible. ansible-playbook -i hostgroups-children-ini site.yml --limit frontends:apps WebApp Installation Instructions for Centos 7 \u00b6 Install Python Pip and dependencies on Centos 7 \u00b6 sudo yum install -y epel-release python python-pip sudo pip install flask flask-mysql If you come across a certification validation error while running the above command, please use the below command. sudo pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org flask flask-mysql Install MySQL Server on Centos 7 \u00b6 wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm sudo yum update sudo yum -y install mysql-server sudo service mysql start The complete playbook to get the same workin on CentOS is here: https://github.com/kodekloudhub/simple_web_application Setting up Ansible to run on localhost always \u00b6 Beneficial when testing roles or playbooks in docker images Remember: To set the hosts parameter matches in ansible.cfg This is useful for debugging ansible modules and syntax without having to use VMs or test in dev environments. Install ansible \u00b6 pip install ansible Make some relevant config files \u00b6 ~/.ansible.cfg : [ defaults ] inventory = ~/.ansible-hosts ~/.ansible-hosts : localhost ansible_connection=local Make a test playbook and run \u00b6 helloworld.yml : --- - hosts : all tasks : - shell : echo 'hello world' - run! ansible-playbook helloworld.yml Executing Ansible Playbook \u00b6 Launching Ansible situational commands \u00b6 # To check the inventory file ansible-inventory --list -y # Test Connection ansible all -m ping -u root # Ask for Sudo password ansible all -m ping --ask-pass # Using a specific SSH private key and a user ansible -m ping hosts --private-key = ~/.ssh/keys/id_rsa -u centos # Check the disk usage of all servers ansible all -a \"df -h\" -u root # Check the time of `uptime` each host in a group **servers** ansible servers -a \"uptime\" -u root # Specify multiple hosts by separating their names with colons ansible server1:server2 -m ping -u root # Get system dat in json format of target ansible target1 -i myhosts -m setup --private-key = ~/.ssh/ansible-user -u root # Filter json output ansible target1 -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root ansible all -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root Launching Ansible Playbook situational commands \u00b6 ansible-playbook -i myhosts site.yml # Ask for Sudo password ansible-playbook myplaybook.yml --ask-become-pass # Or use the -K option ansible-playbook -i inventory myplaybook.yml -u sammy -K # Execute a play without making any changes to the remote servers ansible-playbook myplaybook.yml --list-tasks # List all hosts that would be affected by a play ansible-playbook myplaybook.yml --list-hosts ansible-playbook -i myhosts playbooks/atmo_playbook.yml --user atmouser # Passing variables which executing playbooks ansible-playbook playbooks/atmo_playbook.yml -e \"ATMOUSERNAME=atmouser\" ansible host01 -i myhosts -m copy -a \"src=test.txt dest=/tmp/\" ansible host01 -i myhosts -m file -a \"dest=/tmp/test mode=644 state=directory\" ansible host01 -i myhosts -m apt -a \"name=sudo state=latest\" ansible host01 -i myhosts -m shell -a \"echo $TERM \" ansible host01 -i myhosts -m command -a \"mkdir folder1\" # Run playbook on one host ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1\" # Run playbook on multiple hosts ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1,host2\" # Flush Ansible memory f pevious runs ansible-playbook playbooks/PLAYBOOK_NAME.yml --flush-cache # Dry run mode ansible-playbook playbooks/PLAYBOOK_NAME.yml --check # Starts playbook execution from an intermediate task, task name should match ansible-playbook myplaybook.yml --start-at-task = \"Set Up Nginx\" # Increasing debug verbosity ansible-playbook myplaybook.yml -v ansible-playbook myplaybook.yml -vvvv Launching Ansible Vault situational commands \u00b6 # Create new encrypted file, enter password ansible-vault encrypt credentials.yml # View the contents of encrypted file ansible-vault view credentials.yml # Edit the encrypted file ansible-vault edit credentials.yml # Permanently decrypt the file ansible-vault decrypt credentials.yml # Creating multiple vaults per env like dev, prod # create a new vault ID named dev that uses prompt as password source. # Prompt will ask you to enter a password, or a valid path to a password file. ansible-vault create --vault-id dev@prompt credentials_dev.yml ansible-vault create --vault-id prod@prompt credentials_prod.yml # Editing , Decrypting multiple vaults ansible-vault edit credentials_dev.yml --vault-id dev@prompt # Using Password file when using 3rd party automation ansible-vault create --vault-id dev@path/to/passfile credentials_dev.yml # Running playbooks with encrypted password ansible-playbook myplaybook.yml --ask-vault-pass # Passing password file ansible-playbook myplaybook.yml --vault-password-file my_vault_password.py # Passing multi env password ansible-playbook myplaybook.yml --vault-id dev@prompt ansible-playbook myplaybook.yml --vault-id dev@vault_password.py --vault-id test@prompt --vault-id ci@prompt # To change the vault password for key rotation ansible-vault rekey credentials.yml Understanding the playbook framework \u00b6 A playbook allows you to manage multiple configurations and complex deployments on many machines simply and easily. This is one of the key benefits of using Ansible for the delivery of complex applications. With playbooks, you can organize your tasks in a logical structure as tasks are (generally) executed in the order they are written, allowing you to have a good deal of control over your automation processes. # Example inventory [ frontends ] frt01.example.com https_port=8443 frt02.example.com http_proxy=proxy.example.com [frontends:vars] ntp_server=ntp.frt.example.com proxy=proxy.frt.example.com [apps] app01.example.com app02.example.com [webapp:children] frontends apps [webapp:vars] proxy_server=proxy.webapp.example.com health_check_retry=3 health_check_interal=60 Create a simple playbook to run on the hosts in the frontends host group defined in our inventory file. We can set the user that will access the hosts using the remote_user directive in the playbook --- - hosts : frontends remote_user : danieloh tasks : - name : simple connection test ping : remote_user : danieloh The ignore_errors directive to this task to ensure that our playbook doesn't fail if the ls command fails (for example, if the directory we're trying to list doesn't exist). - name : run a simple command shell : /bin/ls -al /nonexistent ignore_errors : True Defining plays and tasks \u00b6 So far when we have worked with playbooks, we have been creating one single play per playbook (which logically is the minimum you can do). However, you can have more than one play in a playbook, and a \"play\" in Ansible terms is simply a set of tasks (and roles, handlers, and other Ansible facets) associated with a host (or group of hosts). A task is the smallest possible element of a play and is responsible for running a single module with a set of arguments to achieve a specific goal. Understanding roles \u00b6 Roles are designed to enable you to efficiently and effectively reuse Ansible code. They always follow a known structure and often will include sensible default values for variables, error handling, handlers, and so on. The process of creating roles is in fact very simple\u2014Ansible will (by default) look within the same directory as you are running your playbook from for a roles/ directory. The role name is derived from the subdirectory name\u2014there is no need to create complex metadata or anything else\u2014it really is that simple. Within each subdirectory goes a fixed directory structure that tells Ansible what the tasks, default variables, handlers, and so on are for each role. The roles/ directory is not the only play Ansible will look for roles\u2014this is the first directory it will look in, but it will then look in /etc/ansible/roles for any additional roles. Setting up role-based variables and dependencies \u00b6 The Ansible role directory structure allows for role-specific variables to be declared in two locations. Although, at first, the difference between these two locations may not seem obvious, it is of fundamental importance. Roles based variables can go in one of two locations: defaults/main.yml vars/main.yml Variables that go in the defaults/ directory are one of the lowest in terms of precedence and so are easily overwritten. This location is where you would put variables that you want to override easily, but where you don't want to leave a variable undefined. For example, if you are installing Apache Tomcat, you might build a role to install a specific version. However, you don't want the role to exit with an error if someone forgets to set the version\u2014rather, you would prefer to set a sensible default such as 7.0.76, which can then be overridden with inventory variables or on the command line (using the -e or \u2013extra-vars switches). In this way, you know the role will work even without someone explicitly setting this variable, but it can easily be changed to a newer Tomcat version if desired. Variables that go in the vars/ directory, however, come much higher up on Ansible's variable precedence ordering. This will not be overridden by inventory variables, and so should be used for variable data that it is more important to keep static. Of course, this is not to say they can't be overridden\u2014the -e or \u2013extra-vars switches are the highest order of precedence in Ansible and so will override anything else that you define. Most of the time, you will probably make use of the defaults/ based variables alone, but there will doubtless be times when having the option of variables higher up the precedence ordering becomes valuable to your automation, and so it is vital to know that this option is available to you. Note I recommend that you make extensive use of the debug statement and test your playbook design to make sure that you don't fall foul of this during your playbook development. Ansible Playbook Examples \u00b6 Install Software only if it doesn't exist - name : installing python2 minimal raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) Install latest software version - hosts : host01 --- become : true tasks : - name : ensure latest sysstat is installed apt : name : sysstat state : latest Install software on all hosts --- - name : install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name : apache2 update_cache : yes state : latest Copy file only when it does not exists --- - hosts : host01 tasks : - stat : path : /home/ubuntu/folder1/something.j2 register : st - name : Template to copy file template : src : ./something.j2 dest : /home/ubuntu/folder1/something.j2 mode : '0644' when : st.stat.exists == False Add users using Loops # Looping - name : add several users user : name : \"{{ item }}\" state : present groups : \"developer\" with_items : - raj - david - john - lauren Using Looping with debug # show all the hosts in the inventory - debug : msg : \"{{ item }}\" with_items : - \"{{ groups['all'] }}\" # show all the hosts in the current play - debug : msg : \"{{ item }}\" with_items : - \"{{ play_hosts }}\" Conditionals # This Playbook will add Java Packages to different systems (handling Ubuntu/Debian OS) - name : debian | ubuntu | add java ppa repo apt_repository : repo=ppa:webupd8team/java state=present become : yes when : ansible_distribution == 'Ubuntu' - name : debian | ensure the webupd8 launchpad apt repository is present apt_repository : repo=\"{{ item }} http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" update_cache=yes state=present with_items : - deb - deb-src become : yes when : ansible_distribution == 'Debian' Full Play --- - name : install software hosts : host01 sudo : yes tasks : - name : Update and upgrade apt packages apt : upgrade : dist update_cache : yes cache_valid_time : 86400 - name : install software apt : name : \"{{item}}\" update_cache : yes state : installed with_items : - nginx - postgresql - postgresql-contrib - libpq-dev - python-psycopg2 - name : Ensure the Nginx service is running service : name : nginx state : started enabled : yes - name : Ensure the PostgreSQL service is running service : name : postgresql state : started enabled : yes #file: vars.yml --- var : 20 #file: playbook.yml --- - hosts : all vars_files : - vars.yml tasks : - debug : msg=\"Variable 'var' is set to {{ var }}\" #host variables - Variables are defined inline for individual host [ group1 ] host1 http_port=80 host2 http_port=303 #group variables - Variables are applied to entire group of hosts [ group1 : vars ] ntp_server= example.com proxy=proxy.example.com Full Play - Deploying an Nginx static site on Ubuntu # playbook.yml --- - hosts : all become : yes vars : server_name : \"{{ ansible_default_ipv4.address }}\" document_root : /var/www app_root : html_demo_site-main tasks : - name : Update apt cache and install Nginx apt : name : nginx state : latest update_cache : yes - name : Copy website files to the server's document root copy : src : \"{{ app_root }}\" dest : \"{{ document_root }}\" mode : preserve - name : Apply Nginx template template : src : files/nginx.conf.j2 dest : /etc/nginx/sites-available/default notify : Restart Nginx - name : Enable new site file : src : /etc/nginx/sites-available/default dest : /etc/nginx/sites-enabled/default state : link notify : Restart Nginx - name : Allow all access to tcp port 80 ufw : rule : allow port : '80' proto : tcp handlers : - name : Restart Nginx service : name : nginx state : restarted # Copy the static files and unzip to folder root curl -L https://github.com/do-community/html_demo_site/archive/refs/heads/main.zip -o html_demo.zip # files/nginx.conf.j2 server { listen 80; root {{ document_root }}/{{ app_root }}; index index.html index.htm; server_name {{ server_name }}; location / { default_type \"text/html\"; try_files $uri.html $uri $uri/ =404; } } # Executing the playbook with sammy user and prompting for password ansible-playbook -i inventory playbook.yml -u sammy -K Using ansible system variables in Jinja2 Templates \u00b6 Whenever you run Playbook, Ansible by default collects information (facts) about each host like host IP address, CPU type, disk space, operating system information etc. ansible host01 -i myhosts -m setup Create Dynamic templates Consider you need the IP address of all the servers in you web group using 'group' variable { % for host in groups.web % } server {{ host.inventory_hostname }} {{ host.ansible_default_ipv4.address }} :8080 { % endfor % } Create a Webservice entry in Nginx { % for host in groups. [ 'jenkins' ] % } define host { use linux-server host_name {{ host }} alias {{ host }} address {{ hostvars [ host ] .ansible_default_ipv4.address }} hostgroups jenkins } { % endfor % } # service checks to be applied to the webserver { % if jenkins_uses_proxy == true % } define service { use local-service hostgroup_name jenkins service_description HTTP check_command check_jenkins_http notifications_enabled 1 } { % endif % } Get a list of all the variables associated with the current host with the help of hostvars and inventory_hostname variables. --- - name : built-in variables hosts : all tasks : - debug : var=hostvars[inventory_hostname] Using register variables # register variable stores the output, after executing command module, in contents variable # stdout is used to access string content of register variable --- - name : check registered variable for emptiness hosts : all tasks : - name : list contents of the directory in the host command : ls /home/ubuntu register : contents - name : check dir is empty debug : msg=\"Directory is empty\" when : contents.stdout == \"\" - name : check dir has contents debug : msg=\"Directory is not empty\" when : contents.stdout != \"\" Variable Precedence => Command Line > Playbook > Facts > Roles CLI: While running the playbook in Command Line redefine the variable # Passing runtime values in plays ansible-playbook -i myhosts test.yml --extra-vars \"ansible_bios_version=Ansible\" Async \u00b6 async - How long to run the task poll - How frequently to check the task status. Default is 10 seconds async_status - Check status of an async task - name : Deploy a mysql DB hosts : db_server roles : - python - mysql_db - name : Deploy a Web Server hosts : web_server roles : - python - flask_web # Below task will run the async in parallel as poll is 0 and register the output - name : Monitor Web Application for 6 Minutes hosts : web_server command : /opt/monitor_webapp.py async : 360 poll : 0 register : webapp_result - name : Monitor Database for 6 Minutes hosts : db_server command : /opt/monitor_database.py async : 360 poll : 0 register : database_result # To avoid job from completing, async_status can be used to poll all async jobs have completed - name : Check status of async task async_status : jid={{ webapp_result.ansible_job_id }} register : job_result until : job_result.finished retries : 30 Deployment Strategy and Forks \u00b6 Serial - Default: All tasks are run after the previous once completes Free: Once the task completes in a host, it continues next execution without waiting for other hosts Batch: Based on serial, but takes action on multiple host (Rolling Updates) Forks: Deployment on multiple servers # Runs playbook on 2 servers at a time - name : Deploy a web application hosts : app_servers serial : 2 vars : db_name : employee_db db_user : db_user db_password : Passw0rd tasks : - name : Install dependencies - name : Install MySQL database - name : Start Mysql Service - name : Create Application Database - name : Create Application DB User - name : Install Python Flask dependencies - name : Copy web-server code - name : Start web-application # Deploy based on random rolling strategy name : Deploy a web application hosts : app_servers serial : - 2 - 3 - 5 # Deploy based on percentage name : Deploy a web application hosts : app_servers serial : \"20%\" # Runs playbook to fail early, suppose there are 10 servers - name : Deploy a web application hosts : app_servers serial : 5 max_fail_percentage : 50 # The number of failed hosts must exceed the value of max_fail_percentage; if it is equal, the play continues. # So, in our example, if exactly 50% of our hosts failed, the play would still continue. # The first task has a special clause under it that we use to deliberately simulate a failure\u2014this line starts with failed_when and we use it to tell the task that if it runs this task on the first tow hosts in the batch, then it should deliberately fail this task regardless of the result; otherwise, it should allow the task to run as normal. tasks : - name : A task that will sometimes fail debug : msg : This might fail failed_when : inventory_hostname in ansible_play_batch[0:3] # We'll add a second task that will always succeed. - name : A task that will succeed debug : msg : Success! - We have also deliberately set up a failure condition that causes three of the hosts in the first batch of 5 (60%) to fail. ansible-playbook -i morehosts maxfail.yml - We deliberately failed three of the first batch of 5, exceeding the threshold for max_fail_percentage that we set. - This immediately causes the play to abort and the second task is not performed on the first batch of 5. - You will also notice that the second batch of 5, out of the 10 hosts, is never processed, so our play was truly aborted. - This is exactly the behavior you would want to see to prevent a failed update from rolling out across a cluster. - Through the careful use of batches and max_fail_percentage, you can safely run automated tasks across an entire cluster without the fear of breaking the entire cluster in the event of an issue. # Deploy based on completion name : Deploy a web application hosts : app_servers strategy : free Error Handling \u00b6 Playbook Error Handling We would like Ansible to stop execution of the entire playbook if a single server was to fail. # To fail playbook on any failure and stop processing on all servers name : Deploy a web application hosts : app_servers any_errors_fatal : true # This will stop all processing # To avoid failure of playbook due to an insignificant task name : Deploy a web application hosts : app_servers tasks : - mail : to : devops@abc.com subject : Server Deployed! body : Webserver is live! ignore_errors : yes # Add this to ignore task failure - command : cat /var/log/server.log register : command_output failed_when : \"'ERROR' in command_output.stdout\" # Conditional failure of task Jinja2 Templating \u00b6 Templating: A process a generating dynamic content or expressions String Manipulation - Filters # Substitution The name is {{ my_name }} # Uppercase The name is {{ my_name | upper }} # Lowercase The name is {{ my_name | lower }} # Titlecase The name is {{ my_name | title }} # Replace The name is {{ my_name | replace(\"Bond\", \"Bourne\") }} # Default value The name is {{ first_name | default(\"James\") }} {{ my_name }} Filters - List and Set # Min {{ [ 1 , 2 , 3 ] | min }} => 1 # Max {{ [1,2,3] | min }} => 3 # Unique {{ [1,2,3,2] | unique }} => 1,2,3 # Union {{ [1,2,3,4] | union([4,5]) }} => 1,2,3,4,5 # Intersect {{ [1,2,3,4] | intersect([4,5]) }} => 4 {{ 100 | random }} => generates random number between 1 to 100 # Join {{ [\"The\",\"name\",\"is\",\"Bond\"] | join(\" \")}} => The name is Bond Filters - File {{ \"/etc/hosts\" | basename }} => hosts Filters - expanduser tasks : - name : Ensure the SSH key is present on OpenStack os_keypair : state : present name : ansible_key public_key_file : \"{{ '~' | expanduser }}/.ssh/id_rsa.pub\" Lookups \u00b6 Lookups : To get data from another source on the system # Credentials File csv Hostname,Password web_server,Passw0rd db_server,Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('csvfile', 'web_server file=/tmp/credentials.csv delimiter=,') }}\" => Passw0rd # Credentials File ini [ web_server ] password = Passw0rd [ db_server ] password = Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('ini', 'password section=web_server file=/tmp/credentials.ini') }}\" => Passw0rd Tags \u00b6 Tags are names pinned on individual tasks, roles or an entire play, that allows you to run or skip parts of your Playbook. Tags can help you while testing certain parts of your Playbook. # tag.yml --- - name : Play1-install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name=apache2 update_cache=yes state=latest - name : displaying \"hello world\" debug : msg=\"hello world\" tags : - tag1 - name : Play2-install nginx hosts : all sudo : yes tags : - tag2 tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : debug module displays message in control machine debug : msg=\"have a good day\" tags : - mymessage - name : shell module displays message in host machine. shell : echo \"yet another task\" tags : - mymessage Executing above play using tags # displays the list of tasks in the Playbook ansible-playbook -i myhosts tag.yml --list-tasks # displays only tags in your Playbook ansible-playbook -i myhosts tag.yml --list-tags # executes only certain tasks which are tagged as tag1 and mymessage ansible-playbook -i myhosts tag.yml --tags \"tag1,mymessage\" Includes (Outdated after 2.0) \u00b6 Ansible gives you the flexibility of organizing your tasks through include keyword, that introduces more abstraction and make your Playbook more easily maintainable, reusable and powerful. --- - name : testing includes hosts : all sudo : yes tasks : - include : apache.yml - include : content.yml - include : create_folder.yml - include : content.yml - include : nginx.yml # apache.yml will not have hosts & tasks but nginx.yml as a separate play will have tasks and can run independently #nginx.yml --- - name : installing nginx hosts : all sudo : yes tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : displaying message debug : msg=\"yayy!! nginx installed\" Roles \u00b6 A Role is completely self contained or encapsulated and completely reusable # cat ansible.cfg [ defaults ] host_key_checking=False inventory = /etc/ansible/myhosts log_path = /home/scrapbook/tutorial/output.txt roles_path = /home/scrapbook/tutorial/roles/ # master-playbook.yml --- - name : my first role in ansible hosts : all sudo : yes roles : - sample_role - sample_role2 # sample_role/tasks/main.yml --- - include : nginx.yml - include : copy-template.yml - include : copy-static.yml # sample_role/tasks/nginx.yml --- - name : Installs nginx apt : pkg=nginx state=installed update_cache=true notify : - start nginx # sample_role/handlers/main.yml --- - name : start nginx service : name=nginx state=started # sample_role/tasks/copy-template.yml --- - name : sample template - x template : src : template-file.j2 dest : /home/ubuntu/copy-template-file.j2 with_items : var_x # sample_role/vars/main.yml var_x : - 'variable x' var_y : - 'variable y' # sample_role/tasks/copy-static.yml # Ensure some-file.txt is present under files folder of the role --- - name : Copy a file copy : src=some-file.txt dest=/home/ubuntu/file1.txt Executing the play # Let us run the master_playbook and check the output: ansible-playbook -i myhosts master_playbook.yml Ansible Galaxy \u00b6 ansible-galaxy is command line tool for scaffolding the creation of directory structure needed for organizing your code ansible-galaxy init sample_role ansible-galaxy useful commands \u00b6 Install a Role from Ansible Galaxy To use others role, visit https://galaxy.ansible.com/ and search for the role that will achieve your goal. Goal: Install Apache in the host machines. # Ensure that roles_path are defined in ansible.cfg for a role to successfully install. # Here, apache is role name and geerlingguy is name of the user in GitHub who created the role. ansible-galaxy install geerlingguy.apache # Forcefully Recreate Role ansible-galaxy init geerlingguy.apache --force # Listing Installed Roles ansible-galaxy list # Remove an Installed Role ansible-galaxy remove geerlingguy.apache Environment Variables \u00b6 Ansible recommends maintaining inventory file for each environment, instead of keeping all your hosts in a single inventory. Each environment directory has one inventory file (hosts) and group_vars directory. Ansible Best Practises \u00b6 Inventory Files \u00b6 To show you a great way of setting up your directory structure for a simple role-based playbook that has two different inventories \u2014 one for a development environment and one for a production environment. # inventories/development/hosts [ app ] app01.dev.example.com app02.dev.example.com # inventories/development/group_vars --- http_port : 8080 # inventories/production/hosts [ app ] app01.prod.example.com app02.prod.example.com # inventories/production/group_vars --- http_port : 80 # To run it on the development inventory ansible-playbook -i inventories/development/hosts site.yml # To run it on the production inventory ansible-playbook -i inventories/production/hosts site.yml However, there are always differences between the two environments, not just in the hostnames, but also sometimes in the parameters, the load balancer names, the port numbers, and so on\u2014the list can seem endless. Try and reuse the same playbooks for all of your environments that run the same code. For example, if you deploy a web app in your development environment, you should be confident that your playbooks will deploy the same app in the production environment This means that not only are you testing your application deployments and code, you are also testing your Ansible playbooks and roles as part of your overall testing process. Your inventories for each environment should be kept in separate directory trees, but all roles, playbooks, plugins, and modules (if used) should be in the same directory structure (this should be the case for both environments). It is normal for different environments to require different authentication credentials; you should keep these separate not only for security but also to ensure that playbooks are not accidentally run in the wrong environment. Your playbooks should be in your version control system, just as your code is. This enables you to track changes over time and ensure that everyone is working from the same copy of the automation code. The proper approach to defining group and host variables \u00b6 First and foremost, you should always pay attention to variable precedence. Host variables are always of a higher order of precedence than group variables; so, you can override any group variable with a host variable. This behavior is useful if you take advantage of it in a controlled manner, but can yield unexpected results if you are not aware of it. There is a special group variables definition called all, which is applied to all inventory groups. This has a lower order of precedence than specifically defined group variables. What happens if you define the same variable twice in two groups? If this happens, both groups have the same order of precedence, so which one wins? [ app ] app01.dev.example.com app02.dev.example.com # inventories/development/group_vars/all.yml --- http_port : 8080 # inventories/development/group_vars/app.yml --- http_port : 8081 # site.yml --- - name : Play using best practise directory structure hosts : all tasks : - name : Display the value of our inventory variable debug : var : http_port ansible-playbook -i inventories/development/hosts site.yml As expected, the variable definition in the specific group won, which is in line with the order of precedence documented for Ansible. Now, let's see what happens if we define the same variable twice in two specifically named groups. To complete this example, we'll create a child group, called centos, and another group that could notionally contain hosts built to a new build standard, called newcentos, which both application servers will be a member of. [ app ] app01.dev.example.com app02.dev.example.com [centos:children] app [newcentos:children] app # inventories/development/group_vars/centos.yml --- http_port : 8082 # inventories/development/group_vars/newcentos.yml --- http_port : 8083 We've now defined the same variable four times at the group level! ansible-playbook -i inventories/development/hosts site.yml The value we entered in newcentos.yml won\u2014but why? The Ansible documentation states that where identical variables are defined at the group level in the inventory (the one place you can do this), the one from the last-loaded group wins. Groups are processed in alphabetical order and newcentos is the group with the name beginning furthest down the alphabet\u2014so, its value of http_port was the value that won. Just for completeness, we can override all of this by leaving the group_vars directory untouched, but adding a file called inventories/development/host_vars/app01.dev.example.com.yml --- http_port : 9090 We will see that the value we defined at the host level completely overrides any value that we set at the group level for app01.dev.example.com. app02.dev.example.com is unaffected as we did not define a host variable for it, so the next highest level of precedence\u2014the group variable from the newcentos group\u2014won Using top-level playbooks \u00b6 Imagine handing a playbook directory structure with 100 different playbooks to a new system administrator\u2014how would they know which ones to run and in which circumstances? The task of training someone to use the playbooks would be immense and would simply move complexity from one area to another. The most important thing is that, on receipt of a new playbook directory structure, a new operator at least knows what the starting point for both running the playbooks, and understanding the code is. If the top-level playbook they encounter is always site.yml, then at least everyone knows where to start. Through the clever use of roles and the import_* and include_* statements, you can split your playbook up into logical portions of reusable code, all from one playbook file. Leveraging version control tools \u00b6 Any changes to your Ansible code could mean big changes to your environment, and possibly even whether an important production service works or not. As a result, it is vital that you maintain a version history of your Ansible code and that everyone works from the same version. Setting OS and distribution variances \u00b6 This playbook demonstrates how you can group differing plays using an Ansible fact so that the OS distribution determines which play in a playbook gets run. # osvariants.yml - It will also contain a single task. --- - name : Play to demonstrate group_by module hosts : all tasks : - name : Create inventory groups based on host facts group_by : key : os_{{ ansible_facts['distribution'] }} group_by module: It dynamically creates new inventory groups based on the key that we specify \u2014 in this example, we are creating groups based on a key comprised of the os_ fixed string, followed by the OS distribution fact obtained from the Gathering Facts stage. The original inventory group structure is preserved and unmodified, but all the hosts are also added to the newly created groups according to their facts. So, the two servers in our simple inventory remain in the app group, but if they are based on Ubuntu, they will be added to a newly created inventory group called os_Ubuntu. Similarly, if they are based on CentOS, they will be added to a group called os_CentOS. # Play definition to the same playbook file to install Apache on CentOS - name : Play to install Apache on CentOS hosts : os_CentOS # Refer to the Dynamic group become : true tasks : - name : Install Apache on CentOS yum : name : httpd state : present # Add a third Play definition, this time for installing the apache2 package on Ubuntu using the apt module - name : Play to install Apache on Ubuntu hosts : os_Ubuntu become : true tasks : - name : Install Apache on Ubuntu apt : name : apache2 state : present ansible-playbook -i hosts osvariants.yml Notice how the task to install Apache on CentOS was run. It was run this way because the group_by module created a group called os_CentOS and our second play only runs on hosts in the group called os_CentOS. As there were no servers running on Ubuntu in the inventory, the os_Ubuntu group was never created and so the third play does not run. We receive a warning about the fact that there is no host pattern that matches os_Ubuntu, but the playbook does not fail\u2014it simply skips this play. It is up to you to choose the coding style most appropriate to you. You can make use of the group_by module, as detailed here, or write your tasks in blocks and add a when clause to the blocks so that they only run when a certain fact-based condition is met (for example, the OS distribution is CentOS)\u2014or perhaps even a combination of the two. The choice is ultimately yours and these different examples are provided to empower you with multiple options that you can choose between to create the best possible solution for your scenario. Setting task execution delegation \u00b6 We have assumed that all the tasks are executed on each host in the inventory in turn. However, what if you need to run one or two tasks on a different host? For example, we have talked about the concept of automating upgrades on clusters. Logically, however, we would want to automate the entire process, including the removal of each host in turn from the load balancer and its return after the task is completed. Although we still want to run our play across our entire inventory, we certainly don't want to run the load balancer commands from those hosts. Imagine that you have a shell script (or other executables) that you can call that can add and remove hosts to and from a load balancer. # remove_from_loadbalancer.sh #!/bin/sh echo Removing $1 from load balancer... # add_to_loadbalancer.sh #!/bin/sh echo Adding $1 to load balancer... [ frontends ] frt01.example.com frt02.example.com --- - name : Play to demonstrate task delegation hosts : frontends tasks : - name : Remove host from the load balancer command : ./remove_from_loadbalancer.sh {{ inventory_hostname }} args : chdir : \"{{ playbook_dir }}\" delegate_to : localhost - name : Deploy code to host debug : msg : Deployment code would go here.... - name : Add host back to the load balancer command : ./add_to_loadbalancer.sh {{ inventory_hostname }} args : chdir : \"{{ playbook_dir }}\" delegate_to : localhost We are using the command module to call the script we created earlier, passing the hostname from the inventory being removed from the load balancer to the script. We use the chdir argument with the playbook_dir magic variable to tell Ansible that the script is to be run from the same directory as the playbook. The special part of this task is the delegate_to directive, which tells Ansible that even though we're iterating through an inventory that doesn't contain localhost, we should run this action on localhost (we aren't copying the script to our remote hosts, so it won't run if we attempt to run it from there). Deploy task has no delegate_to directive, and so it is actually run on the remote host from the inventory (as desired): Finally, we add the host back to the load balancer using the second script we created earlier. This task is almost identical to the first. ansible-playbook -i hosts delegate.yml Notice how even though Ansible is working through the inventory (which doesn't feature localhost), the load balancer-related scripts are actually run from localhost, while the upgrade task is performed directly on the remote host. In truth, you can delegate any task to localhost, or even another non-inventory host. You could, for example, run an rsync command delegated to localhost to copy files to remote hosts using a similar task definition to the previous one. This is useful because although Ansible has a copy module, it can't perform the advanced recursive copy and update functions that rsync is capable of. Note that you can choose to use a form of shorthand notation in your playbooks (and roles) for delegate_to, called local_action . This allows you to specify a task on a single line that would ordinarily be run with delegate_to: localhost added below it. --- - name : Second task delegation example hosts : frontends tasks : - name : Perform an rsync from localhost to inventory hosts local_action : command rsync -a /tmp/ {{ inventory_hostname }}:/tmp/target/ The preceding shorthand notation is equivalent to the following: tasks : - name : Perform an rsync from localhost to inventory hosts command : rsync -a /tmp/ {{ inventory_hostname }}:/tmp/target/ delegate_to : localhost If we run this playbook, we can see that local_action does indeed run rsync from localhost, enabling us to efficiently copy whole directory trees across to remote servers in the inventory. ansible-playbook -i hosts delegate2.yml Using the run_once option \u00b6 When working with clusters, you will sometimes encounter a task that should only be executed once for the entire cluster. For example, you might want to upgrade the schema of a clustered database. Instead, you can write your code as you normally would, but make use of the special run_once directive for any tasks you want to run only once on your inventory. For example, let's reuse the 10-host inventory. --- - name : Play to demonstrate the run_once directive hosts : frontends tasks : - name : Upgrade database schema debug : msg : Upgrading database schema... run_once : true ansible-playbook -i morehosts runonce.yml Notice that, just as desired, although the playbook was run on all 10 hosts (and, indeed, gathered facts from all 10 hosts), we only ran the upgrade task on one host. It's important to note that the run_once option applies per batch of servers, so if we add serial: 5 to our play definition (running our play in two batches of 5 on our inventory of 10 servers), the schema upgrade task actually runs twice! It runs once as requested, but once per batch of servers, not once for the entire inventory. Be careful of this nuance when working with this directive in a clustered environment. Running playbooks locally \u00b6 It is important to note that when we talk about running a playbook locally with Ansible, it is not the same as talking about running it on localhost. If we run a playbook on localhost, Ansible actually sets up an SSH connection to localhost (it doesn't differentiate its behavior or attempt to detect whether a host in the inventory is local or remote - it simply tries faithfully to connect). # Inventory file [ local ] localhost ansible_connection=local We've added a special variable to our localhost entry ansible_connection variable which defines which protocol is used to connect to this inventory host. So, we have told it to use a direct local connection instead of an SSH-based connectivity (which is the default). Note that this special value for the ansible_connection variable actually overrides the hostname you have put in your inventory. So, if we change our inventory to look as follows, Ansible will not even attempt to connect to the remote host called frt01.example.com it will connect locally to the machine running the playbook (without SSH). [ local ] frt01.example.com ansible_connection=local The presence of ansible_connection=local meant that this command was run on the local machine without using SSH. This ability to run commands locally without the need to set up SSH connectivity, SSH keys, and so on can be incredibly valuable, especially if you need to get things up and running quickly on your local machine. Working with proxies and jump hosts \u00b6 Often, when it comes to configuring core network devices, these are isolated from the main network via a proxy or jump host. Ansible lends itself well to automating network device configuration as most of it is performed over SSH: however, this is only helpful in a scenario where Ansible can either be installed and operated from the jump host or, better yet, can operate via a host such as this. Let's assume that you have two Cumulus Networks switches in your network (these are based on a special distribution of Linux for switching hardware, which is very similar to Debian). These two switches have the cmls01.example.com and cmls02.example.com hostnames, but both can only be accessed from a host called bastion.example.com. [ switches ] cmls01.example.com cmls02.example.com [switches:vars] ansible_ssh_common_args='-o ProxyCommand=\"ssh -W %h:%p -q bastion.example.com\"' This special variable content ansible_ssh_common_args tells Ansible to add extra options when it sets up an SSH connection, including to proxy via the bastion.example.com host. The -W %h:%p options tell SSH to proxy the connection and to connect to the host specified by %h (this is either cmls01.example.com or cmls02.example.com) on the port specified by %p (usually port 22). ansible -i switches -m ping all On the surface, Ansible works just as it normally does and connects successfully to the two hosts. However, behind the scenes it proxies via bastion.example.com. Note that this simple example assumes that you are connecting to both the bastion host and switches using the same username and SSH credentials (or in this case, keys). Configuring playbook prompts \u00b6 All of our playbooks have had their data specified for them at run time in variables we defined within the playbook. However, what if you actually want to obtain information from someone during a playbook run? Perhaps you want to obtain a password from a user for an authentication task without storing it anywhere. Ansible can prompt you for user input and store the input in a variable for future processing. We will prompt for two variables, one for a user ID and one for a password. One will be echoed to the screen, while the other won't be, by setting private: yes --- - name : A simple play to demonstrate prompting in a playbook hosts : frontends vars_prompt : - name : loginid prompt : \"Enter your username\" private : no - name : password prompt : \"Enter your password\" private : yes tasks : - name : Proceed with login debug : msg : \"Logging in as {{ loginid }}...\" ansible-playbook -i hosts prompt.yml Ansible Security Best Practices \u00b6 Working with Ansible Vault \u00b6 It's really important to use Ansible Vault to store all the secret information in our playbooks. Some of the really good use cases include how we can use these playbooks without changing our version control systems, CI/CD integration pipelines, and so on. How to use Ansible Vault with variables and files \u00b6 Let's take an example of installing MySQL server in an Ubuntu operating system using the following playbook. As per the Ansible documentation, it's easy and better to store Vault variables and normal variables differently. \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 mysql.yml # contains vault secret values \u251c\u2500\u2500 hosts \u251c\u2500\u2500 main.yml \u2514\u2500\u2500 roles \u2514\u2500\u2500 mysqlsetup \u2514\u2500\u2500 tasks \u2514\u2500\u2500 main.yml Now, if we see the group_vars/main.yml file, the content looks as shown in the codeblock. It contains the secrets variable to use in the playbook, called mysql_root_password. mysql_root_password : supersecretpassword\u200b To encrypt the vault file, we will use the following command and it then prompts for the password to protect ansible-vault encrypt group_vars/mysql.yml # Now, to execute the playbook run the following command, it will prompt for the vault password ansible-playbook --ask-vault-pass -i hosts main.yml We can also pass the ansible-vault password file with playbook execution by specifying flag, it helps in our continuous integration and pipeline platforms. The following file contains the password which used to encrypt the mysql.yml file. cat ~/.vaultpassword thisisvaultpassword # To pass the vault password file through the command line, use the following command when executing playbooks ansible-playbook --vault-password-file ~/.vaultpassword -i hosts main.yml Note Make sure to give proper permissions for this file, so others cannot access this file using chmod. Also, it's good practice to add this file to your .gitignore, so it will not be version controlled when pushing playbooks. Vault password file can be an executable script, which can retrieve data stored somewhere securely rather than having to keep the key in plain text on disk and relying on file permissions to keep it safe. We can also use system environment variables such as ANSIBLE_VAULT_PASSWORD_FILE=~/.vaultpassword and Ansible will use this while executing playbooks. Ansible Vault single encrypted variable \u00b6 It allows us to use vaulted variables with the !vault tag in YAML files This playbook is used to perform reverse IP lookups using the ViewDNS API. We want to secure api_key as it contains sensitive information. # We use the ansible-vault encrypt_string command to perform this encryption. # Here, we used echo with the -n flag to remove the new line echo -n '53ff4ad63849e6977cb652763g7b7c64e2fa42a' | ansible-vault encrypt_string --stdin-name 'api_key' We can place the variable, inside the playbook variables and execute the playbook as normal, using ansible-playbook with the \u2013ask-vault-pass option. - name : ViewDNS domain information hosts : localhost vars : domain : google.com api_key : !vault | $ANSIBLE_VAULT;1.1;AES256 36623761316238613461326466326162373764353437393733343334376161336630333532626465 6662383435303930303164353664643639303761353664330a393365633237306530653963353764 64626237313738656530373639653739656564316161663831653431623832336635393637653330 6632663563363264340a323537356166653338396135376161323435393730306133626635376539 37383861653239326336613837666237636463396465393662666561393132343166666334653465 6265386136386132363534336532623061646438363235383334 output_type : json tasks : - name : \"getting {{ domain }} server info\" uri : url : \"https://api.viewdns.info/reverseip/?host={{ domain }}&apikey={{ api_key }}&output={{ output_type }}\" method : GET register : results - debug : msg : \"{{ results.json }}\" Playbook being executed will be automatically decrypted after we provide it with the given password. ansible-playbook --ask-vault-pass -i hosts main.yml Setting up and using Ansible Galaxy \u00b6 Is an official centralized hub for finding, sharing, and reusing Ansible roles. This allows the community to share and collaborate on Ansible playbooks, and allows new users to quickly get started with using Ansible. To share our custom-written roles with the community, we can publish them to Ansible Galaxy using GitHub authentication. We can install or include roles direct from GitHub by specifying the GitHub URL. This allows the use of private version control systems as local inventories of playbook roles. ansible-galaxy install git+https://github.com/geerlingguy/ansible-role-composer.git Ansible controller machine security \u00b6 The controller machine for Ansible requires SSH and Python to be installed and configured. Ansible has a very low attack surface. Note In January 2017, multiple security issues were found by a company called Computest . This vulnerability was dubbed owning the farm, since compromising the controller would imply that all the nodes could potentially be compromised. The controller machine should be a hardened server and treated with all the seriousness that it deserves. In the vulnerability that was disclosed, if a node gets compromised attackers could leverage that to attack and gain access to the controller. - Once they have access, the could extend their control over all the other nodes being managed by the controller. Since the attack surface is already very limited, the best we can do is ensure that the server stays secure and hardened. Explanation of Ansible OS hardening playbook \u00b6 The following playbook is created by DevSec for Linux baselines. It covers most of the required hardening checks based on multiple standards, which includes Ubuntu Security Features, NSA Guide to Secure Configuration, ArchLinux System Hardening and other. This can be improved if required by adding more tasks (or) roles. Ansible OS Hardening Playbook covers Configures package management, that is, allows only signed packages Removes packages with known issues Configures pam and the pam_limits module Shadow password suite configuration Configures system path permissions Disables core dumps through soft limits Restricts root logins to system console Sets SUIDs Configures kernel parameters through sysctl # download the os-hardening role from Ansible Galaxy ansible-galaxy install dev-sec.os-hardening Call that role in your playbook and execute it to perform the baseline hardening, and also change the variables as required. Refer to https://galaxy.ansible.com/dev-sec/os-hardening for more detailed options. - hosts : localhost become : yes roles : - dev-sec.os-hardening # Execute the playbook ansible-playbook main.yml Best practices and reference playbook projects \u00b6 Projects such as Algo, DebOps, and OpenStack are large Ansible playbook projects that are well maintained and secure by default. DebOps \u2013 your Debian-based data center in a box \u00b6 DebOps is a project created by Maciej Delmanowski. It contains a collection of various Ansible playbooks that can be used for Debian and Ubuntu hosts. This project has more than 128 Ansible roles, which are customized for production use cases and work with multiple environments. We can see a list of available playbook services at debops/debops-playbooks There are two different ways we can quickly get started with a DebOps setup: Vagrant setup Docker setup Algo \u2013 set up a personal IPSEC VPN in the cloud \u00b6 Algo from Trail of Bits provides Ansible roles and scripts to automate the installation of a personal IPSEC VPN. By running the Ansible playbooks, you get a complete hardened VPN server, and deployments to all major cloud providers are already configured ( https://github.com/trailofbits/algo/blob/master/docs/deploy-from-ansible.md ). OpenStack-Ansible \u00b6 Not only does this project use Ansible playbooks extensively, but their security documentation is also worth reading and emulating. The best part is that all of the security configuration is declarative security codified in Ansible playbooks. Documentation on this project is available at https://docs.openstack.org/project-deploy-guide/openstack-ansible/ocata/app-security.html . AWX \u2013 open source version of Ansible Tower \u00b6 AWX provides a web-based user interface, REST API, and task engine built on top of Ansible. AWX can be used with the tower-CLI tool and client library. Get started with AWX here: ansible/awx . Get started with tower-cli here: ansible/tower-cli .","title":"Ansible"},{"location":"learning/ansible/ansible/#introduction","text":"Note The rule of thumb is if you can script it, you can create a playbook for it. Ansible 101 Ansible Cheat Sheet App Security Ansible Tips and Tricks DO Practise examples & Explaination & Tutorials YAML Spec Ref Card Full Application on Cloud Maintaining Playbooks - Pitfalls","title":"Introduction"},{"location":"learning/ansible/ansible/#documentation-on-cmdline","text":"# System outputs the man page for debug module ansible-doc debug ansible-doc -l | grep aws # List all plugins and then grep for aws plugins ansible-doc -s shell # Shows snippets on how to use the plugins","title":"Documentation on cmdline"},{"location":"learning/ansible/ansible/#setup-server","text":"","title":"Setup Server"},{"location":"learning/ansible/ansible/#default-configuration","text":"ansible.cfg and hosts files are present inside /etc/ansible Testing ansible on Ubuntu WSL ansible localhost -m ping","title":"Default Configuration"},{"location":"learning/ansible/ansible/#enabling-ssh-on-the-vm","text":"If you need SSH enabled on the system, follow the below steps: Ensure the /etc/apt/sources.list file has been updated as per above Run the command: apt-get update Run the command: apt-get install openssh-server Run the command: service sshd start ssh-keygen -t rsa -C \"ansible\" #OR # Generate an SSH key pair for future connections to the VM instances (run the command exactly as it is): ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user -P \"\" #Add the SSH private key to the ssh-agent: ssh-add ~/.ssh/ansible-user #Verify that the key was added to the ssh-agent: $ ssh-add -l","title":"Enabling SSH on the VM"},{"location":"learning/ansible/ansible/#access-vm-over-ssh","text":"ssh vagrant@127.0.0.1 -p 2222 -i ~/.ssh/insecure_private_key","title":"Access VM over SSH"},{"location":"learning/ansible/ansible/#copy-files-recursively-from-local-desktop-to-remote-server","text":"scp -r ./scripts vagrant@127.0.0.1:/home/vagrant -p 2222 -i ~/.ssh/insecure_private_key","title":"Copy files recursively from local desktop to remote server"},{"location":"learning/ansible/ansible/#target-docker-containers-for-ansible-controller","text":"The Docker file used to create the ubuntu-ssh-enabled Docker image is located here.","title":"Target Docker containers for Ansible controller"},{"location":"learning/ansible/ansible/#issues-installing-ansible-and-its-dependencies","text":"Once the Debian VM is up and running make the following changes to the /etc/apt/sources.list file to get the Ansible installation working right. deb http://security.debian.org/ jessie/updates main contrib deb-src http://security.debian.org/ jessie/updates main contrib deb http://ftp.debian.org/debian/ jessie-updates main contrib deb-src http://ftp.debian.org/debian/ jessie-updates main contrib deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main deb http://ftp.de.debian.org/debian sid main","title":"Issues installing Ansible and its dependencies"},{"location":"learning/ansible/ansible/#ansible-directory-structure-as-per-best-practises","text":"This is the directory layout of this repository with explanation. production.ini # inventory file for production stage development.ini # inventory file for development stage test.ini # inventory file for test stage vpass # ansible-vault password file # This file should not be committed into the repository # therefore file is ignored by git ########################## This segration can be done if there are changes in variable values between environments \u251c\u2500\u2500 inventories \u2502 \u251c\u2500\u2500 development \u2502 \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u2502 \u2514\u2500\u2500 app.yml \u2502 \u2502 \u251c\u2500\u2500 hosts \u2502 \u2502 \u2514\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 production \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u2514\u2500\u2500 app.yml \u2502 \u251c\u2500\u2500 hosts \u2502 \u2514\u2500\u2500 host_vars ########################## group_vars/ all/ # variables under this directory belongs all the groups apt.yml # ansible-apt role variable file for all groups webservers/ # here we assign variables to webservers groups apt.yml # Each file will correspond to a role i.e. apt.yml nginx.yml # \"\" postgresql/ # here we assign variables to postgresql groups postgresql.yml # Each file will correspond to a role i.e. postgresql postgresql-password.yml # Encrypted password file plays/ ansible.cfg # Ansible.cfg file that holds all ansible config webservers.yml # playbook for webserver tier postgresql.yml # playbook for postgresql tier roles/ roles_requirements.yml# All the information about the roles external/ # All the roles that are in git or ansible galaxy # Roles that are in roles_requirements.yml file will be downloaded into this directory internal/ # All the roles that are not public common/ # common role tasks/ # main.yml # installing basic tasks scripts/ setup/ # All the setup files for updating roles and ansible dependencies","title":"Ansible Directory Structure as per Best Practises"},{"location":"learning/ansible/ansible/#ansible-inventory","text":"","title":"Ansible Inventory"},{"location":"learning/ansible/ansible/#creating-an-inventory-file-and-adding-hosts","text":"Ansible supports two types of inventory\u2014static and dynamic Static inventories are by their very nature static; they are unchanging unless a human being goes and manually edits them. Even in small, closed environments, static inventories are a great way to manage your environment, especially when changes to the infrastructure are infrequent. # Sample inventory file in INI format target1.example.com ansible_host=192.168.81.142 ansible_port=3333 target2.example.com ansible_port=3333 ansible_user=danieloh target3.example.com ansible_host=192.168.81.143 ansible_port=5555 ansible_host: If the inventory hostname cannot be accessed directly\u2014perhaps because it is not in DNS, for example, this variable contains the hostname or IP address that Ansible will connect to instead. ansible_port: By default, Ansible attempts all communication over port 22 for SSH\u2014if you have an SSH daemon running on another port, you can tell Ansible about it using this variable. ansible_user: By default, Ansible will attempt to connect to the remote host using the current user account you are running the Ansible command from\u2014you can override this in several ways, of which this is one. Hence, the preceding three hosts can be summarized as follows: The target1.example.com host should be connected to using the 192.168.81.142 IP address, on port 3333. The target2.example.com host should be connected to on port 3333 also, but this time using the danieloh user rather than the account running the Ansible command. The target3.example.com host should be connected to using the 192.168.81.143 IP address, on port 5555.","title":"Creating an inventory file and adding hosts"},{"location":"learning/ansible/ansible/#using-host-groups","text":"Let's assume you have a simple three-tier web architecture, with multiple hosts in each tier for high availability and/or load balancing. The three tiers in this architecture might be the following: Frontend servers Application servers Database servers To keep the examples clear and concise, we'll assume that you can access all servers using their Fully Qualified Domain Names (FQDNs) , and hence won't add any host variables into these inventory files. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com We have created three groups called frontends, apps, and databases. Note that, in INI-formatted inventories, group names go inside square braces. Under each group name goes the server names that belong in each group, so the preceding example shows two servers in each group. Notice the outlier at the top, loadbalancer.example.com \u2014 this host isn't in any group. All ungrouped hosts must go at the very top of an INI-formatted file. The preceding inventory stands in its own right, but what if our frontend servers are built on Ubuntu, and the app and database servers are built on CentOS? There will be some fundamental differences in the ways we handle these hosts \u2014 for example, we might use the apt module to manage packages on Ubuntu and the yum module on CentOS. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends With the use of the children keyword in the group definition (inside the square braces), we can create groups of groups; hence, we can perform clever groupings to help our playbook design without having to specify each host more than once. ansible -i hostgroups-yml centos -m shell -a 'echo hello-yaml' -f 5 This is a powerful way of managing your inventory and making it easy to run commands on just the hosts you want to. The possibility of creating multiple groups makes life simple and easy, especially when you want to run different tasks on different groups of servers. Let's assume you have 100 app servers, all named sequentially, as follows: app01 to app100 [ apps ] app[01:100].prod.com The following inventory snippet actually produces an inventory with the same 100 app servers that we could create manually.","title":"Using host groups"},{"location":"learning/ansible/ansible/#adding-host-and-group-variables-to-your-inventory","text":"-Suppose that we need to set two variables for each of our two frontend servers. These are not special Ansible variables, but instead are variables entirely of our own choosing. - https_port, which defines the port that the frontend proxy should listen on - lb_vip, which defines the FQDN of the load-balancer in front of the frontend servers - You can assign variables to a host group as well as to hosts individually. [ frontends ] frt01.example.com frt02.example.com [frontends:vars] https_port=8443 lb_vip=lb.example.com - There will be times when you want to work with host variables for individual hosts, and times when group variables are more relevant. - It is also worth noting that host variables override group variables , so if we need to change the connection port to 8444 on the frt01.example.com one, we could do this as follows [ frontends ] frt01.example.com https_port=8444 frt02.example.com [frontends:vars] https_port=8443 lb_vip=lb.example.com - Right now, our examples are small and compact and only contain a handful of groups and variables; however, when you scale this up to a full infrastructure of servers, using a single flat inventory file could, once again, become unmanageable. - Luckily, Ansible also provides a solution to this. Two specially-named directories, host_vars and group_vars , are automatically searched for appropriate variable content if they exist within the playbook directory. - Under the host_vars directory, we'll create a file with the name of our host that needs the proxy setting, with .yml appended to it (that is, frt01.example.com.yml). --- https_port : 8444 - Under the group_vars directory, create a YAML file named after the group to which we want to assign variables (that is, frontends.yml) --- https_port : 8443 lb_vip : lb.example.com - Finally, we will create our inventory file as before, except that it contains no variables. # Final directory structure should look like this \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 frontends.yml \u251c\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 frt01.example.com.yml \u2514\u2500\u2500 inventory Note If you define the same variable at both a group level and a child group level, the variable at the child group level takes precedence. Consider our earlier inventory where we used child groups to differentiate between CentOS and Ubuntu hosts \u2014 if we add a variable with the same name to both the ubuntu child group and the frontends group (which is a child of the ubuntu group) as follows, what will the outcome be? loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [frontends:vars] testvar=childgroup [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends [ubuntu:vars] testvar=group Debugging variable at host level ansible -i hostgroups-children-vars-ini ubuntu -m debug -a \"var=testvar\" # Output frt01.example.com | SUCCESS = > { \"testvar\" : \"childgroup\" } frt02.example.com | SUCCESS = > { \"testvar\" : \"childgroup\" } It's important to note that the frontends group is a child of the ubuntu group in this inventory (hence, the group definition is [ubuntu:children]), and so the variable value we set at the frontends group level wins as this is the child group in this scenario.","title":"Adding host and group variables to your inventory"},{"location":"learning/ansible/ansible/#special-host-management-using-patterns","text":"Let's look at how Ansible can work with patterns to figure out which hosts a command (or playbook) should be run against. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends We shall use the \u2013list-hosts switch with the ansible command to see which hosts Ansible would operate on. ansible -i hostgroups-children-ini all --list-hosts # The asterisk character has the same effect as all, but needs to be quoted in single quotes for the shell to interpret the command properly ansible -i hostgroups-children-ini '*' --list-hosts # Use : to specify a logical OR, meaning \"apply to hosts either in this group or that group,\" ansible -i hostgroups-children-ini frontends:apps --list-hosts # Use ! to exclude a specific group\u2014you can combine this with other characters such as : to show all hosts except those in the apps group. # Again, ! is a special character in the shell and so you must quote your pattern string in single quotes for it to work. ansible -i hostgroups-children-ini 'all:!apps' --list-hosts # Use :& to specify a logical AND between two groups, for example, if we want all hosts that are in the centos group and the apps group . ansible -i hostgroups-children-ini 'centos:&apps' --list-hosts # Use * wildcards ansible -i hostgroups-children-ini 'db*.example.com' --list-hosts # Another way you can limit which hosts a command is run on is to use the --limit switch with Ansible. ansible-playbook -i hostgroups-children-ini site.yml --limit frontends:apps","title":"Special host management using patterns"},{"location":"learning/ansible/ansible/#webapp--installation-instructions-for-centos-7","text":"","title":"WebApp  Installation Instructions for Centos 7"},{"location":"learning/ansible/ansible/#install-python-pip-and-dependencies-on-centos-7","text":"sudo yum install -y epel-release python python-pip sudo pip install flask flask-mysql If you come across a certification validation error while running the above command, please use the below command. sudo pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org flask flask-mysql","title":"Install Python Pip and dependencies on Centos 7"},{"location":"learning/ansible/ansible/#install-mysql-server-on-centos-7","text":"wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm sudo yum update sudo yum -y install mysql-server sudo service mysql start The complete playbook to get the same workin on CentOS is here: https://github.com/kodekloudhub/simple_web_application","title":"Install MySQL Server on Centos 7"},{"location":"learning/ansible/ansible/#setting-up-ansible-to-run-on-localhost-always","text":"Beneficial when testing roles or playbooks in docker images Remember: To set the hosts parameter matches in ansible.cfg This is useful for debugging ansible modules and syntax without having to use VMs or test in dev environments.","title":"Setting up Ansible to run on localhost always"},{"location":"learning/ansible/ansible/#install-ansible","text":"pip install ansible","title":"Install ansible"},{"location":"learning/ansible/ansible/#make-some-relevant-config-files","text":"~/.ansible.cfg : [ defaults ] inventory = ~/.ansible-hosts ~/.ansible-hosts : localhost ansible_connection=local","title":"Make some relevant config files"},{"location":"learning/ansible/ansible/#make-a-test-playbook-and-run","text":"helloworld.yml : --- - hosts : all tasks : - shell : echo 'hello world' - run! ansible-playbook helloworld.yml","title":"Make a test playbook and run"},{"location":"learning/ansible/ansible/#executing-ansible-playbook","text":"","title":"Executing Ansible Playbook"},{"location":"learning/ansible/ansible/#launching-ansible-situational-commands","text":"# To check the inventory file ansible-inventory --list -y # Test Connection ansible all -m ping -u root # Ask for Sudo password ansible all -m ping --ask-pass # Using a specific SSH private key and a user ansible -m ping hosts --private-key = ~/.ssh/keys/id_rsa -u centos # Check the disk usage of all servers ansible all -a \"df -h\" -u root # Check the time of `uptime` each host in a group **servers** ansible servers -a \"uptime\" -u root # Specify multiple hosts by separating their names with colons ansible server1:server2 -m ping -u root # Get system dat in json format of target ansible target1 -i myhosts -m setup --private-key = ~/.ssh/ansible-user -u root # Filter json output ansible target1 -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root ansible all -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root","title":"Launching Ansible situational commands"},{"location":"learning/ansible/ansible/#launching-ansible-playbook-situational-commands","text":"ansible-playbook -i myhosts site.yml # Ask for Sudo password ansible-playbook myplaybook.yml --ask-become-pass # Or use the -K option ansible-playbook -i inventory myplaybook.yml -u sammy -K # Execute a play without making any changes to the remote servers ansible-playbook myplaybook.yml --list-tasks # List all hosts that would be affected by a play ansible-playbook myplaybook.yml --list-hosts ansible-playbook -i myhosts playbooks/atmo_playbook.yml --user atmouser # Passing variables which executing playbooks ansible-playbook playbooks/atmo_playbook.yml -e \"ATMOUSERNAME=atmouser\" ansible host01 -i myhosts -m copy -a \"src=test.txt dest=/tmp/\" ansible host01 -i myhosts -m file -a \"dest=/tmp/test mode=644 state=directory\" ansible host01 -i myhosts -m apt -a \"name=sudo state=latest\" ansible host01 -i myhosts -m shell -a \"echo $TERM \" ansible host01 -i myhosts -m command -a \"mkdir folder1\" # Run playbook on one host ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1\" # Run playbook on multiple hosts ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1,host2\" # Flush Ansible memory f pevious runs ansible-playbook playbooks/PLAYBOOK_NAME.yml --flush-cache # Dry run mode ansible-playbook playbooks/PLAYBOOK_NAME.yml --check # Starts playbook execution from an intermediate task, task name should match ansible-playbook myplaybook.yml --start-at-task = \"Set Up Nginx\" # Increasing debug verbosity ansible-playbook myplaybook.yml -v ansible-playbook myplaybook.yml -vvvv","title":"Launching Ansible Playbook situational commands"},{"location":"learning/ansible/ansible/#launching-ansible-vault-situational-commands","text":"# Create new encrypted file, enter password ansible-vault encrypt credentials.yml # View the contents of encrypted file ansible-vault view credentials.yml # Edit the encrypted file ansible-vault edit credentials.yml # Permanently decrypt the file ansible-vault decrypt credentials.yml # Creating multiple vaults per env like dev, prod # create a new vault ID named dev that uses prompt as password source. # Prompt will ask you to enter a password, or a valid path to a password file. ansible-vault create --vault-id dev@prompt credentials_dev.yml ansible-vault create --vault-id prod@prompt credentials_prod.yml # Editing , Decrypting multiple vaults ansible-vault edit credentials_dev.yml --vault-id dev@prompt # Using Password file when using 3rd party automation ansible-vault create --vault-id dev@path/to/passfile credentials_dev.yml # Running playbooks with encrypted password ansible-playbook myplaybook.yml --ask-vault-pass # Passing password file ansible-playbook myplaybook.yml --vault-password-file my_vault_password.py # Passing multi env password ansible-playbook myplaybook.yml --vault-id dev@prompt ansible-playbook myplaybook.yml --vault-id dev@vault_password.py --vault-id test@prompt --vault-id ci@prompt # To change the vault password for key rotation ansible-vault rekey credentials.yml","title":"Launching Ansible Vault situational commands"},{"location":"learning/ansible/ansible/#understanding-the-playbook-framework","text":"A playbook allows you to manage multiple configurations and complex deployments on many machines simply and easily. This is one of the key benefits of using Ansible for the delivery of complex applications. With playbooks, you can organize your tasks in a logical structure as tasks are (generally) executed in the order they are written, allowing you to have a good deal of control over your automation processes. # Example inventory [ frontends ] frt01.example.com https_port=8443 frt02.example.com http_proxy=proxy.example.com [frontends:vars] ntp_server=ntp.frt.example.com proxy=proxy.frt.example.com [apps] app01.example.com app02.example.com [webapp:children] frontends apps [webapp:vars] proxy_server=proxy.webapp.example.com health_check_retry=3 health_check_interal=60 Create a simple playbook to run on the hosts in the frontends host group defined in our inventory file. We can set the user that will access the hosts using the remote_user directive in the playbook --- - hosts : frontends remote_user : danieloh tasks : - name : simple connection test ping : remote_user : danieloh The ignore_errors directive to this task to ensure that our playbook doesn't fail if the ls command fails (for example, if the directory we're trying to list doesn't exist). - name : run a simple command shell : /bin/ls -al /nonexistent ignore_errors : True","title":"Understanding the playbook framework"},{"location":"learning/ansible/ansible/#defining-plays-and-tasks","text":"So far when we have worked with playbooks, we have been creating one single play per playbook (which logically is the minimum you can do). However, you can have more than one play in a playbook, and a \"play\" in Ansible terms is simply a set of tasks (and roles, handlers, and other Ansible facets) associated with a host (or group of hosts). A task is the smallest possible element of a play and is responsible for running a single module with a set of arguments to achieve a specific goal.","title":"Defining plays and tasks"},{"location":"learning/ansible/ansible/#understanding-roles","text":"Roles are designed to enable you to efficiently and effectively reuse Ansible code. They always follow a known structure and often will include sensible default values for variables, error handling, handlers, and so on. The process of creating roles is in fact very simple\u2014Ansible will (by default) look within the same directory as you are running your playbook from for a roles/ directory. The role name is derived from the subdirectory name\u2014there is no need to create complex metadata or anything else\u2014it really is that simple. Within each subdirectory goes a fixed directory structure that tells Ansible what the tasks, default variables, handlers, and so on are for each role. The roles/ directory is not the only play Ansible will look for roles\u2014this is the first directory it will look in, but it will then look in /etc/ansible/roles for any additional roles.","title":"Understanding roles"},{"location":"learning/ansible/ansible/#setting-up-role-based-variables-and-dependencies","text":"The Ansible role directory structure allows for role-specific variables to be declared in two locations. Although, at first, the difference between these two locations may not seem obvious, it is of fundamental importance. Roles based variables can go in one of two locations: defaults/main.yml vars/main.yml Variables that go in the defaults/ directory are one of the lowest in terms of precedence and so are easily overwritten. This location is where you would put variables that you want to override easily, but where you don't want to leave a variable undefined. For example, if you are installing Apache Tomcat, you might build a role to install a specific version. However, you don't want the role to exit with an error if someone forgets to set the version\u2014rather, you would prefer to set a sensible default such as 7.0.76, which can then be overridden with inventory variables or on the command line (using the -e or \u2013extra-vars switches). In this way, you know the role will work even without someone explicitly setting this variable, but it can easily be changed to a newer Tomcat version if desired. Variables that go in the vars/ directory, however, come much higher up on Ansible's variable precedence ordering. This will not be overridden by inventory variables, and so should be used for variable data that it is more important to keep static. Of course, this is not to say they can't be overridden\u2014the -e or \u2013extra-vars switches are the highest order of precedence in Ansible and so will override anything else that you define. Most of the time, you will probably make use of the defaults/ based variables alone, but there will doubtless be times when having the option of variables higher up the precedence ordering becomes valuable to your automation, and so it is vital to know that this option is available to you. Note I recommend that you make extensive use of the debug statement and test your playbook design to make sure that you don't fall foul of this during your playbook development.","title":"Setting up role-based variables and dependencies"},{"location":"learning/ansible/ansible/#ansible-playbook-examples","text":"Install Software only if it doesn't exist - name : installing python2 minimal raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) Install latest software version - hosts : host01 --- become : true tasks : - name : ensure latest sysstat is installed apt : name : sysstat state : latest Install software on all hosts --- - name : install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name : apache2 update_cache : yes state : latest Copy file only when it does not exists --- - hosts : host01 tasks : - stat : path : /home/ubuntu/folder1/something.j2 register : st - name : Template to copy file template : src : ./something.j2 dest : /home/ubuntu/folder1/something.j2 mode : '0644' when : st.stat.exists == False Add users using Loops # Looping - name : add several users user : name : \"{{ item }}\" state : present groups : \"developer\" with_items : - raj - david - john - lauren Using Looping with debug # show all the hosts in the inventory - debug : msg : \"{{ item }}\" with_items : - \"{{ groups['all'] }}\" # show all the hosts in the current play - debug : msg : \"{{ item }}\" with_items : - \"{{ play_hosts }}\" Conditionals # This Playbook will add Java Packages to different systems (handling Ubuntu/Debian OS) - name : debian | ubuntu | add java ppa repo apt_repository : repo=ppa:webupd8team/java state=present become : yes when : ansible_distribution == 'Ubuntu' - name : debian | ensure the webupd8 launchpad apt repository is present apt_repository : repo=\"{{ item }} http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" update_cache=yes state=present with_items : - deb - deb-src become : yes when : ansible_distribution == 'Debian' Full Play --- - name : install software hosts : host01 sudo : yes tasks : - name : Update and upgrade apt packages apt : upgrade : dist update_cache : yes cache_valid_time : 86400 - name : install software apt : name : \"{{item}}\" update_cache : yes state : installed with_items : - nginx - postgresql - postgresql-contrib - libpq-dev - python-psycopg2 - name : Ensure the Nginx service is running service : name : nginx state : started enabled : yes - name : Ensure the PostgreSQL service is running service : name : postgresql state : started enabled : yes #file: vars.yml --- var : 20 #file: playbook.yml --- - hosts : all vars_files : - vars.yml tasks : - debug : msg=\"Variable 'var' is set to {{ var }}\" #host variables - Variables are defined inline for individual host [ group1 ] host1 http_port=80 host2 http_port=303 #group variables - Variables are applied to entire group of hosts [ group1 : vars ] ntp_server= example.com proxy=proxy.example.com Full Play - Deploying an Nginx static site on Ubuntu # playbook.yml --- - hosts : all become : yes vars : server_name : \"{{ ansible_default_ipv4.address }}\" document_root : /var/www app_root : html_demo_site-main tasks : - name : Update apt cache and install Nginx apt : name : nginx state : latest update_cache : yes - name : Copy website files to the server's document root copy : src : \"{{ app_root }}\" dest : \"{{ document_root }}\" mode : preserve - name : Apply Nginx template template : src : files/nginx.conf.j2 dest : /etc/nginx/sites-available/default notify : Restart Nginx - name : Enable new site file : src : /etc/nginx/sites-available/default dest : /etc/nginx/sites-enabled/default state : link notify : Restart Nginx - name : Allow all access to tcp port 80 ufw : rule : allow port : '80' proto : tcp handlers : - name : Restart Nginx service : name : nginx state : restarted # Copy the static files and unzip to folder root curl -L https://github.com/do-community/html_demo_site/archive/refs/heads/main.zip -o html_demo.zip # files/nginx.conf.j2 server { listen 80; root {{ document_root }}/{{ app_root }}; index index.html index.htm; server_name {{ server_name }}; location / { default_type \"text/html\"; try_files $uri.html $uri $uri/ =404; } } # Executing the playbook with sammy user and prompting for password ansible-playbook -i inventory playbook.yml -u sammy -K","title":"Ansible Playbook Examples"},{"location":"learning/ansible/ansible/#using-ansible-system-variables-in-jinja2-templates","text":"Whenever you run Playbook, Ansible by default collects information (facts) about each host like host IP address, CPU type, disk space, operating system information etc. ansible host01 -i myhosts -m setup Create Dynamic templates Consider you need the IP address of all the servers in you web group using 'group' variable { % for host in groups.web % } server {{ host.inventory_hostname }} {{ host.ansible_default_ipv4.address }} :8080 { % endfor % } Create a Webservice entry in Nginx { % for host in groups. [ 'jenkins' ] % } define host { use linux-server host_name {{ host }} alias {{ host }} address {{ hostvars [ host ] .ansible_default_ipv4.address }} hostgroups jenkins } { % endfor % } # service checks to be applied to the webserver { % if jenkins_uses_proxy == true % } define service { use local-service hostgroup_name jenkins service_description HTTP check_command check_jenkins_http notifications_enabled 1 } { % endif % } Get a list of all the variables associated with the current host with the help of hostvars and inventory_hostname variables. --- - name : built-in variables hosts : all tasks : - debug : var=hostvars[inventory_hostname] Using register variables # register variable stores the output, after executing command module, in contents variable # stdout is used to access string content of register variable --- - name : check registered variable for emptiness hosts : all tasks : - name : list contents of the directory in the host command : ls /home/ubuntu register : contents - name : check dir is empty debug : msg=\"Directory is empty\" when : contents.stdout == \"\" - name : check dir has contents debug : msg=\"Directory is not empty\" when : contents.stdout != \"\" Variable Precedence => Command Line > Playbook > Facts > Roles CLI: While running the playbook in Command Line redefine the variable # Passing runtime values in plays ansible-playbook -i myhosts test.yml --extra-vars \"ansible_bios_version=Ansible\"","title":"Using ansible system variables in Jinja2 Templates"},{"location":"learning/ansible/ansible/#async","text":"async - How long to run the task poll - How frequently to check the task status. Default is 10 seconds async_status - Check status of an async task - name : Deploy a mysql DB hosts : db_server roles : - python - mysql_db - name : Deploy a Web Server hosts : web_server roles : - python - flask_web # Below task will run the async in parallel as poll is 0 and register the output - name : Monitor Web Application for 6 Minutes hosts : web_server command : /opt/monitor_webapp.py async : 360 poll : 0 register : webapp_result - name : Monitor Database for 6 Minutes hosts : db_server command : /opt/monitor_database.py async : 360 poll : 0 register : database_result # To avoid job from completing, async_status can be used to poll all async jobs have completed - name : Check status of async task async_status : jid={{ webapp_result.ansible_job_id }} register : job_result until : job_result.finished retries : 30","title":"Async"},{"location":"learning/ansible/ansible/#deployment-strategy-and-forks","text":"Serial - Default: All tasks are run after the previous once completes Free: Once the task completes in a host, it continues next execution without waiting for other hosts Batch: Based on serial, but takes action on multiple host (Rolling Updates) Forks: Deployment on multiple servers # Runs playbook on 2 servers at a time - name : Deploy a web application hosts : app_servers serial : 2 vars : db_name : employee_db db_user : db_user db_password : Passw0rd tasks : - name : Install dependencies - name : Install MySQL database - name : Start Mysql Service - name : Create Application Database - name : Create Application DB User - name : Install Python Flask dependencies - name : Copy web-server code - name : Start web-application # Deploy based on random rolling strategy name : Deploy a web application hosts : app_servers serial : - 2 - 3 - 5 # Deploy based on percentage name : Deploy a web application hosts : app_servers serial : \"20%\" # Runs playbook to fail early, suppose there are 10 servers - name : Deploy a web application hosts : app_servers serial : 5 max_fail_percentage : 50 # The number of failed hosts must exceed the value of max_fail_percentage; if it is equal, the play continues. # So, in our example, if exactly 50% of our hosts failed, the play would still continue. # The first task has a special clause under it that we use to deliberately simulate a failure\u2014this line starts with failed_when and we use it to tell the task that if it runs this task on the first tow hosts in the batch, then it should deliberately fail this task regardless of the result; otherwise, it should allow the task to run as normal. tasks : - name : A task that will sometimes fail debug : msg : This might fail failed_when : inventory_hostname in ansible_play_batch[0:3] # We'll add a second task that will always succeed. - name : A task that will succeed debug : msg : Success! - We have also deliberately set up a failure condition that causes three of the hosts in the first batch of 5 (60%) to fail. ansible-playbook -i morehosts maxfail.yml - We deliberately failed three of the first batch of 5, exceeding the threshold for max_fail_percentage that we set. - This immediately causes the play to abort and the second task is not performed on the first batch of 5. - You will also notice that the second batch of 5, out of the 10 hosts, is never processed, so our play was truly aborted. - This is exactly the behavior you would want to see to prevent a failed update from rolling out across a cluster. - Through the careful use of batches and max_fail_percentage, you can safely run automated tasks across an entire cluster without the fear of breaking the entire cluster in the event of an issue. # Deploy based on completion name : Deploy a web application hosts : app_servers strategy : free","title":"Deployment Strategy and Forks"},{"location":"learning/ansible/ansible/#error-handling","text":"Playbook Error Handling We would like Ansible to stop execution of the entire playbook if a single server was to fail. # To fail playbook on any failure and stop processing on all servers name : Deploy a web application hosts : app_servers any_errors_fatal : true # This will stop all processing # To avoid failure of playbook due to an insignificant task name : Deploy a web application hosts : app_servers tasks : - mail : to : devops@abc.com subject : Server Deployed! body : Webserver is live! ignore_errors : yes # Add this to ignore task failure - command : cat /var/log/server.log register : command_output failed_when : \"'ERROR' in command_output.stdout\" # Conditional failure of task","title":"Error Handling"},{"location":"learning/ansible/ansible/#jinja2-templating","text":"Templating: A process a generating dynamic content or expressions String Manipulation - Filters # Substitution The name is {{ my_name }} # Uppercase The name is {{ my_name | upper }} # Lowercase The name is {{ my_name | lower }} # Titlecase The name is {{ my_name | title }} # Replace The name is {{ my_name | replace(\"Bond\", \"Bourne\") }} # Default value The name is {{ first_name | default(\"James\") }} {{ my_name }} Filters - List and Set # Min {{ [ 1 , 2 , 3 ] | min }} => 1 # Max {{ [1,2,3] | min }} => 3 # Unique {{ [1,2,3,2] | unique }} => 1,2,3 # Union {{ [1,2,3,4] | union([4,5]) }} => 1,2,3,4,5 # Intersect {{ [1,2,3,4] | intersect([4,5]) }} => 4 {{ 100 | random }} => generates random number between 1 to 100 # Join {{ [\"The\",\"name\",\"is\",\"Bond\"] | join(\" \")}} => The name is Bond Filters - File {{ \"/etc/hosts\" | basename }} => hosts Filters - expanduser tasks : - name : Ensure the SSH key is present on OpenStack os_keypair : state : present name : ansible_key public_key_file : \"{{ '~' | expanduser }}/.ssh/id_rsa.pub\"","title":"Jinja2 Templating"},{"location":"learning/ansible/ansible/#lookups","text":"Lookups : To get data from another source on the system # Credentials File csv Hostname,Password web_server,Passw0rd db_server,Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('csvfile', 'web_server file=/tmp/credentials.csv delimiter=,') }}\" => Passw0rd # Credentials File ini [ web_server ] password = Passw0rd [ db_server ] password = Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('ini', 'password section=web_server file=/tmp/credentials.ini') }}\" => Passw0rd","title":"Lookups"},{"location":"learning/ansible/ansible/#tags","text":"Tags are names pinned on individual tasks, roles or an entire play, that allows you to run or skip parts of your Playbook. Tags can help you while testing certain parts of your Playbook. # tag.yml --- - name : Play1-install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name=apache2 update_cache=yes state=latest - name : displaying \"hello world\" debug : msg=\"hello world\" tags : - tag1 - name : Play2-install nginx hosts : all sudo : yes tags : - tag2 tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : debug module displays message in control machine debug : msg=\"have a good day\" tags : - mymessage - name : shell module displays message in host machine. shell : echo \"yet another task\" tags : - mymessage Executing above play using tags # displays the list of tasks in the Playbook ansible-playbook -i myhosts tag.yml --list-tasks # displays only tags in your Playbook ansible-playbook -i myhosts tag.yml --list-tags # executes only certain tasks which are tagged as tag1 and mymessage ansible-playbook -i myhosts tag.yml --tags \"tag1,mymessage\"","title":"Tags"},{"location":"learning/ansible/ansible/#includes-outdated-after-20","text":"Ansible gives you the flexibility of organizing your tasks through include keyword, that introduces more abstraction and make your Playbook more easily maintainable, reusable and powerful. --- - name : testing includes hosts : all sudo : yes tasks : - include : apache.yml - include : content.yml - include : create_folder.yml - include : content.yml - include : nginx.yml # apache.yml will not have hosts & tasks but nginx.yml as a separate play will have tasks and can run independently #nginx.yml --- - name : installing nginx hosts : all sudo : yes tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : displaying message debug : msg=\"yayy!! nginx installed\"","title":"Includes (Outdated after 2.0)"},{"location":"learning/ansible/ansible/#roles","text":"A Role is completely self contained or encapsulated and completely reusable # cat ansible.cfg [ defaults ] host_key_checking=False inventory = /etc/ansible/myhosts log_path = /home/scrapbook/tutorial/output.txt roles_path = /home/scrapbook/tutorial/roles/ # master-playbook.yml --- - name : my first role in ansible hosts : all sudo : yes roles : - sample_role - sample_role2 # sample_role/tasks/main.yml --- - include : nginx.yml - include : copy-template.yml - include : copy-static.yml # sample_role/tasks/nginx.yml --- - name : Installs nginx apt : pkg=nginx state=installed update_cache=true notify : - start nginx # sample_role/handlers/main.yml --- - name : start nginx service : name=nginx state=started # sample_role/tasks/copy-template.yml --- - name : sample template - x template : src : template-file.j2 dest : /home/ubuntu/copy-template-file.j2 with_items : var_x # sample_role/vars/main.yml var_x : - 'variable x' var_y : - 'variable y' # sample_role/tasks/copy-static.yml # Ensure some-file.txt is present under files folder of the role --- - name : Copy a file copy : src=some-file.txt dest=/home/ubuntu/file1.txt Executing the play # Let us run the master_playbook and check the output: ansible-playbook -i myhosts master_playbook.yml","title":"Roles"},{"location":"learning/ansible/ansible/#ansible-galaxy","text":"ansible-galaxy is command line tool for scaffolding the creation of directory structure needed for organizing your code ansible-galaxy init sample_role","title":"Ansible Galaxy"},{"location":"learning/ansible/ansible/#ansible-galaxy-useful-commands","text":"Install a Role from Ansible Galaxy To use others role, visit https://galaxy.ansible.com/ and search for the role that will achieve your goal. Goal: Install Apache in the host machines. # Ensure that roles_path are defined in ansible.cfg for a role to successfully install. # Here, apache is role name and geerlingguy is name of the user in GitHub who created the role. ansible-galaxy install geerlingguy.apache # Forcefully Recreate Role ansible-galaxy init geerlingguy.apache --force # Listing Installed Roles ansible-galaxy list # Remove an Installed Role ansible-galaxy remove geerlingguy.apache","title":"ansible-galaxy useful commands"},{"location":"learning/ansible/ansible/#environment-variables","text":"Ansible recommends maintaining inventory file for each environment, instead of keeping all your hosts in a single inventory. Each environment directory has one inventory file (hosts) and group_vars directory.","title":"Environment Variables"},{"location":"learning/ansible/ansible/#ansible-best-practises","text":"","title":"Ansible Best Practises"},{"location":"learning/ansible/ansible/#inventory-files","text":"To show you a great way of setting up your directory structure for a simple role-based playbook that has two different inventories \u2014 one for a development environment and one for a production environment. # inventories/development/hosts [ app ] app01.dev.example.com app02.dev.example.com # inventories/development/group_vars --- http_port : 8080 # inventories/production/hosts [ app ] app01.prod.example.com app02.prod.example.com # inventories/production/group_vars --- http_port : 80 # To run it on the development inventory ansible-playbook -i inventories/development/hosts site.yml # To run it on the production inventory ansible-playbook -i inventories/production/hosts site.yml However, there are always differences between the two environments, not just in the hostnames, but also sometimes in the parameters, the load balancer names, the port numbers, and so on\u2014the list can seem endless. Try and reuse the same playbooks for all of your environments that run the same code. For example, if you deploy a web app in your development environment, you should be confident that your playbooks will deploy the same app in the production environment This means that not only are you testing your application deployments and code, you are also testing your Ansible playbooks and roles as part of your overall testing process. Your inventories for each environment should be kept in separate directory trees, but all roles, playbooks, plugins, and modules (if used) should be in the same directory structure (this should be the case for both environments). It is normal for different environments to require different authentication credentials; you should keep these separate not only for security but also to ensure that playbooks are not accidentally run in the wrong environment. Your playbooks should be in your version control system, just as your code is. This enables you to track changes over time and ensure that everyone is working from the same copy of the automation code.","title":"Inventory Files"},{"location":"learning/ansible/ansible/#the-proper-approach-to-defining-group-and-host-variables","text":"First and foremost, you should always pay attention to variable precedence. Host variables are always of a higher order of precedence than group variables; so, you can override any group variable with a host variable. This behavior is useful if you take advantage of it in a controlled manner, but can yield unexpected results if you are not aware of it. There is a special group variables definition called all, which is applied to all inventory groups. This has a lower order of precedence than specifically defined group variables. What happens if you define the same variable twice in two groups? If this happens, both groups have the same order of precedence, so which one wins? [ app ] app01.dev.example.com app02.dev.example.com # inventories/development/group_vars/all.yml --- http_port : 8080 # inventories/development/group_vars/app.yml --- http_port : 8081 # site.yml --- - name : Play using best practise directory structure hosts : all tasks : - name : Display the value of our inventory variable debug : var : http_port ansible-playbook -i inventories/development/hosts site.yml As expected, the variable definition in the specific group won, which is in line with the order of precedence documented for Ansible. Now, let's see what happens if we define the same variable twice in two specifically named groups. To complete this example, we'll create a child group, called centos, and another group that could notionally contain hosts built to a new build standard, called newcentos, which both application servers will be a member of. [ app ] app01.dev.example.com app02.dev.example.com [centos:children] app [newcentos:children] app # inventories/development/group_vars/centos.yml --- http_port : 8082 # inventories/development/group_vars/newcentos.yml --- http_port : 8083 We've now defined the same variable four times at the group level! ansible-playbook -i inventories/development/hosts site.yml The value we entered in newcentos.yml won\u2014but why? The Ansible documentation states that where identical variables are defined at the group level in the inventory (the one place you can do this), the one from the last-loaded group wins. Groups are processed in alphabetical order and newcentos is the group with the name beginning furthest down the alphabet\u2014so, its value of http_port was the value that won. Just for completeness, we can override all of this by leaving the group_vars directory untouched, but adding a file called inventories/development/host_vars/app01.dev.example.com.yml --- http_port : 9090 We will see that the value we defined at the host level completely overrides any value that we set at the group level for app01.dev.example.com. app02.dev.example.com is unaffected as we did not define a host variable for it, so the next highest level of precedence\u2014the group variable from the newcentos group\u2014won","title":"The proper approach to defining group and host variables"},{"location":"learning/ansible/ansible/#using-top-level-playbooks","text":"Imagine handing a playbook directory structure with 100 different playbooks to a new system administrator\u2014how would they know which ones to run and in which circumstances? The task of training someone to use the playbooks would be immense and would simply move complexity from one area to another. The most important thing is that, on receipt of a new playbook directory structure, a new operator at least knows what the starting point for both running the playbooks, and understanding the code is. If the top-level playbook they encounter is always site.yml, then at least everyone knows where to start. Through the clever use of roles and the import_* and include_* statements, you can split your playbook up into logical portions of reusable code, all from one playbook file.","title":"Using top-level playbooks"},{"location":"learning/ansible/ansible/#leveraging-version-control-tools","text":"Any changes to your Ansible code could mean big changes to your environment, and possibly even whether an important production service works or not. As a result, it is vital that you maintain a version history of your Ansible code and that everyone works from the same version.","title":"Leveraging version control tools"},{"location":"learning/ansible/ansible/#setting-os-and-distribution-variances","text":"This playbook demonstrates how you can group differing plays using an Ansible fact so that the OS distribution determines which play in a playbook gets run. # osvariants.yml - It will also contain a single task. --- - name : Play to demonstrate group_by module hosts : all tasks : - name : Create inventory groups based on host facts group_by : key : os_{{ ansible_facts['distribution'] }} group_by module: It dynamically creates new inventory groups based on the key that we specify \u2014 in this example, we are creating groups based on a key comprised of the os_ fixed string, followed by the OS distribution fact obtained from the Gathering Facts stage. The original inventory group structure is preserved and unmodified, but all the hosts are also added to the newly created groups according to their facts. So, the two servers in our simple inventory remain in the app group, but if they are based on Ubuntu, they will be added to a newly created inventory group called os_Ubuntu. Similarly, if they are based on CentOS, they will be added to a group called os_CentOS. # Play definition to the same playbook file to install Apache on CentOS - name : Play to install Apache on CentOS hosts : os_CentOS # Refer to the Dynamic group become : true tasks : - name : Install Apache on CentOS yum : name : httpd state : present # Add a third Play definition, this time for installing the apache2 package on Ubuntu using the apt module - name : Play to install Apache on Ubuntu hosts : os_Ubuntu become : true tasks : - name : Install Apache on Ubuntu apt : name : apache2 state : present ansible-playbook -i hosts osvariants.yml Notice how the task to install Apache on CentOS was run. It was run this way because the group_by module created a group called os_CentOS and our second play only runs on hosts in the group called os_CentOS. As there were no servers running on Ubuntu in the inventory, the os_Ubuntu group was never created and so the third play does not run. We receive a warning about the fact that there is no host pattern that matches os_Ubuntu, but the playbook does not fail\u2014it simply skips this play. It is up to you to choose the coding style most appropriate to you. You can make use of the group_by module, as detailed here, or write your tasks in blocks and add a when clause to the blocks so that they only run when a certain fact-based condition is met (for example, the OS distribution is CentOS)\u2014or perhaps even a combination of the two. The choice is ultimately yours and these different examples are provided to empower you with multiple options that you can choose between to create the best possible solution for your scenario.","title":"Setting OS and distribution variances"},{"location":"learning/ansible/ansible/#setting-task-execution-delegation","text":"We have assumed that all the tasks are executed on each host in the inventory in turn. However, what if you need to run one or two tasks on a different host? For example, we have talked about the concept of automating upgrades on clusters. Logically, however, we would want to automate the entire process, including the removal of each host in turn from the load balancer and its return after the task is completed. Although we still want to run our play across our entire inventory, we certainly don't want to run the load balancer commands from those hosts. Imagine that you have a shell script (or other executables) that you can call that can add and remove hosts to and from a load balancer. # remove_from_loadbalancer.sh #!/bin/sh echo Removing $1 from load balancer... # add_to_loadbalancer.sh #!/bin/sh echo Adding $1 to load balancer... [ frontends ] frt01.example.com frt02.example.com --- - name : Play to demonstrate task delegation hosts : frontends tasks : - name : Remove host from the load balancer command : ./remove_from_loadbalancer.sh {{ inventory_hostname }} args : chdir : \"{{ playbook_dir }}\" delegate_to : localhost - name : Deploy code to host debug : msg : Deployment code would go here.... - name : Add host back to the load balancer command : ./add_to_loadbalancer.sh {{ inventory_hostname }} args : chdir : \"{{ playbook_dir }}\" delegate_to : localhost We are using the command module to call the script we created earlier, passing the hostname from the inventory being removed from the load balancer to the script. We use the chdir argument with the playbook_dir magic variable to tell Ansible that the script is to be run from the same directory as the playbook. The special part of this task is the delegate_to directive, which tells Ansible that even though we're iterating through an inventory that doesn't contain localhost, we should run this action on localhost (we aren't copying the script to our remote hosts, so it won't run if we attempt to run it from there). Deploy task has no delegate_to directive, and so it is actually run on the remote host from the inventory (as desired): Finally, we add the host back to the load balancer using the second script we created earlier. This task is almost identical to the first. ansible-playbook -i hosts delegate.yml Notice how even though Ansible is working through the inventory (which doesn't feature localhost), the load balancer-related scripts are actually run from localhost, while the upgrade task is performed directly on the remote host. In truth, you can delegate any task to localhost, or even another non-inventory host. You could, for example, run an rsync command delegated to localhost to copy files to remote hosts using a similar task definition to the previous one. This is useful because although Ansible has a copy module, it can't perform the advanced recursive copy and update functions that rsync is capable of. Note that you can choose to use a form of shorthand notation in your playbooks (and roles) for delegate_to, called local_action . This allows you to specify a task on a single line that would ordinarily be run with delegate_to: localhost added below it. --- - name : Second task delegation example hosts : frontends tasks : - name : Perform an rsync from localhost to inventory hosts local_action : command rsync -a /tmp/ {{ inventory_hostname }}:/tmp/target/ The preceding shorthand notation is equivalent to the following: tasks : - name : Perform an rsync from localhost to inventory hosts command : rsync -a /tmp/ {{ inventory_hostname }}:/tmp/target/ delegate_to : localhost If we run this playbook, we can see that local_action does indeed run rsync from localhost, enabling us to efficiently copy whole directory trees across to remote servers in the inventory. ansible-playbook -i hosts delegate2.yml","title":"Setting task execution delegation"},{"location":"learning/ansible/ansible/#using-the-run_once-option","text":"When working with clusters, you will sometimes encounter a task that should only be executed once for the entire cluster. For example, you might want to upgrade the schema of a clustered database. Instead, you can write your code as you normally would, but make use of the special run_once directive for any tasks you want to run only once on your inventory. For example, let's reuse the 10-host inventory. --- - name : Play to demonstrate the run_once directive hosts : frontends tasks : - name : Upgrade database schema debug : msg : Upgrading database schema... run_once : true ansible-playbook -i morehosts runonce.yml Notice that, just as desired, although the playbook was run on all 10 hosts (and, indeed, gathered facts from all 10 hosts), we only ran the upgrade task on one host. It's important to note that the run_once option applies per batch of servers, so if we add serial: 5 to our play definition (running our play in two batches of 5 on our inventory of 10 servers), the schema upgrade task actually runs twice! It runs once as requested, but once per batch of servers, not once for the entire inventory. Be careful of this nuance when working with this directive in a clustered environment.","title":"Using the run_once option"},{"location":"learning/ansible/ansible/#running-playbooks-locally","text":"It is important to note that when we talk about running a playbook locally with Ansible, it is not the same as talking about running it on localhost. If we run a playbook on localhost, Ansible actually sets up an SSH connection to localhost (it doesn't differentiate its behavior or attempt to detect whether a host in the inventory is local or remote - it simply tries faithfully to connect). # Inventory file [ local ] localhost ansible_connection=local We've added a special variable to our localhost entry ansible_connection variable which defines which protocol is used to connect to this inventory host. So, we have told it to use a direct local connection instead of an SSH-based connectivity (which is the default). Note that this special value for the ansible_connection variable actually overrides the hostname you have put in your inventory. So, if we change our inventory to look as follows, Ansible will not even attempt to connect to the remote host called frt01.example.com it will connect locally to the machine running the playbook (without SSH). [ local ] frt01.example.com ansible_connection=local The presence of ansible_connection=local meant that this command was run on the local machine without using SSH. This ability to run commands locally without the need to set up SSH connectivity, SSH keys, and so on can be incredibly valuable, especially if you need to get things up and running quickly on your local machine.","title":"Running playbooks locally"},{"location":"learning/ansible/ansible/#working-with-proxies-and-jump-hosts","text":"Often, when it comes to configuring core network devices, these are isolated from the main network via a proxy or jump host. Ansible lends itself well to automating network device configuration as most of it is performed over SSH: however, this is only helpful in a scenario where Ansible can either be installed and operated from the jump host or, better yet, can operate via a host such as this. Let's assume that you have two Cumulus Networks switches in your network (these are based on a special distribution of Linux for switching hardware, which is very similar to Debian). These two switches have the cmls01.example.com and cmls02.example.com hostnames, but both can only be accessed from a host called bastion.example.com. [ switches ] cmls01.example.com cmls02.example.com [switches:vars] ansible_ssh_common_args='-o ProxyCommand=\"ssh -W %h:%p -q bastion.example.com\"' This special variable content ansible_ssh_common_args tells Ansible to add extra options when it sets up an SSH connection, including to proxy via the bastion.example.com host. The -W %h:%p options tell SSH to proxy the connection and to connect to the host specified by %h (this is either cmls01.example.com or cmls02.example.com) on the port specified by %p (usually port 22). ansible -i switches -m ping all On the surface, Ansible works just as it normally does and connects successfully to the two hosts. However, behind the scenes it proxies via bastion.example.com. Note that this simple example assumes that you are connecting to both the bastion host and switches using the same username and SSH credentials (or in this case, keys).","title":"Working with proxies and jump hosts"},{"location":"learning/ansible/ansible/#configuring-playbook-prompts","text":"All of our playbooks have had their data specified for them at run time in variables we defined within the playbook. However, what if you actually want to obtain information from someone during a playbook run? Perhaps you want to obtain a password from a user for an authentication task without storing it anywhere. Ansible can prompt you for user input and store the input in a variable for future processing. We will prompt for two variables, one for a user ID and one for a password. One will be echoed to the screen, while the other won't be, by setting private: yes --- - name : A simple play to demonstrate prompting in a playbook hosts : frontends vars_prompt : - name : loginid prompt : \"Enter your username\" private : no - name : password prompt : \"Enter your password\" private : yes tasks : - name : Proceed with login debug : msg : \"Logging in as {{ loginid }}...\" ansible-playbook -i hosts prompt.yml","title":"Configuring playbook prompts"},{"location":"learning/ansible/ansible/#ansible-security-best-practices","text":"","title":"Ansible Security Best Practices"},{"location":"learning/ansible/ansible/#working-with-ansible-vault","text":"It's really important to use Ansible Vault to store all the secret information in our playbooks. Some of the really good use cases include how we can use these playbooks without changing our version control systems, CI/CD integration pipelines, and so on.","title":"Working with Ansible Vault"},{"location":"learning/ansible/ansible/#how-to-use-ansible-vault-with-variables-and-files","text":"Let's take an example of installing MySQL server in an Ubuntu operating system using the following playbook. As per the Ansible documentation, it's easy and better to store Vault variables and normal variables differently. \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 mysql.yml # contains vault secret values \u251c\u2500\u2500 hosts \u251c\u2500\u2500 main.yml \u2514\u2500\u2500 roles \u2514\u2500\u2500 mysqlsetup \u2514\u2500\u2500 tasks \u2514\u2500\u2500 main.yml Now, if we see the group_vars/main.yml file, the content looks as shown in the codeblock. It contains the secrets variable to use in the playbook, called mysql_root_password. mysql_root_password : supersecretpassword\u200b To encrypt the vault file, we will use the following command and it then prompts for the password to protect ansible-vault encrypt group_vars/mysql.yml # Now, to execute the playbook run the following command, it will prompt for the vault password ansible-playbook --ask-vault-pass -i hosts main.yml We can also pass the ansible-vault password file with playbook execution by specifying flag, it helps in our continuous integration and pipeline platforms. The following file contains the password which used to encrypt the mysql.yml file. cat ~/.vaultpassword thisisvaultpassword # To pass the vault password file through the command line, use the following command when executing playbooks ansible-playbook --vault-password-file ~/.vaultpassword -i hosts main.yml Note Make sure to give proper permissions for this file, so others cannot access this file using chmod. Also, it's good practice to add this file to your .gitignore, so it will not be version controlled when pushing playbooks. Vault password file can be an executable script, which can retrieve data stored somewhere securely rather than having to keep the key in plain text on disk and relying on file permissions to keep it safe. We can also use system environment variables such as ANSIBLE_VAULT_PASSWORD_FILE=~/.vaultpassword and Ansible will use this while executing playbooks.","title":"How to use Ansible Vault with variables and files"},{"location":"learning/ansible/ansible/#ansible-vault-single-encrypted-variable","text":"It allows us to use vaulted variables with the !vault tag in YAML files This playbook is used to perform reverse IP lookups using the ViewDNS API. We want to secure api_key as it contains sensitive information. # We use the ansible-vault encrypt_string command to perform this encryption. # Here, we used echo with the -n flag to remove the new line echo -n '53ff4ad63849e6977cb652763g7b7c64e2fa42a' | ansible-vault encrypt_string --stdin-name 'api_key' We can place the variable, inside the playbook variables and execute the playbook as normal, using ansible-playbook with the \u2013ask-vault-pass option. - name : ViewDNS domain information hosts : localhost vars : domain : google.com api_key : !vault | $ANSIBLE_VAULT;1.1;AES256 36623761316238613461326466326162373764353437393733343334376161336630333532626465 6662383435303930303164353664643639303761353664330a393365633237306530653963353764 64626237313738656530373639653739656564316161663831653431623832336635393637653330 6632663563363264340a323537356166653338396135376161323435393730306133626635376539 37383861653239326336613837666237636463396465393662666561393132343166666334653465 6265386136386132363534336532623061646438363235383334 output_type : json tasks : - name : \"getting {{ domain }} server info\" uri : url : \"https://api.viewdns.info/reverseip/?host={{ domain }}&apikey={{ api_key }}&output={{ output_type }}\" method : GET register : results - debug : msg : \"{{ results.json }}\" Playbook being executed will be automatically decrypted after we provide it with the given password. ansible-playbook --ask-vault-pass -i hosts main.yml","title":"Ansible Vault single encrypted variable"},{"location":"learning/ansible/ansible/#setting-up-and-using-ansible-galaxy","text":"Is an official centralized hub for finding, sharing, and reusing Ansible roles. This allows the community to share and collaborate on Ansible playbooks, and allows new users to quickly get started with using Ansible. To share our custom-written roles with the community, we can publish them to Ansible Galaxy using GitHub authentication. We can install or include roles direct from GitHub by specifying the GitHub URL. This allows the use of private version control systems as local inventories of playbook roles. ansible-galaxy install git+https://github.com/geerlingguy/ansible-role-composer.git","title":"Setting up and using Ansible Galaxy"},{"location":"learning/ansible/ansible/#ansible-controller-machine-security","text":"The controller machine for Ansible requires SSH and Python to be installed and configured. Ansible has a very low attack surface. Note In January 2017, multiple security issues were found by a company called Computest . This vulnerability was dubbed owning the farm, since compromising the controller would imply that all the nodes could potentially be compromised. The controller machine should be a hardened server and treated with all the seriousness that it deserves. In the vulnerability that was disclosed, if a node gets compromised attackers could leverage that to attack and gain access to the controller. - Once they have access, the could extend their control over all the other nodes being managed by the controller. Since the attack surface is already very limited, the best we can do is ensure that the server stays secure and hardened.","title":"Ansible controller machine security"},{"location":"learning/ansible/ansible/#explanation-of-ansible-os-hardening-playbook","text":"The following playbook is created by DevSec for Linux baselines. It covers most of the required hardening checks based on multiple standards, which includes Ubuntu Security Features, NSA Guide to Secure Configuration, ArchLinux System Hardening and other. This can be improved if required by adding more tasks (or) roles. Ansible OS Hardening Playbook covers Configures package management, that is, allows only signed packages Removes packages with known issues Configures pam and the pam_limits module Shadow password suite configuration Configures system path permissions Disables core dumps through soft limits Restricts root logins to system console Sets SUIDs Configures kernel parameters through sysctl # download the os-hardening role from Ansible Galaxy ansible-galaxy install dev-sec.os-hardening Call that role in your playbook and execute it to perform the baseline hardening, and also change the variables as required. Refer to https://galaxy.ansible.com/dev-sec/os-hardening for more detailed options. - hosts : localhost become : yes roles : - dev-sec.os-hardening # Execute the playbook ansible-playbook main.yml","title":"Explanation of Ansible OS hardening playbook"},{"location":"learning/ansible/ansible/#best-practices-and-reference-playbook-projects","text":"Projects such as Algo, DebOps, and OpenStack are large Ansible playbook projects that are well maintained and secure by default.","title":"Best practices and reference playbook projects"},{"location":"learning/ansible/ansible/#debops--your-debian-based-data-center-in-a-box","text":"DebOps is a project created by Maciej Delmanowski. It contains a collection of various Ansible playbooks that can be used for Debian and Ubuntu hosts. This project has more than 128 Ansible roles, which are customized for production use cases and work with multiple environments. We can see a list of available playbook services at debops/debops-playbooks There are two different ways we can quickly get started with a DebOps setup: Vagrant setup Docker setup","title":"DebOps \u2013 your Debian-based data center in a box"},{"location":"learning/ansible/ansible/#algo--set-up-a-personal-ipsec-vpn-in-the-cloud","text":"Algo from Trail of Bits provides Ansible roles and scripts to automate the installation of a personal IPSEC VPN. By running the Ansible playbooks, you get a complete hardened VPN server, and deployments to all major cloud providers are already configured ( https://github.com/trailofbits/algo/blob/master/docs/deploy-from-ansible.md ).","title":"Algo \u2013 set up a personal IPSEC VPN in the cloud"},{"location":"learning/ansible/ansible/#openstack-ansible","text":"Not only does this project use Ansible playbooks extensively, but their security documentation is also worth reading and emulating. The best part is that all of the security configuration is declarative security codified in Ansible playbooks. Documentation on this project is available at https://docs.openstack.org/project-deploy-guide/openstack-ansible/ocata/app-security.html .","title":"OpenStack-Ansible"},{"location":"learning/ansible/ansible/#awx--open-source-version-of-ansible-tower","text":"AWX provides a web-based user interface, REST API, and task engine built on top of Ansible. AWX can be used with the tower-CLI tool and client library. Get started with AWX here: ansible/awx . Get started with tower-cli here: ansible/tower-cli .","title":"AWX \u2013 open source version of Ansible Tower"},{"location":"learning/ansible/container_cloud/","text":"Container and Cloud Management \u00b6 In the last few years, container-based workloads and cloud workloads have become more and more popular, and for this reason, we are going to look at how you can automate tasks related to those kinds of workloads with Ansible. First of all, even if you are in a very good place in your automation path and you have a lot of Ansible roles written for your infrastructure, you can't leverage them in Dockerfiles, so you would end up replicating your work to create containers. If this is not enough of a problem, this situation quickly deteriorates when you start considering cloud environments. All cloud environments have their own control planes and native automation languages, so in a very short time, you would find yourself rewriting the automation for the same operation over and over, thus wasting time and deteriorating the consistency of your environments. Designing and building containers with playbooks \u00b6 Ansible provides ansible-container so that you can create containers using the same components you would use for creating machines. The first thing you should do is ensure that you have ansible-container installed. sudo pip install ansible-container [ docker,k8s ] The ansible-container tool comes with three supported engines at the time of writing: docker: This is needed if you want to use it with Docker Engine (that is, on your local machine). k8s: This is needed if you want to use it with a Kubernetes cluster, both local (that is, MiniKube) or remote (that is, a production cluster). openshift: This is needed if you want to use it with an OpenShift cluster, both local (that is, MiniShift) or remote (that is, a production cluster). Follow these steps to build the container using playbooks Issuing the ansible-container init command ansible-container init Running this command will also create the following files: ansible.cfg: An empty file to be (eventually) used to override Ansible system configurations ansible-requirements.txt: An empty file to (eventually) list the Python requirements for the building process of your containers container.yml: A file that contains the Ansible code for the build meta.yml: A file that contains the metadata for Ansible Galaxy requirements.yml: An empty file to (eventually) list the Ansible roles that are required for your build Let's try building our own container using this tool \u2013 replace the contents of container.yml with the following version : \"2\" settings : conductor : base : centos:7 project_name : http-server services : web : from : \"centos:7\" roles : - geerlingguy.apache ports : - \"80:80\" command : - \"/usr/bin/dumb-init\" - \"/usr/sbin/apache2ctl\" - \"-D\" - \"FOREGROUND\" dev_overrides : environment : - \"DEBUG=1\" We can now run ansible-container build to initiate the build. At the end of the building process, we will have a container built with the geerlingguy.apache role applied to it. The ansible-container tool performs a multi-stage build capability, spinning up an Ansible container that is then used to build the real container. If we specified more than one role to be applied, the output would be an image with more layers, since Ansible will create a layer for every specified role. In this way, containers can easily be built using your existing Ansible roles rather than Dockerfiles. Managing multiple container platforms \u00b6 To be able to call a deployment \"production-ready,\" you need to be able to demonstrate that the service your application is delivering will run reasonably, even in the case of a single application crash, as well as hardware failure. Often, you'll have even more reliability constraints from your customer. Today, the most successful one is Kubernetes due to its various distributions/versions, so we are going to focus on it primarily. The idea of Kubernetes is that you inform the Kubernetes Control Plane that you want X number of instances of your Y application, and Kubernetes will count how many instances of the Y application are running on the Kubernetes Nodes to ensure that the number of instances are X. If there are too few instances, Kubernetes will take care to start more instances, while if there are too many instances, the exceeding instances will be stopped. Due to the complexity of installing and managing Kubernetes, multiple companies have started to sell distributions of Kubernetes that simplify their operations and that they are willing to support. Deploying to Kubernetes with ansible-container \u00b6 We will assume that you have access to either a Kubernetes cluster for testing. To deploy your application to your cluster, you need to change the container.yml file so that you can add some additional information. We will need to add a section called settings and a section called k8s_namespace to declare our deployment settings. k8s_namespace : name : http-server description : An HTTP server display_name : HTTP server We can proceed with the deployment- ansible-container --engine kubernetes deploy As soon as Ansible has completed its execution, you will be able to find the http-server deployment on your Kubernetes cluster. Based on the image that we built in the previous section and the additional information we added at the beginning of this section, Ansible is able to populate a deployment template and then deploy it using the k8s module. Managing Kubernetes Objects with Ansible \u00b6 You can do kubectl get namespaces with Ansible by creating a file called k8s-ns-show.yaml --- - hosts : localhost tasks : - name : Get information from K8s k8s_info : api_version : v1 kind : Namespace # Specify the k8s object Deployments, Services, Pods register : ns - name : Print info debug : var : ns ansible-playbook k8s-ns-show.yaml # Create a new namespace --- - hosts : localhost tasks : - name : Ensure the myns namespace exists k8s : api_version : v1 kind : Namespace name : myns state : present # Creates a new service --- - hosts : localhost tasks : - name : Ensure the Service mysvc is present k8s : state : present definition : apiVersion : v1 kind : Service metadata : name : mysvc namespace : myns spec : selector : app : myapp service : mysvc ports : - protocol : TCP targetPort : 800 name : port-80-tcp port : 80 Ansible allows you to manage your Kubernetes clusters with some modules: k8s: Allows you to manage any kind of Kubernetes object k8s_auth: Allows you to authenticate to Kubernetes clusters that require an explicit login step k8s_facts: Allows you to inspect Kubernetes objects k8s_scale: Allows you to set a new size for a Deployment, ReplicaSet, Replication Controller, or Job k8s_service: Allows you to manage Services on Kubernetes Automating Docker with Ansible \u00b6 With Ansible, you can easily manage your Docker instance in Development environments. First of all, we need to create a playbook called start-docker-container.yaml that will contain the following code - hosts : localhost tasks : - name : Start a container with a command docker_container : name : test-container image : alpine command : - echo - \"Hello, World!\" Other modules include the following: docker_config: Used to change the configurations of the Docker daemon docker_container_info: Used to gather information from (inspect) a container docker_network: Used to manage Docker networking configuration Automating against Amazon Web Services \u00b6 To be able to use Ansible to automate your Amazon Web Service estate, you'll need to install the boto library. pip install boto Authentication \u00b6 The boto library looks up the necessary credentials in the ~/.aws/credentials file. There are two different ways to ensure that the credentials file is configured properly. It is possible to use the AWS CLI tool. Alternatively, this can be done with a text editor of your choice by creating a file with the following structure [ default ] aws_access_key_id = [ YOUR_KEY_HERE ] aws_secret_access_key = [ YOUR_SECRET_ACCESS_KEY_HERE ] Now that you've created the file with the necessary credentials, boto will be able to work against your AWS environment. Since Ansible uses boto for every single communication with AWS systems, this means that Ansible will be appropriately configured, even without you have to change any Ansible-specific configuration. Creating your first machine \u00b6 To launch a virtual machine in AWS, we need a few things to be in place, as follows: An SSH key pair A network A subnetwork A security group By default, a network and a subnetwork are already available in your accounts, but you need to retrieve their IDs. Create the aws.yaml Playbook with the following content - hosts : localhost tasks : - name : Ensure key pair is present ec2_key : name : fale key_material : \"{{ lookup('file', '~/.ssh/fale.pub') }}\" - name : Gather information of the EC2 VPC net in eu-west-1 ec2_vpc_net_facts : region : eu-west-1 register : aws_simple_net - name : Gather information of the EC2 VPC subnet in eu-west-1 ec2_vpc_subnet_facts : region : eu-west-1 filters : vpc-id : '{{ aws_simple_net.vpcs.0.id }}' register : aws_simple_subnet - name : Ensure wssg Security Group is present ec2_group : name : wssg description : Web Security Group region : eu-west-1 vpc_id : '{{ aws_simple_net.vpcs.0.id }}' rules : - proto : tcp from_port : 22 to_port : 22 cidr_ip : 0.0.0.0/0 - proto : tcp from_port : 80 to_port : 80 cidr_ip : 0.0.0.0/0 - proto : tcp from_port : 443 to_port : 443 cidr_ip : 0.0.0.0/0 rules_egress : - proto : all cidr_ip : 0.0.0.0/0 register : aws_simple_wssg - name : Setup instance ec2 : assign_public_ip : true image : ami-3548444c region : eu-west-1 exact_count : 1 key_name : fale count_tag : Name : ws01.ansible2cookbook.com instance_tags : Name : ws01.ansible2cookbook.coms instance_type : t2.micro group_id : '{{ aws_simple_wssg.group_id }}' vpc_subnet_id : '{{ aws_simple_subnet.subnets.0.id }}' volumes : - device_name : /dev/sda1 volume_type : gp2 volume_size : 10 delete_on_termination : True ansible-playbook aws.yaml We started by uploading the public part of an SSH keypair to AWS, then queried for information about the network and the subnetwork, then ensured that the Security Group we wanted to use was present, and lastly triggered the machine build. Automating against Azure \u00b6 To let Ansible manage the Azure cloud, you need to install the Azure SDK for Python. pip install 'ansible[azure]' ``` ## Authentication - There are different ways to ensure that Ansible is able to manage Azure for you, based on the way your Azure account is set up, but they can all be configured in the ` ~/.azure/credentials ` file. ``` BASH [ default ] subscription_id = [ YOUR_SUBSCIRPTION_ID_HERE ] client_id = [ YOUR_CLIENT_ID_HERE ] secret = [ YOUR_SECRET_HERE ] tenant = [ YOUR_TENANT_HERE ] If you prefer to use Active Directories with a username and password. [ default ] ad_user = [ YOUR_AD_USER_HERE ] password = [ YOUR_AD_PASSWORD_HERE ] You can opt for an Active Directory login with ADFS. [ default ] ad_user = [ YOUR_AD_USER_HERE ] password = [ YOUR_AD_PASSWORD_HERE ] client_id = [ YOUR_CLIENT_ID_HERE ] tenant = [ YOUR_TENANT_HERE ] adfs_authority_url = [ YOUR_ADFS_AUTHORITY_URL_HERE ] Creating your first machine \u00b6 Create the azure.yaml Playbook with the following content. In Azure, you will need all the resources to be ready before you can issue the machine creation command. This is the reason you create the Storage Account, the Virtual Network, the Subnet, the Public IP, the security Group, and the NIC first, and only at that point, the machine itself. - hosts : localhost tasks : - name : Ensure the Storage Account is present azure_rm_storageaccount : resource_group : Testing name : mysa account_type : Standard_LRS - name : Ensure the Virtual Network is present azure_rm_virtualnetwork : resource_group : Testing name : myvn address_prefixes : \"10.10.0.0/16\" - name : Ensure the Subnet is present azure_rm_subnet : resource_group : Testing name : mysn address_prefix : \"10.10.0.0/24\" virtual_network : myvn - name : Ensure that the Public IP is set azure_rm_publicipaddress : resource_group : Testing allocation_method : Static name : myip - name : Ensure a Security Group allowing SSH is present azure_rm_securitygroup : resource_group : Testing name : mysg rules : - name : SSH protocol : Tcp destination_port_range : 22 access : Allow priority : 101 direction : Inbound - name : Ensure the NIC is present azure_rm_networkinterface : resource_group : Testing name : testnic001 virtual_network : myvn subnet : mysn public_ip_name : myip security_group : mysg - name : Ensure the Virtual Machine is present azure_rm_virtualmachine : resource_group : Testing name : myvm01 vm_size : Standard_D1 storage_account : mysa storage_container : myvm01 storage_blob : myvm01.vhd admin_username : admin admin_password : Password! network_interfaces : testnic001 image : offer : CentOS publisher : OpenLogic sku : '8.0' version : latest ansible-playbook azure.yaml","title":"Container and Cloud Management"},{"location":"learning/ansible/container_cloud/#container-and-cloud-management","text":"In the last few years, container-based workloads and cloud workloads have become more and more popular, and for this reason, we are going to look at how you can automate tasks related to those kinds of workloads with Ansible. First of all, even if you are in a very good place in your automation path and you have a lot of Ansible roles written for your infrastructure, you can't leverage them in Dockerfiles, so you would end up replicating your work to create containers. If this is not enough of a problem, this situation quickly deteriorates when you start considering cloud environments. All cloud environments have their own control planes and native automation languages, so in a very short time, you would find yourself rewriting the automation for the same operation over and over, thus wasting time and deteriorating the consistency of your environments.","title":"Container and Cloud Management"},{"location":"learning/ansible/container_cloud/#designing-and-building-containers-with-playbooks","text":"Ansible provides ansible-container so that you can create containers using the same components you would use for creating machines. The first thing you should do is ensure that you have ansible-container installed. sudo pip install ansible-container [ docker,k8s ] The ansible-container tool comes with three supported engines at the time of writing: docker: This is needed if you want to use it with Docker Engine (that is, on your local machine). k8s: This is needed if you want to use it with a Kubernetes cluster, both local (that is, MiniKube) or remote (that is, a production cluster). openshift: This is needed if you want to use it with an OpenShift cluster, both local (that is, MiniShift) or remote (that is, a production cluster). Follow these steps to build the container using playbooks Issuing the ansible-container init command ansible-container init Running this command will also create the following files: ansible.cfg: An empty file to be (eventually) used to override Ansible system configurations ansible-requirements.txt: An empty file to (eventually) list the Python requirements for the building process of your containers container.yml: A file that contains the Ansible code for the build meta.yml: A file that contains the metadata for Ansible Galaxy requirements.yml: An empty file to (eventually) list the Ansible roles that are required for your build Let's try building our own container using this tool \u2013 replace the contents of container.yml with the following version : \"2\" settings : conductor : base : centos:7 project_name : http-server services : web : from : \"centos:7\" roles : - geerlingguy.apache ports : - \"80:80\" command : - \"/usr/bin/dumb-init\" - \"/usr/sbin/apache2ctl\" - \"-D\" - \"FOREGROUND\" dev_overrides : environment : - \"DEBUG=1\" We can now run ansible-container build to initiate the build. At the end of the building process, we will have a container built with the geerlingguy.apache role applied to it. The ansible-container tool performs a multi-stage build capability, spinning up an Ansible container that is then used to build the real container. If we specified more than one role to be applied, the output would be an image with more layers, since Ansible will create a layer for every specified role. In this way, containers can easily be built using your existing Ansible roles rather than Dockerfiles.","title":"Designing and building containers with playbooks"},{"location":"learning/ansible/container_cloud/#managing-multiple-container-platforms","text":"To be able to call a deployment \"production-ready,\" you need to be able to demonstrate that the service your application is delivering will run reasonably, even in the case of a single application crash, as well as hardware failure. Often, you'll have even more reliability constraints from your customer. Today, the most successful one is Kubernetes due to its various distributions/versions, so we are going to focus on it primarily. The idea of Kubernetes is that you inform the Kubernetes Control Plane that you want X number of instances of your Y application, and Kubernetes will count how many instances of the Y application are running on the Kubernetes Nodes to ensure that the number of instances are X. If there are too few instances, Kubernetes will take care to start more instances, while if there are too many instances, the exceeding instances will be stopped. Due to the complexity of installing and managing Kubernetes, multiple companies have started to sell distributions of Kubernetes that simplify their operations and that they are willing to support.","title":"Managing multiple container platforms"},{"location":"learning/ansible/container_cloud/#deploying-to-kubernetes-with-ansible-container","text":"We will assume that you have access to either a Kubernetes cluster for testing. To deploy your application to your cluster, you need to change the container.yml file so that you can add some additional information. We will need to add a section called settings and a section called k8s_namespace to declare our deployment settings. k8s_namespace : name : http-server description : An HTTP server display_name : HTTP server We can proceed with the deployment- ansible-container --engine kubernetes deploy As soon as Ansible has completed its execution, you will be able to find the http-server deployment on your Kubernetes cluster. Based on the image that we built in the previous section and the additional information we added at the beginning of this section, Ansible is able to populate a deployment template and then deploy it using the k8s module.","title":"Deploying to Kubernetes with ansible-container"},{"location":"learning/ansible/container_cloud/#managing-kubernetes-objects-with-ansible","text":"You can do kubectl get namespaces with Ansible by creating a file called k8s-ns-show.yaml --- - hosts : localhost tasks : - name : Get information from K8s k8s_info : api_version : v1 kind : Namespace # Specify the k8s object Deployments, Services, Pods register : ns - name : Print info debug : var : ns ansible-playbook k8s-ns-show.yaml # Create a new namespace --- - hosts : localhost tasks : - name : Ensure the myns namespace exists k8s : api_version : v1 kind : Namespace name : myns state : present # Creates a new service --- - hosts : localhost tasks : - name : Ensure the Service mysvc is present k8s : state : present definition : apiVersion : v1 kind : Service metadata : name : mysvc namespace : myns spec : selector : app : myapp service : mysvc ports : - protocol : TCP targetPort : 800 name : port-80-tcp port : 80 Ansible allows you to manage your Kubernetes clusters with some modules: k8s: Allows you to manage any kind of Kubernetes object k8s_auth: Allows you to authenticate to Kubernetes clusters that require an explicit login step k8s_facts: Allows you to inspect Kubernetes objects k8s_scale: Allows you to set a new size for a Deployment, ReplicaSet, Replication Controller, or Job k8s_service: Allows you to manage Services on Kubernetes","title":"Managing Kubernetes Objects with Ansible"},{"location":"learning/ansible/container_cloud/#automating-docker-with-ansible","text":"With Ansible, you can easily manage your Docker instance in Development environments. First of all, we need to create a playbook called start-docker-container.yaml that will contain the following code - hosts : localhost tasks : - name : Start a container with a command docker_container : name : test-container image : alpine command : - echo - \"Hello, World!\" Other modules include the following: docker_config: Used to change the configurations of the Docker daemon docker_container_info: Used to gather information from (inspect) a container docker_network: Used to manage Docker networking configuration","title":"Automating Docker with Ansible"},{"location":"learning/ansible/container_cloud/#automating-against-amazon-web-services","text":"To be able to use Ansible to automate your Amazon Web Service estate, you'll need to install the boto library. pip install boto","title":"Automating against Amazon Web Services"},{"location":"learning/ansible/container_cloud/#authentication","text":"The boto library looks up the necessary credentials in the ~/.aws/credentials file. There are two different ways to ensure that the credentials file is configured properly. It is possible to use the AWS CLI tool. Alternatively, this can be done with a text editor of your choice by creating a file with the following structure [ default ] aws_access_key_id = [ YOUR_KEY_HERE ] aws_secret_access_key = [ YOUR_SECRET_ACCESS_KEY_HERE ] Now that you've created the file with the necessary credentials, boto will be able to work against your AWS environment. Since Ansible uses boto for every single communication with AWS systems, this means that Ansible will be appropriately configured, even without you have to change any Ansible-specific configuration.","title":"Authentication"},{"location":"learning/ansible/container_cloud/#creating-your-first-machine","text":"To launch a virtual machine in AWS, we need a few things to be in place, as follows: An SSH key pair A network A subnetwork A security group By default, a network and a subnetwork are already available in your accounts, but you need to retrieve their IDs. Create the aws.yaml Playbook with the following content - hosts : localhost tasks : - name : Ensure key pair is present ec2_key : name : fale key_material : \"{{ lookup('file', '~/.ssh/fale.pub') }}\" - name : Gather information of the EC2 VPC net in eu-west-1 ec2_vpc_net_facts : region : eu-west-1 register : aws_simple_net - name : Gather information of the EC2 VPC subnet in eu-west-1 ec2_vpc_subnet_facts : region : eu-west-1 filters : vpc-id : '{{ aws_simple_net.vpcs.0.id }}' register : aws_simple_subnet - name : Ensure wssg Security Group is present ec2_group : name : wssg description : Web Security Group region : eu-west-1 vpc_id : '{{ aws_simple_net.vpcs.0.id }}' rules : - proto : tcp from_port : 22 to_port : 22 cidr_ip : 0.0.0.0/0 - proto : tcp from_port : 80 to_port : 80 cidr_ip : 0.0.0.0/0 - proto : tcp from_port : 443 to_port : 443 cidr_ip : 0.0.0.0/0 rules_egress : - proto : all cidr_ip : 0.0.0.0/0 register : aws_simple_wssg - name : Setup instance ec2 : assign_public_ip : true image : ami-3548444c region : eu-west-1 exact_count : 1 key_name : fale count_tag : Name : ws01.ansible2cookbook.com instance_tags : Name : ws01.ansible2cookbook.coms instance_type : t2.micro group_id : '{{ aws_simple_wssg.group_id }}' vpc_subnet_id : '{{ aws_simple_subnet.subnets.0.id }}' volumes : - device_name : /dev/sda1 volume_type : gp2 volume_size : 10 delete_on_termination : True ansible-playbook aws.yaml We started by uploading the public part of an SSH keypair to AWS, then queried for information about the network and the subnetwork, then ensured that the Security Group we wanted to use was present, and lastly triggered the machine build.","title":"Creating your first machine"},{"location":"learning/ansible/container_cloud/#automating-against-azure","text":"To let Ansible manage the Azure cloud, you need to install the Azure SDK for Python. pip install 'ansible[azure]' ``` ## Authentication - There are different ways to ensure that Ansible is able to manage Azure for you, based on the way your Azure account is set up, but they can all be configured in the ` ~/.azure/credentials ` file. ``` BASH [ default ] subscription_id = [ YOUR_SUBSCIRPTION_ID_HERE ] client_id = [ YOUR_CLIENT_ID_HERE ] secret = [ YOUR_SECRET_HERE ] tenant = [ YOUR_TENANT_HERE ] If you prefer to use Active Directories with a username and password. [ default ] ad_user = [ YOUR_AD_USER_HERE ] password = [ YOUR_AD_PASSWORD_HERE ] You can opt for an Active Directory login with ADFS. [ default ] ad_user = [ YOUR_AD_USER_HERE ] password = [ YOUR_AD_PASSWORD_HERE ] client_id = [ YOUR_CLIENT_ID_HERE ] tenant = [ YOUR_TENANT_HERE ] adfs_authority_url = [ YOUR_ADFS_AUTHORITY_URL_HERE ]","title":"Automating against Azure"},{"location":"learning/ansible/container_cloud/#creating-your-first-machine_1","text":"Create the azure.yaml Playbook with the following content. In Azure, you will need all the resources to be ready before you can issue the machine creation command. This is the reason you create the Storage Account, the Virtual Network, the Subnet, the Public IP, the security Group, and the NIC first, and only at that point, the machine itself. - hosts : localhost tasks : - name : Ensure the Storage Account is present azure_rm_storageaccount : resource_group : Testing name : mysa account_type : Standard_LRS - name : Ensure the Virtual Network is present azure_rm_virtualnetwork : resource_group : Testing name : myvn address_prefixes : \"10.10.0.0/16\" - name : Ensure the Subnet is present azure_rm_subnet : resource_group : Testing name : mysn address_prefix : \"10.10.0.0/24\" virtual_network : myvn - name : Ensure that the Public IP is set azure_rm_publicipaddress : resource_group : Testing allocation_method : Static name : myip - name : Ensure a Security Group allowing SSH is present azure_rm_securitygroup : resource_group : Testing name : mysg rules : - name : SSH protocol : Tcp destination_port_range : 22 access : Allow priority : 101 direction : Inbound - name : Ensure the NIC is present azure_rm_networkinterface : resource_group : Testing name : testnic001 virtual_network : myvn subnet : mysn public_ip_name : myip security_group : mysg - name : Ensure the Virtual Machine is present azure_rm_virtualmachine : resource_group : Testing name : myvm01 vm_size : Standard_D1 storage_account : mysa storage_container : myvm01 storage_blob : myvm01.vhd admin_username : admin admin_password : Password! network_interfaces : testnic001 image : offer : CentOS publisher : OpenLogic sku : '8.0' version : latest ansible-playbook azure.yaml","title":"Creating your first machine"},{"location":"learning/ansible/security_basics/","text":"Introduction \u00b6 Ansible Security Dev-Sec Community Playbooks Ansible Security Automation Why Ansible for this setup? \u00b6 Ansible is made for security automation and hardening. It uses YAML syntax, which helps us to codify our entire process of repeated tasks. By using this, we can automate the process of continuous delivery and deployment of infrastructure using roles and playbooks. The modular approach enables us to perform tasks very simply. For example, the operations teams can write a playbook to set up a WordPress site and the security team can create another role which can harden the WordPress site. It is very easy to use the modules for repeatability, and the output is idempotent, which means creating standards for the servers, applications, and infrastructure. Some use cases include creating base images for organizations using internal policy standards. Ansible uses SSH protocol, which is by default secured with encrypted transmission and host encryption. Also, there are no dependency issues while dealing with different types of operating systems. It uses Python to perform; this can be easily extended, based on our use case. Setting up nginx web server \u00b6 We are adding the signing key, then adding the repository, then installing. This ensures that we can also perform integrity checks while downloading packages from the repositories. Hardening SSH service \u00b6 # Disabling the root user login, and instead creating a different user, and, if required, providing the sudo privilege - name : create new user user : name : \"{{ new_user_name }}\" password : \"{{ new_user_password }}\" shell : /bin/bash groups : sudo append : yes # Using key-based authentication to log in. Unlike with password-based authentication, we can generate SSH keys and add the public key to the authorized keys - name : add ssh key for new user authorized_key : user : \"{{ new_user_name }}\" key : \"{{ lookup('file', '/home/user/.ssh/id_rsa.pub') }}\" state : present # Some of the configuration tweaks using the SSH configuration file; for example, PermitRootLogin, PubkeyAuthentication, and PasswordAuthentication - name : ssh configuration tweaks lineinfile : dest : /etc/ssh/sshd_config state : present line : \"{{ item }}\" backups : yes with_items : - \"PermitRootLogin no\" - \"PasswordAuthentication no\" notify : - restart ssh - We can also set up services like fail2ban for protecting against basic attacks. - Also, we can enable MFA, if required to log in. Digitial Ocean - The following playbook will provide more advanced features for SSH hardening by dev-sec team Hardening nginx \u00b6 We can start looking at things like disabling server tokens to not display version information, adding headers like X-XSS-Protection, and many other configuration tweaks. Most of these changes are done via configuration changes, and Ansible allows us to version and control and automate these changes based on user requirements. The nginx server version information can be blocked by adding the server_tokens off; value to the configuration add_header X-XSS-Protection \"1; mode=block\"; will enable the cross-site scripting (XSS) filter SSLv3 can be disabled by adding ssl_protocols TLSv1 TLSv1.1 TLSv1.2; - name : update the hardened nginx configuration changes template : src : \"hardened-nginx-config.j2\" dest : \"/etc/nginx/sites-available/default\" notify : - restart nginx Mozilla runs an updated web page on guidance for SSL/TLS . The guidance offers an opinion on what cipher suites to use, and other security measures. Additionally, if you trust their judgment, you can also use their SSL/TLS configuration generator to quickly generate a configuration for your web server configuration . Whichever configuration you decide to use, the template needs to be named as hardened-nginx-config.j2 . Hardening WordPress \u00b6 This includes basic checks for WordPress security misconfigurations. Some of them include: # Directory and file permissions - name : update the file permissions file : path : \"{{ WordPress_install_directory }}\" recurse : yes owner : \"{{ new_user_name }}\" group : www-data - name : updating file and directory permissions shell : \"{{ item }}\" with_items : - \"find {{ WordPress_install_directory }} -type d -exec chmod 755 {} \\ ;\" - \"find {{ WordPress_install_directory }} -type f -exec chmod 644 {} \\ ;\" # Username and attachment enumeration blocking. The following code snippet is part of nginx's configuration # Username enumeration block if ($args ~ \"^/?author=([0-9]*)\"){ return 403; } # Attachment enumeration block if ($query_string ~ \"attachment_id=([0-9]*)\"){ return 403; } # Disallowing file edits in the WordPress editor - name : update the WordPress configuration lineinfile : path : /var/www/html/wp-config.php line : \"{{ item }}\" with_items : - define('FS_METHOD', 'direct'); - define('DISALLOW_FILE_EDIT', true); Hardening a database service \u00b6 We can harden the MySQL service by binding it to localhost and the required interfaces for interacting with the application. It then removes the anonymous user and test databases - name : delete anonymous mysql user for localhost mysql_user : user : \"\" state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : secure mysql root user mysql_user : user : \"root\" password : \"{{ mysql_root_password }}\" host : \"{{ item }}\" login_password : \"{{ mysql_root_password }}\" login_user : root with_items : - 127.0.0.1 - localhost - ::1 - \"{{ ansible_fqdn }}\" - name : removes mysql test database mysql_db : db : test state : absent login_password : \"{{ mysql_root_password }}\" login_user : root Hardening a host firewall service \u00b6 Ansible even has a module for UFW, so the following snippet starts with installing this and enabling logging. It follows this by adding default policies, like default denying all incoming and allowing outgoing. Then it will add SSH, HTTP, and HTTPS services to allow incoming. These options are completely configurable, as required. - name : installing ufw package apt : name : \"ufw\" update_cache : yes state : present - name : enable ufw logging ufw : logging : on - name : default ufw setting ufw : direction : \"{{ item.direction }}\" policy : \"{{ item.policy }}\" with_items : - { direction : 'incoming' , policy : 'deny' } - { direction : 'outgoing' , policy : 'allow' } - name : allow required ports to access server ufw : rule : \"{{ item.policy }}\" port : \"{{ item.port }}\" proto : \"{{ item.protocol }}\" with_items : - { port : \"22\" , protocol : \"tcp\" , policy : \"allow\" } - { port : \"80\" , protocol : \"tcp\" , policy : \"allow\" } - { port : \"443\" , protocol : \"tcp\" , policy : \"allow\" } - name : enable ufw ufw : state : enabled - name : restart ufw and add to start up programs service : name : ufw state : restarted enabled : yes Setting up automated encrypted backups in AWS S3 \u00b6 Backups are always something that most of us feel should be done, but they seem quite a chore. Over the years, people have done extensive work to ensure we can have simple enough ways to back up and restore our data. In today's day and age, a great backup solution/software should be able to do the following: Automated: Automation allows for process around it Incremental: While storage is cheap overall, if we want backups at five minute intervals, what has changed should be backed up Encrypted before it leaves our server: This is to ensure that we have security of data at rest and in motion Cheap: While we care about our data, a good back up solution will be much cheaper than the server which needs to be backed up For our backup solution, we will pick up the following stack: Software: Duply - A wrapper over duplicity, a Python script Storage: While duply offers many backends, it works really well with AWS S3 Encryption: By using GPG, we can use asymmetric public and private key pairs - name : installing duply apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-boto - duply - name : check if we already have backup directory stat : path : \"/root/.duply/{{ new_backup_name }}\" register : duply_dir_stats - name : create backup directories shell : duply {{ new_backup_name }} create when : duply_dir_stats.stat.exists == False - name : update the duply configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dest }}\" with_items : - { src : conf.j2 , dest : /root/.duply/ {{ new_backup_name }} /conf } - { src : exclude.j2 , dest : /root/.duply/ {{ new_backup_name }} /exclude } - name : create cron job for automated backups template : src : duply-backup.j2 dest : /etc/cron.hourly/duply-backup LAMP stack playbook \u00b6 The high-level hierarchy structure of the entire playbook: \u00b6 inventory # inventory file group_vars/ # all.yml # variables site.yml # master playbook (contains list of roles) roles/ # common/ # common role tasks/ # main.yml # installing basic tasks web/ # apache2 role tasks/ # main.yml # install apache templates/ # web.conf.j2 # apache2 custom configuration vars/ # main.yml # variables for web role handlers/ # main.yml # start apache2 php/ # php role tasks/ # main.yml # installing php and restart apache2 db/ # db role tasks/ # main.yml # install mysql and include harden.yml harden.yml # security hardening for mysql handlers/ # main.yml # start db and restart apache2 vars/ # main.yml # variables for db role Playbook Files \u00b6 Here is a very basic static inventory file where we will define a since host and set the IP address used to connect to it. Configure the following inventory file as required: [ lamp ] lampstack ansible_host=192.168.56.10 # group_vars/lamp.yml, which has the configuration of all the global variables remote_username : \"hodor\" # site.yml, which is the main playbook file to start - name : LAMP stack setup on Ubuntu 16.04 hosts : lamp gather_facts : False remote_user : \"{{ remote_username }}\" become : True roles : - common - web - db - php # roles/common/tasks/main.yml file, which will install python2, curl, and git # In ubuntu 16.04 by default there is no python2 - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : install curl and git apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - curl - git # roles/web/tasks/main.yml, performs multiple operations, such as installation and configuration of apache2. # It also adds the service to the startup process - name : install apache2 server apt : name : apache2 state : present - name : update the apache2 server configuration template : src : web.conf.j2 dest : /etc/apache2/sites-available/000-default.conf owner : root group : root mode : 0644 - name : enable apache2 on startup systemd : name : apache2 enabled : yes notify : - start apache2 # notify parameter will trigger the handlers found in roles/web/handlers/main.yml - name : start apache2 systemd : state : started name : apache2 - name : stop apache2 systemd : state : stopped name : apache2 - name : restart apache2 systemd : state : restarted name : apache2 daemon_reload : yes # The template files will be taken from role/web/templates/web.conf.j2, which uses Jinja templating, it also takes values from local variables <VirtualHost *:80><VirtualHost *:80> ServerAdmin {{server_admin_email}} DocumentRoot {{server_document_root}} ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined </VirtualHost> # The local variables file is located in roles/web/vars/main.yml server_admin_email : hodor@localhost.local server_document_root : /var/www/html # File roles/db/tasks/main.yml includes installation of the database server with assigned passwords when prompted. # At the end of the file, we included harden.yml, which executes another set of tasks - name : set mysql root password debconf : name : mysql-server question : mysql-server/root_password value : \"{{ mysql_root_password | quote }}\" vtype : password - name : confirm mysql root password debconf : name : mysql-server question : mysql-server/root_password_again value : \"{{ mysql_root_password | quote }}\" vtype : password - name : install mysqlserver apt : name : \"{{ item }}\" state : present with_items : - mysql-server - mysql-client - include : harden.yml # harden.yml performs hardening of MySQL server configuration - name : deletes anonymous mysql user mysql_user : user : \"\" state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : secures the mysql root user mysql_user : user : root password : \"{{ mysql_root_password }}\" host : \"{{ item }}\" login_password : \"{{mysql_root_password}}\" login_user : root with_items : - 127.0.0.1 - localhost - ::1 - \"{{ ansible_fqdn }}\" - name : removes the mysql test database mysql_db : db : test state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : enable mysql on startup systemd : name : mysql enabled : yes notify : - start mysql # db server role also has roles/db/handlers/main.yml and local variables similar to the web role - name : start mysql systemd : state : started name : mysql - name : stop mysql systemd : state : stopped name : mysql - name : restart mysql systemd : state : restarted name : mysql daemon_reload : yes # roles/db/vars/main.yml, which has the mysql_root_password while configuring the server. # we can secure these plaintext passwords using ansible-vault in future. mysql_root_password : R4nd0mP4$$w0rd # we will install PHP and configure it to work with apache2 by restarting the roles/php/tasks/main.yml service - name : install php7 apt : name : \"{{ item }}\" state : present with_items : - php7.0-mysql - php7.0-curl - php7.0-json - php7.0-cgi - php7.0 - libapache2-mod-php7 - name : restart apache2 systemd : state : restarted name : apache2 daemon_reload : yes # Execute the following command against the Ubuntu 16.04 server to set up LAMP stack. # Provide the password when it prompts for system access for user hodor. ansible-playbook -i inventory site.yml Setting up Ansible Tower \u00b6 # Make sure you have Vagrant installed in your host system before running the following command: vagrant init ansible/tower vagrant up vagrant ssh # It will prompt you to enter IP address, username, and password to login to the Ansible Tower dashboard. - Then navigate the browser to https://10.42.0.42 and accept the SSL error to proceed. - This SSL error can be fixed by providing the valid certificates in the configuration at /etc/tower and need to restart the Ansible Tower service. - Enter the login credentials to access the Ansible Tower dashboard. - Once you log in, it will prompt you for the Ansible Tower license. - Ansible Tower also provides Role-Based Authentication Control (RBAC) , which provides a granular level of control for different users and groups to manage Tower. - Create a new user with the System Administrator privilege - To add inventory into Ansible Tower, we can simply enter it manually - Add credentials (or) keys to the tower by providing them in credential management, which can be reused as well. - Secrets store in Ansible Tower are encrypted with a symmetric key unique to each Ansible Tower cluster. - Once stored in the Ansible Tower database, the credentials may only be used, not viewed, in the web interface. - The types of credentials that Ansible Tower can store are passwords, SSH keys, Ansible Vault keys, and cloud credentials. - Once we have the inventory gathered, we can create jobs to perform the playbook or ad-hoc command operations. - The Ansible Tower REST API is a pretty powerful way to interact with the system - Get started with the pip install ansible-tower-cli command. Setting up AWX \u00b6 Ansible is very powerful, but it does require the user to use the CLI. In some situations, this is not the best option, such as in cases where you need to trigger an Ansible job from another job (where APIs would be better) or in cases where the person that should trigger a job should only be able to trigger that specific job. For these cases, AWX or Ansible Tower are better options to use. The only differences between AWX and Ansible Tower are that AWX is the upstream and open source version, while Ansible Tower is the Red Hat and downstream product that is officially supported but for a price, and also the delivery method. We will use AWX and talk about AWX, but everything we discuss also applies to Ansible Tower. Although there are several ways to install AWX, we are going to use the suggested AWX installation, which is container-based. For this reason, the following software needs to be installed on your machine: Ansible 2.4+. Docker. The docker Python module. The docker-compose Python module. If your system uses Security-Enhanced Linux (SELinux), you also need the libselinux Python module. Installing AWX \u00b6 We need to clone the AWX Git repository. git clone https://github.com/ansible/awx.git Modify the installer/inventory file by setting sensible values for the passwords and secrets (such as pg_password, rabbitmq_password, admin_password, and secret_key) Now that we have downloaded the Ansible AWX code and installer, we can move into the installer folder and execute the installation by running the following code. cd awx/installer ansible-playbook -i inventory install.yml The install.yml playbook performs the whole installation for us. It starts by checking the environment for possible misconfigurations or missing dependencies. If everything seems to be correct, it moves on to downloading several Docker images (including PostgreSQL, memcached, RabbitMQ, AWX Web, and AWX workers) and then runs them all. As soon as the playbook completes, you can check the installation by issuing the docker ps command. As you can see from docker ps output, our system now has a container called awx_web, which has bound itself to port 80. You can now access AWX by browsing to http://<ip address of your AWX host>/ and using the credentials you specified in the inventory file earlier on and that the default administrator username is admin unless you change it in the inventory. Running your first playbook from AWX \u00b6 As in Ansible, in AWX, the goal is running an Ansible playbook and each playbook that is run is called a job. Since AWX gives you more flexibility and automation than Ansible, it requires a little bit more configuration before you can run your first job. Creating an AWX project \u00b6 AWX uses the term project to identify a repository of Ansible playbooks. AWX projects support the placement of playbooks in all major Source Control Management (SCM) systems, such as Git, Mercurial, and SVN, but also support playbooks on the filesystem or playbooks provided by Red Hat Insights. Projects are the system to store and use playbooks in AWX. As you can imagine, there are many interesting additional configurations for AWX projects\u2014and the most interesting one, in my view\u2014is update revision on launch. If flagged, this option instructs Ansible to always update the playbook's repository before running any playbook from that project. This ensures it always executes the latest version of the playbook. This is an important feature to enable as if you don't have it checked, there is the possibility (and sooner or later, this will happen in your environment) that someone notices that there is a problem in a playbook and fixes it, then they run the playbook feeling sure that they are running the latest version. They then forget to run the synchronization task before running the playbook, effectively running the older version of the playbook. This could lead to major problems if the previous version was fairly buggy. The downside of using this option is that every time you execute a playbook, two playbooks are effectively run, adding time to your task execution. I think this is a very small downside and one that does not offset the benefits of using this option. Creating an inventory \u00b6 As with Ansible Core, to make AWX aware of the machines present in your environment, we use inventories. Inventories, in the AWX world, are not that different from their equivalents in Ansible Core. Since an empty inventory is not useful in any way, we are going to add localhost to it. We then need to add the hostname (localhost) and instruct Ansible to use the local connection by adding the following code to the VARIABLES box. --- ansible_connection : local ansible_python_interpreter : '{{ ansible_playbook_python }}' Creating a job template \u00b6 A job template in AWX is a collection of the configurations that are needed to perform a job. This is very similar to the ansible-playbook command-line options. The reason why we need to create a job template is so that playbook runs can be launched with little or no user input, meaning they can be delegated to teams who might not know all the details of how a playbook works, or can even be run on a scheduled basis without anyone present. Note that because we are running it using the local connection to localhost, we don't need to create or specify any credentials. However, if you were running a job template against one or more remote hosts, you would need to create a machine credential and associate it with your job template. A machine credential is, for example, an SSH username and password or an SSH username and a private key\u2014these are stored securely in the backend database of AWX, meaning you can again delegate playbook-related tasks to other teams without actually giving them passwords or SSH keys. The first thing we had to choose was whether we are creating a job template or a workflow template. We chose Job Template since we want to be able to create simple jobs out of this template. It's also possible to create more complex jobs, which are the composition of multiple job templates, with flow control features between one job and the next. This allows more complex situations and scenarios where you might want to have multiple jobs (such as the creation of an instance, company customization, the setup of Oracle Database, the setup of a MySQL database, and so on), but you also want to have a one-click deployment that would, for instance, set up the machine, apply all the company customization, and install the MySQL database. Obviously, you might also have another deployment that uses all the same components except the last one and in its place, it uses the Oracle Database piece to create an Oracle Database machine. This allows you to have extreme flexibility and to reuse a lot of components, creating multiple, nearly identical playbooks. It's interesting to note that many fields in the Job Template creation window have an option with the Prompt on launch caption. This is to be able to set this value optionally during the creation of the job template, but also allow the user running the job to enter/override it at runtime. This can be incredibly valuable when you have a field that changes on each run (perhaps the limit field, which operates in the same way as \u2013limit when used with the ansible-playbook command) or can also be used as a sanity check, as it prompts the user with the value (and gives them a chance to modify it) before the playbook is actually run. However, it could potentially block scheduled job runs, so exercise caution when enabling this feature. Running a job \u00b6 A job is an instance of a job template. This means that to perform any action on our machine, we have to create a job template instance or, more simply, a job. One of the great things about AWX and Ansible Tower is that they archive this job execution output in the backend database, meaning you can, at any point in the future, come back and query a job run to see what changed and what happened. This is incredibly powerful and useful for occasions such as auditing and policy enforcement. Controlling access to AWX \u00b6 In my opinion, one of the biggest advantages of AWX compared to Ansible is the fact that AWX allows multiple users to connect and control/perform actions. This allows a company to have a single AWX installation for different teams, a whole organization, or even multiple organizations. A Role-Based Access Control (RBAC) system is in place to manage the users' permissions. Both AWX and Ansible Tower can link to central directories, such as Lightweight Directory Access Protocol (LDAP) and Azure Active Directory however, we can also create user accounts locally on the AWX server itself. Creating a user \u00b6 One of the big advantages of AWX is the ability to manage multiple users. This allows us to create a user in AWX for each person that is using the AWX system so that we can ensure they are only granted the permissions that they need. Also, by using individual accounts, we can ensure that we can see who carried out what action by using the audit logs. By adding the email address, the username, and the password (with confirmation), you can create the new user. Users can be of three types: A normal user: Users of this type do not have any inherited permissions and they need to be awarded specific permissions to be able to do anything. A system auditor: Users of this type have full read-only privileges on the whole AWX installation. A system administrator: Users of this type have full privileges on the whole AWX installation. Creating a team \u00b6 Although having individual user accounts is an incredibly powerful tool, especially for enterprise use cases, it would be incredibly inconvenient and cumbersome to have to set permissions for each object (such as a job template or an inventory) on an individual basis. Every time someone joins a team, their user account has to be manually configured with the correct permissions against every object and, similarly, be removed if they leave. AWX and Ansible Tower have the same concept of user grouping that you would find in most other RBAC systems. The only slight difference is that in the user interface, they are referred to as teams, rather than groups. However, you can create teams simply and easily and then add and remove users as you need to. Doing this through the user interface is very straightforward and you will find the process similar to the way that most RBAC systems handle user groups, so we won't go into any more specific details here. Once you have your teams set up, I recommend that you assign your permissions to teams, rather than through individual users, as this will make your management of AWX object permissions much easier as your organization grows. Creating an organization \u00b6 Sometimes, you have multiple independent groups of people that you need to manage independent machines. For those kinds of scenarios, the use of organizations can help you. An organization is basically a tenant of AWX, with its own unique user accounts, teams, projects, inventories, and job templates\u2014it's almost like having a separate instance of AWX! After you create the organization, you can assign any kind of resource to an organization, such as projects, templates, inventories, users, and so on. Organizations are a simple concept to grasp, but also powerful in terms of segregating roles and responsibilities in AWX. Assigning permissions in AWX \u00b6 Individual users (or the teams that they belong to) can be granted permissions on a per-object basis. So, for example, you could have a team of database administrators who only have access to see and execute playbooks on an inventory of database servers, using job templates that are specific to their role. Linux system administrators could then have access to the inventories, projects, and job templates that are specific to their role. AWX hides objects that users don't have the privileges to, which means the database administrators never see the Linux system administrator objects and vice versa. There are a number of different privilege levels that you can award users (or teams) with, which include the following: Admin: This is the organization-level equivalent of a system administrator. Execute: This kind of user can only execute templates that are part of the organization. Project admin: This kind of user can alter any project that is part of the organization. Inventory admin: This kind of user can alter any inventory that is part of the organization. Credential admin: This kind of user can alter any credential that is part of the organization. Workflow admin: This kind of user can alter any workflow that is part of the organization. Notification admin: This kind of user can alter any notification that is part of the organization. Job template admin: This kind of user can alter any job template that is part of the organization. Auditor: This is the organization-level equivalent to a system auditor. Member: This is the organization-level equivalent of a normal user. Read: This kind of user is able to view non-sensible objects that are part of the organization. AWX is a great addition to the power of Ansible in an enterprise setting and really helps ensure that your users can run Ansible playbooks in a manner that is well managed, secure, and auditable. Setting up Jenkins \u00b6 - name : installing jenkins in ubuntu 16.04 hosts : \"192.168.1.7\" remote_user : ubuntu gather_facts : False become : True tasks : - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : install curl and git apt : name={{ item }} state=present update_cache=yes with_items : - curl - git - name : adding jenkins gpg key apt_key : url : https://pkg.jenkins.io/debian/jenkins-ci.org.key state : present - name : jeknins repository to system apt_repository : repo : http://pkg.jenkins.io/debian-stable binary/ state : present - name : installing jenkins apt : name : jenkins state : present update_cache : yes - name : adding jenkins to startup service : name : jenkins state : started enabled : yes - name : printing jenkins default administration password command : cat /var/lib/jenkins/secrets/initialAdminPassword register : jenkins_default_admin_password - debug : msg : \"{{ jenkins_default_admin_password.stdout }}\" # To set up Jenkins, run the following command. Where 192.168.1.7 is the server IP address where Jenkins will be installed: ansible-playbook -i '192.168.1.7,' site.yml --ask-sudo-pass # we have to navigate to the Jenkins dashboard by browsing to http://192.168.1.7:8080 and providing the auto-generated password. # If the playbook runs without any errors, it will display the password at the end of the play. - Create the new user by filling in the details and confirming to log in to the Jenkins console. - We can install custom plugins in Jenkins, navigate to the Manage Jenkins tab, select Manage Plugins, then navigate to the Available tab. - In the Filter: enter the plugin name as Ansible. Then select the checkbox and click Install without restart. - Now we are ready to work with the Ansible plugin for Jenkins. - Create a new project in the main dashboard, give it a name, and select Freestyle project to proceed: - We can configure the build options, this is where Jenkins will give us more flexibility to define our own triggers, build instructions, and post build scripts - Once the build triggers based on an event, this can be sent to some artifact storage, it can also be available in the Jenkins build console output - This is a really very powerful way to perform dynamic operations such as triggering automated server and stacks setup based on a code push to the repository, as well as scheduled scans and automated reporting. Security automation use cases \u00b6 Here is a list of tasks that will prepare you to build layers of automation for the stuff that is important to you: Adding playbooks or connecting your source code management (SCM) tools, such as GitHub/GitLab/BitBucket Authentication and data security Logging output and managing reports for the automation jobs Job scheduling Alerting, notifications, and webhooks Ansible Tower configuration \u00b6 To add playbooks into Ansible Tower, we have to start by creating projects, then select the SCM TYPE as Manual We can choose the SCM TYPE set to Git and provide a github.com URL pointing to a playbook Git SCM to add playbooks into projects We can also change the PROJECTS_ROOT under CONFIGURE TOWER to change this location. The added playbooks are executed by creating a job template. Then we can schedule these jobs (or) we can launch directly. Jenkins Ansible integration configuration \u00b6 Jenkins supports SCM to use playbooks and local directories for manual playbooks too. This can be configured with the build options. Jenkins supports both ad-hoc commands and playbooks to trigger as a build (or) post-build action. We can also specify credentials if we want to access private repositories We can add the Playbook path by specifying the location of the playbook and defining inventory and variables as required Authentication and data security \u00b6 Some of the security features the tools offer include: RBAC (authentication and authorization) Web application over TLS/SSL (security for data in motion) Encryption for storing secrets (security for data at rest) RBAC for Ansible Tower \u00b6 Ansible Tower supports RBAC to manage multiple users with different permissions and roles. It also supports Lightweight Directory Access Protocol (LDAP) integration in the enterprise version to support Active Directory. This feature allows us to create different levels of users for accessing Ansible Tower. For example: The operations team requires a system administrator role to perform playbook execution and other activities like monitoring The security team requires a system auditor role to perform audit check for compliance standards such as Payment Card Industry Data Security Standard (PCI DSS) or even internal policy validation Normal users, such as team members, might just want to see how things are going, in the form of status updates and failure (or) success of job status TLS/SSL for Ansible Tower \u00b6 By default, Ansible Tower uses HTTPS using self-signed certificates at /etc/tower/tower.cert and /etc/tower/tower.key. Encryption and data security for Ansible Tower \u00b6 Ansible Tower has been created with built-in security for handling encryption of credentials that includes passwords and keys. It uses Ansible Vault to perform this operation. It encrypts passwords and key information in the database. TLS/SSL for Jenkins \u00b6 By default, Jenkins runs as plain old HTTP. To enable HTTPS, we can use a reverse proxy, such as Nginx, in front of Jenkins to serve as HTTPS. Digital Ocean Reference for TLS Encryption and data security for Jenkins \u00b6 We are using Jenkins' default credential feature. This will store the keys and passwords in the local filesystem. Output of the playbooks \u00b6 We would like to know where can we see the output of the playbooks executing and if any other logs that get created. Report management for Ansible Tower \u00b6 By default, Ansible Tower itself is a reporting platform for the status of the playbooks, job executions, and inventory collection. The Ansible Tower dashboard gives an overview of the total projects, inventories, hosts, and status of the jobs. The output can be consumed in the dashboard, standard out, or by using the REST API and we can get this via tower-cli command line tool as well, which is just a pre-built command line tool for interfacing with the REST API. Scheduling of jobs \u00b6 The scheduling of jobs is simple and straightforward in Ansible Tower. For a job, you can specify a schedule and the options are mostly like cron. For example, you can say that you have a daily scan template and would like it to be executed at 4 a.m. every day for the next three months. This kind of schedule makes our meta automation very flexible and powerful. Alerting, notifications, and webhooks \u00b6 Tower supports multiple ways of alerting and notifying users as per configuration. This can even be configured to make an HTTP POST request to a URL of your choice using a webhook. Ansible Tower notification using slack webhook is a popular option. Setting Up a Hardened WordPress with Encrypted Automated Backups \u00b6 Automating our server's patches is the most obvious, and possibly popular, requirement. We will apply security automation techniques and approaches to set up a hardened WordPress and enable encrypted backups. Everyone would agree that setting up a secure website and keeping it secured is a fairly common security requirement. And since it is so common, it would be useful for a lot of people who are tasked with building and managing websites to stay secure to look at that specific scenario. Note Are you aware that, according to Wikipedia, 27.5% of the top 10 million websites use WordPress? According to another statistic, 58.7% of all websites with known software on the entire web run WordPress. For us, setting up a hardened WordPress with encrypted automated backups can be broken down into the following steps: Setting up a Linux/Windows server with security measures in place. Setting up a web server (Apache/Nginx on Linux and IIS on Windows). Setting up a database server (MySQL) on the same host. Setting up WordPress using a command-line utility called WP-CLI. Setting up backup for the site files and the database which is incremental, encrypted, and most importantly, automated. Note We will assume that the server that we plan to deploy our WordPress website on is already up and running and we are able to connect to it. We will store the backup in an already configured AWS S3 bucket, for which the access key and secret access key is already provisioned. Refer the examples in Introduction to set this up. Secure automated the WordPress updates \u00b6 Run the backups and update WordPress core, themes, and plugins. This can be scheduled via an Ansible Tower job for every day - name : running backup using duply command : /etc/cron.hourly/duply-backup - name : updating WordPress core command : wp core update register : wp_core_update_output ignore_errors : yes - name : wp core update output debug : msg : \"{{ wp_core_update_output.stdout }}\" - name : updating WordPress themes command : wp theme update --all register : wp_theme_update_output ignore_errors : yes - name : wp themes update output debug : msg : \"{{ wp_theme_update_output.stdout }}\" - name : updating WordPress plugins command : wp plugin update --all register : wp_plugin_update_output ignore_errors : yes - name : wp plugins update output debug : msg : \"{{ wp_plugin_update_output.stdout }}\" Scheduling via Ansible Tower for daily updates \u00b6 Ansible Tower job scheduling for automated WordPress updates OR We can use the cron job template to perform this daily and add this template while deploying the WordPress setup #!/bin/bash /etc/cron.hourly/duply-backup wp core update wp theme update --all wp plugin update --all Setting up Apache2 web server \u00b6 Shows how we can use templating to perform configuration updates in the server - name : installing apache2 server apt : name : \"apache2\" update_cache : yes state : present - name : updating customized templates for apache2 configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" mode : 0644 with_tems : - { src : apache2.conf.j2 , dst : /etc/apache2/conf.d/apache2.conf } - { src : 000-default.conf.j2 , dst : /etc/apache2/sites-available/000-default.conf } - { src : default-ssl.conf.j2 , dst : /etc/apache2/sites-available/default-ssl.conf } - name : adding custom link for sites-enabled from sites-available file : src : \"{{ item.src }}\" dest : \"{{ item.dest }}\" state : link with_items : - { src : '/etc/apache2/sites-available/000-default.conf' , dest : '/etc/apache2/sites-enabled/000-default.conf' } - { src : '/etc/apache2/sites-available/default-ssl.conf' , dest : '/etc/apache2/sites-enabled/default-ssl.conf' } notify : - start apache2 - startup apache2 Enabling TLS/SSL with Let's Encrypt \u00b6 We can use a command-line tool offered by Let's Encrypt to get free SSL/TLS certificates in an open, automated manner. The tool is capable of reading and understanding an nginx virtual host file and generating the relevant certificates completely automatically, without any kind of manual intervention. - name : adding certbot ppa apt_repository : repo : \"ppa:certbot/certbot\" - name : install certbot apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-certbot-nginx - name : check if we have generated a cert already stat : path : \"/etc/letsencrypt/live/{{ website_domain_name }}/fullchain.pem\" register : cert_stats - name : run certbot to generate the certificates shell : \"certbot certonly --standalone -d {{ website_domain_name }} --email {{ service_admin_email }} --non-interactive --agree-tos\" when : cert_stats.stat.exists == False - name : configuring site files template : src : website.conf dest : \"/etc/nginx/sites-available/{{ website_domain_name }}\" - name : restart nginx service : name : nginx state : restarted Log Monitoring \u00b6 Log monitoring is the perfect place to think about security automation. For monitoring to be effective, a few things need to happen. We should be able to move logs from different devices to a central location. We should be able to make sense of what a regular log entry is and what could possibly be an attack. We should be able to store the logs, and also operate on them for things such as aggregation, normalization, and eventually, analysis. Traditional logging systems find it difficult to log for all applications, systems, and devices. The variety of time formats, log output formats, and so on, makes the task pretty complicated. The biggest roadblock is finding a way to be able to centralize logs. This gets in the way of being able to process log entries in real time, or near real time effectively. Some of the problematic points are as follows: Access is often difficult High expertise in mined data is required Logs can be difficult to find Log data is immense in size Introduction to Elastic Stack \u00b6 Elastic Stack is a group of open source products from the Elastic company. It takes data from any type of source and in any format and searches, analyzes, and visualizes that data in real time. It consists of four major components, as follows: Elasticsearch Logstash Kibana Beats It helps users/admins to collect, analyze, and visualize data in (near) real time. Each module fits based on your use case and environment. Elasticsearch \u00b6 Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected Main plus points of Elastic Stack: Distributed and highly available search engine, written in Java, and uses Groovy Built on top of Lucene Multi-tenant, with multi types and a set of APIs Document-oriented, providing (near) real-time search Logstash \u00b6 Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources, simultaneously transforms it, and then sends it to your favorite stash. Centralized data processing of all types of logs Consists of the following three main components: Input: Passing logs to process them into machine-understandable format Filter: A set of conditions to perform a specific action on an event Output: The decision maker for processed events/logs Kibana \u00b6 Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack, so you can do anything from learning why you're getting paged at 2:00 a.m. to understanding the impact rain might have on your quarterly numbers. Kibana's list of features: Powerful frontend dashboard is written in JavaScript Browser-based analytics and search dashboard for Elasticsearch A flexible analytics and visualization platform Provides data in the form of charts, graphs, counts, maps, and so on, in real time Beats \u00b6 Beats is the platform for single-purpose data shippers. They install as lightweight agents and send data from hundreds or thousands of machines to Logstash or Elasticsearch. Beats are: Lightweight shippers for Elasticsearch and Logstash Capture all sorts of operational data, like logs or network packet data They can send logs to either Elasticsearch or Logstash ElastAlert \u00b6 ElastAlert is a Python tool which also bundles with the different types of integrations to support with alerting and notifications. Some of them include Command, Email, JIRA, OpsGenie, AWS SNS, HipChat, Slack, Telegram, and so on. It also provides a modular approach to creating our own integrations. Why should we use Elastic Stack for security monitoring and alerting? \u00b6 The Elastic Stack solves most of the problems that we have discussed before, such as: Ability to store large amounts of data Ability to understand and read a variety of log formats Ability to ship the log information from a variety of devices in near real time to one central location A visualization dashboard for log analysis Prerequisites for setting up Elastic Stack \u00b6 - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : accepting oracle java license agreement debconf : name : 'oracle-java8-installer' question : 'shared/accepted-oracle-license-v1-1' value : 'true' vtype : 'select' - name : adding ppa repo for oracle java by webupd8team apt_repository : repo : 'ppa:webupd8team/java' state : present update_cache : yes - name : installing java nginx apache2-utils and git apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-software-properties - oracle-java8-installer - nginx - apache2-utils - python-pip - python-passlib Setting up the Elastic Stack \u00b6 The stack is a combination of: The Elasticsearch service The Logstash service The Kibana service The Beats service on all the devices We are going to set up Elasticsearch, Logstash, and Kibana on a single machine. This is the main log collection machine: It requires a minimum of 4 GB RAM, as we are using a single machine to serve three services (Elasticsearch, Logstash, and Kibana) It requires a minimum of 20 GB disk space, and, based on your log size, you can add the disk space Installing Elasticsearch \u00b6 Install Elasticsearch from the repository with gpg key and add it to the startup programs - name : adding elastic gpg key for elasticsearch apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing elasticsearch apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - elasticsearch - name : adding elasticsearch to the startup programs service : name : elasticsearch enabled : yes notify : - start elasticsearch Configure the Elasticsearch cluster with the required settings. Also, set up the JVM options for the Elasticsearch cluster. Also, create a backup directory for Elasticsearch cluster backups and snapshots - name : creating elasticsearch backup repo directory at {{ elasticsearch_backups_repo_path }} file : path : \"{{ elasticsearch_backups_repo_path }}\" state : directory mode : 0755 owner : elasticsearch group : elasticsearch - name : configuring elasticsearch.yml file template : src : \"{{ item.src }}\" dest : /etc/elasticsearch/\"{{ item.dst }}\" with_items : - { src : 'elasticsearch.yml.j2' , dst : 'elasticsearch.yml' } - { src : 'jvm.options.j2' , dst : 'jvm.options' } notify : - restart elasticsearch The notify part will trigger the restart elasticsearch handler and the handler file will look as follows. - name : start elasticsearch service : name : elasticsearch state : started - name : restart elasticsearch service : name : elasticsearch state : restarted Installing Logstash \u00b6 Install Logstash from the repository with gpg key and add it to the startup programs - name : adding elastic gpg key for logstash apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing logstash apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - logstash - name : adding logstash to the startup programs service : name : logstash enabled : yes notify : - start logstash Configure the Logstash service with input, output, and filter settings. This enables receiving logs, processing logs, and sending logs to the Elasticsearch cluster - name : logstash configuration files template : src : \"{{ item.src }}\" dest : /etc/logstash/conf.d/\"{{ item.dst }}\" with_items : - { src : '02-beats-input.conf.j2' , dst : '02-beats-input.conf' } - { src : '10-sshlog-filter.conf.j2' , dst : '10-sshlog-filter.conf' } - { src : '11-weblog-filter.conf.j2' , dst : '11-weblog-filter.conf' } - { src : '30-elasticsearch-output.conf.j2' , dst : '10-elasticsearch-output.conf' } notify : - restart logstash Logstash configuration \u00b6 To receive logs from different systems, we use the Beats service from Elastic. The following configuration is to receive logs from different servers to the Logstash server. Logstash runs on port 5044 and we can use SSL certificates to ensure logs are transferred via an encrypted channel. # 02-beats-input.conf.j2 input { beats { port => 5044 ssl => true ssl_certificate => \"/etc/pki/tls/certs/logstash-forwarder.crt\" ssl_key => \"/etc/pki/tls/private/logstash-forwarder.key\" } } The following configuration is to parse the system SSH service logs (auth.log) using grok filters. It also applies filters like geoip, while providing additional information like country, location, longitude, latitude, and so on. #10-sshlog-filter.conf.j2 filter { if [type] == \"sshlog\" { grok { match => [ \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : message repeated 2 times : \\[ %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } password for invalid user %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } % { WORD : auth_method } for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\" ] } date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] locale => en } geoip { source => \"ip\" } } } The following configuration is to parse web server logs (nginx, apache2). We will also apply filters for geoip and useragent. The useragent filter allows us to get information about the agent, OS type, version information, and so on. #11-weblog-filter.conf.j2 filter { if [type] == \"weblog\" { grok { match => { \"message\" => '%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}' } } date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] locale => en } geoip { source => \"clientip\" } useragent { source => \"agent\" target => \"useragent\" } } } The following configuration will send the log output into the Elasticsearch cluster with daily index formats. #30-elasticsearch-output.conf.j2 output { elasticsearch { hosts => [\"localhost:9200\"] manage_template => false index => \"%{[@metadata][beat]}-%{+YYYY.MM.dd}\" document_type => \"%{[@metadata][type]}\" } } Installing Kibana \u00b6 By default we are not making any changes in Kibana, as it works out of the box with Elasticsearch. - name : adding elastic gpg key for kibana apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing kibana apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - kibana - name : adding kibana to the startup programs service : name : kibana enabled : yes notify : - start kibana By default Kibana doesn't have any authentication, X-Pack is the commercial plug-in by Elastic for RBAC (role-based access control) with security. Also, some open source options include https://readonlyrest.com/ and Search Guard ( https://floragunn.com ) to interact with Elasticsearch. Using TLS/SSL and custom authentication and aauthorization is highly recommended. Some of the open source options includes Oauth2 Proxy ( bitly/oauth2_proxy ) and Auth0, and so on. Setting up nginx reverse proxy \u00b6 The following configuration is to enable basic authentication for Kibana using nginx reverse proxy. server { listen 80; server_name localhost; auth_basic \"Restricted Access\"; auth_basic_user_file /etc/nginx/htpasswd.users; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } Setting up and configuring the nginx service looks as follows. #command: htpasswd -c /etc/nginx/htpasswd.users - name : htpasswd generation htpasswd : path : \"/etc/nginx/htpasswd.users\" name : \"{{ basic_auth_username }}\" password : \"{{ basic_auth_password }}\" owner : root group : root mode : 0644 - name : nginx virtualhost configuration template : src : \"templates/nginxdefault.j2\" dest : \"/etc/nginx/sites-available/default\" notify : - restart nginx Installing Beats to send logs to Elastic Stack \u00b6 We are going to install Filebeat to send SSH and web server logs to the Elastic Stack: - name : adding elastic gpg key for filebeat apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing filebeat apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - apt-transport-https - filebeat - name : adding filebeat to the startup programs service : name : filebeat enabled : yes notify : - start filebeat Configure the Filebeat to send both SSH and web server logs to Elastic Stack, to process and index in near real-time. filebeat : prospectors : - paths : - /var/log/auth.log # - /var/log/syslog # - /var/log/*.log document_type : sshlog - paths : - /var/log/nginx/access.log document_type : weblog registry_file : /var/lib/filebeat/registry output : logstash : hosts : [ \"{{ logstash_server_ip }}:5044\" ] bulk_max_size : 1024 ssl : certificate_authorities : [ \"/etc/pki/tls/certs/logstash-forwarder.crt\" ] logging : files : rotateeverybytes : 10485760 # = 10MB ElastAlert for alerting \u00b6 First, we need to install the prerequisites for setting up ElastAlert. - name : installing pre requisuites for elastalert apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-pip - python-dev - libffi-dev - libssl-dev - python-setuptools - build-essential - name : installing elastalert pip : name : elastalert - name : creating elastalert directories file : path : \"{{ item }}\" state : directory mode : 0755 with_items : - /opt/elastalert/rules - /opt/elastalert/config - name : creating elastalert configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : 'elastalert-config.j2' , dst : '/opt/elastalert/config/config.yml' } - { src : 'elastalert-service.j2' , dst : '/lib/systemd/system/elastalert.service' } - { src : 'elastalert-sshrule.j2' , dst : '/opt/elastalert/rules/ssh-bruteforce.yml' } - name : enable elastalert service service : name : elastalert state : started enabled : yes Creating a simple startup script so that ElastAlert will be used as a system service. [ Unit ] Description = elastalert After = multi-user.target [ Service ] Type = simple WorkingDirectory = /opt/elastalert ExecStart = /usr/local/bin/elastalert --config /opt/elastalert/config/config.yml [ Install ] WantedBy = multi-user.target Configuring the Let's Encrypt service \u00b6 - name : adding certbot ppa apt_repository : repo : \"ppa:certbot/certbot\" - name : install certbot apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-certbot-nginx - name : check if we have generated a cert already stat : path : \"/etc/letsencrypt/live/{{ website_domain_name }}/fullchain.pem\" register : cert_stats - name : run certbot to generate the certificates shell : \"certbot certonly --standalone -d {{ website_domain_name }} --email {{ service_admin_email }} --non-interactive --agree-tos\" when : cert_stats.stat.exists == False - name : configuring site files template : src : website.conf dest : \"/etc/nginx/sites-available/{{ website_domain_name }}\" - name : restart nginx service : name : nginx state : restarted ElastAlert rule configuration \u00b6 Assuming that you already have Elastic Stack installed and logging SSH logs, use the following ElastAlert rule to trigger SSH attack IP blacklisting. es_host : localhost es_port : 9200 name : \"SSH Bruteforce attack alert\" type : frequency index : filebeat-* num_events : 20 timeframe : minutes : 1 # For more info: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.html filter : - query : query_string : query : '_type:sshlog AND login:failed AND (username: \"ubuntu\" OR username: \"root\")' alert : - slack : slack_webhook_url : \"https://hooks.slack.com/services/xxxxx\" slack_username_override : \"attack-bot\" slack_emoji_override : \"robot_face\" - command : [ \"/usr/bin/curl\" , \"https://xxxxxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/zzzzzzzzzzzzzz/ip/inframonitor/%(ip)s\" ] realert : minutes : 0 For more references, visit https://elastalert.readthedocs.io/en/latest/running_elastalert.html . Serverless Automated Defense \u00b6 If we can get a notification for an attack, we can set up and do the following: Call an AWS Lambda function Send the attacker's IP address information to this AWS Lambda function endpoint Use the code deployed in the Lambda function to call the VPC network access list API and block the attacker's IP address To ensure that we don't fill up the ACLs with attacker IPs, we can combine this approach with AWS DynamoDB to store this information for a short duration and remove it from the block list. As soon as an attack is detected, the alerter sends the IP to the blacklist lambda endpoint via an HTTPS request. The IP is blocked using the network ACL and the record of it is maintained in DynamoDB. If the IP is currently blocked already, then the expiry time for the rule will be extended in the DynamoDB. An expiry handler function is periodically triggered, which removes expired rules from DynamoDB and ACL accordingly. Setup \u00b6 The setup involves the following steps: Obtain IAM credentials Create a table in DynamoDB Configure the lambda function based on requirement Deploy code to AWS Lambda Configure Cloudwatch to periodic invocation The entire setup is automated, except for obtaining the IAM credentials and configuring the function based on requirements. Configuration \u00b6 The following parameters are configurable before deployment: region: AWS region to deploy in. This needs to be the same as the region where the VPC network resides. accessToken: The accessToken that will be used to authenticate the requests to the blacklist endpoint. aclLimit: The maximum number of rules an ACL can handle. The maximum limit in AWS is 20 by default. ruleStartId: The starting ID for rules in the ACL. aclID: The ACL ID of the network where the rules will be applied. tableName: The unique table name in DynamoDB, created for each VPC to be defended. ruleValidity: The duration for which a rule is valid, after which the IP will be unblocked. // Configure the following in the config.js file module . exports = { region : \"us-east-1\" , // AWS Region to deploy in accessToken : \"YOUR_R4NDOM_S3CR3T_ACCESS_TOKEN_GOES_HERE\" , // Accesstoken to make requests to blacklist aclLimit : 20 , // Maximum number of acl rules ruleStartId : 10 , // Starting id for acl entries aclId : \"YOUR_ACL_ID\" , // AclId that you want to be managed tableName : \"blacklist_ip\" , // DynamoDB table that will be created ruleValidity : 5 // Validity of Blacklist rule in minutes } Make sure to modify at least the aclId, accessToken, and region based on your setup. To modify the lambda deployment configuration use the serverless.yml file ... functions : blacklist : handler : handler.blacklistip events : - http : path : blacklistip method : get handleexpiry : handler : handler.handleexpiry events : - schedule : rate(1 minute) ... For example, the rate at which the expiry function is triggered and the endpoint URL for the blacklist function can be modified using the YML file. But the defaults are already optimal. # The playbook looks as follows: - name : installing node run time and npm apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - nodejs - npm - name : installing serverless package npm : name : \"{{ item }}\" global : yes state : present with_items : - serverless - aws-sdk - name : copy the setup files template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : 'config.js.j2' , dst : '/opt/serverless/config.js' } - { src : 'handler.js.j2' , dst : '/opt/serverless/handler.js' } - { src : 'iamRoleStatements.json.j2' , dst : '/opt/serverless/iamRoleStatements.json' } - { src : 'initDb.js.j2' , dst : '/opt/serverless/initDb.js' } - { src : 'serverless.yml.j2' , dst : '/opt/serverless/serverless.yml' } - { src : 'aws-credentials.j2' , dst : '~/.aws/credentials' } - name : create dynamo db table command : node initDb.js args : chdir : /opt/serverless/ - name : deploy the serverless command : serverless deploy args : chdir : /opt/serverless/ The current setup for AWS Lambda is to block the IP address against network ACL. This can be reused with other API endpoints, like a firewall dynamic block list and other security devices. The blacklist endpoint is responsible for blocking an IP address. The URL looks like the following: https://lambda_url/blacklistipaccessToken=ACCESS_TOKEN&ip=IP_ADDRESS The query parameters are as follows: IP_ADDRESS: This is the IP address to be blocked ACCESS_TOKEN: The accessToken to authenticate the request Automated defense lambda in action \u00b6 When the ElastAlert detects an SSH brute force attack, it will trigger a request to lambda endpoint by providing the attacker's IP address. Then our automated defense platform will trigger a network ACL blocklist rule. This can be configurable to say for how much time it should be blocked.","title":"Introduction"},{"location":"learning/ansible/security_basics/#introduction","text":"Ansible Security Dev-Sec Community Playbooks Ansible Security Automation","title":"Introduction"},{"location":"learning/ansible/security_basics/#why-ansible-for-this-setup","text":"Ansible is made for security automation and hardening. It uses YAML syntax, which helps us to codify our entire process of repeated tasks. By using this, we can automate the process of continuous delivery and deployment of infrastructure using roles and playbooks. The modular approach enables us to perform tasks very simply. For example, the operations teams can write a playbook to set up a WordPress site and the security team can create another role which can harden the WordPress site. It is very easy to use the modules for repeatability, and the output is idempotent, which means creating standards for the servers, applications, and infrastructure. Some use cases include creating base images for organizations using internal policy standards. Ansible uses SSH protocol, which is by default secured with encrypted transmission and host encryption. Also, there are no dependency issues while dealing with different types of operating systems. It uses Python to perform; this can be easily extended, based on our use case.","title":"Why Ansible for this setup?"},{"location":"learning/ansible/security_basics/#setting-up-nginx-web-server","text":"We are adding the signing key, then adding the repository, then installing. This ensures that we can also perform integrity checks while downloading packages from the repositories.","title":"Setting up nginx web server"},{"location":"learning/ansible/security_basics/#hardening-ssh-service","text":"# Disabling the root user login, and instead creating a different user, and, if required, providing the sudo privilege - name : create new user user : name : \"{{ new_user_name }}\" password : \"{{ new_user_password }}\" shell : /bin/bash groups : sudo append : yes # Using key-based authentication to log in. Unlike with password-based authentication, we can generate SSH keys and add the public key to the authorized keys - name : add ssh key for new user authorized_key : user : \"{{ new_user_name }}\" key : \"{{ lookup('file', '/home/user/.ssh/id_rsa.pub') }}\" state : present # Some of the configuration tweaks using the SSH configuration file; for example, PermitRootLogin, PubkeyAuthentication, and PasswordAuthentication - name : ssh configuration tweaks lineinfile : dest : /etc/ssh/sshd_config state : present line : \"{{ item }}\" backups : yes with_items : - \"PermitRootLogin no\" - \"PasswordAuthentication no\" notify : - restart ssh - We can also set up services like fail2ban for protecting against basic attacks. - Also, we can enable MFA, if required to log in. Digitial Ocean - The following playbook will provide more advanced features for SSH hardening by dev-sec team","title":"Hardening SSH service"},{"location":"learning/ansible/security_basics/#hardening-nginx","text":"We can start looking at things like disabling server tokens to not display version information, adding headers like X-XSS-Protection, and many other configuration tweaks. Most of these changes are done via configuration changes, and Ansible allows us to version and control and automate these changes based on user requirements. The nginx server version information can be blocked by adding the server_tokens off; value to the configuration add_header X-XSS-Protection \"1; mode=block\"; will enable the cross-site scripting (XSS) filter SSLv3 can be disabled by adding ssl_protocols TLSv1 TLSv1.1 TLSv1.2; - name : update the hardened nginx configuration changes template : src : \"hardened-nginx-config.j2\" dest : \"/etc/nginx/sites-available/default\" notify : - restart nginx Mozilla runs an updated web page on guidance for SSL/TLS . The guidance offers an opinion on what cipher suites to use, and other security measures. Additionally, if you trust their judgment, you can also use their SSL/TLS configuration generator to quickly generate a configuration for your web server configuration . Whichever configuration you decide to use, the template needs to be named as hardened-nginx-config.j2 .","title":"Hardening nginx"},{"location":"learning/ansible/security_basics/#hardening-wordpress","text":"This includes basic checks for WordPress security misconfigurations. Some of them include: # Directory and file permissions - name : update the file permissions file : path : \"{{ WordPress_install_directory }}\" recurse : yes owner : \"{{ new_user_name }}\" group : www-data - name : updating file and directory permissions shell : \"{{ item }}\" with_items : - \"find {{ WordPress_install_directory }} -type d -exec chmod 755 {} \\ ;\" - \"find {{ WordPress_install_directory }} -type f -exec chmod 644 {} \\ ;\" # Username and attachment enumeration blocking. The following code snippet is part of nginx's configuration # Username enumeration block if ($args ~ \"^/?author=([0-9]*)\"){ return 403; } # Attachment enumeration block if ($query_string ~ \"attachment_id=([0-9]*)\"){ return 403; } # Disallowing file edits in the WordPress editor - name : update the WordPress configuration lineinfile : path : /var/www/html/wp-config.php line : \"{{ item }}\" with_items : - define('FS_METHOD', 'direct'); - define('DISALLOW_FILE_EDIT', true);","title":"Hardening WordPress"},{"location":"learning/ansible/security_basics/#hardening-a-database-service","text":"We can harden the MySQL service by binding it to localhost and the required interfaces for interacting with the application. It then removes the anonymous user and test databases - name : delete anonymous mysql user for localhost mysql_user : user : \"\" state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : secure mysql root user mysql_user : user : \"root\" password : \"{{ mysql_root_password }}\" host : \"{{ item }}\" login_password : \"{{ mysql_root_password }}\" login_user : root with_items : - 127.0.0.1 - localhost - ::1 - \"{{ ansible_fqdn }}\" - name : removes mysql test database mysql_db : db : test state : absent login_password : \"{{ mysql_root_password }}\" login_user : root","title":"Hardening a database service"},{"location":"learning/ansible/security_basics/#hardening-a-host-firewall-service","text":"Ansible even has a module for UFW, so the following snippet starts with installing this and enabling logging. It follows this by adding default policies, like default denying all incoming and allowing outgoing. Then it will add SSH, HTTP, and HTTPS services to allow incoming. These options are completely configurable, as required. - name : installing ufw package apt : name : \"ufw\" update_cache : yes state : present - name : enable ufw logging ufw : logging : on - name : default ufw setting ufw : direction : \"{{ item.direction }}\" policy : \"{{ item.policy }}\" with_items : - { direction : 'incoming' , policy : 'deny' } - { direction : 'outgoing' , policy : 'allow' } - name : allow required ports to access server ufw : rule : \"{{ item.policy }}\" port : \"{{ item.port }}\" proto : \"{{ item.protocol }}\" with_items : - { port : \"22\" , protocol : \"tcp\" , policy : \"allow\" } - { port : \"80\" , protocol : \"tcp\" , policy : \"allow\" } - { port : \"443\" , protocol : \"tcp\" , policy : \"allow\" } - name : enable ufw ufw : state : enabled - name : restart ufw and add to start up programs service : name : ufw state : restarted enabled : yes","title":"Hardening a host firewall service"},{"location":"learning/ansible/security_basics/#setting-up-automated-encrypted-backups-in-aws-s3","text":"Backups are always something that most of us feel should be done, but they seem quite a chore. Over the years, people have done extensive work to ensure we can have simple enough ways to back up and restore our data. In today's day and age, a great backup solution/software should be able to do the following: Automated: Automation allows for process around it Incremental: While storage is cheap overall, if we want backups at five minute intervals, what has changed should be backed up Encrypted before it leaves our server: This is to ensure that we have security of data at rest and in motion Cheap: While we care about our data, a good back up solution will be much cheaper than the server which needs to be backed up For our backup solution, we will pick up the following stack: Software: Duply - A wrapper over duplicity, a Python script Storage: While duply offers many backends, it works really well with AWS S3 Encryption: By using GPG, we can use asymmetric public and private key pairs - name : installing duply apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-boto - duply - name : check if we already have backup directory stat : path : \"/root/.duply/{{ new_backup_name }}\" register : duply_dir_stats - name : create backup directories shell : duply {{ new_backup_name }} create when : duply_dir_stats.stat.exists == False - name : update the duply configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dest }}\" with_items : - { src : conf.j2 , dest : /root/.duply/ {{ new_backup_name }} /conf } - { src : exclude.j2 , dest : /root/.duply/ {{ new_backup_name }} /exclude } - name : create cron job for automated backups template : src : duply-backup.j2 dest : /etc/cron.hourly/duply-backup","title":"Setting up automated encrypted backups in AWS S3"},{"location":"learning/ansible/security_basics/#lamp-stack-playbook","text":"","title":"LAMP stack playbook"},{"location":"learning/ansible/security_basics/#the-high-level-hierarchy-structure-of-the-entire-playbook","text":"inventory # inventory file group_vars/ # all.yml # variables site.yml # master playbook (contains list of roles) roles/ # common/ # common role tasks/ # main.yml # installing basic tasks web/ # apache2 role tasks/ # main.yml # install apache templates/ # web.conf.j2 # apache2 custom configuration vars/ # main.yml # variables for web role handlers/ # main.yml # start apache2 php/ # php role tasks/ # main.yml # installing php and restart apache2 db/ # db role tasks/ # main.yml # install mysql and include harden.yml harden.yml # security hardening for mysql handlers/ # main.yml # start db and restart apache2 vars/ # main.yml # variables for db role","title":"The high-level hierarchy structure of the entire playbook:"},{"location":"learning/ansible/security_basics/#playbook-files","text":"Here is a very basic static inventory file where we will define a since host and set the IP address used to connect to it. Configure the following inventory file as required: [ lamp ] lampstack ansible_host=192.168.56.10 # group_vars/lamp.yml, which has the configuration of all the global variables remote_username : \"hodor\" # site.yml, which is the main playbook file to start - name : LAMP stack setup on Ubuntu 16.04 hosts : lamp gather_facts : False remote_user : \"{{ remote_username }}\" become : True roles : - common - web - db - php # roles/common/tasks/main.yml file, which will install python2, curl, and git # In ubuntu 16.04 by default there is no python2 - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : install curl and git apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - curl - git # roles/web/tasks/main.yml, performs multiple operations, such as installation and configuration of apache2. # It also adds the service to the startup process - name : install apache2 server apt : name : apache2 state : present - name : update the apache2 server configuration template : src : web.conf.j2 dest : /etc/apache2/sites-available/000-default.conf owner : root group : root mode : 0644 - name : enable apache2 on startup systemd : name : apache2 enabled : yes notify : - start apache2 # notify parameter will trigger the handlers found in roles/web/handlers/main.yml - name : start apache2 systemd : state : started name : apache2 - name : stop apache2 systemd : state : stopped name : apache2 - name : restart apache2 systemd : state : restarted name : apache2 daemon_reload : yes # The template files will be taken from role/web/templates/web.conf.j2, which uses Jinja templating, it also takes values from local variables <VirtualHost *:80><VirtualHost *:80> ServerAdmin {{server_admin_email}} DocumentRoot {{server_document_root}} ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined </VirtualHost> # The local variables file is located in roles/web/vars/main.yml server_admin_email : hodor@localhost.local server_document_root : /var/www/html # File roles/db/tasks/main.yml includes installation of the database server with assigned passwords when prompted. # At the end of the file, we included harden.yml, which executes another set of tasks - name : set mysql root password debconf : name : mysql-server question : mysql-server/root_password value : \"{{ mysql_root_password | quote }}\" vtype : password - name : confirm mysql root password debconf : name : mysql-server question : mysql-server/root_password_again value : \"{{ mysql_root_password | quote }}\" vtype : password - name : install mysqlserver apt : name : \"{{ item }}\" state : present with_items : - mysql-server - mysql-client - include : harden.yml # harden.yml performs hardening of MySQL server configuration - name : deletes anonymous mysql user mysql_user : user : \"\" state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : secures the mysql root user mysql_user : user : root password : \"{{ mysql_root_password }}\" host : \"{{ item }}\" login_password : \"{{mysql_root_password}}\" login_user : root with_items : - 127.0.0.1 - localhost - ::1 - \"{{ ansible_fqdn }}\" - name : removes the mysql test database mysql_db : db : test state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : enable mysql on startup systemd : name : mysql enabled : yes notify : - start mysql # db server role also has roles/db/handlers/main.yml and local variables similar to the web role - name : start mysql systemd : state : started name : mysql - name : stop mysql systemd : state : stopped name : mysql - name : restart mysql systemd : state : restarted name : mysql daemon_reload : yes # roles/db/vars/main.yml, which has the mysql_root_password while configuring the server. # we can secure these plaintext passwords using ansible-vault in future. mysql_root_password : R4nd0mP4$$w0rd # we will install PHP and configure it to work with apache2 by restarting the roles/php/tasks/main.yml service - name : install php7 apt : name : \"{{ item }}\" state : present with_items : - php7.0-mysql - php7.0-curl - php7.0-json - php7.0-cgi - php7.0 - libapache2-mod-php7 - name : restart apache2 systemd : state : restarted name : apache2 daemon_reload : yes # Execute the following command against the Ubuntu 16.04 server to set up LAMP stack. # Provide the password when it prompts for system access for user hodor. ansible-playbook -i inventory site.yml","title":"Playbook Files"},{"location":"learning/ansible/security_basics/#setting-up-ansible-tower","text":"# Make sure you have Vagrant installed in your host system before running the following command: vagrant init ansible/tower vagrant up vagrant ssh # It will prompt you to enter IP address, username, and password to login to the Ansible Tower dashboard. - Then navigate the browser to https://10.42.0.42 and accept the SSL error to proceed. - This SSL error can be fixed by providing the valid certificates in the configuration at /etc/tower and need to restart the Ansible Tower service. - Enter the login credentials to access the Ansible Tower dashboard. - Once you log in, it will prompt you for the Ansible Tower license. - Ansible Tower also provides Role-Based Authentication Control (RBAC) , which provides a granular level of control for different users and groups to manage Tower. - Create a new user with the System Administrator privilege - To add inventory into Ansible Tower, we can simply enter it manually - Add credentials (or) keys to the tower by providing them in credential management, which can be reused as well. - Secrets store in Ansible Tower are encrypted with a symmetric key unique to each Ansible Tower cluster. - Once stored in the Ansible Tower database, the credentials may only be used, not viewed, in the web interface. - The types of credentials that Ansible Tower can store are passwords, SSH keys, Ansible Vault keys, and cloud credentials. - Once we have the inventory gathered, we can create jobs to perform the playbook or ad-hoc command operations. - The Ansible Tower REST API is a pretty powerful way to interact with the system - Get started with the pip install ansible-tower-cli command.","title":"Setting up Ansible Tower"},{"location":"learning/ansible/security_basics/#setting-up-awx","text":"Ansible is very powerful, but it does require the user to use the CLI. In some situations, this is not the best option, such as in cases where you need to trigger an Ansible job from another job (where APIs would be better) or in cases where the person that should trigger a job should only be able to trigger that specific job. For these cases, AWX or Ansible Tower are better options to use. The only differences between AWX and Ansible Tower are that AWX is the upstream and open source version, while Ansible Tower is the Red Hat and downstream product that is officially supported but for a price, and also the delivery method. We will use AWX and talk about AWX, but everything we discuss also applies to Ansible Tower. Although there are several ways to install AWX, we are going to use the suggested AWX installation, which is container-based. For this reason, the following software needs to be installed on your machine: Ansible 2.4+. Docker. The docker Python module. The docker-compose Python module. If your system uses Security-Enhanced Linux (SELinux), you also need the libselinux Python module.","title":"Setting up AWX"},{"location":"learning/ansible/security_basics/#installing-awx","text":"We need to clone the AWX Git repository. git clone https://github.com/ansible/awx.git Modify the installer/inventory file by setting sensible values for the passwords and secrets (such as pg_password, rabbitmq_password, admin_password, and secret_key) Now that we have downloaded the Ansible AWX code and installer, we can move into the installer folder and execute the installation by running the following code. cd awx/installer ansible-playbook -i inventory install.yml The install.yml playbook performs the whole installation for us. It starts by checking the environment for possible misconfigurations or missing dependencies. If everything seems to be correct, it moves on to downloading several Docker images (including PostgreSQL, memcached, RabbitMQ, AWX Web, and AWX workers) and then runs them all. As soon as the playbook completes, you can check the installation by issuing the docker ps command. As you can see from docker ps output, our system now has a container called awx_web, which has bound itself to port 80. You can now access AWX by browsing to http://<ip address of your AWX host>/ and using the credentials you specified in the inventory file earlier on and that the default administrator username is admin unless you change it in the inventory.","title":"Installing AWX"},{"location":"learning/ansible/security_basics/#running-your-first-playbook-from-awx","text":"As in Ansible, in AWX, the goal is running an Ansible playbook and each playbook that is run is called a job. Since AWX gives you more flexibility and automation than Ansible, it requires a little bit more configuration before you can run your first job.","title":"Running your first playbook from AWX"},{"location":"learning/ansible/security_basics/#creating-an-awx-project","text":"AWX uses the term project to identify a repository of Ansible playbooks. AWX projects support the placement of playbooks in all major Source Control Management (SCM) systems, such as Git, Mercurial, and SVN, but also support playbooks on the filesystem or playbooks provided by Red Hat Insights. Projects are the system to store and use playbooks in AWX. As you can imagine, there are many interesting additional configurations for AWX projects\u2014and the most interesting one, in my view\u2014is update revision on launch. If flagged, this option instructs Ansible to always update the playbook's repository before running any playbook from that project. This ensures it always executes the latest version of the playbook. This is an important feature to enable as if you don't have it checked, there is the possibility (and sooner or later, this will happen in your environment) that someone notices that there is a problem in a playbook and fixes it, then they run the playbook feeling sure that they are running the latest version. They then forget to run the synchronization task before running the playbook, effectively running the older version of the playbook. This could lead to major problems if the previous version was fairly buggy. The downside of using this option is that every time you execute a playbook, two playbooks are effectively run, adding time to your task execution. I think this is a very small downside and one that does not offset the benefits of using this option.","title":"Creating an AWX project"},{"location":"learning/ansible/security_basics/#creating-an-inventory","text":"As with Ansible Core, to make AWX aware of the machines present in your environment, we use inventories. Inventories, in the AWX world, are not that different from their equivalents in Ansible Core. Since an empty inventory is not useful in any way, we are going to add localhost to it. We then need to add the hostname (localhost) and instruct Ansible to use the local connection by adding the following code to the VARIABLES box. --- ansible_connection : local ansible_python_interpreter : '{{ ansible_playbook_python }}'","title":"Creating an inventory"},{"location":"learning/ansible/security_basics/#creating-a-job-template","text":"A job template in AWX is a collection of the configurations that are needed to perform a job. This is very similar to the ansible-playbook command-line options. The reason why we need to create a job template is so that playbook runs can be launched with little or no user input, meaning they can be delegated to teams who might not know all the details of how a playbook works, or can even be run on a scheduled basis without anyone present. Note that because we are running it using the local connection to localhost, we don't need to create or specify any credentials. However, if you were running a job template against one or more remote hosts, you would need to create a machine credential and associate it with your job template. A machine credential is, for example, an SSH username and password or an SSH username and a private key\u2014these are stored securely in the backend database of AWX, meaning you can again delegate playbook-related tasks to other teams without actually giving them passwords or SSH keys. The first thing we had to choose was whether we are creating a job template or a workflow template. We chose Job Template since we want to be able to create simple jobs out of this template. It's also possible to create more complex jobs, which are the composition of multiple job templates, with flow control features between one job and the next. This allows more complex situations and scenarios where you might want to have multiple jobs (such as the creation of an instance, company customization, the setup of Oracle Database, the setup of a MySQL database, and so on), but you also want to have a one-click deployment that would, for instance, set up the machine, apply all the company customization, and install the MySQL database. Obviously, you might also have another deployment that uses all the same components except the last one and in its place, it uses the Oracle Database piece to create an Oracle Database machine. This allows you to have extreme flexibility and to reuse a lot of components, creating multiple, nearly identical playbooks. It's interesting to note that many fields in the Job Template creation window have an option with the Prompt on launch caption. This is to be able to set this value optionally during the creation of the job template, but also allow the user running the job to enter/override it at runtime. This can be incredibly valuable when you have a field that changes on each run (perhaps the limit field, which operates in the same way as \u2013limit when used with the ansible-playbook command) or can also be used as a sanity check, as it prompts the user with the value (and gives them a chance to modify it) before the playbook is actually run. However, it could potentially block scheduled job runs, so exercise caution when enabling this feature.","title":"Creating a job template"},{"location":"learning/ansible/security_basics/#running-a-job","text":"A job is an instance of a job template. This means that to perform any action on our machine, we have to create a job template instance or, more simply, a job. One of the great things about AWX and Ansible Tower is that they archive this job execution output in the backend database, meaning you can, at any point in the future, come back and query a job run to see what changed and what happened. This is incredibly powerful and useful for occasions such as auditing and policy enforcement.","title":"Running a job"},{"location":"learning/ansible/security_basics/#controlling-access-to-awx","text":"In my opinion, one of the biggest advantages of AWX compared to Ansible is the fact that AWX allows multiple users to connect and control/perform actions. This allows a company to have a single AWX installation for different teams, a whole organization, or even multiple organizations. A Role-Based Access Control (RBAC) system is in place to manage the users' permissions. Both AWX and Ansible Tower can link to central directories, such as Lightweight Directory Access Protocol (LDAP) and Azure Active Directory however, we can also create user accounts locally on the AWX server itself.","title":"Controlling access to AWX"},{"location":"learning/ansible/security_basics/#creating-a-user","text":"One of the big advantages of AWX is the ability to manage multiple users. This allows us to create a user in AWX for each person that is using the AWX system so that we can ensure they are only granted the permissions that they need. Also, by using individual accounts, we can ensure that we can see who carried out what action by using the audit logs. By adding the email address, the username, and the password (with confirmation), you can create the new user. Users can be of three types: A normal user: Users of this type do not have any inherited permissions and they need to be awarded specific permissions to be able to do anything. A system auditor: Users of this type have full read-only privileges on the whole AWX installation. A system administrator: Users of this type have full privileges on the whole AWX installation.","title":"Creating a user"},{"location":"learning/ansible/security_basics/#creating-a-team","text":"Although having individual user accounts is an incredibly powerful tool, especially for enterprise use cases, it would be incredibly inconvenient and cumbersome to have to set permissions for each object (such as a job template or an inventory) on an individual basis. Every time someone joins a team, their user account has to be manually configured with the correct permissions against every object and, similarly, be removed if they leave. AWX and Ansible Tower have the same concept of user grouping that you would find in most other RBAC systems. The only slight difference is that in the user interface, they are referred to as teams, rather than groups. However, you can create teams simply and easily and then add and remove users as you need to. Doing this through the user interface is very straightforward and you will find the process similar to the way that most RBAC systems handle user groups, so we won't go into any more specific details here. Once you have your teams set up, I recommend that you assign your permissions to teams, rather than through individual users, as this will make your management of AWX object permissions much easier as your organization grows.","title":"Creating a team"},{"location":"learning/ansible/security_basics/#creating-an-organization","text":"Sometimes, you have multiple independent groups of people that you need to manage independent machines. For those kinds of scenarios, the use of organizations can help you. An organization is basically a tenant of AWX, with its own unique user accounts, teams, projects, inventories, and job templates\u2014it's almost like having a separate instance of AWX! After you create the organization, you can assign any kind of resource to an organization, such as projects, templates, inventories, users, and so on. Organizations are a simple concept to grasp, but also powerful in terms of segregating roles and responsibilities in AWX.","title":"Creating an organization"},{"location":"learning/ansible/security_basics/#assigning-permissions-in-awx","text":"Individual users (or the teams that they belong to) can be granted permissions on a per-object basis. So, for example, you could have a team of database administrators who only have access to see and execute playbooks on an inventory of database servers, using job templates that are specific to their role. Linux system administrators could then have access to the inventories, projects, and job templates that are specific to their role. AWX hides objects that users don't have the privileges to, which means the database administrators never see the Linux system administrator objects and vice versa. There are a number of different privilege levels that you can award users (or teams) with, which include the following: Admin: This is the organization-level equivalent of a system administrator. Execute: This kind of user can only execute templates that are part of the organization. Project admin: This kind of user can alter any project that is part of the organization. Inventory admin: This kind of user can alter any inventory that is part of the organization. Credential admin: This kind of user can alter any credential that is part of the organization. Workflow admin: This kind of user can alter any workflow that is part of the organization. Notification admin: This kind of user can alter any notification that is part of the organization. Job template admin: This kind of user can alter any job template that is part of the organization. Auditor: This is the organization-level equivalent to a system auditor. Member: This is the organization-level equivalent of a normal user. Read: This kind of user is able to view non-sensible objects that are part of the organization. AWX is a great addition to the power of Ansible in an enterprise setting and really helps ensure that your users can run Ansible playbooks in a manner that is well managed, secure, and auditable.","title":"Assigning permissions in AWX"},{"location":"learning/ansible/security_basics/#setting-up-jenkins","text":"- name : installing jenkins in ubuntu 16.04 hosts : \"192.168.1.7\" remote_user : ubuntu gather_facts : False become : True tasks : - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : install curl and git apt : name={{ item }} state=present update_cache=yes with_items : - curl - git - name : adding jenkins gpg key apt_key : url : https://pkg.jenkins.io/debian/jenkins-ci.org.key state : present - name : jeknins repository to system apt_repository : repo : http://pkg.jenkins.io/debian-stable binary/ state : present - name : installing jenkins apt : name : jenkins state : present update_cache : yes - name : adding jenkins to startup service : name : jenkins state : started enabled : yes - name : printing jenkins default administration password command : cat /var/lib/jenkins/secrets/initialAdminPassword register : jenkins_default_admin_password - debug : msg : \"{{ jenkins_default_admin_password.stdout }}\" # To set up Jenkins, run the following command. Where 192.168.1.7 is the server IP address where Jenkins will be installed: ansible-playbook -i '192.168.1.7,' site.yml --ask-sudo-pass # we have to navigate to the Jenkins dashboard by browsing to http://192.168.1.7:8080 and providing the auto-generated password. # If the playbook runs without any errors, it will display the password at the end of the play. - Create the new user by filling in the details and confirming to log in to the Jenkins console. - We can install custom plugins in Jenkins, navigate to the Manage Jenkins tab, select Manage Plugins, then navigate to the Available tab. - In the Filter: enter the plugin name as Ansible. Then select the checkbox and click Install without restart. - Now we are ready to work with the Ansible plugin for Jenkins. - Create a new project in the main dashboard, give it a name, and select Freestyle project to proceed: - We can configure the build options, this is where Jenkins will give us more flexibility to define our own triggers, build instructions, and post build scripts - Once the build triggers based on an event, this can be sent to some artifact storage, it can also be available in the Jenkins build console output - This is a really very powerful way to perform dynamic operations such as triggering automated server and stacks setup based on a code push to the repository, as well as scheduled scans and automated reporting.","title":"Setting up Jenkins"},{"location":"learning/ansible/security_basics/#security-automation-use-cases","text":"Here is a list of tasks that will prepare you to build layers of automation for the stuff that is important to you: Adding playbooks or connecting your source code management (SCM) tools, such as GitHub/GitLab/BitBucket Authentication and data security Logging output and managing reports for the automation jobs Job scheduling Alerting, notifications, and webhooks","title":"Security automation use cases"},{"location":"learning/ansible/security_basics/#ansible-tower-configuration","text":"To add playbooks into Ansible Tower, we have to start by creating projects, then select the SCM TYPE as Manual We can choose the SCM TYPE set to Git and provide a github.com URL pointing to a playbook Git SCM to add playbooks into projects We can also change the PROJECTS_ROOT under CONFIGURE TOWER to change this location. The added playbooks are executed by creating a job template. Then we can schedule these jobs (or) we can launch directly.","title":"Ansible Tower configuration"},{"location":"learning/ansible/security_basics/#jenkins-ansible-integration-configuration","text":"Jenkins supports SCM to use playbooks and local directories for manual playbooks too. This can be configured with the build options. Jenkins supports both ad-hoc commands and playbooks to trigger as a build (or) post-build action. We can also specify credentials if we want to access private repositories We can add the Playbook path by specifying the location of the playbook and defining inventory and variables as required","title":"Jenkins Ansible integration configuration"},{"location":"learning/ansible/security_basics/#authentication-and--data-security","text":"Some of the security features the tools offer include: RBAC (authentication and authorization) Web application over TLS/SSL (security for data in motion) Encryption for storing secrets (security for data at rest)","title":"Authentication and  data security"},{"location":"learning/ansible/security_basics/#rbac-for-ansible-tower","text":"Ansible Tower supports RBAC to manage multiple users with different permissions and roles. It also supports Lightweight Directory Access Protocol (LDAP) integration in the enterprise version to support Active Directory. This feature allows us to create different levels of users for accessing Ansible Tower. For example: The operations team requires a system administrator role to perform playbook execution and other activities like monitoring The security team requires a system auditor role to perform audit check for compliance standards such as Payment Card Industry Data Security Standard (PCI DSS) or even internal policy validation Normal users, such as team members, might just want to see how things are going, in the form of status updates and failure (or) success of job status","title":"RBAC for Ansible Tower"},{"location":"learning/ansible/security_basics/#tlsssl-for-ansible-tower","text":"By default, Ansible Tower uses HTTPS using self-signed certificates at /etc/tower/tower.cert and /etc/tower/tower.key.","title":"TLS/SSL for Ansible Tower"},{"location":"learning/ansible/security_basics/#encryption-and-data-security-for-ansible-tower","text":"Ansible Tower has been created with built-in security for handling encryption of credentials that includes passwords and keys. It uses Ansible Vault to perform this operation. It encrypts passwords and key information in the database.","title":"Encryption and data security for Ansible Tower"},{"location":"learning/ansible/security_basics/#tlsssl-for-jenkins","text":"By default, Jenkins runs as plain old HTTP. To enable HTTPS, we can use a reverse proxy, such as Nginx, in front of Jenkins to serve as HTTPS. Digital Ocean Reference for TLS","title":"TLS/SSL for Jenkins"},{"location":"learning/ansible/security_basics/#encryption-and-data-security-for-jenkins","text":"We are using Jenkins' default credential feature. This will store the keys and passwords in the local filesystem.","title":"Encryption and data security for Jenkins"},{"location":"learning/ansible/security_basics/#output-of-the-playbooks","text":"We would like to know where can we see the output of the playbooks executing and if any other logs that get created.","title":"Output of the playbooks"},{"location":"learning/ansible/security_basics/#report-management-for-ansible-tower","text":"By default, Ansible Tower itself is a reporting platform for the status of the playbooks, job executions, and inventory collection. The Ansible Tower dashboard gives an overview of the total projects, inventories, hosts, and status of the jobs. The output can be consumed in the dashboard, standard out, or by using the REST API and we can get this via tower-cli command line tool as well, which is just a pre-built command line tool for interfacing with the REST API.","title":"Report management for Ansible Tower"},{"location":"learning/ansible/security_basics/#scheduling-of-jobs","text":"The scheduling of jobs is simple and straightforward in Ansible Tower. For a job, you can specify a schedule and the options are mostly like cron. For example, you can say that you have a daily scan template and would like it to be executed at 4 a.m. every day for the next three months. This kind of schedule makes our meta automation very flexible and powerful.","title":"Scheduling of jobs"},{"location":"learning/ansible/security_basics/#alerting-notifications-and-webhooks","text":"Tower supports multiple ways of alerting and notifying users as per configuration. This can even be configured to make an HTTP POST request to a URL of your choice using a webhook. Ansible Tower notification using slack webhook is a popular option.","title":"Alerting, notifications, and webhooks"},{"location":"learning/ansible/security_basics/#setting-up-a-hardened-wordpress-with-encrypted-automated-backups","text":"Automating our server's patches is the most obvious, and possibly popular, requirement. We will apply security automation techniques and approaches to set up a hardened WordPress and enable encrypted backups. Everyone would agree that setting up a secure website and keeping it secured is a fairly common security requirement. And since it is so common, it would be useful for a lot of people who are tasked with building and managing websites to stay secure to look at that specific scenario. Note Are you aware that, according to Wikipedia, 27.5% of the top 10 million websites use WordPress? According to another statistic, 58.7% of all websites with known software on the entire web run WordPress. For us, setting up a hardened WordPress with encrypted automated backups can be broken down into the following steps: Setting up a Linux/Windows server with security measures in place. Setting up a web server (Apache/Nginx on Linux and IIS on Windows). Setting up a database server (MySQL) on the same host. Setting up WordPress using a command-line utility called WP-CLI. Setting up backup for the site files and the database which is incremental, encrypted, and most importantly, automated. Note We will assume that the server that we plan to deploy our WordPress website on is already up and running and we are able to connect to it. We will store the backup in an already configured AWS S3 bucket, for which the access key and secret access key is already provisioned. Refer the examples in Introduction to set this up.","title":"Setting Up a Hardened WordPress with Encrypted Automated Backups"},{"location":"learning/ansible/security_basics/#secure-automated-the-wordpress-updates","text":"Run the backups and update WordPress core, themes, and plugins. This can be scheduled via an Ansible Tower job for every day - name : running backup using duply command : /etc/cron.hourly/duply-backup - name : updating WordPress core command : wp core update register : wp_core_update_output ignore_errors : yes - name : wp core update output debug : msg : \"{{ wp_core_update_output.stdout }}\" - name : updating WordPress themes command : wp theme update --all register : wp_theme_update_output ignore_errors : yes - name : wp themes update output debug : msg : \"{{ wp_theme_update_output.stdout }}\" - name : updating WordPress plugins command : wp plugin update --all register : wp_plugin_update_output ignore_errors : yes - name : wp plugins update output debug : msg : \"{{ wp_plugin_update_output.stdout }}\"","title":"Secure automated the WordPress updates"},{"location":"learning/ansible/security_basics/#scheduling-via-ansible-tower-for-daily-updates","text":"Ansible Tower job scheduling for automated WordPress updates OR We can use the cron job template to perform this daily and add this template while deploying the WordPress setup #!/bin/bash /etc/cron.hourly/duply-backup wp core update wp theme update --all wp plugin update --all","title":"Scheduling via Ansible Tower for daily updates"},{"location":"learning/ansible/security_basics/#setting-up-apache2-web-server","text":"Shows how we can use templating to perform configuration updates in the server - name : installing apache2 server apt : name : \"apache2\" update_cache : yes state : present - name : updating customized templates for apache2 configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" mode : 0644 with_tems : - { src : apache2.conf.j2 , dst : /etc/apache2/conf.d/apache2.conf } - { src : 000-default.conf.j2 , dst : /etc/apache2/sites-available/000-default.conf } - { src : default-ssl.conf.j2 , dst : /etc/apache2/sites-available/default-ssl.conf } - name : adding custom link for sites-enabled from sites-available file : src : \"{{ item.src }}\" dest : \"{{ item.dest }}\" state : link with_items : - { src : '/etc/apache2/sites-available/000-default.conf' , dest : '/etc/apache2/sites-enabled/000-default.conf' } - { src : '/etc/apache2/sites-available/default-ssl.conf' , dest : '/etc/apache2/sites-enabled/default-ssl.conf' } notify : - start apache2 - startup apache2","title":"Setting up Apache2 web server"},{"location":"learning/ansible/security_basics/#enabling-tlsssl-with-lets-encrypt","text":"We can use a command-line tool offered by Let's Encrypt to get free SSL/TLS certificates in an open, automated manner. The tool is capable of reading and understanding an nginx virtual host file and generating the relevant certificates completely automatically, without any kind of manual intervention. - name : adding certbot ppa apt_repository : repo : \"ppa:certbot/certbot\" - name : install certbot apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-certbot-nginx - name : check if we have generated a cert already stat : path : \"/etc/letsencrypt/live/{{ website_domain_name }}/fullchain.pem\" register : cert_stats - name : run certbot to generate the certificates shell : \"certbot certonly --standalone -d {{ website_domain_name }} --email {{ service_admin_email }} --non-interactive --agree-tos\" when : cert_stats.stat.exists == False - name : configuring site files template : src : website.conf dest : \"/etc/nginx/sites-available/{{ website_domain_name }}\" - name : restart nginx service : name : nginx state : restarted","title":"Enabling TLS/SSL with Let's Encrypt"},{"location":"learning/ansible/security_basics/#log-monitoring","text":"Log monitoring is the perfect place to think about security automation. For monitoring to be effective, a few things need to happen. We should be able to move logs from different devices to a central location. We should be able to make sense of what a regular log entry is and what could possibly be an attack. We should be able to store the logs, and also operate on them for things such as aggregation, normalization, and eventually, analysis. Traditional logging systems find it difficult to log for all applications, systems, and devices. The variety of time formats, log output formats, and so on, makes the task pretty complicated. The biggest roadblock is finding a way to be able to centralize logs. This gets in the way of being able to process log entries in real time, or near real time effectively. Some of the problematic points are as follows: Access is often difficult High expertise in mined data is required Logs can be difficult to find Log data is immense in size","title":"Log Monitoring"},{"location":"learning/ansible/security_basics/#introduction-to-elastic-stack","text":"Elastic Stack is a group of open source products from the Elastic company. It takes data from any type of source and in any format and searches, analyzes, and visualizes that data in real time. It consists of four major components, as follows: Elasticsearch Logstash Kibana Beats It helps users/admins to collect, analyze, and visualize data in (near) real time. Each module fits based on your use case and environment.","title":"Introduction to Elastic Stack"},{"location":"learning/ansible/security_basics/#elasticsearch","text":"Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected Main plus points of Elastic Stack: Distributed and highly available search engine, written in Java, and uses Groovy Built on top of Lucene Multi-tenant, with multi types and a set of APIs Document-oriented, providing (near) real-time search","title":"Elasticsearch"},{"location":"learning/ansible/security_basics/#logstash","text":"Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources, simultaneously transforms it, and then sends it to your favorite stash. Centralized data processing of all types of logs Consists of the following three main components: Input: Passing logs to process them into machine-understandable format Filter: A set of conditions to perform a specific action on an event Output: The decision maker for processed events/logs","title":"Logstash"},{"location":"learning/ansible/security_basics/#kibana","text":"Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack, so you can do anything from learning why you're getting paged at 2:00 a.m. to understanding the impact rain might have on your quarterly numbers. Kibana's list of features: Powerful frontend dashboard is written in JavaScript Browser-based analytics and search dashboard for Elasticsearch A flexible analytics and visualization platform Provides data in the form of charts, graphs, counts, maps, and so on, in real time","title":"Kibana"},{"location":"learning/ansible/security_basics/#beats","text":"Beats is the platform for single-purpose data shippers. They install as lightweight agents and send data from hundreds or thousands of machines to Logstash or Elasticsearch. Beats are: Lightweight shippers for Elasticsearch and Logstash Capture all sorts of operational data, like logs or network packet data They can send logs to either Elasticsearch or Logstash","title":"Beats"},{"location":"learning/ansible/security_basics/#elastalert","text":"ElastAlert is a Python tool which also bundles with the different types of integrations to support with alerting and notifications. Some of them include Command, Email, JIRA, OpsGenie, AWS SNS, HipChat, Slack, Telegram, and so on. It also provides a modular approach to creating our own integrations.","title":"ElastAlert"},{"location":"learning/ansible/security_basics/#why-should-we-use-elastic-stack-for-security-monitoring-and-alerting","text":"The Elastic Stack solves most of the problems that we have discussed before, such as: Ability to store large amounts of data Ability to understand and read a variety of log formats Ability to ship the log information from a variety of devices in near real time to one central location A visualization dashboard for log analysis","title":"Why should we use Elastic Stack for security monitoring and alerting?"},{"location":"learning/ansible/security_basics/#prerequisites-for-setting-up-elastic-stack","text":"- name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : accepting oracle java license agreement debconf : name : 'oracle-java8-installer' question : 'shared/accepted-oracle-license-v1-1' value : 'true' vtype : 'select' - name : adding ppa repo for oracle java by webupd8team apt_repository : repo : 'ppa:webupd8team/java' state : present update_cache : yes - name : installing java nginx apache2-utils and git apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-software-properties - oracle-java8-installer - nginx - apache2-utils - python-pip - python-passlib","title":"Prerequisites for setting up Elastic Stack"},{"location":"learning/ansible/security_basics/#setting-up-the-elastic-stack","text":"The stack is a combination of: The Elasticsearch service The Logstash service The Kibana service The Beats service on all the devices We are going to set up Elasticsearch, Logstash, and Kibana on a single machine. This is the main log collection machine: It requires a minimum of 4 GB RAM, as we are using a single machine to serve three services (Elasticsearch, Logstash, and Kibana) It requires a minimum of 20 GB disk space, and, based on your log size, you can add the disk space","title":"Setting up the Elastic Stack"},{"location":"learning/ansible/security_basics/#installing-elasticsearch","text":"Install Elasticsearch from the repository with gpg key and add it to the startup programs - name : adding elastic gpg key for elasticsearch apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing elasticsearch apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - elasticsearch - name : adding elasticsearch to the startup programs service : name : elasticsearch enabled : yes notify : - start elasticsearch Configure the Elasticsearch cluster with the required settings. Also, set up the JVM options for the Elasticsearch cluster. Also, create a backup directory for Elasticsearch cluster backups and snapshots - name : creating elasticsearch backup repo directory at {{ elasticsearch_backups_repo_path }} file : path : \"{{ elasticsearch_backups_repo_path }}\" state : directory mode : 0755 owner : elasticsearch group : elasticsearch - name : configuring elasticsearch.yml file template : src : \"{{ item.src }}\" dest : /etc/elasticsearch/\"{{ item.dst }}\" with_items : - { src : 'elasticsearch.yml.j2' , dst : 'elasticsearch.yml' } - { src : 'jvm.options.j2' , dst : 'jvm.options' } notify : - restart elasticsearch The notify part will trigger the restart elasticsearch handler and the handler file will look as follows. - name : start elasticsearch service : name : elasticsearch state : started - name : restart elasticsearch service : name : elasticsearch state : restarted","title":"Installing Elasticsearch"},{"location":"learning/ansible/security_basics/#installing-logstash","text":"Install Logstash from the repository with gpg key and add it to the startup programs - name : adding elastic gpg key for logstash apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing logstash apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - logstash - name : adding logstash to the startup programs service : name : logstash enabled : yes notify : - start logstash Configure the Logstash service with input, output, and filter settings. This enables receiving logs, processing logs, and sending logs to the Elasticsearch cluster - name : logstash configuration files template : src : \"{{ item.src }}\" dest : /etc/logstash/conf.d/\"{{ item.dst }}\" with_items : - { src : '02-beats-input.conf.j2' , dst : '02-beats-input.conf' } - { src : '10-sshlog-filter.conf.j2' , dst : '10-sshlog-filter.conf' } - { src : '11-weblog-filter.conf.j2' , dst : '11-weblog-filter.conf' } - { src : '30-elasticsearch-output.conf.j2' , dst : '10-elasticsearch-output.conf' } notify : - restart logstash","title":"Installing Logstash"},{"location":"learning/ansible/security_basics/#logstash-configuration","text":"To receive logs from different systems, we use the Beats service from Elastic. The following configuration is to receive logs from different servers to the Logstash server. Logstash runs on port 5044 and we can use SSL certificates to ensure logs are transferred via an encrypted channel. # 02-beats-input.conf.j2 input { beats { port => 5044 ssl => true ssl_certificate => \"/etc/pki/tls/certs/logstash-forwarder.crt\" ssl_key => \"/etc/pki/tls/private/logstash-forwarder.key\" } } The following configuration is to parse the system SSH service logs (auth.log) using grok filters. It also applies filters like geoip, while providing additional information like country, location, longitude, latitude, and so on. #10-sshlog-filter.conf.j2 filter { if [type] == \"sshlog\" { grok { match => [ \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : message repeated 2 times : \\[ %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } password for invalid user %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } % { WORD : auth_method } for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\" ] } date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] locale => en } geoip { source => \"ip\" } } } The following configuration is to parse web server logs (nginx, apache2). We will also apply filters for geoip and useragent. The useragent filter allows us to get information about the agent, OS type, version information, and so on. #11-weblog-filter.conf.j2 filter { if [type] == \"weblog\" { grok { match => { \"message\" => '%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}' } } date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] locale => en } geoip { source => \"clientip\" } useragent { source => \"agent\" target => \"useragent\" } } } The following configuration will send the log output into the Elasticsearch cluster with daily index formats. #30-elasticsearch-output.conf.j2 output { elasticsearch { hosts => [\"localhost:9200\"] manage_template => false index => \"%{[@metadata][beat]}-%{+YYYY.MM.dd}\" document_type => \"%{[@metadata][type]}\" } }","title":"Logstash configuration"},{"location":"learning/ansible/security_basics/#installing-kibana","text":"By default we are not making any changes in Kibana, as it works out of the box with Elasticsearch. - name : adding elastic gpg key for kibana apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing kibana apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - kibana - name : adding kibana to the startup programs service : name : kibana enabled : yes notify : - start kibana By default Kibana doesn't have any authentication, X-Pack is the commercial plug-in by Elastic for RBAC (role-based access control) with security. Also, some open source options include https://readonlyrest.com/ and Search Guard ( https://floragunn.com ) to interact with Elasticsearch. Using TLS/SSL and custom authentication and aauthorization is highly recommended. Some of the open source options includes Oauth2 Proxy ( bitly/oauth2_proxy ) and Auth0, and so on.","title":"Installing Kibana"},{"location":"learning/ansible/security_basics/#setting-up-nginx-reverse-proxy","text":"The following configuration is to enable basic authentication for Kibana using nginx reverse proxy. server { listen 80; server_name localhost; auth_basic \"Restricted Access\"; auth_basic_user_file /etc/nginx/htpasswd.users; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } Setting up and configuring the nginx service looks as follows. #command: htpasswd -c /etc/nginx/htpasswd.users - name : htpasswd generation htpasswd : path : \"/etc/nginx/htpasswd.users\" name : \"{{ basic_auth_username }}\" password : \"{{ basic_auth_password }}\" owner : root group : root mode : 0644 - name : nginx virtualhost configuration template : src : \"templates/nginxdefault.j2\" dest : \"/etc/nginx/sites-available/default\" notify : - restart nginx","title":"Setting up nginx reverse proxy"},{"location":"learning/ansible/security_basics/#installing-beats-to-send-logs-to-elastic-stack","text":"We are going to install Filebeat to send SSH and web server logs to the Elastic Stack: - name : adding elastic gpg key for filebeat apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing filebeat apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - apt-transport-https - filebeat - name : adding filebeat to the startup programs service : name : filebeat enabled : yes notify : - start filebeat Configure the Filebeat to send both SSH and web server logs to Elastic Stack, to process and index in near real-time. filebeat : prospectors : - paths : - /var/log/auth.log # - /var/log/syslog # - /var/log/*.log document_type : sshlog - paths : - /var/log/nginx/access.log document_type : weblog registry_file : /var/lib/filebeat/registry output : logstash : hosts : [ \"{{ logstash_server_ip }}:5044\" ] bulk_max_size : 1024 ssl : certificate_authorities : [ \"/etc/pki/tls/certs/logstash-forwarder.crt\" ] logging : files : rotateeverybytes : 10485760 # = 10MB","title":"Installing Beats to send logs to Elastic Stack"},{"location":"learning/ansible/security_basics/#elastalert-for-alerting","text":"First, we need to install the prerequisites for setting up ElastAlert. - name : installing pre requisuites for elastalert apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-pip - python-dev - libffi-dev - libssl-dev - python-setuptools - build-essential - name : installing elastalert pip : name : elastalert - name : creating elastalert directories file : path : \"{{ item }}\" state : directory mode : 0755 with_items : - /opt/elastalert/rules - /opt/elastalert/config - name : creating elastalert configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : 'elastalert-config.j2' , dst : '/opt/elastalert/config/config.yml' } - { src : 'elastalert-service.j2' , dst : '/lib/systemd/system/elastalert.service' } - { src : 'elastalert-sshrule.j2' , dst : '/opt/elastalert/rules/ssh-bruteforce.yml' } - name : enable elastalert service service : name : elastalert state : started enabled : yes Creating a simple startup script so that ElastAlert will be used as a system service. [ Unit ] Description = elastalert After = multi-user.target [ Service ] Type = simple WorkingDirectory = /opt/elastalert ExecStart = /usr/local/bin/elastalert --config /opt/elastalert/config/config.yml [ Install ] WantedBy = multi-user.target","title":"ElastAlert for alerting"},{"location":"learning/ansible/security_basics/#configuring-the-lets-encrypt-service","text":"- name : adding certbot ppa apt_repository : repo : \"ppa:certbot/certbot\" - name : install certbot apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-certbot-nginx - name : check if we have generated a cert already stat : path : \"/etc/letsencrypt/live/{{ website_domain_name }}/fullchain.pem\" register : cert_stats - name : run certbot to generate the certificates shell : \"certbot certonly --standalone -d {{ website_domain_name }} --email {{ service_admin_email }} --non-interactive --agree-tos\" when : cert_stats.stat.exists == False - name : configuring site files template : src : website.conf dest : \"/etc/nginx/sites-available/{{ website_domain_name }}\" - name : restart nginx service : name : nginx state : restarted","title":"Configuring the Let's Encrypt service"},{"location":"learning/ansible/security_basics/#elastalert-rule-configuration","text":"Assuming that you already have Elastic Stack installed and logging SSH logs, use the following ElastAlert rule to trigger SSH attack IP blacklisting. es_host : localhost es_port : 9200 name : \"SSH Bruteforce attack alert\" type : frequency index : filebeat-* num_events : 20 timeframe : minutes : 1 # For more info: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.html filter : - query : query_string : query : '_type:sshlog AND login:failed AND (username: \"ubuntu\" OR username: \"root\")' alert : - slack : slack_webhook_url : \"https://hooks.slack.com/services/xxxxx\" slack_username_override : \"attack-bot\" slack_emoji_override : \"robot_face\" - command : [ \"/usr/bin/curl\" , \"https://xxxxxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/zzzzzzzzzzzzzz/ip/inframonitor/%(ip)s\" ] realert : minutes : 0 For more references, visit https://elastalert.readthedocs.io/en/latest/running_elastalert.html .","title":"ElastAlert rule configuration"},{"location":"learning/ansible/security_basics/#serverless-automated-defense","text":"If we can get a notification for an attack, we can set up and do the following: Call an AWS Lambda function Send the attacker's IP address information to this AWS Lambda function endpoint Use the code deployed in the Lambda function to call the VPC network access list API and block the attacker's IP address To ensure that we don't fill up the ACLs with attacker IPs, we can combine this approach with AWS DynamoDB to store this information for a short duration and remove it from the block list. As soon as an attack is detected, the alerter sends the IP to the blacklist lambda endpoint via an HTTPS request. The IP is blocked using the network ACL and the record of it is maintained in DynamoDB. If the IP is currently blocked already, then the expiry time for the rule will be extended in the DynamoDB. An expiry handler function is periodically triggered, which removes expired rules from DynamoDB and ACL accordingly.","title":"Serverless Automated Defense"},{"location":"learning/ansible/security_basics/#setup","text":"The setup involves the following steps: Obtain IAM credentials Create a table in DynamoDB Configure the lambda function based on requirement Deploy code to AWS Lambda Configure Cloudwatch to periodic invocation The entire setup is automated, except for obtaining the IAM credentials and configuring the function based on requirements.","title":"Setup"},{"location":"learning/ansible/security_basics/#configuration","text":"The following parameters are configurable before deployment: region: AWS region to deploy in. This needs to be the same as the region where the VPC network resides. accessToken: The accessToken that will be used to authenticate the requests to the blacklist endpoint. aclLimit: The maximum number of rules an ACL can handle. The maximum limit in AWS is 20 by default. ruleStartId: The starting ID for rules in the ACL. aclID: The ACL ID of the network where the rules will be applied. tableName: The unique table name in DynamoDB, created for each VPC to be defended. ruleValidity: The duration for which a rule is valid, after which the IP will be unblocked. // Configure the following in the config.js file module . exports = { region : \"us-east-1\" , // AWS Region to deploy in accessToken : \"YOUR_R4NDOM_S3CR3T_ACCESS_TOKEN_GOES_HERE\" , // Accesstoken to make requests to blacklist aclLimit : 20 , // Maximum number of acl rules ruleStartId : 10 , // Starting id for acl entries aclId : \"YOUR_ACL_ID\" , // AclId that you want to be managed tableName : \"blacklist_ip\" , // DynamoDB table that will be created ruleValidity : 5 // Validity of Blacklist rule in minutes } Make sure to modify at least the aclId, accessToken, and region based on your setup. To modify the lambda deployment configuration use the serverless.yml file ... functions : blacklist : handler : handler.blacklistip events : - http : path : blacklistip method : get handleexpiry : handler : handler.handleexpiry events : - schedule : rate(1 minute) ... For example, the rate at which the expiry function is triggered and the endpoint URL for the blacklist function can be modified using the YML file. But the defaults are already optimal. # The playbook looks as follows: - name : installing node run time and npm apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - nodejs - npm - name : installing serverless package npm : name : \"{{ item }}\" global : yes state : present with_items : - serverless - aws-sdk - name : copy the setup files template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : 'config.js.j2' , dst : '/opt/serverless/config.js' } - { src : 'handler.js.j2' , dst : '/opt/serverless/handler.js' } - { src : 'iamRoleStatements.json.j2' , dst : '/opt/serverless/iamRoleStatements.json' } - { src : 'initDb.js.j2' , dst : '/opt/serverless/initDb.js' } - { src : 'serverless.yml.j2' , dst : '/opt/serverless/serverless.yml' } - { src : 'aws-credentials.j2' , dst : '~/.aws/credentials' } - name : create dynamo db table command : node initDb.js args : chdir : /opt/serverless/ - name : deploy the serverless command : serverless deploy args : chdir : /opt/serverless/ The current setup for AWS Lambda is to block the IP address against network ACL. This can be reused with other API endpoints, like a firewall dynamic block list and other security devices. The blacklist endpoint is responsible for blocking an IP address. The URL looks like the following: https://lambda_url/blacklistipaccessToken=ACCESS_TOKEN&ip=IP_ADDRESS The query parameters are as follows: IP_ADDRESS: This is the IP address to be blocked ACCESS_TOKEN: The accessToken to authenticate the request","title":"Configuration"},{"location":"learning/ansible/security_basics/#automated-defense-lambda-in-action","text":"When the ElastAlert detects an SSH brute force attack, it will trigger a request to lambda endpoint by providing the attacker's IP address. Then our automated defense platform will trigger a network ACL blocklist rule. This can be configurable to say for how much time it should be blocked.","title":"Automated defense lambda in action"},{"location":"learning/ansible/security_hardening/","text":"Security Hardening for Applications and Networks \u00b6 Security hardening is the most obvious task for any security-conscious endeavor. By doing the effort of securing systems, applications, and networks, one can achieve multiple security goals given as follows: Ensuring that applications and networks are not compromised (sometimes) Making it difficult for compromises to stay hidden for long Securing by default ensures that compromises in one part of the network don't propagate further and more We will build playbooks that will allow us to do the following things: Secure our master images so that as soon as the applications and systems are part of the network, they offer decent security Execute audit processes so that we can verify and measure periodically if the applications, systems, and networks are in line with the security policies that are required by the organization Security hardening with benchmarks such as Center for Internet Security (CIS), Security Technical Implementation Guides (STIG), and National Institute of Standards and Technology (NIST) Automating security audit checks for networking devices using Ansible Automating security audit checks for applications using Ansible Automated patching approaches using Ansible Security hardening with benchmarks such as CIS, STIGs, and NIST \u00b6 Benchmarks provide a great way for anyone to gain assurance of their individual security efforts. Hardening for security mostly boils down to do the following: Agreeing on what is the minimal set of configuration that qualifies as secure configuration. This is usually defined as a hardening benchmark or framework. Making changes to all the aspects of the system that are touched by such configuration. Measuring periodically if the application and system are still in line with the configuration or if there is any deviation. If any deviation is found, take corrective action to fix that. If no deviation is found, log that. Since software is always getting upgraded, staying on top of the latest configuration guidelines and benchmarks is most important. Operating system hardening for baseline using an Ansible playbook \u00b6 We will see how we can use existing playbooks from the community (Ansible Galaxy). The following playbook provides multiple security configurations, standards, and ways to protect operating system against different attacks and security vulnerabilities. Some of the tasks it will perform include the following: Configures package management, for example, allows only signed packages Remove packages with known issues Configures pam and pam_limits modules Shadow password suite configuration Configures system path permissions Disable core dumps via soft limits Restrict root logins to system console Set SUIDs Configures kernel parameters via sysctl Downloading and executing Ansible playbooks from galaxy is as simple as follows: ansible-galaxy install dev-sec.os-hardening - hosts : localhost become : yes roles : - dev-sec.os-hardening The preceding playbook will detect the operating system and perform hardening steps based on the different guidelines. This can be configured as required by updating the default variables values. Refer to dev-sec/ansible-os-hardening for more details about the playbook. STIGs Ansible role for automated security hardening for Linux hosts \u00b6 OpenStack has an awesome project named ansible-hardening , which applies the security configuration changes as per the STIGs standards. It performs security hardening for the following domains: accounts: User account security controls aide: Advanced Intrusion Detection Environment auditd: Audit daemon auth: Authentication file_perms: Filesystem permissions graphical: Graphical login security controls kernel: Kernel parameters lsm: Linux Security Modules misc: Miscellaneous security controls packages: Package managers sshd: SSH daemon Download the role from the GitHub repository itself using ansible-galaxy as follows: ansible-galaxy install git+https://github.com/openstack/ansible-hardening - name : STIGs ansible-hardening for automated security hardening hosts : servers become : yes remote_user : \"{{ remote_user_name }}\" vars : remote_user_name : vagrant security_ntp_servers : - time.nist.gov - time.google.com roles : - ansible-hardening Continuous security scans and reports for OpenSCAP using Ansible Tower \u00b6 OpenSCAP is a set of security tools, policies, and standards to perform security compliance checks against the systems by following SCAP. SCAP is the U.S. standard maintained by NIST. OpenSCAP follows these steps to perform scanning on your system: Install SCAP Workbench or OpenSCAP Base (for more information, visit https://www.open-scap.org ) Choose a policy Adjust your settings Evaluate the system Check playbook reference at https://medium.com/@jackprice/ansible-openscap-for-compliance-automation-14200fe70663 . - hosts : all become : yes vars : oscap_profile : xccdf_org.ssgproject.content_profile_pci-dss oscap_policy : ssg-rhel7-ds tasks : - name : install openscap scanner package : name : \"{{ item }}\" state : latest with_items : - openscap-scanner - scap-security-guide - block : - name : run openscap command : > oscap xccdf eval --profile {{ oscap_profile }} --results-arf /tmp/oscap-arf.xml --report /tmp/oscap-report.html --fetch-remote-resources /usr/share/xml/scap/ssg/content/{{ oscap_policy }}.xml always : - name : download report fetch : src : /tmp/oscap-report.html dest : ./{{ inventory_hostname }}.html flat : yes We can use this playbook to perform continuously automated checks using Ansible Tower First, we need to create a directory in Ansible Tower server in order to store this playbook with the awx user permission to add the custom playbook. Create a new project in Ansible Tower to perform the OpenSCAP setup and scan against the checks. Then, we have to create a new job to execute the playbook. Here, we can include the list of hosts, credentials for login, and other details required to perform the execution. This audit can be scheduled to perform frequently. We can also launch this job on demand when required. The output of the playbook will generate the OpenSCAP report, and it will be fetched to Ansible Tower. We can access this playbook at the /tmp/ location. Also, we can send this report to the other centralized reporting server if required. We can also set up notifications based on playbook execution results. By doing this, we can send this notifications to respective channels, such as email, slack, and message. CIS Benchmarks \u00b6 CIS has benchmarks for different type OS, software, and services. The following are some high-level categories: Desktops and web browsers Mobile devices Network devices Security metrics Servers \u2013 operating systems Servers \u2013 other Virtualization platforms, cloud, and other Ubuntu CIS Benchmarks (server level) \u00b6 CIS Benchmarks Ubuntu provides prescriptive guidance to establish a secure configuration posture for Ubuntu Linux systems running on x86 and x64 platforms. This benchmark is intended for system and application administrators, security specialists, auditors, help desk, and platform deployment personnel who plan to develop, deploy, assess, or secure solutions that incorporate Linux platform. Here are the high-level six domains that are part of CIS Ubuntu 16.04 LTS benchmarks: Initial setup: Filesystem configuration Configure software updates Filesystem integrity checking Secure boot settings Additional process hardening Mandatory access control Warning banners Services: Inted Services Special purpose services Service clients Network configuration: Network parameters (host only) Network parameters (host and router) IPv6 TCP wrappers Uncommon network protocols Logging and auditing: Configure system accounting (auditd) Configure logging Access, authentication, and authorization: Configure cron SSH server configuration Configure PAM User accounts and environment System maintenance: System file permissions User and group settings # Playbooks git clone https://github.com/oguya/cis-ubuntu-14-ansible.git cd cis-ubuntu-14-ansible Then, update the variables and inventory and execute the playbook using the following command. The variables are not required mostly, as this performs against different CIS checks unless, if we wanted to customize the benchmarks as per the organization. ansible-playbook -i inventory cis.yml The preceding playbook will execute the CIS security benchmark against an Ubuntu server and performs all the checks listed in the CIS guidelines. AWS benchmarks (cloud provider level) \u00b6 AWS CIS Benchmarks provides prescriptive guidance to configure security options for a subset of AWS with an emphasis on foundational, testable, and architecture agnostic settings. Here are the high-level domains, which are part of AWS CIS Benchmarks: Identity and access management Logging Monitoring Networking Extra Currently, there is a tool named prowler ( Alfresco/prowler ) based on AWS-CLI commands for AWS account security assessment and hardening. Before running the playbook, we have to provide AWS API keys to perform security audit. This can be created using IAM role in AWS service. If you have an already existing account with required privileges, these steps can be skipped. Create a new user in your AWS account with programmatic access. Apply the SecurityAudit policy for the user from existing policies in IAM console. Create the new user by following the steps. Make sure that you safely save the Access key ID and Secret access key for later use. The following playbook assume that you already have installed python and pip in your local system. - name : AWS CIS Benchmarks playbook hosts : localhost become : yes vars : aws_access_key : XXXXXXXX aws_secret_key : XXXXXXXX tasks : - name : installing aws cli and ansi2html pip : name : \"{{ item }}\" with_items : - awscli - ansi2html - name : downloading and setting up prowler get_url : url : https://raw.githubusercontent.com/Alfresco/prowler/master/prowler dest : /usr/bin/prowler mode : 0755 - name : running prowler full scan shell : \"prowler | ansi2html -la > ./aws-cis-report-{{ ansible_date_time.epoch }}.html\" environment : AWS_ACCESS_KEY_ID : \"{{ aws_access_key }}\" AWS_SECRET_ACCESS_KEY : \"{{ aws_secret_key }}\" - name : AWS CIS Benchmarks report downloaded debug : msg : \"Report can be found at ./aws-cis-report-{{ ansible_date_time.epoch }}.html\" The playbook will trigger the setup and security audit scan for AWS CIS Benchmarks using the prowler tool. Prowler-generated HTML report can be downloaded in different formats as required and also scanning checks can be configured as required. More reference about the tool can be found at Alfresco/prowler . Automating security audit checks for networking devices using Ansible \u00b6 We can use this to do security audit checks for networking devices. Nmap scanning and NSE \u00b6 Network Mapper (Nmap) is a free open source software to perform network discovery, scanning, audit, and many others. It has a various amount of features such as OS detection, system fingerprinting, firewall detection, and many other features. Nmap Scripting Engine (Nmap NSE) provides advanced capabilities like scanning for particular vulnerabilities and attacks. We can also write and extend Nmap using our own custom script. Nmap is a swiss army knife for pen testers (security testers) and network security teams. - name : Basic NMAP Scan Playbook hosts : localhost gather_facts : false vars : top_ports : 1000 network_hosts : - 192.168.1.1 - scanme.nmap.org - 127.0.0.1 - 192.168.11.0/24 tasks : - name : check if nmap installed and install apt : name : nmap update_cache : yes state : present become : yes - name : top ports scan shell : \"nmap --top-ports {{ top_ports }} -Pn -oA nmap-scan-%Y-%m-%d {{ network_hosts|join(' ') }}\" {{ network_hosts|join(' ') }} is a Jinja2 feature named filter arguments to parse the given network_hosts by space delimited network_hosts variable holds the list of IPs, network range (CIDR), hosts, and so on to perform scan using Nmap top_ports is the number that is ranging from 0 to 65535. Nmap by default picks commonly opened top ports -Pn specifies that scans the host if ping (ICMP) doesn't work also -oA gets the output in all formats, which includes gnmap (greppable format), Nmap, and XML Nmap NSE scanning playbook \u00b6 This playbook will perform enumeration of directories used by popular web applications and servers using http-enum and finds options that are supported by an HTTP server using http-methods using Nmap scripts. - name : Advanced NMAP Scan using NSE hosts : localhost vars : ports : - 80 - 443 scan_host : scanme.nmap.org tasks : - name : Running Nmap NSE scan shell : \"nmap -Pn -p {{ ports|join(',') }} --script {{ item }} -oA nmap-{{ item }}-results-%Y-%m-%d {{ scan_host }}\" with_items : - http-methods - http-enum The http-enum script runs additional tests against network ports where web servers are detected. We can see that two folders were discovered by the script and additionally all HTTP methods that are supported got enumerated as well. Automation security audit checks for applications using Ansible \u00b6 Modern applications can get pretty complex fairly quickly. Having the ability to run automation to do security tasks is almost a mandatory requirement. The different types of application security scanning we can do can range from the following: Run CI/CD scanning against the source code (for example, RIPS and brakeman). Dependency checking scanners (for example, OWASP dependency checker and snyk.io ( https://snyk.io/ )). Once deployed then run the web application scanner (for example, Nikto, Arachni, and w3af). Framework-specific security scanners (for example, WPScan and Droopscan) and many other. Source code analysis scanners \u00b6 This is one of the first and common way to minimize the security risk while applications going to production. Source code analysis scanner also known as Static Application Security Testing (SAST) will help to find security issues by analyzing the source code of the application. Source code analysis is kind of white box testing and looking through code. This kind of testing methodology may not find 100% coverage of security vulnerabilities, and it requires manual testing as well. For example, finding logical vulnerabilities requires some kind of user interactions such as dynamic functionalities. For example, if you are scanning PHP code, then RIPS ; if it's Ruby on Rails code, then it's Brakeman ; and if it's python, then Bandit Dependency-checking scanners \u00b6 Most of the developers use third-party libraries while developing applications, and it's very common to see using open source plugins and modules inside their code. So dependency checks will allow us to find using components with known vulnerabilities (OWASP A9) issues in application code by scanning the libraries against the CVE and NIST vulnerability database. There are multiple projects out there in the market for performing these checks, and some of them includes the following: OWASP Dependency-Check Snyk.io ( https://snyk.io/ ) Retire.js [:] SourceClear and many other Running web application security scanners \u00b6 This is the phase where the application went live to QA, stage, (or) Production. Then, we wanted to perform security scans like an attacker (black box view). At this stage, an application will have all the dynamic functionalities and server configurations applied. These scanner results tell us how good the server configured and any other application security issues before releasing the replica copy into the production. There are many tools in the market to do these jobs for you in both open source and commercial world. Nikto Arachni w3af Acunetix Framework-specific security scanners \u00b6 This kind of check and scanning is to perform against specific to framework, CMS, and platforms. It allows to get more detailed results by validating against multiple security test cases and checks. Scanning against WordPress CMS using WPScan Scanning against JavaScript libraries using Retire.js Scanning against Drupal CMS using Droopescan Automated patching approaches using Ansible \u00b6 Patching and updating is a task that everyone who has to manage production systems has to deal with. There are two approaches that we will look are as follows: Rolling updates BlueGreen deployments Rolling updates \u00b6 Imagine that we have five web servers behind a load balancer. What we would like to do is a zero downtime upgrade of our web application. We want to achieve the following: Tell the load balancer that web server node is down Bring down the web server on that node Copy the updated application files to that node Bring up the web server on that node The first keyword for us to look at is serial. This ensures that the execution of the playbook is done serially rather than in parallel. We can choose to provide a percentage value or numeric value to serial. A great example for this way of doing updates is given in the following link Episode #47 - Zero-downtime Deployments with Ansible BlueGreen deployments \u00b6 The concept of BlueGreen is attributed to Martin Fowler . The idea is to consider our current live production workload as blue. Now what we want to do is upgrade the application. So a replica of blue is brought up behind the same load balancer. The replica of the infrastructure has the updated application. Once it is up and running, the load balancer configuration is switched from current blue to point to green. Blue keeps running in case there are any operational issues. Once we are happy with the progress, we can tear down the older host. BlueGreen deployment setup playbook \u00b6 The following playbook will set up three nodes, which includes load balancer and two web server nodes. The first playbook brings up three hosts. Two web servers running nginx behind a load balancer The second playbook switches what is live (blue) to green # inventory.yml [ proxyserver ] proxy ansible_host=192.168.100.100 ansible_user=ubuntu ansible_password=passwordgoeshere [blue] blueserver ansible_host=192.168.100.10 ansible_user=ubuntu ansible_password=passwordgoeshere [green] greenserver ansible_host=192.168.100.20 ansible_user=ubuntu ansible_password=passwordgoeshere [webservers:children] blue green [prod:children] webservers proxyserver # main.yml - name : running common role hosts : prod gather_facts : false become : yes serial : 100% roles : - common - name : running haproxy role hosts : proxyserver become : yes roles : - haproxy - name : running webserver role hosts : webservers become : yes serial : 100% roles : - nginx - name : updating blue code hosts : blue become : yes roles : - bluecode - name : updating green code hosts : green become : yes roles : - greencode # common role - name : installing python if not installed raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : updating and installing git, curl apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - git - curl # Also we can include common any monitoring and security hardening tasks # haproxy role - name : adding haproxy repo apt_repository : repo : ppa:vbernat/haproxy-1.7 - name : updating and installing haproxy apt : name : haproxy state : present update_cache : yes - name : updating the haproxy configuration template : src : haproxy.cfg.j2 dest : /etc/haproxy/haproxy.cfg - name : starting the haproxy service service : name : haproxy state : started enabled : yes # haproxy.cfg.j2 global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). This list is from : # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ # An alternative list with additional directives can be obtained from # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http frontend http_front bind *:80 stats uri /haproxy?stats default_backend http_back backend http_back balance roundrobin server {{ hostvars.blueserver.ansible_host }} {{ hostvars.blueserver.ansible_host }}:80 check #server {{ hostvars.greenserver.ansible_host }} {{ hostvars.greenserver.ansible_host }}:80 check # nginx role - name : installing nginx apt : name : nginx state : present update_cache : yes - name : starting the nginx service service : name : nginx state : started enabled : yes # Code snipet for blue <html> <body bgcolor=\"blue\"> <h1 align=\"center\">Welcome to Blue Deployment</h1> </body> </html> # Code snipet for green <html> <body bgcolor=\"green\"> <h1 align=\"center\">Welcome to Green Deployment</h1> </body> </html> We want to deploy the new version of production site with green deployment. The playbook looks very simple as follows, it will update the configuration and reloads the haproxy service to serve the new production deployment. - name : Updating to GREEN deployment hosts : proxyserver become : yes tasks : - name : updating proxy configuration template : src : haproxy.cfg.j2 dest : /etc/haproxy/haproxy.cfg - name : updating the service service : name : haproxy state : reloaded - debug : msg : \"GREEN deployment successful. Please check your server :)\" Continuous Security Scanning for Docker Containers \u00b6 Docker containers are the new way developers package applications. The best feature of containers is the fact that they contain the code, runtime, system libraries, and all the settings that are required for the application to work. Due to the ease of use and deployment, more and more applications are getting deployed in containers for production use. With so many moving parts, it becomes imperative that we have the capability to continuously scan Docker containers for security issues. Understanding continuous security concepts \u00b6 One of the key approaches to emerge out of DevOps is the idea of immutable infrastructure. It means that every time there needs to be a runtime change , either in application code or configuration, the containers are built and deployed again and the existing running ones are torn down. Since that allows for predictability, resilience, and simplifies deployment choices at runtime, it is no surprise that many operations teams are moving toward it. With that comes the question of when these containers should be tested for security and compliance. By embracing the process of continuous security scanning and monitoring, you can automate for a variety of workloads and workflows. Automating vulnerability assessments of Docker containers using Ansible \u00b6 Tool: Description There are many different ways of evaluating the security of containers. Docker Bench: A security shell script to perform checks based on CIS Clair: A tool to perform vulnerability analysis based on the CVE database Anchore: A tool to perform security evaluation and make runtime policy decisions vuls: An agent-less vulnerability scanner with CVE, OVAL database osquery: OS instrumentation framework for OS analytics to do HIDS-type activities Docker Bench for Security \u00b6 Docker Bench for Security is a shell script to perform multiple checks against the Docker container environment. It will give a more detailed view of the security configuration based on CIS benchmarks. This script supports most of the Unix operating systems as it was built based on the POSIX 2004 compliant. The following are the high-level areas of checks this script will perform: Host configuration Docker daemon configuration and files Docker container images Docker runtime Docker security operations Docker swarm configuration - name : Docker bench security playbook hosts : docker remote_user : ubuntu become : yes tasks : - name : make sure git installed apt : name : git state : present - name : download the docker bench security git : repo : https://github.com/docker/docker-bench-security.git dest : /opt/docker-bench-security - name : running docker-bench-security scan command : docker-bench-security.sh -l /tmp/output.log args : chdir : /opt/docker-bench-security/ - name : downloading report locally fetch : src : /tmp/output.log dest : \"{{ playbook_dir }}/{{ inventory_hostname }}-docker-report-{{ ansible_date_time.date }}.log\" flat : yes - name : report location debug : msg : \"Report can be found at {{ playbook_dir }}/{{ inventory_hostname }}-docker-report-{{ ansible_date_time.date }}.log\" </mark> The output of the playbook will download and scan the containers based on the CIS benchmark and store the results in a log file Clair \u00b6 Clair allows us to perform static vulnerability analysis against containers by checking with the existing vulnerability database. It allows us to perform vulnerability analysis checks against our Docker container images using the Clair database. Setting up Clair itself is really difficult and scanning using the API with Docker images makes more difficult. Here comes clair-scanner , it makes really simple to set up and perform scans using the REST API. Clair-scanner can trigger a simple scan against a container based on certain events, to check for existing vulnerabilities. Furthermore, this report can be forwarded to perform the team responsible for fixes and so on. # It assumes that the target system has Docker and the required libraries installed - name : Clair Scanner Server Setup hosts : docker remote_user : ubuntu become : yes tasks : - name : setting up clair-db docker_container : name : clair_db image : arminc/clair-db exposed_ports : - 5432 - name : setting up clair-local-scan docker_container : name : clair image : arminc/clair-local-scan:v2.0.1 ports : - \"6060:6060\" links : - \"clair_db:postgres\" # Setting up clair-scanner with Docker containers using Ansible # It will take a while to download and setup the CVE database after playbook execution. This playbook will be used to run clair-scanner to perform an analysis on the containers by making an API request to the server. - name : Scanning containers using clair-scanner hosts : docker remote_user : ubuntu become : yes vars : image_to_scan : \"debian:sid\" # container to scan for vulnerabilities clair_server : \"http://192.168.1.10:6060\" # clair server api endpoint tasks : - name : downloading and setting up clair-scanner binary get_url : url : https://github.com/arminc/clair-scanner/releases/download/v6/clair-scanner_linux_amd64 dest : /usr/local/bin/clair-scanner mode : 0755 - name : scanning {{ image_to_scan }} container for vulnerabilities command : clair-scanner -r /tmp/{{ image_to_scan }}-scan-report.json -c {{ clair_server }} --ip 0.0.0.0 {{ image_to_scan }} register : scan_output ignore_errors : yes - name : downloading the report locally fetch : src : /tmp/{{ image_to_scan }}-scan-report.json dest : {{ playbook_dir }} /{{ image_to_scan }}-scan-report.json flat : yes Scheduled scans using Ansible Tower for Docker security \u00b6 Continuous security processes are all about the loop of planning, doing, measuring, and acting By following standard checklists and benchmarks and using Ansible to execute them on containers, we can check for security issues and act on them. Anchore \u2013 open container compliance platform \u00b6 Anchore is one of the most popular tools and services to perform analysis, inspection, and certification of container images. Anchore is an analysis and inspection platform for containers. It provides multiple services and platforms to set up, the most stable and powerful way is to set up the local service using Anchore Engine, which can be accessed via the REST API. High level operations Anchore can perform: Policy evaluation operations Image operations Policy operations Registry operations Subscription operations System operations Anchore Engine service setup \u00b6 This playbook will set up the Anchore Engine service, which contains the engine container as well as the postgres to store database information. The admin_password variable is the admin user password to access the REST API of Anchore. - name : anchore server setup hosts : anchore become : yes vars : db_password : changeme admin_password : secretpassword tasks : - name : creating volumes file : path : \"{{ item }}\" recurse : yes state : directory with_items : - /root/aevolume/db - /root/aevolume/config - name : copying anchore-engine configuration template : src : config.yaml.j2 dest : /root/aevolume/config/config.yaml - name : starting anchore-db container docker_container : name : anchore-db image : postgres:9 volumes : - \"/root/aevolume/db/:/var/lib/postgresql/data/pgdata/\" env : POSTGRES_PASSWORD : \"{{ db_password }}\" PGDATA : \"/var/lib/postgresql/data/pgdata/\" - name : starting anchore-engine container docker_container : name : anchore-engine image : anchore/anchore-engine ports : - 8228:8228 - 8338:8338 volumes : - \"/root/aevolume/config/config.yaml:/config/config.yaml:ro\" - \"/var/run/docker.sock:/var/run/docker.sock:ro\" links : - anchore-db:anchore-db Anchore CLI scanner \u00b6 Now that we have the Anchore Engine service REST API with access details, we can use this to perform the scanning of container images in any host. The following steps are the Ansible Tower setup to perform continuous scanning of container images for vulnerabilities. - name : anchore-cli scan hosts : anchore become : yes vars : scan_image_name : \"docker.io/library/ubuntu:latest\" anchore_vars : ANCHORE_CLI_URL : http://localhost:8228/v1 ANCHORE_CLI_USER : admin ANCHORE_CLI_PASS : secretpassword tasks : - name : installing anchore-cli pip : name : \"{{ item }}\" with_items : - anchorecli - pyyaml - name : downloading image docker_image : name : \"{{ scan_image_name }}\" - name : adding image for analysis command : \"anchore-cli image add {{ scan_image_name }}\" environment : \"{{anchore_vars}}\" - name : wait for analysis to compelte command : \"anchore-cli image content {{ scan_image_name }} os\" register : analysis until : analysis.rc != 1 retries : 10 delay : 30 ignore_errors : yes environment : \"{{anchore_vars}}\" - name : vulnerabilities results command : \"anchore-cli image vuln {{ scan_image_name }} os\" register : vuln_output environment : \"{{anchore_vars}}\" - name : \"vulnerabilities in {{ scan_image_name }}\" debug : msg : \"{{ vuln_output.stdout_lines }}\" Scheduled scans using Ansible Tower for operating systems and kernel security \u00b6 While most of the discussed tools can be used for scanning and maintaining a benchmark for security, we should think about the entire process of the incident response and threat detection workflow: Preparation Detection and analysis Containment, eradication, and recovery Post-incident activity Setting up all such scanners is our preparation. Using the output of these scanners gives us the ability to detect and analyze. Both containment and recovery are beyond the scope of such tools. For the process of recovery and post-incident activity, you may want to consider playbooks that can trash the current infrastructure and recreate it as it is. As part of our preparation, it may be useful to get familiar with the following terms as you will see them being used repeatedly in the world of vulnerability scanners and vulnerability management tools: Term: Full form (if any): Description of the term CVE: Common Vulnerabilities and Exposures: It is a list of cybersecurity vulnerability identifiers. Usage typically includes CVE IDs. OVAL: Open Vulnerability and Assessment Language: A language for finding out and naming vulnerabilities and configuration issues in computer systems. CWE: Common Weakness Enumeration: A common list of software security weaknesses. NVD: National Vulnerability Database: A US government vulnerability management database available for public use in XML format. Vuls \u2013 vulnerability scanner \u00b6 Vuls is an agent-less scanner written in golang. It supports a different variety of Linux operating systems. It performs the complete end-to-end security system administrative tasks such as scanning for security vulnerabilities and security software updates. It analyzes the system for required security vulnerabilities, performs security risk analysis based on the CVE score, sends notifications via Slack and email, and also provides a simple web report with historical data. The playbook has mainly two roles for setting up vuls using Docker containers. vuls_containers_download vuls_database_download - name : setting up vuls using docker containers hosts : vuls become : yes roles : - vuls_containers_download - vuls_database_download # Pulling the Docker containers locally using the docker_image module: - name : pulling containers locally docker_image : name : \"{{ item }}\" pull : yes with_items : - vuls/go-cve-dictionary - vuls/goval-dictionary - vuls/vuls # Then downloading the CVE and OVAL databases for the required operating systems and distributions versions - name : fetching NVD database locally docker_container : name : \"cve-{{ item }}\" image : vuls/go-cve-dictionary auto_remove : yes interactive : yes state : started command : fetchnvd -years \"{{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/go-cve-dictionary-log:/var/log/vuls\" with_sequence : start=2002 end=\"{{ nvd_database_years }}\" - name : fetching redhat oval data docker_container : name : \"redhat-oval-{{ item }}\" image : vuls/goval-dictionary auto_remove : yes interactive : yes state : started command : fetch-redhat \"{{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/goval-dictionary-log:/var/log/vuls\" with_items : \"{{ redhat_oval_versions }}\" - name : fetching ubuntu oval data docker_container : name : \"ubuntu-oval-{{ item }}\" image : vuls/goval-dictionary auto_remove : yes interactive : yes state : started command : \"fetch-ubuntu {{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/goval-dictionary-log:/var/log/vuls\" with_items : \"{{ ubuntu_oval_versions }}\" The global variables file looks as follows. We can add more redhat_oval_versions, such as 5. The nvd_database_years will download the CVE database up until the end of 2017: vuls_data_directory : \"/vuls_data\" nvd_database_years : 2017 redhat_oval_versions : - 6 - 7 ubuntu_oval_versions : - 12 - 14 - 16 Now, it's time to perform the scanning and reporting using the vuls Docker containers. The following playbook contains simple steps to perform the vuls scan against virtual machines and containers, and send the report to slack and web: - name : scanning and reporting using vuls hosts : vuls become : yes vars : vuls_data_directory : \"/vuls_data\" slack_web_hook_url : https://hooks.slack.com/services/XXXXXXX/XXXXXXXXXXXXXXXXXXXXX slack_channel : \"#vuls\" slack_emoji : \":ghost:\" server_to_scan : 192.168.33.80 server_username : vagrant server_key_file_name : 192-168-33-80 tasks : - name : copying configuraiton file and ssh keys template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" mode : 0400 with_items : - { src : 'config.toml' , dst : '/root/config.toml' } - { src : '192-168-33-80' , dst : '/root/.ssh/192-168-33-80' } - name : running config test docker_container : name : configtest image : vuls/vuls auto_remove : yes interactive : yes state : started command : configtest -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - name : running vuls scanner docker_container : name : vulsscan image : vuls/vuls auto_remove : yes interactive : yes state : started command : scan -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - \"/etc/localtime:/etc/localtime:ro\" env : TZ : \"Asia/Kolkata\" - name : sending slack report docker_container : name : vulsreport image : vuls/vuls auto_remove : yes interactive : yes state : started command : report -cvedb-path=/vuls/cve.sqlite3 -ovaldb-path=/vuls/oval.sqlite3 --to-slack -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - \"/etc/localtime:/etc/localtime:ro\" - name : vuls webui report docker_container : name : vulswebui image : vuls/vulsrepo interactive : yes volumes : - \"{{ vuls_data_directory }}:/vuls\" ports : - \"80:5111\" The following file is the configuration file for vuls to perform the scanning. This holds the configuration for slack alerting and also the server to perform scanning. This can be configured very effectively as required using vuls documentation: [ slack ] hookURL = \"{{ slack_web_hook_url}}\" channel = \"{{ slack_channel }}\" iconEmoji = \"{{ slack_emoji }}\" [servers] [servers.{{ server_key_file_name }}] host = \"{{ server_to_scan }}\" user = \"{{ server_username }}\" keyPath = \"/root/.ssh/{{ server_key_file_name }}\" We can also visit the web UI interface of the vuls server IP address to see the detailed results in tabular and portable format. This is very useful to manage large amount of servers and patches at scale. This can be part of the CI/CD life cycle as an infrastructure code and then we can run this as a scheduled scan using Ansible Tower or Jenkins. Scheduled scans for file integrity checks, host-level monitoring using Ansible for various compliance initiatives \u00b6 One of the many advantages of being able to execute commands on the host using Ansible is the ability to get internal system information, such as: File hashes Network connections List of running processes It can act as a lightweight Host-Based Intrusion Detection System (HIDS). While this may not eliminate the case for a purpose-built HIDS in many cases, we can execute the same kind of security tasks using a tool such as Facebook's osquery along with Ansible. osquery \u00b6 osquery is an operating system instrumentation framework by Facebook and written in C++, that supports Windows, Linux, OS X (macOS), and other operating systems. It provides an interface to query an operating system using an SQL like syntax. By using this, we can perform low-level activities such as running processes, kernel configurations, network connections, and file integrity checks. Overall it's like a host-based intrusion detection system (HIDS) endpoint security. It provides osquery as a service, system interactive shell, and so on. Hence we can use this to perform centralized monitoring and security management solutions. This playbook is to set up and configure the osquery agent in your Linux servers to monitor and look for vulnerabilities, file integrity monitoring, and many other compliance activities, and then log them for sending to a centralized logging monitoring system. The reference tutorial can be followed at DigitalOcean . - name : setting up osquery hosts : linuxservers become : yes tasks : - name : installing osquery apt : deb : https://pkg.osquery.io/deb/osquery_2.10.2_1.linux.amd64.deb update_cache : yes - name : adding osquery configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : fim.conf , dst : /usr/share/osquery/packs/fim.conf } - { src : osquery.conf , dst : /etc/osquery/osquery.conf } - name : starting and enabling osquery service service : name : osqueryd state : started enabled : yes The following fim.conf code snippet is the pack for file integrity monitoring and it monitors for file events in the /home, /etc, and /tmp directories every 300 seconds. It uses Secure Hash Algorithm (SHA) checksum to validate the changes. This can be used to find out whether attackers add their own SSH keys or audit log changes against system configuration changes for compliance and other activities. { \"queries\" : { \"file_events\" : { \"query\" : \"select * from file_events;\" , \"removed\" : false , \"interval\" : 300 } }, \"file_paths\" : { \"homes\" : [ \"/root/.ssh/%%\" , \"/home/%/.ssh/%%\" ], \"etc\" : [ \"/etc/%%\" ], \"home\" : [ \"/home/%%\" ], \"tmp\" : [ \"/tmp/%%\" ] } } The following code snippet is the osquery service configuration. This can be modified as required to monitor and log by osquery service. { \"options\" : { \"config_plugin\" : \"filesystem\" , \"logger_plugin\" : \"filesystem\" , \"logger_path\" : \"/var/log/osquery\" , \"disable_logging\" : \"false\" , \"log_result_events\" : \"true\" , \"schedule_splay_percent\" : \"10\" , \"pidfile\" : \"/var/osquery/osquery.pidfile\" , \"events_expiry\" : \"3600\" , \"database_path\" : \"/var/osquery/osquery.db\" , \"verbose\" : \"false\" , \"worker_threads\" : \"2\" , \"enable_monitor\" : \"true\" , \"disable_events\" : \"false\" , \"disable_audit\" : \"false\" , \"audit_allow_config\" : \"true\" , \"host_identifier\" : \"hostname\" , \"enable_syslog\" : \"true\" , \"audit_allow_sockets\" : \"true\" , \"schedule_default_interval\" : \"3600\" }, \"schedule\" : { \"crontab\" : { \"query\" : \"SELECT * FROM crontab;\" , \"interval\" : 300 }, \"system_profile\" : { \"query\" : \"SELECT * FROM osquery_schedule;\" }, \"system_info\" : { \"query\" : \"SELECT hostname, cpu_brand, physical_memory FROM system_info;\" , \"interval\" : 3600 } }, \"decorators\" : { \"load\" : [ \"SELECT uuid AS host_uuid FROM system_info;\" , \"SELECT user AS username FROM logged_in_users ORDER BY time DESC LIMIT 1;\" ] }, \"packs\" : { \"fim\" : \"/usr/share/osquery/packs/fim.conf\" , \"osquery-monitoring\" : \"/usr/share/osquery/packs/osquery-monitoring.conf\" , \"incident-response\" : \"/usr/share/osquery/packs/incident-response.conf\" , \"it-compliance\" : \"/usr/share/osquery/packs/it-compliance.conf\" , \"vuln-management\" : \"/usr/share/osquery/packs/vuln-management.conf\" } } The goal is not just setting up osquery, we can use the logs to build a centralized real-time monitoring system using our Elastic stack. We can use the Filebeat agent to forward these logs to our Elastic stack and we can view them and build a centralized dashboard for alerting and monitoring. This idea can be extended for building some automated defences by taking actions against attacks by using automated Ansible playbooks for known actions. The world is moving toward containers and this kind of monitoring gives us a look at low-level things such as kernel security checks, and file integrity checks on host level. When attackers try to bypass containers and get access to hosts to escalate privileges, we can detect and defend them using this kind of setup. Summary \u00b6 Containers are rapidly changing the world of developers and operations teams. The rate of change is accelerating, and in this new world, security automation gets to play a front and center role. By leveraging our knowledge of using Ansible for scripting play-by-play commands along with excellent tools such as Anchore and osquery , we can measure, analyze, and benchmark our containers for security. This allows us to build end-to-end automatic processes of securing, scanning and remediating containers. Automating Lab Setups for Forensics Collection and Malware Analysis \u00b6 Malware is one of the biggest challenges faced by the security community. It impacts everyone who gets to interact with information systems. While there is a massive effort required in keeping computers safe from malware for operational systems, a big chunk of work in malware defenses is about understanding where they come from and what they are capable of. Another important aspect of malware analysis is the ability to collaborate and share threats using the Malware Information Sharing Platform (MISP). One of the initial phases of malware analysis is identification and classification. The most popular source is using VirusTotal to scan and get the results of the malware samples, domain information, and so on. It has a very rich API and a lot of people have written custom apps that leverage the API to perform the automated scans using the API key for identifying the malware type. It generally checks using more than 60 antivirus scanners and tools and provides detailed information. VirusTotal API tool set up \u00b6 The following playbook will set up the VirusTotal API tool - name : setting up VirusTotal hosts : malware remote_user : ubuntu become : yes tasks : - name : installing pip apt : name : \"{{ item }}\" with_items : - python-pip - unzip - name : checking if vt already exists stat : path : /usr/local/bin/vt register : vt_status - name : downloading VirusTotal api tool repo unarchive : src : \"https://github.com/doomedraven/VirusTotalApi/archive/master.zip\" dest : /tmp/ remote_src : yes when : vt_status.stat.exists == False - name : installing the dependencies pip : requirements : /tmp/VirusTotalApi-master/requirements.txt when : vt_status.stat.exists == False - name : installing vt command : python /tmp/VirusTotalApi-master/setup.py install when : vt_status.stat.exists == False The playbook execution will download the repository and set up the VirusTotal API tool. The following playbook will find and copy the local malware samples to a remote system and scan them recursively and return the results. Once the scan has been completed, it will remove the samples from the remote system. - name : scanning file in VirusTotal hosts : malware remote_user : ubuntu vars : vt_api_key : XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX #use Ansible-vault vt_api_type : public # public/private vt_intelligence_access : False # True/False files_in_local_system : /tmp/samples/ files_in_remote_system : /tmp/sample-file/ tasks : - name : creating samples directory file : path : \"{{ files_in_remote_system }}\" state : directory - name : copying file to remote system copy : src : \"{{ files_in_local_system }}\" dest : \"{{ files_in_remote_system }}\" directory_mode : yes - name : copying configuration template : src : config.j2 dest : \"{{ files_in_remote_system }}/.vtapi\" - name : running VirusTotal scan command : \"vt -fr {{ files_in_remote_system }}\" args : chdir : \"{{ files_in_remote_system }}\" register : vt_scan - name : removing the samples file : path : \"{{ files_in_remote_system }}\" state : absent - name : VirusTotal scan results debug : msg : \"{{ vt_scan.stdout_lines }}\" Creating Ansible playbooks for collection and storage with secure backup of forensic artifacts \u00b6 Ansible is an apt replacement for all kinds of bash scripts. Typically, for most activities that require analysis, we follow a set pattern: Collect logs from running processes into files with a path we already know Copy the content from these log files periodically to a secure storage locally or accessible remotely over SSH or a network file share Once copied successfully, rotate the logs Since there is a bit of network activity involved, our bash scripts are usually written to be fault tolerant with regard to network connections and become complex very soon. Ansible playbooks can be used to do all of that while being simple to read for everyone. Collecting log artifacts for incident response \u00b6 The key phase in incident response is log analysis. This playbook will collect the logs from all the hosts and store it locally. This allows responders to perform the further analysis. # Reference https://www.Ansible.com/security-automation-with-Ansible - name : Gather log files hosts : servers become : yes tasks : - name : List files to grab find : paths : - /var/log patterns : - '*.log*' recurse : yes register : log_files - name : Grab files fetch : src : \"{{ item.path }}\" dest : \"/tmp/LOGS_{{ Ansible_fqdn }}/\" with_items : \"{{ log_files.files }}\" This playbook execution will collect a list of logs in specified locations in remote hosts using Ansible modules and store them in the local system. Secure backups for data collection \u00b6 When collecting multiple sets of data from servers, it's important to store them securely with encrypted backups. This can be achieved by backing up the data to storage services such as S3. This Ansible playbook allows us to install and copy the collected data to the AWS S3 service with encryption enabled. - name : backing up the log data hosts : localhost gather_facts : false become : yes vars : s3_access_key : XXXXXXX # Use Ansible-vault to encrypt s3_access_secret : XXXXXXX # Use Ansible-vault to encrypt localfolder : /tmp/LOGS/ # Trailing slash is important remotebucket : secretforensicsdatausingAnsible # This should be unique in s3 tasks : - name : installing s3cmd if not installed apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-magic - python-dateutil - s3cmd - name : create s3cmd config file template : src : s3cmd.j2 dest : /root/.s3cfg owner : root group : root mode : 0640 - name : make sure \"{{ remotebucket }}\" is avilable command : \"s3cmd mb s3://{{ remotebucket }}/ -c /root/.s3cfg\" - name : running the s3 backup to \"{{ remotebucket }}\" command : \"s3cmd sync {{ localfolder }} --preserve s3://{{ remotebucket }}/ -c /root/.s3cfg\" The Ansible playbook installing s3cmd, creating the new bucket called secretforensicsdatausingAnsible, and copying the local log data to the remote S3 bucket. The configuration file looks like the following for the s3cmd configuration [ default ] access_key = {{ s3_access_key }} secret_key = {{ s3_access_secret }} host_base = s3.amazonaws.com host_bucket = %(bucket)s.s3.amazonaws.com website_endpoint = http://%(bucket)s.s3-website-%(location)s.amazonaws.com/ use_https = True signature_v2 = True We can see that the logs are successfully uploaded into the secretforensicsdatausingAnsible S3 bucket in AWS S3.","title":"Security Hardening for Applications and Networks"},{"location":"learning/ansible/security_hardening/#security-hardening-for-applications-and-networks","text":"Security hardening is the most obvious task for any security-conscious endeavor. By doing the effort of securing systems, applications, and networks, one can achieve multiple security goals given as follows: Ensuring that applications and networks are not compromised (sometimes) Making it difficult for compromises to stay hidden for long Securing by default ensures that compromises in one part of the network don't propagate further and more We will build playbooks that will allow us to do the following things: Secure our master images so that as soon as the applications and systems are part of the network, they offer decent security Execute audit processes so that we can verify and measure periodically if the applications, systems, and networks are in line with the security policies that are required by the organization Security hardening with benchmarks such as Center for Internet Security (CIS), Security Technical Implementation Guides (STIG), and National Institute of Standards and Technology (NIST) Automating security audit checks for networking devices using Ansible Automating security audit checks for applications using Ansible Automated patching approaches using Ansible","title":"Security Hardening for Applications and Networks"},{"location":"learning/ansible/security_hardening/#security-hardening-with-benchmarks-such-as-cis-stigs-and-nist","text":"Benchmarks provide a great way for anyone to gain assurance of their individual security efforts. Hardening for security mostly boils down to do the following: Agreeing on what is the minimal set of configuration that qualifies as secure configuration. This is usually defined as a hardening benchmark or framework. Making changes to all the aspects of the system that are touched by such configuration. Measuring periodically if the application and system are still in line with the configuration or if there is any deviation. If any deviation is found, take corrective action to fix that. If no deviation is found, log that. Since software is always getting upgraded, staying on top of the latest configuration guidelines and benchmarks is most important.","title":"Security hardening with benchmarks such as CIS, STIGs, and NIST"},{"location":"learning/ansible/security_hardening/#operating-system-hardening-for-baseline-using-an-ansible-playbook","text":"We will see how we can use existing playbooks from the community (Ansible Galaxy). The following playbook provides multiple security configurations, standards, and ways to protect operating system against different attacks and security vulnerabilities. Some of the tasks it will perform include the following: Configures package management, for example, allows only signed packages Remove packages with known issues Configures pam and pam_limits modules Shadow password suite configuration Configures system path permissions Disable core dumps via soft limits Restrict root logins to system console Set SUIDs Configures kernel parameters via sysctl Downloading and executing Ansible playbooks from galaxy is as simple as follows: ansible-galaxy install dev-sec.os-hardening - hosts : localhost become : yes roles : - dev-sec.os-hardening The preceding playbook will detect the operating system and perform hardening steps based on the different guidelines. This can be configured as required by updating the default variables values. Refer to dev-sec/ansible-os-hardening for more details about the playbook.","title":"Operating system hardening for baseline using an Ansible playbook"},{"location":"learning/ansible/security_hardening/#stigs-ansible-role-for-automated-security-hardening-for-linux-hosts","text":"OpenStack has an awesome project named ansible-hardening , which applies the security configuration changes as per the STIGs standards. It performs security hardening for the following domains: accounts: User account security controls aide: Advanced Intrusion Detection Environment auditd: Audit daemon auth: Authentication file_perms: Filesystem permissions graphical: Graphical login security controls kernel: Kernel parameters lsm: Linux Security Modules misc: Miscellaneous security controls packages: Package managers sshd: SSH daemon Download the role from the GitHub repository itself using ansible-galaxy as follows: ansible-galaxy install git+https://github.com/openstack/ansible-hardening - name : STIGs ansible-hardening for automated security hardening hosts : servers become : yes remote_user : \"{{ remote_user_name }}\" vars : remote_user_name : vagrant security_ntp_servers : - time.nist.gov - time.google.com roles : - ansible-hardening","title":"STIGs Ansible role for automated security hardening for Linux hosts"},{"location":"learning/ansible/security_hardening/#continuous-security-scans-and-reports-for-openscap-using-ansible-tower","text":"OpenSCAP is a set of security tools, policies, and standards to perform security compliance checks against the systems by following SCAP. SCAP is the U.S. standard maintained by NIST. OpenSCAP follows these steps to perform scanning on your system: Install SCAP Workbench or OpenSCAP Base (for more information, visit https://www.open-scap.org ) Choose a policy Adjust your settings Evaluate the system Check playbook reference at https://medium.com/@jackprice/ansible-openscap-for-compliance-automation-14200fe70663 . - hosts : all become : yes vars : oscap_profile : xccdf_org.ssgproject.content_profile_pci-dss oscap_policy : ssg-rhel7-ds tasks : - name : install openscap scanner package : name : \"{{ item }}\" state : latest with_items : - openscap-scanner - scap-security-guide - block : - name : run openscap command : > oscap xccdf eval --profile {{ oscap_profile }} --results-arf /tmp/oscap-arf.xml --report /tmp/oscap-report.html --fetch-remote-resources /usr/share/xml/scap/ssg/content/{{ oscap_policy }}.xml always : - name : download report fetch : src : /tmp/oscap-report.html dest : ./{{ inventory_hostname }}.html flat : yes We can use this playbook to perform continuously automated checks using Ansible Tower First, we need to create a directory in Ansible Tower server in order to store this playbook with the awx user permission to add the custom playbook. Create a new project in Ansible Tower to perform the OpenSCAP setup and scan against the checks. Then, we have to create a new job to execute the playbook. Here, we can include the list of hosts, credentials for login, and other details required to perform the execution. This audit can be scheduled to perform frequently. We can also launch this job on demand when required. The output of the playbook will generate the OpenSCAP report, and it will be fetched to Ansible Tower. We can access this playbook at the /tmp/ location. Also, we can send this report to the other centralized reporting server if required. We can also set up notifications based on playbook execution results. By doing this, we can send this notifications to respective channels, such as email, slack, and message.","title":"Continuous security scans and reports for OpenSCAP using Ansible Tower"},{"location":"learning/ansible/security_hardening/#cis-benchmarks","text":"CIS has benchmarks for different type OS, software, and services. The following are some high-level categories: Desktops and web browsers Mobile devices Network devices Security metrics Servers \u2013 operating systems Servers \u2013 other Virtualization platforms, cloud, and other","title":"CIS Benchmarks"},{"location":"learning/ansible/security_hardening/#ubuntu-cis-benchmarks-server-level","text":"CIS Benchmarks Ubuntu provides prescriptive guidance to establish a secure configuration posture for Ubuntu Linux systems running on x86 and x64 platforms. This benchmark is intended for system and application administrators, security specialists, auditors, help desk, and platform deployment personnel who plan to develop, deploy, assess, or secure solutions that incorporate Linux platform. Here are the high-level six domains that are part of CIS Ubuntu 16.04 LTS benchmarks: Initial setup: Filesystem configuration Configure software updates Filesystem integrity checking Secure boot settings Additional process hardening Mandatory access control Warning banners Services: Inted Services Special purpose services Service clients Network configuration: Network parameters (host only) Network parameters (host and router) IPv6 TCP wrappers Uncommon network protocols Logging and auditing: Configure system accounting (auditd) Configure logging Access, authentication, and authorization: Configure cron SSH server configuration Configure PAM User accounts and environment System maintenance: System file permissions User and group settings # Playbooks git clone https://github.com/oguya/cis-ubuntu-14-ansible.git cd cis-ubuntu-14-ansible Then, update the variables and inventory and execute the playbook using the following command. The variables are not required mostly, as this performs against different CIS checks unless, if we wanted to customize the benchmarks as per the organization. ansible-playbook -i inventory cis.yml The preceding playbook will execute the CIS security benchmark against an Ubuntu server and performs all the checks listed in the CIS guidelines.","title":"Ubuntu CIS Benchmarks (server level)"},{"location":"learning/ansible/security_hardening/#aws-benchmarks-cloud-provider-level","text":"AWS CIS Benchmarks provides prescriptive guidance to configure security options for a subset of AWS with an emphasis on foundational, testable, and architecture agnostic settings. Here are the high-level domains, which are part of AWS CIS Benchmarks: Identity and access management Logging Monitoring Networking Extra Currently, there is a tool named prowler ( Alfresco/prowler ) based on AWS-CLI commands for AWS account security assessment and hardening. Before running the playbook, we have to provide AWS API keys to perform security audit. This can be created using IAM role in AWS service. If you have an already existing account with required privileges, these steps can be skipped. Create a new user in your AWS account with programmatic access. Apply the SecurityAudit policy for the user from existing policies in IAM console. Create the new user by following the steps. Make sure that you safely save the Access key ID and Secret access key for later use. The following playbook assume that you already have installed python and pip in your local system. - name : AWS CIS Benchmarks playbook hosts : localhost become : yes vars : aws_access_key : XXXXXXXX aws_secret_key : XXXXXXXX tasks : - name : installing aws cli and ansi2html pip : name : \"{{ item }}\" with_items : - awscli - ansi2html - name : downloading and setting up prowler get_url : url : https://raw.githubusercontent.com/Alfresco/prowler/master/prowler dest : /usr/bin/prowler mode : 0755 - name : running prowler full scan shell : \"prowler | ansi2html -la > ./aws-cis-report-{{ ansible_date_time.epoch }}.html\" environment : AWS_ACCESS_KEY_ID : \"{{ aws_access_key }}\" AWS_SECRET_ACCESS_KEY : \"{{ aws_secret_key }}\" - name : AWS CIS Benchmarks report downloaded debug : msg : \"Report can be found at ./aws-cis-report-{{ ansible_date_time.epoch }}.html\" The playbook will trigger the setup and security audit scan for AWS CIS Benchmarks using the prowler tool. Prowler-generated HTML report can be downloaded in different formats as required and also scanning checks can be configured as required. More reference about the tool can be found at Alfresco/prowler .","title":"AWS benchmarks (cloud provider level)"},{"location":"learning/ansible/security_hardening/#automating-security-audit-checks-for-networking-devices-using-ansible","text":"We can use this to do security audit checks for networking devices.","title":"Automating security audit checks for networking devices using Ansible"},{"location":"learning/ansible/security_hardening/#nmap-scanning-and-nse","text":"Network Mapper (Nmap) is a free open source software to perform network discovery, scanning, audit, and many others. It has a various amount of features such as OS detection, system fingerprinting, firewall detection, and many other features. Nmap Scripting Engine (Nmap NSE) provides advanced capabilities like scanning for particular vulnerabilities and attacks. We can also write and extend Nmap using our own custom script. Nmap is a swiss army knife for pen testers (security testers) and network security teams. - name : Basic NMAP Scan Playbook hosts : localhost gather_facts : false vars : top_ports : 1000 network_hosts : - 192.168.1.1 - scanme.nmap.org - 127.0.0.1 - 192.168.11.0/24 tasks : - name : check if nmap installed and install apt : name : nmap update_cache : yes state : present become : yes - name : top ports scan shell : \"nmap --top-ports {{ top_ports }} -Pn -oA nmap-scan-%Y-%m-%d {{ network_hosts|join(' ') }}\" {{ network_hosts|join(' ') }} is a Jinja2 feature named filter arguments to parse the given network_hosts by space delimited network_hosts variable holds the list of IPs, network range (CIDR), hosts, and so on to perform scan using Nmap top_ports is the number that is ranging from 0 to 65535. Nmap by default picks commonly opened top ports -Pn specifies that scans the host if ping (ICMP) doesn't work also -oA gets the output in all formats, which includes gnmap (greppable format), Nmap, and XML","title":"Nmap scanning and NSE"},{"location":"learning/ansible/security_hardening/#nmap-nse-scanning-playbook","text":"This playbook will perform enumeration of directories used by popular web applications and servers using http-enum and finds options that are supported by an HTTP server using http-methods using Nmap scripts. - name : Advanced NMAP Scan using NSE hosts : localhost vars : ports : - 80 - 443 scan_host : scanme.nmap.org tasks : - name : Running Nmap NSE scan shell : \"nmap -Pn -p {{ ports|join(',') }} --script {{ item }} -oA nmap-{{ item }}-results-%Y-%m-%d {{ scan_host }}\" with_items : - http-methods - http-enum The http-enum script runs additional tests against network ports where web servers are detected. We can see that two folders were discovered by the script and additionally all HTTP methods that are supported got enumerated as well.","title":"Nmap NSE scanning playbook"},{"location":"learning/ansible/security_hardening/#automation-security-audit-checks-for-applications-using-ansible","text":"Modern applications can get pretty complex fairly quickly. Having the ability to run automation to do security tasks is almost a mandatory requirement. The different types of application security scanning we can do can range from the following: Run CI/CD scanning against the source code (for example, RIPS and brakeman). Dependency checking scanners (for example, OWASP dependency checker and snyk.io ( https://snyk.io/ )). Once deployed then run the web application scanner (for example, Nikto, Arachni, and w3af). Framework-specific security scanners (for example, WPScan and Droopscan) and many other.","title":"Automation security audit checks for applications using Ansible"},{"location":"learning/ansible/security_hardening/#source-code-analysis-scanners","text":"This is one of the first and common way to minimize the security risk while applications going to production. Source code analysis scanner also known as Static Application Security Testing (SAST) will help to find security issues by analyzing the source code of the application. Source code analysis is kind of white box testing and looking through code. This kind of testing methodology may not find 100% coverage of security vulnerabilities, and it requires manual testing as well. For example, finding logical vulnerabilities requires some kind of user interactions such as dynamic functionalities. For example, if you are scanning PHP code, then RIPS ; if it's Ruby on Rails code, then it's Brakeman ; and if it's python, then Bandit","title":"Source code analysis scanners"},{"location":"learning/ansible/security_hardening/#dependency-checking-scanners","text":"Most of the developers use third-party libraries while developing applications, and it's very common to see using open source plugins and modules inside their code. So dependency checks will allow us to find using components with known vulnerabilities (OWASP A9) issues in application code by scanning the libraries against the CVE and NIST vulnerability database. There are multiple projects out there in the market for performing these checks, and some of them includes the following: OWASP Dependency-Check Snyk.io ( https://snyk.io/ ) Retire.js [:] SourceClear and many other","title":"Dependency-checking scanners"},{"location":"learning/ansible/security_hardening/#running-web-application-security-scanners","text":"This is the phase where the application went live to QA, stage, (or) Production. Then, we wanted to perform security scans like an attacker (black box view). At this stage, an application will have all the dynamic functionalities and server configurations applied. These scanner results tell us how good the server configured and any other application security issues before releasing the replica copy into the production. There are many tools in the market to do these jobs for you in both open source and commercial world. Nikto Arachni w3af Acunetix","title":"Running web application security scanners"},{"location":"learning/ansible/security_hardening/#framework-specific-security-scanners","text":"This kind of check and scanning is to perform against specific to framework, CMS, and platforms. It allows to get more detailed results by validating against multiple security test cases and checks. Scanning against WordPress CMS using WPScan Scanning against JavaScript libraries using Retire.js Scanning against Drupal CMS using Droopescan","title":"Framework-specific security scanners"},{"location":"learning/ansible/security_hardening/#automated-patching-approaches-using-ansible","text":"Patching and updating is a task that everyone who has to manage production systems has to deal with. There are two approaches that we will look are as follows: Rolling updates BlueGreen deployments","title":"Automated patching approaches using Ansible"},{"location":"learning/ansible/security_hardening/#rolling-updates","text":"Imagine that we have five web servers behind a load balancer. What we would like to do is a zero downtime upgrade of our web application. We want to achieve the following: Tell the load balancer that web server node is down Bring down the web server on that node Copy the updated application files to that node Bring up the web server on that node The first keyword for us to look at is serial. This ensures that the execution of the playbook is done serially rather than in parallel. We can choose to provide a percentage value or numeric value to serial. A great example for this way of doing updates is given in the following link Episode #47 - Zero-downtime Deployments with Ansible","title":"Rolling updates"},{"location":"learning/ansible/security_hardening/#bluegreen-deployments","text":"The concept of BlueGreen is attributed to Martin Fowler . The idea is to consider our current live production workload as blue. Now what we want to do is upgrade the application. So a replica of blue is brought up behind the same load balancer. The replica of the infrastructure has the updated application. Once it is up and running, the load balancer configuration is switched from current blue to point to green. Blue keeps running in case there are any operational issues. Once we are happy with the progress, we can tear down the older host.","title":"BlueGreen deployments"},{"location":"learning/ansible/security_hardening/#bluegreen-deployment-setup-playbook","text":"The following playbook will set up three nodes, which includes load balancer and two web server nodes. The first playbook brings up three hosts. Two web servers running nginx behind a load balancer The second playbook switches what is live (blue) to green # inventory.yml [ proxyserver ] proxy ansible_host=192.168.100.100 ansible_user=ubuntu ansible_password=passwordgoeshere [blue] blueserver ansible_host=192.168.100.10 ansible_user=ubuntu ansible_password=passwordgoeshere [green] greenserver ansible_host=192.168.100.20 ansible_user=ubuntu ansible_password=passwordgoeshere [webservers:children] blue green [prod:children] webservers proxyserver # main.yml - name : running common role hosts : prod gather_facts : false become : yes serial : 100% roles : - common - name : running haproxy role hosts : proxyserver become : yes roles : - haproxy - name : running webserver role hosts : webservers become : yes serial : 100% roles : - nginx - name : updating blue code hosts : blue become : yes roles : - bluecode - name : updating green code hosts : green become : yes roles : - greencode # common role - name : installing python if not installed raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : updating and installing git, curl apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - git - curl # Also we can include common any monitoring and security hardening tasks # haproxy role - name : adding haproxy repo apt_repository : repo : ppa:vbernat/haproxy-1.7 - name : updating and installing haproxy apt : name : haproxy state : present update_cache : yes - name : updating the haproxy configuration template : src : haproxy.cfg.j2 dest : /etc/haproxy/haproxy.cfg - name : starting the haproxy service service : name : haproxy state : started enabled : yes # haproxy.cfg.j2 global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). This list is from : # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ # An alternative list with additional directives can be obtained from # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http frontend http_front bind *:80 stats uri /haproxy?stats default_backend http_back backend http_back balance roundrobin server {{ hostvars.blueserver.ansible_host }} {{ hostvars.blueserver.ansible_host }}:80 check #server {{ hostvars.greenserver.ansible_host }} {{ hostvars.greenserver.ansible_host }}:80 check # nginx role - name : installing nginx apt : name : nginx state : present update_cache : yes - name : starting the nginx service service : name : nginx state : started enabled : yes # Code snipet for blue <html> <body bgcolor=\"blue\"> <h1 align=\"center\">Welcome to Blue Deployment</h1> </body> </html> # Code snipet for green <html> <body bgcolor=\"green\"> <h1 align=\"center\">Welcome to Green Deployment</h1> </body> </html> We want to deploy the new version of production site with green deployment. The playbook looks very simple as follows, it will update the configuration and reloads the haproxy service to serve the new production deployment. - name : Updating to GREEN deployment hosts : proxyserver become : yes tasks : - name : updating proxy configuration template : src : haproxy.cfg.j2 dest : /etc/haproxy/haproxy.cfg - name : updating the service service : name : haproxy state : reloaded - debug : msg : \"GREEN deployment successful. Please check your server :)\"","title":"BlueGreen deployment setup playbook"},{"location":"learning/ansible/security_hardening/#continuous-security-scanning-for-docker-containers","text":"Docker containers are the new way developers package applications. The best feature of containers is the fact that they contain the code, runtime, system libraries, and all the settings that are required for the application to work. Due to the ease of use and deployment, more and more applications are getting deployed in containers for production use. With so many moving parts, it becomes imperative that we have the capability to continuously scan Docker containers for security issues.","title":"Continuous Security Scanning for Docker Containers"},{"location":"learning/ansible/security_hardening/#understanding-continuous-security-concepts","text":"One of the key approaches to emerge out of DevOps is the idea of immutable infrastructure. It means that every time there needs to be a runtime change , either in application code or configuration, the containers are built and deployed again and the existing running ones are torn down. Since that allows for predictability, resilience, and simplifies deployment choices at runtime, it is no surprise that many operations teams are moving toward it. With that comes the question of when these containers should be tested for security and compliance. By embracing the process of continuous security scanning and monitoring, you can automate for a variety of workloads and workflows.","title":"Understanding continuous security concepts"},{"location":"learning/ansible/security_hardening/#automating-vulnerability-assessments-of-docker-containers-using-ansible","text":"Tool: Description There are many different ways of evaluating the security of containers. Docker Bench: A security shell script to perform checks based on CIS Clair: A tool to perform vulnerability analysis based on the CVE database Anchore: A tool to perform security evaluation and make runtime policy decisions vuls: An agent-less vulnerability scanner with CVE, OVAL database osquery: OS instrumentation framework for OS analytics to do HIDS-type activities","title":"Automating vulnerability assessments of Docker containers using Ansible"},{"location":"learning/ansible/security_hardening/#docker-bench-for-security","text":"Docker Bench for Security is a shell script to perform multiple checks against the Docker container environment. It will give a more detailed view of the security configuration based on CIS benchmarks. This script supports most of the Unix operating systems as it was built based on the POSIX 2004 compliant. The following are the high-level areas of checks this script will perform: Host configuration Docker daemon configuration and files Docker container images Docker runtime Docker security operations Docker swarm configuration - name : Docker bench security playbook hosts : docker remote_user : ubuntu become : yes tasks : - name : make sure git installed apt : name : git state : present - name : download the docker bench security git : repo : https://github.com/docker/docker-bench-security.git dest : /opt/docker-bench-security - name : running docker-bench-security scan command : docker-bench-security.sh -l /tmp/output.log args : chdir : /opt/docker-bench-security/ - name : downloading report locally fetch : src : /tmp/output.log dest : \"{{ playbook_dir }}/{{ inventory_hostname }}-docker-report-{{ ansible_date_time.date }}.log\" flat : yes - name : report location debug : msg : \"Report can be found at {{ playbook_dir }}/{{ inventory_hostname }}-docker-report-{{ ansible_date_time.date }}.log\" </mark> The output of the playbook will download and scan the containers based on the CIS benchmark and store the results in a log file","title":"Docker Bench for Security"},{"location":"learning/ansible/security_hardening/#clair","text":"Clair allows us to perform static vulnerability analysis against containers by checking with the existing vulnerability database. It allows us to perform vulnerability analysis checks against our Docker container images using the Clair database. Setting up Clair itself is really difficult and scanning using the API with Docker images makes more difficult. Here comes clair-scanner , it makes really simple to set up and perform scans using the REST API. Clair-scanner can trigger a simple scan against a container based on certain events, to check for existing vulnerabilities. Furthermore, this report can be forwarded to perform the team responsible for fixes and so on. # It assumes that the target system has Docker and the required libraries installed - name : Clair Scanner Server Setup hosts : docker remote_user : ubuntu become : yes tasks : - name : setting up clair-db docker_container : name : clair_db image : arminc/clair-db exposed_ports : - 5432 - name : setting up clair-local-scan docker_container : name : clair image : arminc/clair-local-scan:v2.0.1 ports : - \"6060:6060\" links : - \"clair_db:postgres\" # Setting up clair-scanner with Docker containers using Ansible # It will take a while to download and setup the CVE database after playbook execution. This playbook will be used to run clair-scanner to perform an analysis on the containers by making an API request to the server. - name : Scanning containers using clair-scanner hosts : docker remote_user : ubuntu become : yes vars : image_to_scan : \"debian:sid\" # container to scan for vulnerabilities clair_server : \"http://192.168.1.10:6060\" # clair server api endpoint tasks : - name : downloading and setting up clair-scanner binary get_url : url : https://github.com/arminc/clair-scanner/releases/download/v6/clair-scanner_linux_amd64 dest : /usr/local/bin/clair-scanner mode : 0755 - name : scanning {{ image_to_scan }} container for vulnerabilities command : clair-scanner -r /tmp/{{ image_to_scan }}-scan-report.json -c {{ clair_server }} --ip 0.0.0.0 {{ image_to_scan }} register : scan_output ignore_errors : yes - name : downloading the report locally fetch : src : /tmp/{{ image_to_scan }}-scan-report.json dest : {{ playbook_dir }} /{{ image_to_scan }}-scan-report.json flat : yes","title":"Clair"},{"location":"learning/ansible/security_hardening/#scheduled-scans-using-ansible-tower-for-docker-security","text":"Continuous security processes are all about the loop of planning, doing, measuring, and acting By following standard checklists and benchmarks and using Ansible to execute them on containers, we can check for security issues and act on them.","title":"Scheduled scans using Ansible Tower for Docker security"},{"location":"learning/ansible/security_hardening/#anchore--open-container-compliance-platform","text":"Anchore is one of the most popular tools and services to perform analysis, inspection, and certification of container images. Anchore is an analysis and inspection platform for containers. It provides multiple services and platforms to set up, the most stable and powerful way is to set up the local service using Anchore Engine, which can be accessed via the REST API. High level operations Anchore can perform: Policy evaluation operations Image operations Policy operations Registry operations Subscription operations System operations","title":"Anchore \u2013 open container compliance platform"},{"location":"learning/ansible/security_hardening/#anchore-engine-service-setup","text":"This playbook will set up the Anchore Engine service, which contains the engine container as well as the postgres to store database information. The admin_password variable is the admin user password to access the REST API of Anchore. - name : anchore server setup hosts : anchore become : yes vars : db_password : changeme admin_password : secretpassword tasks : - name : creating volumes file : path : \"{{ item }}\" recurse : yes state : directory with_items : - /root/aevolume/db - /root/aevolume/config - name : copying anchore-engine configuration template : src : config.yaml.j2 dest : /root/aevolume/config/config.yaml - name : starting anchore-db container docker_container : name : anchore-db image : postgres:9 volumes : - \"/root/aevolume/db/:/var/lib/postgresql/data/pgdata/\" env : POSTGRES_PASSWORD : \"{{ db_password }}\" PGDATA : \"/var/lib/postgresql/data/pgdata/\" - name : starting anchore-engine container docker_container : name : anchore-engine image : anchore/anchore-engine ports : - 8228:8228 - 8338:8338 volumes : - \"/root/aevolume/config/config.yaml:/config/config.yaml:ro\" - \"/var/run/docker.sock:/var/run/docker.sock:ro\" links : - anchore-db:anchore-db","title":"Anchore Engine service setup"},{"location":"learning/ansible/security_hardening/#anchore-cli-scanner","text":"Now that we have the Anchore Engine service REST API with access details, we can use this to perform the scanning of container images in any host. The following steps are the Ansible Tower setup to perform continuous scanning of container images for vulnerabilities. - name : anchore-cli scan hosts : anchore become : yes vars : scan_image_name : \"docker.io/library/ubuntu:latest\" anchore_vars : ANCHORE_CLI_URL : http://localhost:8228/v1 ANCHORE_CLI_USER : admin ANCHORE_CLI_PASS : secretpassword tasks : - name : installing anchore-cli pip : name : \"{{ item }}\" with_items : - anchorecli - pyyaml - name : downloading image docker_image : name : \"{{ scan_image_name }}\" - name : adding image for analysis command : \"anchore-cli image add {{ scan_image_name }}\" environment : \"{{anchore_vars}}\" - name : wait for analysis to compelte command : \"anchore-cli image content {{ scan_image_name }} os\" register : analysis until : analysis.rc != 1 retries : 10 delay : 30 ignore_errors : yes environment : \"{{anchore_vars}}\" - name : vulnerabilities results command : \"anchore-cli image vuln {{ scan_image_name }} os\" register : vuln_output environment : \"{{anchore_vars}}\" - name : \"vulnerabilities in {{ scan_image_name }}\" debug : msg : \"{{ vuln_output.stdout_lines }}\"","title":"Anchore CLI scanner"},{"location":"learning/ansible/security_hardening/#scheduled-scans-using-ansible-tower-for-operating-systems-and-kernel-security","text":"While most of the discussed tools can be used for scanning and maintaining a benchmark for security, we should think about the entire process of the incident response and threat detection workflow: Preparation Detection and analysis Containment, eradication, and recovery Post-incident activity Setting up all such scanners is our preparation. Using the output of these scanners gives us the ability to detect and analyze. Both containment and recovery are beyond the scope of such tools. For the process of recovery and post-incident activity, you may want to consider playbooks that can trash the current infrastructure and recreate it as it is. As part of our preparation, it may be useful to get familiar with the following terms as you will see them being used repeatedly in the world of vulnerability scanners and vulnerability management tools: Term: Full form (if any): Description of the term CVE: Common Vulnerabilities and Exposures: It is a list of cybersecurity vulnerability identifiers. Usage typically includes CVE IDs. OVAL: Open Vulnerability and Assessment Language: A language for finding out and naming vulnerabilities and configuration issues in computer systems. CWE: Common Weakness Enumeration: A common list of software security weaknesses. NVD: National Vulnerability Database: A US government vulnerability management database available for public use in XML format.","title":"Scheduled scans using Ansible Tower for operating systems and kernel security"},{"location":"learning/ansible/security_hardening/#vuls--vulnerability-scanner","text":"Vuls is an agent-less scanner written in golang. It supports a different variety of Linux operating systems. It performs the complete end-to-end security system administrative tasks such as scanning for security vulnerabilities and security software updates. It analyzes the system for required security vulnerabilities, performs security risk analysis based on the CVE score, sends notifications via Slack and email, and also provides a simple web report with historical data. The playbook has mainly two roles for setting up vuls using Docker containers. vuls_containers_download vuls_database_download - name : setting up vuls using docker containers hosts : vuls become : yes roles : - vuls_containers_download - vuls_database_download # Pulling the Docker containers locally using the docker_image module: - name : pulling containers locally docker_image : name : \"{{ item }}\" pull : yes with_items : - vuls/go-cve-dictionary - vuls/goval-dictionary - vuls/vuls # Then downloading the CVE and OVAL databases for the required operating systems and distributions versions - name : fetching NVD database locally docker_container : name : \"cve-{{ item }}\" image : vuls/go-cve-dictionary auto_remove : yes interactive : yes state : started command : fetchnvd -years \"{{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/go-cve-dictionary-log:/var/log/vuls\" with_sequence : start=2002 end=\"{{ nvd_database_years }}\" - name : fetching redhat oval data docker_container : name : \"redhat-oval-{{ item }}\" image : vuls/goval-dictionary auto_remove : yes interactive : yes state : started command : fetch-redhat \"{{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/goval-dictionary-log:/var/log/vuls\" with_items : \"{{ redhat_oval_versions }}\" - name : fetching ubuntu oval data docker_container : name : \"ubuntu-oval-{{ item }}\" image : vuls/goval-dictionary auto_remove : yes interactive : yes state : started command : \"fetch-ubuntu {{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/goval-dictionary-log:/var/log/vuls\" with_items : \"{{ ubuntu_oval_versions }}\" The global variables file looks as follows. We can add more redhat_oval_versions, such as 5. The nvd_database_years will download the CVE database up until the end of 2017: vuls_data_directory : \"/vuls_data\" nvd_database_years : 2017 redhat_oval_versions : - 6 - 7 ubuntu_oval_versions : - 12 - 14 - 16 Now, it's time to perform the scanning and reporting using the vuls Docker containers. The following playbook contains simple steps to perform the vuls scan against virtual machines and containers, and send the report to slack and web: - name : scanning and reporting using vuls hosts : vuls become : yes vars : vuls_data_directory : \"/vuls_data\" slack_web_hook_url : https://hooks.slack.com/services/XXXXXXX/XXXXXXXXXXXXXXXXXXXXX slack_channel : \"#vuls\" slack_emoji : \":ghost:\" server_to_scan : 192.168.33.80 server_username : vagrant server_key_file_name : 192-168-33-80 tasks : - name : copying configuraiton file and ssh keys template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" mode : 0400 with_items : - { src : 'config.toml' , dst : '/root/config.toml' } - { src : '192-168-33-80' , dst : '/root/.ssh/192-168-33-80' } - name : running config test docker_container : name : configtest image : vuls/vuls auto_remove : yes interactive : yes state : started command : configtest -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - name : running vuls scanner docker_container : name : vulsscan image : vuls/vuls auto_remove : yes interactive : yes state : started command : scan -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - \"/etc/localtime:/etc/localtime:ro\" env : TZ : \"Asia/Kolkata\" - name : sending slack report docker_container : name : vulsreport image : vuls/vuls auto_remove : yes interactive : yes state : started command : report -cvedb-path=/vuls/cve.sqlite3 -ovaldb-path=/vuls/oval.sqlite3 --to-slack -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - \"/etc/localtime:/etc/localtime:ro\" - name : vuls webui report docker_container : name : vulswebui image : vuls/vulsrepo interactive : yes volumes : - \"{{ vuls_data_directory }}:/vuls\" ports : - \"80:5111\" The following file is the configuration file for vuls to perform the scanning. This holds the configuration for slack alerting and also the server to perform scanning. This can be configured very effectively as required using vuls documentation: [ slack ] hookURL = \"{{ slack_web_hook_url}}\" channel = \"{{ slack_channel }}\" iconEmoji = \"{{ slack_emoji }}\" [servers] [servers.{{ server_key_file_name }}] host = \"{{ server_to_scan }}\" user = \"{{ server_username }}\" keyPath = \"/root/.ssh/{{ server_key_file_name }}\" We can also visit the web UI interface of the vuls server IP address to see the detailed results in tabular and portable format. This is very useful to manage large amount of servers and patches at scale. This can be part of the CI/CD life cycle as an infrastructure code and then we can run this as a scheduled scan using Ansible Tower or Jenkins.","title":"Vuls \u2013 vulnerability scanner"},{"location":"learning/ansible/security_hardening/#scheduled-scans-for-file-integrity-checks-host-level-monitoring-using-ansible-for-various-compliance-initiatives","text":"One of the many advantages of being able to execute commands on the host using Ansible is the ability to get internal system information, such as: File hashes Network connections List of running processes It can act as a lightweight Host-Based Intrusion Detection System (HIDS). While this may not eliminate the case for a purpose-built HIDS in many cases, we can execute the same kind of security tasks using a tool such as Facebook's osquery along with Ansible.","title":"Scheduled scans for file integrity checks, host-level monitoring using Ansible for various compliance initiatives"},{"location":"learning/ansible/security_hardening/#osquery","text":"osquery is an operating system instrumentation framework by Facebook and written in C++, that supports Windows, Linux, OS X (macOS), and other operating systems. It provides an interface to query an operating system using an SQL like syntax. By using this, we can perform low-level activities such as running processes, kernel configurations, network connections, and file integrity checks. Overall it's like a host-based intrusion detection system (HIDS) endpoint security. It provides osquery as a service, system interactive shell, and so on. Hence we can use this to perform centralized monitoring and security management solutions. This playbook is to set up and configure the osquery agent in your Linux servers to monitor and look for vulnerabilities, file integrity monitoring, and many other compliance activities, and then log them for sending to a centralized logging monitoring system. The reference tutorial can be followed at DigitalOcean . - name : setting up osquery hosts : linuxservers become : yes tasks : - name : installing osquery apt : deb : https://pkg.osquery.io/deb/osquery_2.10.2_1.linux.amd64.deb update_cache : yes - name : adding osquery configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : fim.conf , dst : /usr/share/osquery/packs/fim.conf } - { src : osquery.conf , dst : /etc/osquery/osquery.conf } - name : starting and enabling osquery service service : name : osqueryd state : started enabled : yes The following fim.conf code snippet is the pack for file integrity monitoring and it monitors for file events in the /home, /etc, and /tmp directories every 300 seconds. It uses Secure Hash Algorithm (SHA) checksum to validate the changes. This can be used to find out whether attackers add their own SSH keys or audit log changes against system configuration changes for compliance and other activities. { \"queries\" : { \"file_events\" : { \"query\" : \"select * from file_events;\" , \"removed\" : false , \"interval\" : 300 } }, \"file_paths\" : { \"homes\" : [ \"/root/.ssh/%%\" , \"/home/%/.ssh/%%\" ], \"etc\" : [ \"/etc/%%\" ], \"home\" : [ \"/home/%%\" ], \"tmp\" : [ \"/tmp/%%\" ] } } The following code snippet is the osquery service configuration. This can be modified as required to monitor and log by osquery service. { \"options\" : { \"config_plugin\" : \"filesystem\" , \"logger_plugin\" : \"filesystem\" , \"logger_path\" : \"/var/log/osquery\" , \"disable_logging\" : \"false\" , \"log_result_events\" : \"true\" , \"schedule_splay_percent\" : \"10\" , \"pidfile\" : \"/var/osquery/osquery.pidfile\" , \"events_expiry\" : \"3600\" , \"database_path\" : \"/var/osquery/osquery.db\" , \"verbose\" : \"false\" , \"worker_threads\" : \"2\" , \"enable_monitor\" : \"true\" , \"disable_events\" : \"false\" , \"disable_audit\" : \"false\" , \"audit_allow_config\" : \"true\" , \"host_identifier\" : \"hostname\" , \"enable_syslog\" : \"true\" , \"audit_allow_sockets\" : \"true\" , \"schedule_default_interval\" : \"3600\" }, \"schedule\" : { \"crontab\" : { \"query\" : \"SELECT * FROM crontab;\" , \"interval\" : 300 }, \"system_profile\" : { \"query\" : \"SELECT * FROM osquery_schedule;\" }, \"system_info\" : { \"query\" : \"SELECT hostname, cpu_brand, physical_memory FROM system_info;\" , \"interval\" : 3600 } }, \"decorators\" : { \"load\" : [ \"SELECT uuid AS host_uuid FROM system_info;\" , \"SELECT user AS username FROM logged_in_users ORDER BY time DESC LIMIT 1;\" ] }, \"packs\" : { \"fim\" : \"/usr/share/osquery/packs/fim.conf\" , \"osquery-monitoring\" : \"/usr/share/osquery/packs/osquery-monitoring.conf\" , \"incident-response\" : \"/usr/share/osquery/packs/incident-response.conf\" , \"it-compliance\" : \"/usr/share/osquery/packs/it-compliance.conf\" , \"vuln-management\" : \"/usr/share/osquery/packs/vuln-management.conf\" } } The goal is not just setting up osquery, we can use the logs to build a centralized real-time monitoring system using our Elastic stack. We can use the Filebeat agent to forward these logs to our Elastic stack and we can view them and build a centralized dashboard for alerting and monitoring. This idea can be extended for building some automated defences by taking actions against attacks by using automated Ansible playbooks for known actions. The world is moving toward containers and this kind of monitoring gives us a look at low-level things such as kernel security checks, and file integrity checks on host level. When attackers try to bypass containers and get access to hosts to escalate privileges, we can detect and defend them using this kind of setup.","title":"osquery"},{"location":"learning/ansible/security_hardening/#summary","text":"Containers are rapidly changing the world of developers and operations teams. The rate of change is accelerating, and in this new world, security automation gets to play a front and center role. By leveraging our knowledge of using Ansible for scripting play-by-play commands along with excellent tools such as Anchore and osquery , we can measure, analyze, and benchmark our containers for security. This allows us to build end-to-end automatic processes of securing, scanning and remediating containers.","title":"Summary"},{"location":"learning/ansible/security_hardening/#automating-lab-setups-for-forensics-collection-and-malware-analysis","text":"Malware is one of the biggest challenges faced by the security community. It impacts everyone who gets to interact with information systems. While there is a massive effort required in keeping computers safe from malware for operational systems, a big chunk of work in malware defenses is about understanding where they come from and what they are capable of. Another important aspect of malware analysis is the ability to collaborate and share threats using the Malware Information Sharing Platform (MISP). One of the initial phases of malware analysis is identification and classification. The most popular source is using VirusTotal to scan and get the results of the malware samples, domain information, and so on. It has a very rich API and a lot of people have written custom apps that leverage the API to perform the automated scans using the API key for identifying the malware type. It generally checks using more than 60 antivirus scanners and tools and provides detailed information.","title":"Automating Lab Setups for Forensics Collection and Malware Analysis"},{"location":"learning/ansible/security_hardening/#virustotal--api-tool-set-up","text":"The following playbook will set up the VirusTotal API tool - name : setting up VirusTotal hosts : malware remote_user : ubuntu become : yes tasks : - name : installing pip apt : name : \"{{ item }}\" with_items : - python-pip - unzip - name : checking if vt already exists stat : path : /usr/local/bin/vt register : vt_status - name : downloading VirusTotal api tool repo unarchive : src : \"https://github.com/doomedraven/VirusTotalApi/archive/master.zip\" dest : /tmp/ remote_src : yes when : vt_status.stat.exists == False - name : installing the dependencies pip : requirements : /tmp/VirusTotalApi-master/requirements.txt when : vt_status.stat.exists == False - name : installing vt command : python /tmp/VirusTotalApi-master/setup.py install when : vt_status.stat.exists == False The playbook execution will download the repository and set up the VirusTotal API tool. The following playbook will find and copy the local malware samples to a remote system and scan them recursively and return the results. Once the scan has been completed, it will remove the samples from the remote system. - name : scanning file in VirusTotal hosts : malware remote_user : ubuntu vars : vt_api_key : XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX #use Ansible-vault vt_api_type : public # public/private vt_intelligence_access : False # True/False files_in_local_system : /tmp/samples/ files_in_remote_system : /tmp/sample-file/ tasks : - name : creating samples directory file : path : \"{{ files_in_remote_system }}\" state : directory - name : copying file to remote system copy : src : \"{{ files_in_local_system }}\" dest : \"{{ files_in_remote_system }}\" directory_mode : yes - name : copying configuration template : src : config.j2 dest : \"{{ files_in_remote_system }}/.vtapi\" - name : running VirusTotal scan command : \"vt -fr {{ files_in_remote_system }}\" args : chdir : \"{{ files_in_remote_system }}\" register : vt_scan - name : removing the samples file : path : \"{{ files_in_remote_system }}\" state : absent - name : VirusTotal scan results debug : msg : \"{{ vt_scan.stdout_lines }}\"","title":"VirusTotal  API tool set up"},{"location":"learning/ansible/security_hardening/#creating-ansible-playbooks-for-collection-and-storage-with-secure-backup-of-forensic-artifacts","text":"Ansible is an apt replacement for all kinds of bash scripts. Typically, for most activities that require analysis, we follow a set pattern: Collect logs from running processes into files with a path we already know Copy the content from these log files periodically to a secure storage locally or accessible remotely over SSH or a network file share Once copied successfully, rotate the logs Since there is a bit of network activity involved, our bash scripts are usually written to be fault tolerant with regard to network connections and become complex very soon. Ansible playbooks can be used to do all of that while being simple to read for everyone.","title":"Creating Ansible playbooks for collection and storage with secure backup of forensic artifacts"},{"location":"learning/ansible/security_hardening/#collecting-log-artifacts-for-incident-response","text":"The key phase in incident response is log analysis. This playbook will collect the logs from all the hosts and store it locally. This allows responders to perform the further analysis. # Reference https://www.Ansible.com/security-automation-with-Ansible - name : Gather log files hosts : servers become : yes tasks : - name : List files to grab find : paths : - /var/log patterns : - '*.log*' recurse : yes register : log_files - name : Grab files fetch : src : \"{{ item.path }}\" dest : \"/tmp/LOGS_{{ Ansible_fqdn }}/\" with_items : \"{{ log_files.files }}\" This playbook execution will collect a list of logs in specified locations in remote hosts using Ansible modules and store them in the local system.","title":"Collecting log artifacts for incident response"},{"location":"learning/ansible/security_hardening/#secure-backups-for-data-collection","text":"When collecting multiple sets of data from servers, it's important to store them securely with encrypted backups. This can be achieved by backing up the data to storage services such as S3. This Ansible playbook allows us to install and copy the collected data to the AWS S3 service with encryption enabled. - name : backing up the log data hosts : localhost gather_facts : false become : yes vars : s3_access_key : XXXXXXX # Use Ansible-vault to encrypt s3_access_secret : XXXXXXX # Use Ansible-vault to encrypt localfolder : /tmp/LOGS/ # Trailing slash is important remotebucket : secretforensicsdatausingAnsible # This should be unique in s3 tasks : - name : installing s3cmd if not installed apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-magic - python-dateutil - s3cmd - name : create s3cmd config file template : src : s3cmd.j2 dest : /root/.s3cfg owner : root group : root mode : 0640 - name : make sure \"{{ remotebucket }}\" is avilable command : \"s3cmd mb s3://{{ remotebucket }}/ -c /root/.s3cfg\" - name : running the s3 backup to \"{{ remotebucket }}\" command : \"s3cmd sync {{ localfolder }} --preserve s3://{{ remotebucket }}/ -c /root/.s3cfg\" The Ansible playbook installing s3cmd, creating the new bucket called secretforensicsdatausingAnsible, and copying the local log data to the remote S3 bucket. The configuration file looks like the following for the s3cmd configuration [ default ] access_key = {{ s3_access_key }} secret_key = {{ s3_access_secret }} host_base = s3.amazonaws.com host_bucket = %(bucket)s.s3.amazonaws.com website_endpoint = http://%(bucket)s.s3-website-%(location)s.amazonaws.com/ use_https = True signature_v2 = True We can see that the logs are successfully uploaded into the secretforensicsdatausingAnsible S3 bucket in AWS S3.","title":"Secure backups for data collection"},{"location":"learning/ansible/security_testing/","text":"Automating Web Application Security Testing Using OWASP ZAP \u00b6 The OWASP Zed Attack Proxy (commonly known as ZAP) is one of the most popular web application security testing tools. It has many features that allow it to be used for manual security testing; it also fits nicely into continuous integration/continuous delivery (CI/CD) environments after some tweaking and configuration. More details about the project can be found at https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project . Open Web Application Security Project (OWASP) is a worldwide not-for-profit charitable organization focused on improving the security of software. OWASP ZAP includes many different tools and features in one package. For a pentester tasked with doing the security testing of web applications, the following features are invaluable Feature: Use case Intercepting proxy: This allows us to intercept requests and responses in the browser Active scanner: Automatically run web security scans against targets Passive scanner: Glean information about security issues from pages that get downloaded using spider tools and so on Spiders: Before ZAP can attack an application, it creates a site map of the application by crawling all the possible web pages on it REST API: Allows ZAP to be run in headless mode and to be controlled for running automated scanner, spider, and get the results ZAP is a Java-based software. The typical way of using it will involve the following: Java Runtime Environment (JRE) 7 or more recent installed in the operating system of your choice (macOS, Windows, Linux) Install ZAP using package managers, installers from the official downloads page The best way to achieve that is to use OWASP ZAP as a container. In fact, this is the kind of setup Mozilla uses ZAP in a CI/CD pipeline to verify the baseline security controls at every release. Installing OWASP ZAP \u00b6 We are going to use OWASP ZAP as a container, which requires container runtime in the host operating system. The team behind OWASP ZAP releases ZAP Docker images on a weekly basis via Docker Hub. The approach of pulling Docker images based on tags is popular in modern DevOps environments and it makes sense that we talk about automation with respect to that. Installing Docker runtime \u00b6 # The following playbook will install Docker Community Edition software in Ubuntu 16.04 - name : installing docker on ubuntu hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu apt_repo_data : \"deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable\" apt_gpg_key : https://download.docker.com/linux/ubuntu/gpg tasks : - name : adding docker gpg key apt_key : url : \"{{ apt_gpg_key }}\" state : present - name : add docker repository apt_repository : repo : \"{{ apt_repo_data }}\" state : present - name : installing docker-ce apt : name : docker-ce state : present update_cache : yes - name : install python-pip apt : name : python-pip state : present - name : install docker-py pip : name : \"{{ item }}\" state : present with_items : - docker-py OWASP ZAP Docker container setup \u00b6 The two new modules to deal with Docker containers that we will be using here are docker_image and docker_container. # The following playbook will take some time to complete as it has to download about 1 GB of data from the internet - name : setting up owasp zap container hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly tasks : - name : pulling {{ owasp_zap_image_name }} container docker_image : name : \"{{ owasp_zap_image_name }}\" - name : running owasp zap container docker_container : name : owasp-zap image : \"{{ owasp_zap_image_name }}\" interactive : yes state : started user : zap command : zap.sh -daemon -host 0.0.0.0 -port 8090 -config api.disablekey=true -config api.addrs.addr.name=.* -config api.addrs.addr.regex=true ports : - \"8090:8090\" You can access the ZAP API interface by navigating to http://ZAPSERVERIPADDRESS:8090 A specialized tool for working with Containers - Ansible Container \u00b6 Currently, we are using Docker modules to perform container operations. A new tool, ansible-container, provides an Ansible-centric workflow for building, running, testing, and deploying containers. This allows us to build, push, and run containers using existing playbooks. Dockerfiles are like writing shell scripts, therefore, ansible-container will allow us to codify those Dockerfiles and build them using existing playbooks rather writing complex scripts. The ansible-container supports various orchestration tools, such as Kubernetes and OpenShift. It can also be used to push the build images to private registries such as Google Container Registry and Docker Hub. Running an OWASP ZAP Baseline scan \u00b6 The following playbook runs the Docker Baseline scan against a given website URL. It also stores the output of the Baseline's scan in the host system in HTML, Markdown, and XML formats. - name : Running OWASP ZAP Baseline Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-base-line-scan-dvws tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0770 - name : running owasp zap baseline scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-baseline.py -t {{ website_url }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan command : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\" Explore the parameters of the preceding playbook: website_url is the domain (or) URL that you want to perform the Baseline scan, we can pass this via \u2013extra-vars \"website_url: http://192.168.33.111 \" from the ansible-playbook command reports_location is the path to ZAP host machine where reports get stored Security testing against web applications and websites \u00b6 An active scan may cause the vulnerability to be exploited in the application. Also, this type of scan requires extra configuration, which includes authentication and sensitive functionalities. The following playbook will run the full scan against the DVWS application. Now we can see that the playbook looks almost similar, except the flags sent to command: - name : Running OWASP ZAP Full Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-full-scan-dvws tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0777 - name : running owasp zap full scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-full-scan.py -t {{ website_url }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan raw : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\" Testing web APIs \u00b6 Similar to the ZAP Baseline scan, the fine folks behind ZAP provide a script as part of their live and weekly Docker images. We can use it to run scans against API endpoints defined either by OpenAPI specification or Simple Object Access Protocol (SOAP). The script can understand the API specifications and import all the definitions. Based on this, it runs an active scan against all the URLs found. - name : Running OWASP ZAP API Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-api-scan-dvws api_type : openapi tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0777 - name : running owasp zap api scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-api-scan.py -t {{ website_url }} -f {{ api_type }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan raw : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\" Vulnerability Scanning with Nessus \u00b6 Scanning for vulnerabilities is one of the best understood periodic activities security teams take up on their computers. There are well-documented strategies and best practices for doing regular scanning for vulnerabilities in computers, networks, operating system software, and application software: Basic network scans Credentials patch audit Correlating system information with known vulnerabilities With networked systems, this type of scanning is usually executed from a connected host that has the right kind of permissions to scan for security issues. One of the most popular vulnerability scanning tools is Nessus. Nessus started as a network vulnerability scanning tool, but now incorporates features such as the following: Port scanning Network vulnerability scanning Web application-specific scanning Host-based vulnerability scanning Introduction to Nessus \u00b6 The vulnerability database that Nessus has is its main advantage. While the techniques to understanding which service is running and what version of the software is running the service are known to us, answering the question, \"Does this service have a known vulnerability\" is the important one. Apart from a regularly updated vulnerability database, Nessus also has information on default credentials found in applications, default paths, and locations. All of this fine-tuned in an easy way to use CLI or web-based tool. We will try out the standard activities required for that and see what steps are needed to automate them using Ansible. Installing Nessus using a playbook. Configuring Nessus. Running a scan. Running a scan using AutoNessus. Installing the Nessus REST API Python client. Downloading a report using the API. Installing Nessus for vulnerability assessments \u00b6 - name : installing nessus server hosts : nessus remote_user : \"{{ remote_user_name }}\" gather_facts : no vars : remote_user_name : ubuntu nessus_download_url : \"http://downloads.nessus.org/nessus3dl.php?file=Nessus-6.11.2-ubuntu1110_amd64.deb&licence_accept=yes&t=84ed6ee87f926f3d17a218b2e52b61f0\" tasks : - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : downloading the package and installing apt : deb : \"{{ nessus_download_url }}\" - name : start the nessus daemon service : name : \"nessusd\" enabled : yes state : started Configuring Nessus for vulnerability scanning \u00b6 Perform the following steps to configure Nessus for vulnerability scanning: We have to navigate to https://NESSUSSERVERIP:8834 to confirm and start the service It returns with an SSL error and we need to accept the SSL error and confirm the security exception and continue with the installation Click on Confirm Security Exception and continue to proceed with the installation steps. Click on Continue and provide the details of the user, this user has full administrator access. Then finally, we have to provide the registration code (Activation Code), which can be obtained from registering at https://www.tenable.com/products/nessus-home Now it will install the required plugins. It will take a while to install, and once it is done we can log in to use the application. Now, we have successfully set up the Nessus vulnerability scanner. Basic network scanning \u00b6 Nessus has a wide variety of scans, some of them are free and some of them will be available only in a paid version. So, we can also customize the scanning if required. We can start with a basic network scan to see what's happening in the network. This scan will perform a basic full system scan for the given hosts. We have to mention the scan name and targets. Targets are just the hosts we want. Targets can be given in different formats, such as 192.168.33.1 for a single host, 192.168.33.1-10 for a range of hosts, and also we can upload the target file from our computer. Running a scan using AutoNessus \u00b6 With the AutoNessus script, we can do the following: List scans List scan policies Do actions on scans such as start, stop, pause, and resume The best part of AutoNessus is that since this is a command-line tool, it can easily become part of scheduled tasks and other automation workflows. Setting up AutoNessus \u00b6 The following code is the Ansible playbook snippet to set up AutoNessus and configure it to use Nessus using credentials. - name : installing python-pip apt : name : python-pip update_cache : yes state : present - name : install python requests pip : name : requests - name : setting up autonessus get_url : url : \"https://github.com/redteamsecurity/AutoNessus/raw/master/autoNessus.py\" dest : /usr/bin/autoNessus mode : 0755 - name : updating the credentials replace : path : /usr/bin/autoNessus regexp : \"{{ item.src }}\" replace : \"{{ item.dst }}\" backup : yes no_log : True with_items : - { src : \"token = ''\" , dst : \"token = '{{ nessus_user_token }}'\" } - { src : \"url = 'https://localhost:8834'\" , dst : \"url = '{{ nessus_url }}'\" } - { src : \"username = 'xxxxx'\" , dst : \"username = '{{ nessus_user_name }}'\" } - { src : \"password = 'xxxxx'\" , dst : \"password = '{{ nessus_user_password }}'\" } no_log : True will censor the output in the log console of Ansible output. It will be very useful when we are using secrets and keys inside playbooks. Before running the automated scans using AutoNessus, we have to create them in the Nessus portal with required customization, and we can use these automated playbooks to perform tasks on top of it. Listing current available scans and IDs \u00b6 - name : list current scans and IDs using autoNessus command : \"autoNessus -l\" register : list_scans_output - debug : msg : \"{{ list_scans_output.stdout_lines }}\" Starting a specified scan using scan ID \u00b6 - name : starting nessus scan \"{{ scan_id }}\" using autoNessus command : \"autoNessus -sS {{ scan_id }}\" register : start_scan_output - debug : msg : \"{{ start_scan_output.stdout_lines }}\" - Similarly, we can perform pause, resume, stop, list policies, and so on. Using the AutoNessus program, these playbooks are available. This can be improved by advancing the Nessus API scripts. Storing results \u00b6 The entire report can be exported into multiple formats, such as HTML, CSV, and Nessus. This helps to give more a detailed structure of vulnerabilities found, solutions with risk rating, and other references The output report can be customized based on the audience, if it goes to the technical team, we can list all the vulnerabilities and remediation. For example, if management wants to get the report, we can only get the executive summary of the issues. Reports can be sent by email as well using notification options in Nessus configuration. Installing the Nessus REST API Python client \u00b6 Official API documentation can be obtained by connecting to your Nessus server under 8834/nessus6-api.html. To perform any operations using the Nessus REST API, we have to obtain the API keys from the portal. This can be found in user settings. Please make sure to save these keys Downloading reports using the Nessus REST API \u00b6 - name : working with nessus rest api connection : local hosts : localhost gather_facts : no vars : scan_id : 17 nessus_access_key : 620fe4ffaed47e9fe429ed749207967ecd7a77471105d8 nessus_secret_key : 295414e22dc9a56abc7a89dab713487bd397cf860751a2 nessus_url : https://192.168.33.109:8834 nessus_report_format : html tasks : - name : export the report for given scan \"{{ scan_id }}\" uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export\" method : POST validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" body : \"format={{ nessus_report_format }}&chapters=vuln_by_host;remediations\" register : export_request - debug : msg : \"File id is {{ export_request.json.file }} and scan id is {{ scan_id }}\" - name : check the report status for \"{{ export_request.json.file }}\" uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export/{{ export_request.json.file }}/status\" method : GET validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" register : report_status - debug : msg : \"Report status is {{ report_status.json.status }}\" - name : downloading the report locally uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export/{{ export_request.json.file }}/download\" method : GET validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" return_content : yes dest : \"./{{ scan_id }}_{{ export_request.json.file }}.{{ nessus_report_format }}\" register : report_output - debug : msg : \"Report can be found at ./{{ scan_id }}_{{ export_request.json.file }}.{{ nessus_report_format }}\" Nessus configuration \u00b6 Nessus allows us to create different users with role-based authentication to perform scans and review with different access levels. Summary \u00b6 Security teams and IT teams rely on tools for vulnerability scanning, management, remediation, and continuous security processes. Nessus, by being one of the most popular and useful tools, was an automatic choice for the authors to try and automate. Writing an Ansible Module for Security Testing \u00b6 Ansible primarily works by pushing small bits of code to the nodes it connects to. These codes/programs are what we know as Ansible modules. Typically in the case of a Linux host these are copied over SSH, executed, and then removed from the node. We will look at the following: How to set up the development environment -Writing an Ansible hello world module to understand the basics -Where to seek further help -Defining a security problem statement -Addressing that problem by writing a module of our own Along with that, we will try to understand and attempt to answer the following questions: What are the good use cases for modules? When does it make sense to use roles? How do modules differ from plugins? Getting started with a hello world Ansible module \u00b6 We will pass one argument to our custom module and show if we have success or failure for the module executing based on that. Since all of this is new to us, we will look at the following things: The source code of the hello world module The output of that module for both success and failure The command that we will use to invoke it Setting up the development environment \u00b6 The primary requirement for Ansible 2.4 is Python 2.6 or higher and Python 3.5 or higher. If you have either of them installed, we can follow the simple steps to get the development environment going. From the Ansible Developer Guide: Clone the Ansible repository: $ git clone ansible/ansible.git Change the directory into the repository root directory: $ cd ansible Create a virtual environment: $ python3 -m venv venv (or for Python 2 $ virtualenv venv Note, this requires you to install the virtualenv package: $ pip install virtualenv Activate the virtual environment: $ . venv/bin/activate Install the development requirements: $ pip install -r requirements.txt Run the environment setup script for each new dev shell process: $ . hacking/env-setup This playbook will set up the developer environment by installing and setting up the virtual environment. - name : Setting Developer Environment hosts : dev remote_user : madhu become : yes vars : ansible_code_path : \"/home/madhu/ansible-code\" tasks : - name : installing prerequirements if not installed apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - git - virtualenv - python-pip - name : downloading ansible repo locally git : repo : https://github.com/ansible/ansible.git dest : \"{{ ansible_code_path }}/venv\" - name : creating virtual environment pip : virtualenv : \"{{ ansible_code_path }}\" virtualenv_command : virtualenv requirements : \"{{ ansible_code_path }}/venv/requirements.txt\"","title":"Automating Web Application Security Testing Using OWASP ZAP"},{"location":"learning/ansible/security_testing/#automating-web-application-security-testing-using-owasp-zap","text":"The OWASP Zed Attack Proxy (commonly known as ZAP) is one of the most popular web application security testing tools. It has many features that allow it to be used for manual security testing; it also fits nicely into continuous integration/continuous delivery (CI/CD) environments after some tweaking and configuration. More details about the project can be found at https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project . Open Web Application Security Project (OWASP) is a worldwide not-for-profit charitable organization focused on improving the security of software. OWASP ZAP includes many different tools and features in one package. For a pentester tasked with doing the security testing of web applications, the following features are invaluable Feature: Use case Intercepting proxy: This allows us to intercept requests and responses in the browser Active scanner: Automatically run web security scans against targets Passive scanner: Glean information about security issues from pages that get downloaded using spider tools and so on Spiders: Before ZAP can attack an application, it creates a site map of the application by crawling all the possible web pages on it REST API: Allows ZAP to be run in headless mode and to be controlled for running automated scanner, spider, and get the results ZAP is a Java-based software. The typical way of using it will involve the following: Java Runtime Environment (JRE) 7 or more recent installed in the operating system of your choice (macOS, Windows, Linux) Install ZAP using package managers, installers from the official downloads page The best way to achieve that is to use OWASP ZAP as a container. In fact, this is the kind of setup Mozilla uses ZAP in a CI/CD pipeline to verify the baseline security controls at every release.","title":"Automating Web Application Security Testing Using OWASP ZAP"},{"location":"learning/ansible/security_testing/#installing-owasp-zap","text":"We are going to use OWASP ZAP as a container, which requires container runtime in the host operating system. The team behind OWASP ZAP releases ZAP Docker images on a weekly basis via Docker Hub. The approach of pulling Docker images based on tags is popular in modern DevOps environments and it makes sense that we talk about automation with respect to that.","title":"Installing OWASP ZAP"},{"location":"learning/ansible/security_testing/#installing-docker-runtime","text":"# The following playbook will install Docker Community Edition software in Ubuntu 16.04 - name : installing docker on ubuntu hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu apt_repo_data : \"deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable\" apt_gpg_key : https://download.docker.com/linux/ubuntu/gpg tasks : - name : adding docker gpg key apt_key : url : \"{{ apt_gpg_key }}\" state : present - name : add docker repository apt_repository : repo : \"{{ apt_repo_data }}\" state : present - name : installing docker-ce apt : name : docker-ce state : present update_cache : yes - name : install python-pip apt : name : python-pip state : present - name : install docker-py pip : name : \"{{ item }}\" state : present with_items : - docker-py","title":"Installing Docker runtime"},{"location":"learning/ansible/security_testing/#owasp-zap-docker-container-setup","text":"The two new modules to deal with Docker containers that we will be using here are docker_image and docker_container. # The following playbook will take some time to complete as it has to download about 1 GB of data from the internet - name : setting up owasp zap container hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly tasks : - name : pulling {{ owasp_zap_image_name }} container docker_image : name : \"{{ owasp_zap_image_name }}\" - name : running owasp zap container docker_container : name : owasp-zap image : \"{{ owasp_zap_image_name }}\" interactive : yes state : started user : zap command : zap.sh -daemon -host 0.0.0.0 -port 8090 -config api.disablekey=true -config api.addrs.addr.name=.* -config api.addrs.addr.regex=true ports : - \"8090:8090\" You can access the ZAP API interface by navigating to http://ZAPSERVERIPADDRESS:8090","title":"OWASP ZAP Docker container setup"},{"location":"learning/ansible/security_testing/#a-specialized-tool-for-working-with-containers---ansible-container","text":"Currently, we are using Docker modules to perform container operations. A new tool, ansible-container, provides an Ansible-centric workflow for building, running, testing, and deploying containers. This allows us to build, push, and run containers using existing playbooks. Dockerfiles are like writing shell scripts, therefore, ansible-container will allow us to codify those Dockerfiles and build them using existing playbooks rather writing complex scripts. The ansible-container supports various orchestration tools, such as Kubernetes and OpenShift. It can also be used to push the build images to private registries such as Google Container Registry and Docker Hub.","title":"A specialized tool for working with Containers - Ansible Container"},{"location":"learning/ansible/security_testing/#running-an-owasp-zap-baseline-scan","text":"The following playbook runs the Docker Baseline scan against a given website URL. It also stores the output of the Baseline's scan in the host system in HTML, Markdown, and XML formats. - name : Running OWASP ZAP Baseline Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-base-line-scan-dvws tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0770 - name : running owasp zap baseline scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-baseline.py -t {{ website_url }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan command : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\" Explore the parameters of the preceding playbook: website_url is the domain (or) URL that you want to perform the Baseline scan, we can pass this via \u2013extra-vars \"website_url: http://192.168.33.111 \" from the ansible-playbook command reports_location is the path to ZAP host machine where reports get stored","title":"Running an OWASP ZAP Baseline scan"},{"location":"learning/ansible/security_testing/#security-testing-against-web-applications-and-websites","text":"An active scan may cause the vulnerability to be exploited in the application. Also, this type of scan requires extra configuration, which includes authentication and sensitive functionalities. The following playbook will run the full scan against the DVWS application. Now we can see that the playbook looks almost similar, except the flags sent to command: - name : Running OWASP ZAP Full Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-full-scan-dvws tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0777 - name : running owasp zap full scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-full-scan.py -t {{ website_url }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan raw : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\"","title":"Security testing against web applications and websites"},{"location":"learning/ansible/security_testing/#testing-web-apis","text":"Similar to the ZAP Baseline scan, the fine folks behind ZAP provide a script as part of their live and weekly Docker images. We can use it to run scans against API endpoints defined either by OpenAPI specification or Simple Object Access Protocol (SOAP). The script can understand the API specifications and import all the definitions. Based on this, it runs an active scan against all the URLs found. - name : Running OWASP ZAP API Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-api-scan-dvws api_type : openapi tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0777 - name : running owasp zap api scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-api-scan.py -t {{ website_url }} -f {{ api_type }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan raw : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\"","title":"Testing web APIs"},{"location":"learning/ansible/security_testing/#vulnerability-scanning-with-nessus","text":"Scanning for vulnerabilities is one of the best understood periodic activities security teams take up on their computers. There are well-documented strategies and best practices for doing regular scanning for vulnerabilities in computers, networks, operating system software, and application software: Basic network scans Credentials patch audit Correlating system information with known vulnerabilities With networked systems, this type of scanning is usually executed from a connected host that has the right kind of permissions to scan for security issues. One of the most popular vulnerability scanning tools is Nessus. Nessus started as a network vulnerability scanning tool, but now incorporates features such as the following: Port scanning Network vulnerability scanning Web application-specific scanning Host-based vulnerability scanning","title":"Vulnerability Scanning with Nessus"},{"location":"learning/ansible/security_testing/#introduction-to-nessus","text":"The vulnerability database that Nessus has is its main advantage. While the techniques to understanding which service is running and what version of the software is running the service are known to us, answering the question, \"Does this service have a known vulnerability\" is the important one. Apart from a regularly updated vulnerability database, Nessus also has information on default credentials found in applications, default paths, and locations. All of this fine-tuned in an easy way to use CLI or web-based tool. We will try out the standard activities required for that and see what steps are needed to automate them using Ansible. Installing Nessus using a playbook. Configuring Nessus. Running a scan. Running a scan using AutoNessus. Installing the Nessus REST API Python client. Downloading a report using the API.","title":"Introduction to Nessus"},{"location":"learning/ansible/security_testing/#installing-nessus-for-vulnerability-assessments","text":"- name : installing nessus server hosts : nessus remote_user : \"{{ remote_user_name }}\" gather_facts : no vars : remote_user_name : ubuntu nessus_download_url : \"http://downloads.nessus.org/nessus3dl.php?file=Nessus-6.11.2-ubuntu1110_amd64.deb&licence_accept=yes&t=84ed6ee87f926f3d17a218b2e52b61f0\" tasks : - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : downloading the package and installing apt : deb : \"{{ nessus_download_url }}\" - name : start the nessus daemon service : name : \"nessusd\" enabled : yes state : started","title":"Installing Nessus for vulnerability assessments"},{"location":"learning/ansible/security_testing/#configuring-nessus-for-vulnerability-scanning","text":"Perform the following steps to configure Nessus for vulnerability scanning: We have to navigate to https://NESSUSSERVERIP:8834 to confirm and start the service It returns with an SSL error and we need to accept the SSL error and confirm the security exception and continue with the installation Click on Confirm Security Exception and continue to proceed with the installation steps. Click on Continue and provide the details of the user, this user has full administrator access. Then finally, we have to provide the registration code (Activation Code), which can be obtained from registering at https://www.tenable.com/products/nessus-home Now it will install the required plugins. It will take a while to install, and once it is done we can log in to use the application. Now, we have successfully set up the Nessus vulnerability scanner.","title":"Configuring Nessus for vulnerability scanning"},{"location":"learning/ansible/security_testing/#basic-network-scanning","text":"Nessus has a wide variety of scans, some of them are free and some of them will be available only in a paid version. So, we can also customize the scanning if required. We can start with a basic network scan to see what's happening in the network. This scan will perform a basic full system scan for the given hosts. We have to mention the scan name and targets. Targets are just the hosts we want. Targets can be given in different formats, such as 192.168.33.1 for a single host, 192.168.33.1-10 for a range of hosts, and also we can upload the target file from our computer.","title":"Basic network scanning"},{"location":"learning/ansible/security_testing/#running-a-scan-using-autonessus","text":"With the AutoNessus script, we can do the following: List scans List scan policies Do actions on scans such as start, stop, pause, and resume The best part of AutoNessus is that since this is a command-line tool, it can easily become part of scheduled tasks and other automation workflows.","title":"Running a scan using AutoNessus"},{"location":"learning/ansible/security_testing/#setting-up-autonessus","text":"The following code is the Ansible playbook snippet to set up AutoNessus and configure it to use Nessus using credentials. - name : installing python-pip apt : name : python-pip update_cache : yes state : present - name : install python requests pip : name : requests - name : setting up autonessus get_url : url : \"https://github.com/redteamsecurity/AutoNessus/raw/master/autoNessus.py\" dest : /usr/bin/autoNessus mode : 0755 - name : updating the credentials replace : path : /usr/bin/autoNessus regexp : \"{{ item.src }}\" replace : \"{{ item.dst }}\" backup : yes no_log : True with_items : - { src : \"token = ''\" , dst : \"token = '{{ nessus_user_token }}'\" } - { src : \"url = 'https://localhost:8834'\" , dst : \"url = '{{ nessus_url }}'\" } - { src : \"username = 'xxxxx'\" , dst : \"username = '{{ nessus_user_name }}'\" } - { src : \"password = 'xxxxx'\" , dst : \"password = '{{ nessus_user_password }}'\" } no_log : True will censor the output in the log console of Ansible output. It will be very useful when we are using secrets and keys inside playbooks. Before running the automated scans using AutoNessus, we have to create them in the Nessus portal with required customization, and we can use these automated playbooks to perform tasks on top of it.","title":"Setting up AutoNessus"},{"location":"learning/ansible/security_testing/#listing-current-available-scans-and-ids","text":"- name : list current scans and IDs using autoNessus command : \"autoNessus -l\" register : list_scans_output - debug : msg : \"{{ list_scans_output.stdout_lines }}\"","title":"Listing current available scans and IDs"},{"location":"learning/ansible/security_testing/#starting-a-specified-scan-using-scan-id","text":"- name : starting nessus scan \"{{ scan_id }}\" using autoNessus command : \"autoNessus -sS {{ scan_id }}\" register : start_scan_output - debug : msg : \"{{ start_scan_output.stdout_lines }}\" - Similarly, we can perform pause, resume, stop, list policies, and so on. Using the AutoNessus program, these playbooks are available. This can be improved by advancing the Nessus API scripts.","title":"Starting a specified scan using scan ID"},{"location":"learning/ansible/security_testing/#storing-results","text":"The entire report can be exported into multiple formats, such as HTML, CSV, and Nessus. This helps to give more a detailed structure of vulnerabilities found, solutions with risk rating, and other references The output report can be customized based on the audience, if it goes to the technical team, we can list all the vulnerabilities and remediation. For example, if management wants to get the report, we can only get the executive summary of the issues. Reports can be sent by email as well using notification options in Nessus configuration.","title":"Storing results"},{"location":"learning/ansible/security_testing/#installing-the-nessus-rest-api-python-client","text":"Official API documentation can be obtained by connecting to your Nessus server under 8834/nessus6-api.html. To perform any operations using the Nessus REST API, we have to obtain the API keys from the portal. This can be found in user settings. Please make sure to save these keys","title":"Installing the Nessus REST API Python client"},{"location":"learning/ansible/security_testing/#downloading-reports-using-the-nessus-rest-api","text":"- name : working with nessus rest api connection : local hosts : localhost gather_facts : no vars : scan_id : 17 nessus_access_key : 620fe4ffaed47e9fe429ed749207967ecd7a77471105d8 nessus_secret_key : 295414e22dc9a56abc7a89dab713487bd397cf860751a2 nessus_url : https://192.168.33.109:8834 nessus_report_format : html tasks : - name : export the report for given scan \"{{ scan_id }}\" uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export\" method : POST validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" body : \"format={{ nessus_report_format }}&chapters=vuln_by_host;remediations\" register : export_request - debug : msg : \"File id is {{ export_request.json.file }} and scan id is {{ scan_id }}\" - name : check the report status for \"{{ export_request.json.file }}\" uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export/{{ export_request.json.file }}/status\" method : GET validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" register : report_status - debug : msg : \"Report status is {{ report_status.json.status }}\" - name : downloading the report locally uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export/{{ export_request.json.file }}/download\" method : GET validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" return_content : yes dest : \"./{{ scan_id }}_{{ export_request.json.file }}.{{ nessus_report_format }}\" register : report_output - debug : msg : \"Report can be found at ./{{ scan_id }}_{{ export_request.json.file }}.{{ nessus_report_format }}\"","title":"Downloading reports using the Nessus REST API"},{"location":"learning/ansible/security_testing/#nessus-configuration","text":"Nessus allows us to create different users with role-based authentication to perform scans and review with different access levels.","title":"Nessus configuration"},{"location":"learning/ansible/security_testing/#summary","text":"Security teams and IT teams rely on tools for vulnerability scanning, management, remediation, and continuous security processes. Nessus, by being one of the most popular and useful tools, was an automatic choice for the authors to try and automate.","title":"Summary"},{"location":"learning/ansible/security_testing/#writing-an-ansible-module-for-security-testing","text":"Ansible primarily works by pushing small bits of code to the nodes it connects to. These codes/programs are what we know as Ansible modules. Typically in the case of a Linux host these are copied over SSH, executed, and then removed from the node. We will look at the following: How to set up the development environment -Writing an Ansible hello world module to understand the basics -Where to seek further help -Defining a security problem statement -Addressing that problem by writing a module of our own Along with that, we will try to understand and attempt to answer the following questions: What are the good use cases for modules? When does it make sense to use roles? How do modules differ from plugins?","title":"Writing an Ansible Module for Security Testing"},{"location":"learning/ansible/security_testing/#getting-started-with-a-hello-world-ansible-module","text":"We will pass one argument to our custom module and show if we have success or failure for the module executing based on that. Since all of this is new to us, we will look at the following things: The source code of the hello world module The output of that module for both success and failure The command that we will use to invoke it","title":"Getting started with a hello world Ansible module"},{"location":"learning/ansible/security_testing/#setting-up-the-development-environment","text":"The primary requirement for Ansible 2.4 is Python 2.6 or higher and Python 3.5 or higher. If you have either of them installed, we can follow the simple steps to get the development environment going. From the Ansible Developer Guide: Clone the Ansible repository: $ git clone ansible/ansible.git Change the directory into the repository root directory: $ cd ansible Create a virtual environment: $ python3 -m venv venv (or for Python 2 $ virtualenv venv Note, this requires you to install the virtualenv package: $ pip install virtualenv Activate the virtual environment: $ . venv/bin/activate Install the development requirements: $ pip install -r requirements.txt Run the environment setup script for each new dev shell process: $ . hacking/env-setup This playbook will set up the developer environment by installing and setting up the virtual environment. - name : Setting Developer Environment hosts : dev remote_user : madhu become : yes vars : ansible_code_path : \"/home/madhu/ansible-code\" tasks : - name : installing prerequirements if not installed apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - git - virtualenv - python-pip - name : downloading ansible repo locally git : repo : https://github.com/ansible/ansible.git dest : \"{{ ansible_code_path }}/venv\" - name : creating virtual environment pip : virtualenv : \"{{ ansible_code_path }}\" virtualenv_command : virtualenv requirements : \"{{ ansible_code_path }}/venv/requirements.txt\"","title":"Setting up the development environment"},{"location":"learning/ansible/troubleshooting/","text":"Troubleshooting \u00b6 It's also important to remember that, due to its nature, in Ansible code, we describe the desired state rather than stating a sequence of steps to obtain the desired state. This difference means that the system is less prone to logical errors. Nevertheless, a bug in a Playbook could mean a potential misconfiguration on all of your machines. This should be taken very seriously. It is even more critical when critical parts of the system are changed, such as SSH daemon or sudo configuration, since the risk is you locking yourself out of the system. Digging into playbook execution problems \u00b6 There are cases where an Ansible execution will interrupt. Many things can cause these situations. The single most frequent cause of problems I've found while executing Ansible playbooks is the network. Since the machine that is issuing the commands and the one that is performing them are usually linked through the network, a problem in the network will immediately show itself as an Ansible execution problem. For instance, if you run the /bin/false command, it will always return 1. To execute this in a playbook so that you can avoid it blocking there, you can write something like the following: - name : Run a command that will return 1 command : /bin/false ignore_errors : yes Be aware that this is a particular case, and often, the best approach is to fix your application so that you're following UNIX standards and return 0 if the application runs appropriately, instead of putting a workaround in your Playbooks. Using host facts to diagnose failures \u00b6 Some execution failures derive from the state of the target machine. The most common problem of this kind is the case where Ansible expects a file or variable to be present, but it's not. Sometimes, it can be enough to print the machine facts to find the problem. - hosts : target_host tasks : - name : Display all variables/facts known for a host debug : var : hostvars[inventory_hostname] This technique will give you a lot of information about the state of the target machine during Ansible execution. Testing with a playbook \u00b6 One of the most complex things in the IT field is not creating software and systems, but debugging them when they have problems. Ansible is not an exception. No matter how good you are at creating Ansible playbooks, sooner or later, you'll find yourself debugging a playbook that is not behaving as you thought it would. The simplest way of performing basic tests is to print out the values of variables during execution. - hosts : localhost tasks : - shell : /usr/bin/uptime register : result - debug : var : result The debug module is the module that allows you to print the value of a variable (by using the var option) or a fixed string (by using the msg option) during Ansible's execution. The debug module also provides the verbosity option. - hosts : localhost tasks : - shell : /usr/bin/uptime register : result - debug : var : result verbosity : 2 We set the minimum required verbosity to 2, and by default, Ansible runs with a verbosity of 0. To see the result of using the debug module with this new playbook ansible-playbook debug2.yaml -vv By putting two -v options in the command line, we will be running Ansible with verbosity of 2. This will not only affect this specific module but all the modules (or Ansible itself) that are set to behave differently at different debug levels. Using check mode \u00b6 Although you might be confident in the code you have written, it still pays to test it before running it for real in a production environment. In such cases, it is a good idea to be able to run your code, but with a safety net in place. This is what check mode is for. - hosts : localhost tasks : - name : Touch a file file : path : /tmp/myfile state : touch ansible-playbook check-mode.yaml --check Ansible check mode is usually called a dry run. The idea is that the run won't change the state of the machine and will only highlight the differences between the current status and the status declared in the playbook. Not all modules support check mode, but all major modules do, and more and more modules are being added at every release. In particular, note that the command and shell modules do not support it because it is impossible for the module to tell what commands will result in a change, and what won't. Therefore, these modules will always return changed when they're run outside of check mode because they assume a change has been made. A similar feature to check mode is the \u2013diff flag. What this flag allows us to do is track what exactly changed during an Ansible execution. ansible-playbook check-mode.yaml --diff The output says changed, which means that something was changed (more specifically, the file was created), and in the output, we can see a diff-like output that tells us that the state moved from absent to touch, which means the file was created. mtime and atime also changed, but this is probably due to how files are created and checked. Solving host connection issues \u00b6 Ansible is often used to manage remote hosts or systems. To do this, Ansible will need to be able to connect to the remote host, and only after that will it be able to issue commands. Sometimes, the problem is that Ansible is unable to connect to the remote host. A typical example of this is when you try to manage a machine that hasn't booted yet. Being able to quickly recognize these kinds of problems and fix them promptly will help you save a lot of time. - hosts : all tasks : - name : Touch a file file : path : /tmp/myfile state : touch We can try to run the remote.yaml playbook against a non-existent FQDN ansible-playbook -i host.example.com, remote.yaml The output will clearly inform us that the SSH service did not reply in time. SSH connections usually fail for one of two reasons: The SSH client is unable to establish a connection with the SSH server The SSH server refuses the credentials provided by the SSH client It's very probable that the IP address or the port is wrong, so the TCP connection isn't feasible. Usually, double-checking the IP and the hostname (if it's a DNS, check that it resolves to the right IP) solves the problem. To investigate this further, you can try performing an SSH connection from the same machine to check if there are problems. ssh host.example.com -vvv The second problem might be a little bit more complex to debug since it can happen for multiple reasons. One of those is that you are trying to connect to the wrong host and you don't have the credentials for that machine. Another common case is that the username is wrong. To debug it, you can take the user@host address that is shown in the error (in my case, fale@host.example.com ) and use the same command you used previously. ssh fale@host.example.com -vvv This should raise the same error that Ansible reported to you, but with much more details. Passing working variables via the CLI \u00b6 One thing that can help during debugging, and definitely helps for code reusability, is passing variables to playbooks via the command line. Every time your application \u2013 either an Ansible playbook or any kind of application \u2013 receives an input from a third party (a human, in this case), it should ensure that the value is reasonable. An example of this would be to check that the variable has been set and therefore is not an empty string. This is a security golden rule, but should also be applied when the user is trusted since the user might mistype the variable name . The application should identify this and protect the whole system by protecting itself. - hosts : localhost tasks : - debug : var : variable Now that we have an Ansible playbook that allows us to see if a variable has been set to what we were expecting, let's run it with variable declared in the execution statement. ansible-playbook printvar.yaml --extra-vars = '{\"variable\": \"Hello, World!\"}' Ansible allows variables to be set in various modes and with different priorities. More specifically, you can set them with the following. Command-line values (lowest priority) Role defaults Inventory files or script group vars Inventory group_vars/all Playbook group_vars/all Inventory group_vars/* Playbook group_vars/* Inventory files or script host vars Inventory host_vars/* Playbook host_vars/* Host facts/cached set_facts Play vars Play vars_prompt Play vars_files Role vars (defined in role/vars/main.yml) Block vars (only for tasks in block) Task vars (only for the task) include_vars set_facts/registered vars Role (and include_role) params include params Extra vars (highest priority) Limiting the host's execution \u00b6 While testing a playbook, it might make sense to test on a restricted number of machines; for instance, just one. - hosts : all tasks : - debug : msg : \"Hello, World!\" [ hosts ] host1.example.com host2.example.com host3.example.com If we just want to run it against host3.example.com, we will need to specify this on the command line. ansible-playbook -i inventory helloworld.yaml --limit = host3.example.com By using the \u2013limit keyword, we can force Ansible to ignore all the hosts that are outside what is specified in the limit parameter. It's possible to specify multiple hosts as a list or with patterns, so both of the following commands will execute the playbook against host2.example.com and host3.example.com ansible-playbook -i inventory helloworld.yaml --limit = host2.example.com,host3.example.com ansible-playbook -i inventory helloworld.yaml --limit = host [ 2 -3 ] .example.com Flushing the code cache \u00b6 Everywhere in IT, caches are used to speed up operations, and Ansible is not an exception. Usually, caches are good, and for this reason, they are heavily used ubiquitously. However, they might create some problems if they cache a value they should not have cached or if they are not flushed, even if the value has changed. Flushing caches in Ansible is very straightforward, and it's enough to run ansible-playbook, which we are already running, with the addition of the \u2013flush-cache option ansible-playbook -i inventory helloworld.yaml --flush-cache Ansible uses Redis to save host variables, as well as execution variables. Sometimes, those variables might be left behind and influence the following executions. When Ansible finds a variable that should be set in the step it just started, Ansible might assume that the step has already been completed, and therefore pick up that old variable as if it has just been created. By using the \u2013flush-cache option, we can avoid this since it will ensure that Ansible flushes the Redis cache during its execution. Checking for bad syntax \u00b6 Defining whether a file has the right syntax or not is fairly easy for a machine, but might be more complex for humans. This does not mean that machines are able to fix the code for you, but they can quickly identify whether a problem is present or not. To use Ansible's built-in syntax checker, we need a playbook with a syntax error. - hosts : all tasks : - debug : msg : \"Hello, World!\" We can use the \u2013syntax-check command ansible-playbook syntaxcheck.yaml --syntax-check Since Ansible knows all the supported options in all the supported modules, it can quickly read your code and validate whether the YAML you provided contains all the required fields and that it does not contain any unsupported fields.","title":"Troubleshooting"},{"location":"learning/ansible/troubleshooting/#troubleshooting","text":"It's also important to remember that, due to its nature, in Ansible code, we describe the desired state rather than stating a sequence of steps to obtain the desired state. This difference means that the system is less prone to logical errors. Nevertheless, a bug in a Playbook could mean a potential misconfiguration on all of your machines. This should be taken very seriously. It is even more critical when critical parts of the system are changed, such as SSH daemon or sudo configuration, since the risk is you locking yourself out of the system.","title":"Troubleshooting"},{"location":"learning/ansible/troubleshooting/#digging-into-playbook-execution-problems","text":"There are cases where an Ansible execution will interrupt. Many things can cause these situations. The single most frequent cause of problems I've found while executing Ansible playbooks is the network. Since the machine that is issuing the commands and the one that is performing them are usually linked through the network, a problem in the network will immediately show itself as an Ansible execution problem. For instance, if you run the /bin/false command, it will always return 1. To execute this in a playbook so that you can avoid it blocking there, you can write something like the following: - name : Run a command that will return 1 command : /bin/false ignore_errors : yes Be aware that this is a particular case, and often, the best approach is to fix your application so that you're following UNIX standards and return 0 if the application runs appropriately, instead of putting a workaround in your Playbooks.","title":"Digging into playbook execution problems"},{"location":"learning/ansible/troubleshooting/#using-host-facts-to-diagnose-failures","text":"Some execution failures derive from the state of the target machine. The most common problem of this kind is the case where Ansible expects a file or variable to be present, but it's not. Sometimes, it can be enough to print the machine facts to find the problem. - hosts : target_host tasks : - name : Display all variables/facts known for a host debug : var : hostvars[inventory_hostname] This technique will give you a lot of information about the state of the target machine during Ansible execution.","title":"Using host facts to diagnose failures"},{"location":"learning/ansible/troubleshooting/#testing-with-a-playbook","text":"One of the most complex things in the IT field is not creating software and systems, but debugging them when they have problems. Ansible is not an exception. No matter how good you are at creating Ansible playbooks, sooner or later, you'll find yourself debugging a playbook that is not behaving as you thought it would. The simplest way of performing basic tests is to print out the values of variables during execution. - hosts : localhost tasks : - shell : /usr/bin/uptime register : result - debug : var : result The debug module is the module that allows you to print the value of a variable (by using the var option) or a fixed string (by using the msg option) during Ansible's execution. The debug module also provides the verbosity option. - hosts : localhost tasks : - shell : /usr/bin/uptime register : result - debug : var : result verbosity : 2 We set the minimum required verbosity to 2, and by default, Ansible runs with a verbosity of 0. To see the result of using the debug module with this new playbook ansible-playbook debug2.yaml -vv By putting two -v options in the command line, we will be running Ansible with verbosity of 2. This will not only affect this specific module but all the modules (or Ansible itself) that are set to behave differently at different debug levels.","title":"Testing with a playbook"},{"location":"learning/ansible/troubleshooting/#using-check-mode","text":"Although you might be confident in the code you have written, it still pays to test it before running it for real in a production environment. In such cases, it is a good idea to be able to run your code, but with a safety net in place. This is what check mode is for. - hosts : localhost tasks : - name : Touch a file file : path : /tmp/myfile state : touch ansible-playbook check-mode.yaml --check Ansible check mode is usually called a dry run. The idea is that the run won't change the state of the machine and will only highlight the differences between the current status and the status declared in the playbook. Not all modules support check mode, but all major modules do, and more and more modules are being added at every release. In particular, note that the command and shell modules do not support it because it is impossible for the module to tell what commands will result in a change, and what won't. Therefore, these modules will always return changed when they're run outside of check mode because they assume a change has been made. A similar feature to check mode is the \u2013diff flag. What this flag allows us to do is track what exactly changed during an Ansible execution. ansible-playbook check-mode.yaml --diff The output says changed, which means that something was changed (more specifically, the file was created), and in the output, we can see a diff-like output that tells us that the state moved from absent to touch, which means the file was created. mtime and atime also changed, but this is probably due to how files are created and checked.","title":"Using check mode"},{"location":"learning/ansible/troubleshooting/#solving-host-connection-issues","text":"Ansible is often used to manage remote hosts or systems. To do this, Ansible will need to be able to connect to the remote host, and only after that will it be able to issue commands. Sometimes, the problem is that Ansible is unable to connect to the remote host. A typical example of this is when you try to manage a machine that hasn't booted yet. Being able to quickly recognize these kinds of problems and fix them promptly will help you save a lot of time. - hosts : all tasks : - name : Touch a file file : path : /tmp/myfile state : touch We can try to run the remote.yaml playbook against a non-existent FQDN ansible-playbook -i host.example.com, remote.yaml The output will clearly inform us that the SSH service did not reply in time. SSH connections usually fail for one of two reasons: The SSH client is unable to establish a connection with the SSH server The SSH server refuses the credentials provided by the SSH client It's very probable that the IP address or the port is wrong, so the TCP connection isn't feasible. Usually, double-checking the IP and the hostname (if it's a DNS, check that it resolves to the right IP) solves the problem. To investigate this further, you can try performing an SSH connection from the same machine to check if there are problems. ssh host.example.com -vvv The second problem might be a little bit more complex to debug since it can happen for multiple reasons. One of those is that you are trying to connect to the wrong host and you don't have the credentials for that machine. Another common case is that the username is wrong. To debug it, you can take the user@host address that is shown in the error (in my case, fale@host.example.com ) and use the same command you used previously. ssh fale@host.example.com -vvv This should raise the same error that Ansible reported to you, but with much more details.","title":"Solving host connection issues"},{"location":"learning/ansible/troubleshooting/#passing-working-variables-via-the-cli","text":"One thing that can help during debugging, and definitely helps for code reusability, is passing variables to playbooks via the command line. Every time your application \u2013 either an Ansible playbook or any kind of application \u2013 receives an input from a third party (a human, in this case), it should ensure that the value is reasonable. An example of this would be to check that the variable has been set and therefore is not an empty string. This is a security golden rule, but should also be applied when the user is trusted since the user might mistype the variable name . The application should identify this and protect the whole system by protecting itself. - hosts : localhost tasks : - debug : var : variable Now that we have an Ansible playbook that allows us to see if a variable has been set to what we were expecting, let's run it with variable declared in the execution statement. ansible-playbook printvar.yaml --extra-vars = '{\"variable\": \"Hello, World!\"}' Ansible allows variables to be set in various modes and with different priorities. More specifically, you can set them with the following. Command-line values (lowest priority) Role defaults Inventory files or script group vars Inventory group_vars/all Playbook group_vars/all Inventory group_vars/* Playbook group_vars/* Inventory files or script host vars Inventory host_vars/* Playbook host_vars/* Host facts/cached set_facts Play vars Play vars_prompt Play vars_files Role vars (defined in role/vars/main.yml) Block vars (only for tasks in block) Task vars (only for the task) include_vars set_facts/registered vars Role (and include_role) params include params Extra vars (highest priority)","title":"Passing working variables via the CLI"},{"location":"learning/ansible/troubleshooting/#limiting-the-hosts-execution","text":"While testing a playbook, it might make sense to test on a restricted number of machines; for instance, just one. - hosts : all tasks : - debug : msg : \"Hello, World!\" [ hosts ] host1.example.com host2.example.com host3.example.com If we just want to run it against host3.example.com, we will need to specify this on the command line. ansible-playbook -i inventory helloworld.yaml --limit = host3.example.com By using the \u2013limit keyword, we can force Ansible to ignore all the hosts that are outside what is specified in the limit parameter. It's possible to specify multiple hosts as a list or with patterns, so both of the following commands will execute the playbook against host2.example.com and host3.example.com ansible-playbook -i inventory helloworld.yaml --limit = host2.example.com,host3.example.com ansible-playbook -i inventory helloworld.yaml --limit = host [ 2 -3 ] .example.com","title":"Limiting the host's execution"},{"location":"learning/ansible/troubleshooting/#flushing-the-code-cache","text":"Everywhere in IT, caches are used to speed up operations, and Ansible is not an exception. Usually, caches are good, and for this reason, they are heavily used ubiquitously. However, they might create some problems if they cache a value they should not have cached or if they are not flushed, even if the value has changed. Flushing caches in Ansible is very straightforward, and it's enough to run ansible-playbook, which we are already running, with the addition of the \u2013flush-cache option ansible-playbook -i inventory helloworld.yaml --flush-cache Ansible uses Redis to save host variables, as well as execution variables. Sometimes, those variables might be left behind and influence the following executions. When Ansible finds a variable that should be set in the step it just started, Ansible might assume that the step has already been completed, and therefore pick up that old variable as if it has just been created. By using the \u2013flush-cache option, we can avoid this since it will ensure that Ansible flushes the Redis cache during its execution.","title":"Flushing the code cache"},{"location":"learning/ansible/troubleshooting/#checking-for-bad-syntax","text":"Defining whether a file has the right syntax or not is fairly easy for a machine, but might be more complex for humans. This does not mean that machines are able to fix the code for you, but they can quickly identify whether a problem is present or not. To use Ansible's built-in syntax checker, we need a playbook with a syntax error. - hosts : all tasks : - debug : msg : \"Hello, World!\" We can use the \u2013syntax-check command ansible-playbook syntaxcheck.yaml --syntax-check Since Ansible knows all the supported options in all the supported modules, it can quickly read your code and validate whether the YAML you provided contains all the required fields and that it does not contain any unsupported fields.","title":"Checking for bad syntax"},{"location":"learning/docker/docker-notes/","text":"Important Links Docker Projects Master list of Docker Resources and Projects Play With Docker , a great resource for web-based docker testing and also has a library of labs built by Docker Captains and others, and supported by Docker Inc. Play With Docker Labs DockerHub Recipes Docker Cloud: CI/CD and Server Ops Docker and Pi Projects Docker Training Content Docker Mastery Bret's Podcast Bret's Youtube Docker Shell Config Docker Devops Containers vs VM ebook Docker Refernce Docker Certificated Associate Dockerfile Reference Dockerfile Best Practice Formatting Docker CLI Output Docker Image Docker Registry Config Docker Registry Garbage Collection Docker Registry as cache Docker Storage Docker secrets Docker CLI to kubectl Software Design Immutable Software 12 factor App 12 Fractured Apps Devops Roadmap Docker Best Practises Docker Best practise inside a code repo Healthcheck in Dockerfile Starting container process caused \"exec: \\\"ping\\\": executable file not found in $PATH \" : unknown apt-get update && apt-get install -y iputils-ping Starting mysql container and running ps causes \"ps: command not found\" apt-get update && apt-get install procps Creating and Using Containers Like a Boss \u00b6 Check Our Docker Install and Config \u00b6 docker version - verified cli can talk to engine docker info - most config values of engine Image vs. Container \u00b6 An Image is the application we want to run A Container is an instance of that image running as a process You can have many containers running off the same image Docker's default image \"registry\" is called Docker Hub docker container run --publish 80 :80 --detach --name webhost nginx docker container run -it # start new container interactively docker container exec -it # run additional command in existing container docker container ls -a docker container logs webhost Container VS. VM: It's Just a Process \u00b6 docker run --name mongo -d mongo docker container top # process list in one container docker stop mongo docker ps docker start mongo docker container inspect # details of one container config docker container stats # performance stats for all containers The Mighty Hub: Using Docker Hub Registry Images \u00b6 docker pull nginx docker image ls Images and Their Layers: Discover the Image Cache \u00b6 docker history nginx:latest docker image inspect nginx Image Tagging and Pushing to Docker Hub \u00b6 docker pull nginx:latest docker image ls docker image tag nginx bretfisher/nginx docker login cat .docker/config.json docker image push bretfisher/nginx docker image push bretfisher/nginx bretfisher/nginx:testing Getting a Shell Inside Containers: No Need for SSH \u00b6 docker container exec -it mysql -- bash docker container run -it alpine -- bash docker container run -it alpine -- sh Cleaning Docker images \u00b6 Use docker system df to see space usage. docker image prune to clean up just \"dangling\" images. The big one is usually docker image prune -a which will remove all images you're not using. docker volume prune to remove unused volumes docker system prune will clean up everything (Nuke everything that is not used currently). docker system prune -a wipe everything. Docker Networks: Concepts for Private and Public Comms in Containers \u00b6 Each container connected to a private virtual network \"bridge\" Each virtual network routes through NAT firewall on host IP All containers on a virtual network can talk to each other without -p Best practice is to create a new virtual network for each app: network \"my_web_app\" for mysql and php/apache containers network \"my_api\" for mongo and nodejs containers docker container run -p 80 :80 --name webhost -d nginx docker container port webhost docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost Docker Networks: CLI Management of Virtual Networks \u00b6 Show networks docker network ls Inspect a network docker network inspect Create a network docker network create --driver Attach a network to container docker network connect Detach a network from container docker network disconnect docker network ls docker network inspect bridge docker network create my_app_net docker container run -d --name new_nginx --network my_app_net nginx docker network inspect my_app_net docker network connect <new network id> <container id> docker container disconnect <new network id> <container id> Docker Networks: DNS and How Containers Find Each Other \u00b6 Create your apps so frontend/backend sit on same Docker network Their inter-communication never leaves host All externally exposed ports closed by default You must manually expose via -p , which is better default security! Containers shouldn't rely on IP's for inter-communication DNS for friendly names is built-in if you use custom networks docker container run -d --name my_nginx --network my_app_net nginx docker container exec -it my_nginx ping new_nginx docker container exec -it new_nginx ping my_nginx DNS Round Robin Testing \u00b6 docker network create dude docker container run -d --net dude --net-alias search elasticsearch:2 docker container ls docker container run --rm -- net dude alpine nslookup search docker container run --rm --net dude centos curl -s search:9200 Container Lifetime & Persistent Data: Volumes, Volumes, Volumes \u00b6 Persistent Data: Data Volumes \u00b6 Containers are usually immutable and ephemeral \"immutable infrastructure\": only re-deploy containers, never change This is the ideal scenario, but what about databases, or unique data? Docker gives us features to ensure these \"separation of concerns\" This is known as \"persistent data\" Two ways: Volumes and Bind Mounts Volumes : make special location outside of container UFS Bind Mounts : link container path to host path docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD = True -v mysql-db:/var/lib/mysql mysql docker volume ls docker volume inspect mysql-db Persistent Data: Bind Mounting \u00b6 Used for local development Usecase: When you are changing files on laptop which you want to serve in the app It can be run only during docker run as there is no explicit volume command in the dockerfile the volume will be mounted in the working directory of the container docker container run -d --name nginx -p 80 :80 -v $( pwd ) :/usr/share/nginx/html nginx docker container exec -it nginx -- bash cd /usr/share/nginx/html && ls -la docker log streaming docker container logs -f <container name> Database Passwords in Containers \u00b6 When running postgres now, you'll need to either set a password, or tell it to allow any connection (which was the default before this change). -you need to either set a password with the environment variable: POSTGRES_PASSWORD=mypasswd Or tell it to ignore passwords with the environment variable: POSTGRES_HOST_AUTH_METHOD=trust Making It Easier with Docker Compose: The Multi-Container Tool \u00b6 Why: configure relationships between containers Why: save our docker container run settings in easy-to-read file Why: create one-liner developer environment startups YAML-formatted file that describes our solution options for: containers, networks, volumes A CLI tool docker-compose used for local dev/test automation with those YAML files docker-compose.yml is default filename, but any can be used with docker-compose -f Not a production-grade tool but ideal for local development and test Two most common commands are: docker-compose up # setup volumes/networks and start all containers docker-compose down # stop all containers and remove cont/vol/net Trying Out Basic Compose Commands \u00b6 docker-compose up docker-compose up -d # Running compose in bacground docker-compose down docker-compose down -v --rmi local/all # Removes images and volumes # Compose operations docker-compose logs docker-compose ps docker-compose top docker-compose build # Build images or docker-compose up --build Swarm Intro and Creating a 3-Node Swarm Cluster \u00b6 Swarm Mode is a clustering solution built inside Docker Not enabled by default docker swarm init: What Just Happened? \u00b6 Lots of PKI and security automation Root Signing Certificate created for our Swarm Certificate is issued for first Manager node Join tokens are created Raft database created to store root CA, configs and secrets Encrypted by default on disk (1.13+) No need for another key/value system to hold orchestration/secrets Replicates logs amongst Managers via mutual TLS in \"control plane\" Create Your First Service and Scale it Locally \u00b6 docker info # swarm is down by default docker swarm init # start swarm docker node ls docker service create alpine ping 8 .8.8.8 # creates service frosty_newton docker service ls docker service ps frosty_newton docker container ls docker service update frosty_newton --replicas 3 # creates 3 replicas docker service ls docker service rm frosty_newton # deletes the service docker service ls docker container ls Creating a 3-Node Swarm Cluster \u00b6 docker-machine + VirtualBox - Free and runs locally, but requires a machine with 8GB memory docker-machine create node1 docker-machine ssh node1 docker-machine env node1 docker swarm init docker swarm init --advertise-addr node1 docker node ls docker node update --role manager node2 # Update role to existing node docker swarm join-token manager # Shows join token for manager role docker service create --replicas 3 alpine ping 8 .8.8.8 # Creates service with 3 replicas and starts ping process docker service ls docker service ps <service name> docker node ps docker node ps node2 Scaling Out with Overlay Networking \u00b6 # Create Backend network docker network create --driver overlay mydrupal docker network ls docker service create --name psql --netowrk mydrupal -e POSTGRES_PASSWORD = mypass postgres docker service ls docker service ps psql docker container logs psql <container name> # Create Frontend network docker service create --name drupal --network mydrupal -p 80 :80 drupal docker service inspect drupal Scaling Out with Routing Mesh \u00b6 docker service create --name search --replicas 3 -p 9200 :9200 elasticsearch:2 docker service ps search Create a Multi-Service Multi-Node Web App \u00b6 docker network create -d overlay backend docker network create -d overlay frontend docker service create --name vote -p 80 :80 --network frontend \\ -- replica 2 dockersamples/examplevotingapp_vote:before docker service create --name redis --network frontend \\ --replica 1 redis:3.2 docker service create --name worker --network frontend --network backend dockersamples/examplevotingapp_worker docker service create --name db --network backend \\ --mount type = volume,source = db-data,target = /var/lib/postgresql/data postgres:9.4 docker service create --name result --network backend -p 5001 :80 COPY INFO docker service ls docker service logs worker Swarm Stacks and Production Grade Compose \u00b6 Docker adds a new layer of abstraction to Swarm called Stacks Stacks accept Compose files as their declarative definition for services, networks, and volumes We use docker stack deploy rather then docker service create Stacks manages all those objects for us, including overlay network per stack. Adds stack name to start of their name Compose now ignores deploy: , Swarm ignores build: docker stack deploy -c example-voting-app-stack.yml voteapp docker stack ls docker stack services voteapp docker stack ps voteapp Using Secrets in Swarm Services \u00b6 What is a Secret? - Usernames and passwords - TLS certificates and keys - SSH keys - Any data you would prefer not be \"on front page of news\" docker secret create psql_usr psql_usr.txt echo \"myDBpassWORD\" | docker secret create psql_pass - TAB COMPLETION docker secret inspect psql_usr docker service create --name psql --secret psql_user \\ --secret psql_pass -e POSTGRES_PASSWORD_FILE = /run/secrets/psql_pass \\ -e POSTGRES_USER_FILE = /run/secrets/psql_user postgres docker exec -it <container name> bash cat /run/secrets/psql_user Swarm App Lifecycle \u00b6 Full App Lifecycle: Dev, Build and Deploy With a Single Compose Design \u00b6 Single set of Compose files for: - Local docker-compose up development environment - Remote docker-compose up CI environment - Remote docker stack deploy production environment docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d docker-compose -f docker-compose.yml -f docker-compose.prod.yml config Service Updates: Changing Things In Flight \u00b6 Provides rolling replacement of tasks/containers in a service Limits downtime (be careful with \"prevents\" downtime) Will replace containers for most changes Has many, many cli options to control the update Create options will usually change, adding -add or -rm to them Includes rollback and healthcheck options Also has scale & rollback subcommand for quicker access docker service scale web=4 and docker service rollback web Just update the image used to a newer version docker service update --image myapp:1.2.1 <servicename> Adding an environment variable and remove a port docker service update --env-add NODE_ENV=production --publish-rm 8080 Change number of replicas of two services docker service scale web=8 api=6 docker service create -p 8088 :80 --name web nginx:1.13.7 docker service scale web = 5 docker service update --image nginx:1.13.6 web docker service update --publish-rm 8088 --publish-add 9090 :80 docker service update --force web # forces rebalancing of the service without changing anything docker service rm web Healthchecks in Dockerfiles \u00b6 HEALTHCHECK was added in 1.12 Supported in Dockerfile, Compose YAML, docker run, and Swarm Services Docker engine will exec's the command in the container (e.g. curl localhost) It expects exit 0 (OK) or exit 1 (Error) Three container states: starting, healthy, unhealthy Much better then \"is binary still running?\" Options for healthcheck command --interval = DURATION ( default: 30s ) --timeout = DURATION ( default: 30s ) --start-period = DURATION ( default: 0s ) ( 17 .09+ ) --retries = N ( default: 3 ) docker container run --name p2 -d --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres docker service create --name p2 --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres Container Registries: Image Storage and Distribution \u00b6 Run a Private Docker Registry \u00b6 Secure your Registry with TLS Storage cleanup via Garbage Collection Enable Hub caching via \"\u2013registry-mirror\" # Run the registry image docker container run -d -p 5000 :5000 --name registry registry # Re-tag an existing image and push it to your new registry docker pull hello-world docker run hello-world docker tag hello-world 127 .0.0.1:5000/hello-world docker push 127 .0.0.1:5000/hello-world # Remove that image from local cache and pull it from new registry docker image remove hello-world docker image remove 127 .0.0.1:5000/hello-world docker pull 127 .0.0.1:5000/hello-world:latest # Re-create registry using a bind mount and see how it stores data docker container kill registry docker container rm registry docker container run -d -p 5000 :5000 --name registry -v $( pwd ) /registry-data:/var/lib/registry registry Using Docker Registry With Swarm \u00b6 docker node ls docker service create --name registry --publish 5000 :5000 registry docker service ps registry docker pull nginx docker tag nginx 127 .0.0.1:5000/nginx docker push 127 .0.0.1:5000/nginx docker service create --name nginx -p 80 :80 --replicas 5 --detach = false 127 .0.0.1:5000/nginx docker service ps nginx Using Docker in Production \u00b6 Focus on Dockerfiles first. Study ENTRYPOINT of Hub official images. Use it for config of images before CMD is executed. use ENTRYPOINT to set default values for all environments and then overide using ENV values. EntryPoint vs CMD FROM official distros. Make it == start, log all things in stdout/stderr, documented in file, lean and scale. Using SaaS for - Image Registry, Logging, Monitoring, Look at CNCF Landscape Using Layer 7 Reverse Proxy if port 80 and 443 are used by multiple apps Docker Security \u00b6 Docker Security Checklist Docker Engine Security Docker Security Tools Seccomp App Armor Docker Bench CIS Docker checklist Running Docker as non root user # Creating non root user in alpine RUN addgroup -g 1000 node \\ && adduser -u 1000 -G node -s /bin/sh -D node # Creating non root user in stretch RUN groupadd --gid 1000 node \\ && useradd --uid 1000 --gid node --shell /bin/bash --create-home node Sample Dockerfile with USER User Namespaces Shift Left Security Trivy - Image Scanning Sysdig Falco Appamror Profiles Seccomp Profile Docker Context \u00b6 Start a node on play with Docker Copy the IP of the node Set the Docker Context with the Host Name of the node and port 2375 Contexts are created in the home folder of user called .docker/context docker context create --docker \"host=tcp://<Host Name>:2375\" <context-name> docker context ls docker context use <context-name> docker ps # Should show the new context of play with docker # Overriding Context to default in commandline docker -c default ps docker -c <context-name> ps # Looping through all the context and executing ps for c in ` docker context ls -q ` ; do ` docker -c $c ps ` ; done # Creates the image in all context for c in ` docker context ls -q ` ; do ` docker -c $c run hello-world ` ; done Recommendations \u00b6 To change permissions on file system (chown or chmod) use a Entrypoint script. Look up to official images for examples for Entrypoint One App or Website use one container, specially if using an orchestrator like K8s or Docker Swarm. Scaling is also a benefit due to one-one relationship. Changing Docker IP range Use Cloud DB as service instead of in containers Run one process per container Strict Separation of Config from Code. Use Env variables to achieve this. Using Development workflow in Compose Write all the ENV variables at the top of Dockerfile Using Env variables in Dockerfile Override Env variables in Docker Compose file say for Dev testing Using Env variables in Docker Entrypoint to write into Application config files during start up. Secrets and Application specific config goes into specific ENV var blocks. This can be changed. Defaults or data specific to SERVER or LANGUAGE goes to another ENV block and can be kept static. This avoids them being set for each ENV. Encrypting traffic for local development use Lets Encrypt ad store them in .cert folder in Home Directory. Encrypting traffic for production use Lets Encrypt and maybe Traefik as Front proxy. See example using Swarm COPY vs ADD. Use COPY to copy artefacts in the same repo to the image. Use ADD when you want to download something from the Internet or to untar or unzip. You can also replace using wget statements with ADD. Combine multiple RUN into a single statement. Delete packages which are downloaded and installed also in a single command to save image size. No secrets like configs, certificates should be saved in Image. Pass them during runtime. Always have a CMD in the image, even if its inheriting it from BASE image Version apt packages and BASE images Use multistage Dokcer builds to have Dev dependencies and Prod dependencies separate. Have healthchecks in K8s instead of Dockerfile Use DNS RoundRobin for Database inside Compose file so it switches of Virtual IP on the Overlay network and gives direct access from FrontEnd Service to Backend container. Setting resource limits inside Compose file DRY your compose files using templates","title":"Docker"},{"location":"learning/docker/docker-notes/#creating-and-using-containers-like-a-boss","text":"","title":"Creating and Using Containers Like a Boss"},{"location":"learning/docker/docker-notes/#check-our-docker-install-and-config","text":"docker version - verified cli can talk to engine docker info - most config values of engine","title":"Check Our Docker Install and Config"},{"location":"learning/docker/docker-notes/#image-vs-container","text":"An Image is the application we want to run A Container is an instance of that image running as a process You can have many containers running off the same image Docker's default image \"registry\" is called Docker Hub docker container run --publish 80 :80 --detach --name webhost nginx docker container run -it # start new container interactively docker container exec -it # run additional command in existing container docker container ls -a docker container logs webhost","title":"Image vs. Container"},{"location":"learning/docker/docker-notes/#container-vs-vm-its-just-a-process","text":"docker run --name mongo -d mongo docker container top # process list in one container docker stop mongo docker ps docker start mongo docker container inspect # details of one container config docker container stats # performance stats for all containers","title":"Container VS. VM: It's Just a Process"},{"location":"learning/docker/docker-notes/#the-mighty-hub-using-docker-hub-registry-images","text":"docker pull nginx docker image ls","title":"The Mighty Hub: Using Docker Hub Registry Images"},{"location":"learning/docker/docker-notes/#images-and-their-layers-discover-the-image-cache","text":"docker history nginx:latest docker image inspect nginx","title":"Images and Their Layers: Discover the Image Cache"},{"location":"learning/docker/docker-notes/#image-tagging-and-pushing-to-docker-hub","text":"docker pull nginx:latest docker image ls docker image tag nginx bretfisher/nginx docker login cat .docker/config.json docker image push bretfisher/nginx docker image push bretfisher/nginx bretfisher/nginx:testing","title":"Image Tagging and Pushing to Docker Hub"},{"location":"learning/docker/docker-notes/#getting-a-shell-inside-containers-no-need-for-ssh","text":"docker container exec -it mysql -- bash docker container run -it alpine -- bash docker container run -it alpine -- sh","title":"Getting a Shell Inside Containers: No Need for SSH"},{"location":"learning/docker/docker-notes/#cleaning-docker-images","text":"Use docker system df to see space usage. docker image prune to clean up just \"dangling\" images. The big one is usually docker image prune -a which will remove all images you're not using. docker volume prune to remove unused volumes docker system prune will clean up everything (Nuke everything that is not used currently). docker system prune -a wipe everything.","title":"Cleaning Docker images"},{"location":"learning/docker/docker-notes/#docker-networks-concepts-for-private-and-public-comms-in-containers","text":"Each container connected to a private virtual network \"bridge\" Each virtual network routes through NAT firewall on host IP All containers on a virtual network can talk to each other without -p Best practice is to create a new virtual network for each app: network \"my_web_app\" for mysql and php/apache containers network \"my_api\" for mongo and nodejs containers docker container run -p 80 :80 --name webhost -d nginx docker container port webhost docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost","title":"Docker Networks: Concepts for Private and Public Comms in Containers"},{"location":"learning/docker/docker-notes/#docker-networks-cli-management-of-virtual-networks","text":"Show networks docker network ls Inspect a network docker network inspect Create a network docker network create --driver Attach a network to container docker network connect Detach a network from container docker network disconnect docker network ls docker network inspect bridge docker network create my_app_net docker container run -d --name new_nginx --network my_app_net nginx docker network inspect my_app_net docker network connect <new network id> <container id> docker container disconnect <new network id> <container id>","title":"Docker Networks: CLI Management of Virtual Networks"},{"location":"learning/docker/docker-notes/#docker-networks-dns-and-how-containers-find-each-other","text":"Create your apps so frontend/backend sit on same Docker network Their inter-communication never leaves host All externally exposed ports closed by default You must manually expose via -p , which is better default security! Containers shouldn't rely on IP's for inter-communication DNS for friendly names is built-in if you use custom networks docker container run -d --name my_nginx --network my_app_net nginx docker container exec -it my_nginx ping new_nginx docker container exec -it new_nginx ping my_nginx","title":"Docker Networks: DNS and How Containers Find Each Other"},{"location":"learning/docker/docker-notes/#dns-round-robin-testing","text":"docker network create dude docker container run -d --net dude --net-alias search elasticsearch:2 docker container ls docker container run --rm -- net dude alpine nslookup search docker container run --rm --net dude centos curl -s search:9200","title":"DNS Round Robin Testing"},{"location":"learning/docker/docker-notes/#container-lifetime--persistent-data-volumes-volumes-volumes","text":"","title":"Container Lifetime &amp; Persistent Data: Volumes, Volumes, Volumes"},{"location":"learning/docker/docker-notes/#persistent-data-data-volumes","text":"Containers are usually immutable and ephemeral \"immutable infrastructure\": only re-deploy containers, never change This is the ideal scenario, but what about databases, or unique data? Docker gives us features to ensure these \"separation of concerns\" This is known as \"persistent data\" Two ways: Volumes and Bind Mounts Volumes : make special location outside of container UFS Bind Mounts : link container path to host path docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD = True -v mysql-db:/var/lib/mysql mysql docker volume ls docker volume inspect mysql-db","title":"Persistent Data: Data Volumes"},{"location":"learning/docker/docker-notes/#persistent-data-bind-mounting","text":"Used for local development Usecase: When you are changing files on laptop which you want to serve in the app It can be run only during docker run as there is no explicit volume command in the dockerfile the volume will be mounted in the working directory of the container docker container run -d --name nginx -p 80 :80 -v $( pwd ) :/usr/share/nginx/html nginx docker container exec -it nginx -- bash cd /usr/share/nginx/html && ls -la docker log streaming docker container logs -f <container name>","title":"Persistent Data: Bind Mounting"},{"location":"learning/docker/docker-notes/#database-passwords-in-containers","text":"When running postgres now, you'll need to either set a password, or tell it to allow any connection (which was the default before this change). -you need to either set a password with the environment variable: POSTGRES_PASSWORD=mypasswd Or tell it to ignore passwords with the environment variable: POSTGRES_HOST_AUTH_METHOD=trust","title":"Database Passwords in Containers"},{"location":"learning/docker/docker-notes/#making-it-easier-with-docker-compose-the-multi-container-tool","text":"Why: configure relationships between containers Why: save our docker container run settings in easy-to-read file Why: create one-liner developer environment startups YAML-formatted file that describes our solution options for: containers, networks, volumes A CLI tool docker-compose used for local dev/test automation with those YAML files docker-compose.yml is default filename, but any can be used with docker-compose -f Not a production-grade tool but ideal for local development and test Two most common commands are: docker-compose up # setup volumes/networks and start all containers docker-compose down # stop all containers and remove cont/vol/net","title":"Making It Easier with Docker Compose: The Multi-Container Tool"},{"location":"learning/docker/docker-notes/#trying-out-basic-compose-commands","text":"docker-compose up docker-compose up -d # Running compose in bacground docker-compose down docker-compose down -v --rmi local/all # Removes images and volumes # Compose operations docker-compose logs docker-compose ps docker-compose top docker-compose build # Build images or docker-compose up --build","title":"Trying Out Basic Compose Commands"},{"location":"learning/docker/docker-notes/#swarm-intro-and-creating-a-3-node-swarm-cluster","text":"Swarm Mode is a clustering solution built inside Docker Not enabled by default","title":"Swarm Intro and Creating a 3-Node Swarm Cluster"},{"location":"learning/docker/docker-notes/#docker-swarm-init-what-just-happened","text":"Lots of PKI and security automation Root Signing Certificate created for our Swarm Certificate is issued for first Manager node Join tokens are created Raft database created to store root CA, configs and secrets Encrypted by default on disk (1.13+) No need for another key/value system to hold orchestration/secrets Replicates logs amongst Managers via mutual TLS in \"control plane\"","title":"docker swarm init: What Just Happened?"},{"location":"learning/docker/docker-notes/#create-your-first-service-and-scale-it-locally","text":"docker info # swarm is down by default docker swarm init # start swarm docker node ls docker service create alpine ping 8 .8.8.8 # creates service frosty_newton docker service ls docker service ps frosty_newton docker container ls docker service update frosty_newton --replicas 3 # creates 3 replicas docker service ls docker service rm frosty_newton # deletes the service docker service ls docker container ls","title":"Create Your First Service and Scale it Locally"},{"location":"learning/docker/docker-notes/#creating-a-3-node-swarm-cluster","text":"docker-machine + VirtualBox - Free and runs locally, but requires a machine with 8GB memory docker-machine create node1 docker-machine ssh node1 docker-machine env node1 docker swarm init docker swarm init --advertise-addr node1 docker node ls docker node update --role manager node2 # Update role to existing node docker swarm join-token manager # Shows join token for manager role docker service create --replicas 3 alpine ping 8 .8.8.8 # Creates service with 3 replicas and starts ping process docker service ls docker service ps <service name> docker node ps docker node ps node2","title":"Creating a 3-Node Swarm Cluster"},{"location":"learning/docker/docker-notes/#scaling-out-with-overlay-networking","text":"# Create Backend network docker network create --driver overlay mydrupal docker network ls docker service create --name psql --netowrk mydrupal -e POSTGRES_PASSWORD = mypass postgres docker service ls docker service ps psql docker container logs psql <container name> # Create Frontend network docker service create --name drupal --network mydrupal -p 80 :80 drupal docker service inspect drupal","title":"Scaling Out with Overlay Networking"},{"location":"learning/docker/docker-notes/#scaling-out-with-routing-mesh","text":"docker service create --name search --replicas 3 -p 9200 :9200 elasticsearch:2 docker service ps search","title":"Scaling Out with Routing Mesh"},{"location":"learning/docker/docker-notes/#create-a-multi-service-multi-node-web-app","text":"docker network create -d overlay backend docker network create -d overlay frontend docker service create --name vote -p 80 :80 --network frontend \\ -- replica 2 dockersamples/examplevotingapp_vote:before docker service create --name redis --network frontend \\ --replica 1 redis:3.2 docker service create --name worker --network frontend --network backend dockersamples/examplevotingapp_worker docker service create --name db --network backend \\ --mount type = volume,source = db-data,target = /var/lib/postgresql/data postgres:9.4 docker service create --name result --network backend -p 5001 :80 COPY INFO docker service ls docker service logs worker","title":"Create a Multi-Service Multi-Node Web App"},{"location":"learning/docker/docker-notes/#swarm-stacks-and-production-grade-compose","text":"Docker adds a new layer of abstraction to Swarm called Stacks Stacks accept Compose files as their declarative definition for services, networks, and volumes We use docker stack deploy rather then docker service create Stacks manages all those objects for us, including overlay network per stack. Adds stack name to start of their name Compose now ignores deploy: , Swarm ignores build: docker stack deploy -c example-voting-app-stack.yml voteapp docker stack ls docker stack services voteapp docker stack ps voteapp","title":"Swarm Stacks and Production Grade Compose"},{"location":"learning/docker/docker-notes/#using-secrets-in-swarm-services","text":"What is a Secret? - Usernames and passwords - TLS certificates and keys - SSH keys - Any data you would prefer not be \"on front page of news\" docker secret create psql_usr psql_usr.txt echo \"myDBpassWORD\" | docker secret create psql_pass - TAB COMPLETION docker secret inspect psql_usr docker service create --name psql --secret psql_user \\ --secret psql_pass -e POSTGRES_PASSWORD_FILE = /run/secrets/psql_pass \\ -e POSTGRES_USER_FILE = /run/secrets/psql_user postgres docker exec -it <container name> bash cat /run/secrets/psql_user","title":"Using Secrets in Swarm Services"},{"location":"learning/docker/docker-notes/#swarm-app-lifecycle","text":"","title":"Swarm App Lifecycle"},{"location":"learning/docker/docker-notes/#full-app-lifecycle-dev-build-and-deploy-with-a-single-compose-design","text":"Single set of Compose files for: - Local docker-compose up development environment - Remote docker-compose up CI environment - Remote docker stack deploy production environment docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d docker-compose -f docker-compose.yml -f docker-compose.prod.yml config","title":"Full App Lifecycle: Dev, Build and Deploy With a Single Compose Design"},{"location":"learning/docker/docker-notes/#service-updates-changing-things-in-flight","text":"Provides rolling replacement of tasks/containers in a service Limits downtime (be careful with \"prevents\" downtime) Will replace containers for most changes Has many, many cli options to control the update Create options will usually change, adding -add or -rm to them Includes rollback and healthcheck options Also has scale & rollback subcommand for quicker access docker service scale web=4 and docker service rollback web Just update the image used to a newer version docker service update --image myapp:1.2.1 <servicename> Adding an environment variable and remove a port docker service update --env-add NODE_ENV=production --publish-rm 8080 Change number of replicas of two services docker service scale web=8 api=6 docker service create -p 8088 :80 --name web nginx:1.13.7 docker service scale web = 5 docker service update --image nginx:1.13.6 web docker service update --publish-rm 8088 --publish-add 9090 :80 docker service update --force web # forces rebalancing of the service without changing anything docker service rm web","title":"Service Updates: Changing Things In Flight"},{"location":"learning/docker/docker-notes/#healthchecks-in-dockerfiles","text":"HEALTHCHECK was added in 1.12 Supported in Dockerfile, Compose YAML, docker run, and Swarm Services Docker engine will exec's the command in the container (e.g. curl localhost) It expects exit 0 (OK) or exit 1 (Error) Three container states: starting, healthy, unhealthy Much better then \"is binary still running?\" Options for healthcheck command --interval = DURATION ( default: 30s ) --timeout = DURATION ( default: 30s ) --start-period = DURATION ( default: 0s ) ( 17 .09+ ) --retries = N ( default: 3 ) docker container run --name p2 -d --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres docker service create --name p2 --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres","title":"Healthchecks in Dockerfiles"},{"location":"learning/docker/docker-notes/#container-registries-image-storage-and-distribution","text":"","title":"Container Registries: Image Storage and Distribution"},{"location":"learning/docker/docker-notes/#run-a-private-docker-registry","text":"Secure your Registry with TLS Storage cleanup via Garbage Collection Enable Hub caching via \"\u2013registry-mirror\" # Run the registry image docker container run -d -p 5000 :5000 --name registry registry # Re-tag an existing image and push it to your new registry docker pull hello-world docker run hello-world docker tag hello-world 127 .0.0.1:5000/hello-world docker push 127 .0.0.1:5000/hello-world # Remove that image from local cache and pull it from new registry docker image remove hello-world docker image remove 127 .0.0.1:5000/hello-world docker pull 127 .0.0.1:5000/hello-world:latest # Re-create registry using a bind mount and see how it stores data docker container kill registry docker container rm registry docker container run -d -p 5000 :5000 --name registry -v $( pwd ) /registry-data:/var/lib/registry registry","title":"Run a Private Docker Registry"},{"location":"learning/docker/docker-notes/#using-docker-registry-with-swarm","text":"docker node ls docker service create --name registry --publish 5000 :5000 registry docker service ps registry docker pull nginx docker tag nginx 127 .0.0.1:5000/nginx docker push 127 .0.0.1:5000/nginx docker service create --name nginx -p 80 :80 --replicas 5 --detach = false 127 .0.0.1:5000/nginx docker service ps nginx","title":"Using Docker Registry With Swarm"},{"location":"learning/docker/docker-notes/#using-docker-in-production","text":"Focus on Dockerfiles first. Study ENTRYPOINT of Hub official images. Use it for config of images before CMD is executed. use ENTRYPOINT to set default values for all environments and then overide using ENV values. EntryPoint vs CMD FROM official distros. Make it == start, log all things in stdout/stderr, documented in file, lean and scale. Using SaaS for - Image Registry, Logging, Monitoring, Look at CNCF Landscape Using Layer 7 Reverse Proxy if port 80 and 443 are used by multiple apps","title":"Using Docker in Production"},{"location":"learning/docker/docker-notes/#docker-security","text":"Docker Security Checklist Docker Engine Security Docker Security Tools Seccomp App Armor Docker Bench CIS Docker checklist Running Docker as non root user # Creating non root user in alpine RUN addgroup -g 1000 node \\ && adduser -u 1000 -G node -s /bin/sh -D node # Creating non root user in stretch RUN groupadd --gid 1000 node \\ && useradd --uid 1000 --gid node --shell /bin/bash --create-home node Sample Dockerfile with USER User Namespaces Shift Left Security Trivy - Image Scanning Sysdig Falco Appamror Profiles Seccomp Profile","title":"Docker Security"},{"location":"learning/docker/docker-notes/#docker-context","text":"Start a node on play with Docker Copy the IP of the node Set the Docker Context with the Host Name of the node and port 2375 Contexts are created in the home folder of user called .docker/context docker context create --docker \"host=tcp://<Host Name>:2375\" <context-name> docker context ls docker context use <context-name> docker ps # Should show the new context of play with docker # Overriding Context to default in commandline docker -c default ps docker -c <context-name> ps # Looping through all the context and executing ps for c in ` docker context ls -q ` ; do ` docker -c $c ps ` ; done # Creates the image in all context for c in ` docker context ls -q ` ; do ` docker -c $c run hello-world ` ; done","title":"Docker Context"},{"location":"learning/docker/docker-notes/#recommendations","text":"To change permissions on file system (chown or chmod) use a Entrypoint script. Look up to official images for examples for Entrypoint One App or Website use one container, specially if using an orchestrator like K8s or Docker Swarm. Scaling is also a benefit due to one-one relationship. Changing Docker IP range Use Cloud DB as service instead of in containers Run one process per container Strict Separation of Config from Code. Use Env variables to achieve this. Using Development workflow in Compose Write all the ENV variables at the top of Dockerfile Using Env variables in Dockerfile Override Env variables in Docker Compose file say for Dev testing Using Env variables in Docker Entrypoint to write into Application config files during start up. Secrets and Application specific config goes into specific ENV var blocks. This can be changed. Defaults or data specific to SERVER or LANGUAGE goes to another ENV block and can be kept static. This avoids them being set for each ENV. Encrypting traffic for local development use Lets Encrypt ad store them in .cert folder in Home Directory. Encrypting traffic for production use Lets Encrypt and maybe Traefik as Front proxy. See example using Swarm COPY vs ADD. Use COPY to copy artefacts in the same repo to the image. Use ADD when you want to download something from the Internet or to untar or unzip. You can also replace using wget statements with ADD. Combine multiple RUN into a single statement. Delete packages which are downloaded and installed also in a single command to save image size. No secrets like configs, certificates should be saved in Image. Pass them during runtime. Always have a CMD in the image, even if its inheriting it from BASE image Version apt packages and BASE images Use multistage Dokcer builds to have Dev dependencies and Prod dependencies separate. Have healthchecks in K8s instead of Dockerfile Use DNS RoundRobin for Database inside Compose file so it switches of Virtual IP on the Overlay network and gives direct access from FrontEnd Service to Backend container. Setting resource limits inside Compose file DRY your compose files using templates","title":"Recommendations"},{"location":"learning/k8s/helm-templating/","text":"Go Template Dry run values of a template helm template RELEASE .<Chart path> # Will render the template along with the default values Template Macros : Used for reusing code in the helm templates and is written in _helpers.tpl","title":"Helm templating"},{"location":"learning/k8s/k8s-notes/","text":"Tailing logs from multiple containers on laptop K8s Tutorials K8s DNS Kubectl Usage Convention K8s API Reference Operator Hub Awesome Operator List Creating EKS using Terraform Kubernetes Prompt Why Kubernetes \u00b6 Orchestration: Next logical step in journey to faster DevOps First, understand why you may need orchestration Not every solution needs orchestration Servers + Change Rate = Benefit of orchestration K8s Learning Resources \u00b6 Play with k8s Katacoda Install Kubernetes \u00b6 Linux - Microk8s Install SNAP first using apt-get sudo snap install microk8s --classic --channel = 1 .17/stable # Install specific k8s version microk8s.enable dns # Enbale DNS microk8s.status # Check status Windows - Minikube minikube start --kubernetes-version = '1.17.4' # Install specific k8s version minikube ip # IP of the machine minikube status # Check status minikube stop # Stop minkube service Kubernetes Container Abstractions \u00b6 Pod: one or more containers running together on one Node. Basic unit of deployment. Containers are always in pods Controller: For creating/updating pods and other objects. Many types of Controllers inc. Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob, etc. Service: network endpoint to connect to a pod Namespace: Filtered group of objects in cluster - Secrets, ConfigMaps, and more. Our First Pod With Kubectl run \u00b6 Two ways to deploy Pods (containers): Via commands, or via YAML Object hieracrhy - Pods -> ReplicaSet -> Deployment kubectl run my-nginx --image nginx # Creates a single pod kubectl run nginx-pod --generator = run-pod/v1 -- image nginx # Another way to create pod kubectl get pods # list the pod kubectl create deployment nginx --image nginx # Creates a deployment kubectl deployment deployment nginx # Deletes a deployment kubectl create deployment nginx --image nginx --dry-run --port 80 -- expose # Using Dry run option Scaling ReplicaSets \u00b6 kubectl create deployment my-apache --image httpd kubectl scale deploy/my-apache --replicas 2 # Scale up by 2 kubectl scale deployment my-apache --replicas 2 # Scale up by 2 kubectl get all Inspecting Kubernetes Objects \u00b6 kubectl get deploy,pods # Get multiple resources in one line kubectl get pods -o wide # Get all pods, in wide format ( gives more info ) kubectl get pods --show-labels # Get all pods and show labels kubectl logs deployment/my-apache kubectl logs deployment/my-apache --follow --tail 1 # Show the last line kubectl logs -l run = my-apache # Show logs using label kubectl describe pod/my-apache-<pod id> # Shows the pod configuration including events kubectl get pods -w # Watches the pods in real time kubectl delete pod/my-apache-<pod id> # Deletes a single instance Exposing Kubernetes Ports \u00b6 A service is a stable address for pod(s) If we want to connect to pod(s), we need a service CoreDNS allows us to resolve services by name There are different types of services ClusterIP NodePort LoadBalancer ExternalName ClusterIP and NodePort services are always available in Kubernetes kubectl expose creates a service for existing pods Basic Service Types \u00b6 ClusterIP (default) Single, internal virtual IP allocated Only reachable from within cluster (nodes and pods) Pods can reach service on apps port number NodePort High port allocated on each node Port is open on every node\u2019s IP Anyone can connect (if they can reach node) Other pods need to be updated to this port LoadBalancer Controls a LB endpoint external to the cluster Only available when infra provider gives you a LB (AWS ELB, etc) Creates NodePort+ClusterIP services, tells LB to send to NodePort ExternalName Adds CNAME DNS record to CoreDNS only Not used for Pods, but for giving pods a DNS name to use for something outside Kubernetes # To show how to reach a ClusterIP deployment which is only accessible from the cluster in a Laptop kubectl create deployment httpenv --image = bretfisher/httpenv # simple http server kubectl scale deployment/httpenv --replicas = 5 kubectl expose deployment/httpenv --port 8888 # Create a ClusterIP service ( default ) kubectl get service # Shows services # Uses Generator option and launches the pod and gives BASH terminal kubectl run --generator run-pod/v1 tmp-shell --rm -it --image bretfisher/netshoot -- bash # Launch another pod to run curl curl httpenv:8888 curl [ ip of service ] :8888 # Creating a NodePort and LoadBalancer Service \u00b6 Nodeport Port Range: 30000 to 32767 Did you know that a NodePort service also creates a ClusterIP? These three service types are additive, each one creates the ones above it: ClusterIP NodePort LoadBalancer If you're on Docker Desktop, it provides a built-in LoadBalancer that publishes the \u2013port on localhost If you're on kubeadm, minikube, or microk8s No built-in LB You can still run the command, it'll just stay at LoadBalancer recieves the packet on 8888, then transfers it to the Nodeport of the Node and then to the ClusterIP of the service. \"pending\" (but its NodePort works) kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort kubectl get services curl localhost:<Node Port> # Get this from svc output kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer kubectl get services curl localhost:8888 # Pod Port kubectl delete service/httpenv service/httpenv-np kubectl delete service/httpenv-lb deployment/httpenv Kubernetes Services DNS \u00b6 Internal DNS is provided by CoreDNS Services also have a FQDN curl <hostname>.<namespace>.svc.cluster.local curl <hostname> kubectl get namespaces curl <hostname>.<namespace>.svc.cluster.local Kubernetes Management Techniques \u00b6 Run, Expose and Create Generators \u00b6 These commands use helper templates called \"generators\" Every resource in Kubernetes has a specification or \"spec\" You can output those templates with \u2013dry-run -o yaml kubectl create deployment sample \u2013image nginx \u2013dry-run -o yaml You can use those YAML defaults as a starting point Generators are \"opinionated defaults\" Generator Examples \u00b6 \u2022 Using dry-run with yaml output we can see the generators kubectl create deployment test \u2013image nginx \u2013dry-run -o yaml kubectl create job test \u2013image nginx \u2013dry-run -o yaml kubectl expose deployment/test \u2013port 80 \u2013dry-run -o yaml - You need the deployment to exist before this works Imperative vs. Declarative \u00b6 Imperative : Focus on how a program operates Declarative : Focus on what a program should accomplish - Example: \"I'd like a cup of coffee\" Imperative : I boil water, scoop out 42 grams of medium-fine grounds, pour over 700 grams of water, etc. Declarative : \"Barista, I'd like a a cup of coffee\". (Barista is the engine that works through the steps, including retrying to make a cup, and is only finished when I have a cup) Kubernetes Imperative \u00b6 Examples: kubectl run, kubectl create deployment, kubectl update We start with a state we know (no deployment exists) We ask kubectl run to create a deployment Different commands are required to change that deployment Different commands are required per object Imperative is easier when you know the state Imperative is easier to get started Imperative is easier for humans at the CLI Imperative is NOT easy to automate Kubernetes Declarative \u00b6 Example: kubectl apply -f my-resources.yaml We don't know the current state We only know what we want the end result to be (yaml contents) Same command each time (tiny exception for delete) Resources can be all in a file, or many files (apply a whole dir) Requires understanding the YAML keys and values More work than kubectl run for just starting a pod The easiest way to automate The eventual path to GitOps happiness Three Management Approaches \u00b6 Imperative commands: run, expose, scale, edit, create deployment Best for dev/learning/personal projects Easy to learn, hardest to manage over time Imperative Commands Imperative objects: create -f file.yml, replace -f file.yml, delete\u2026 Good for prod of small environments, single file per command Store your changes in git-based yaml files Hard to automate Imperative Config File Declarative objects: apply -f file.yml or dir, diff Best for prod, easier to automate Harder to understand and predict changes Declarative Config File Recommendations \u00b6 Most Important Rule : Don't mix the three approaches Recommendations: Learn the Imperative CLI for easy control of local and test setups Move to apply -f file.yml and apply -f directory for prod Store yaml in git, git commit each change before you apply This trains you for later doing GitOps (where git commits are automatically applied to clusters) Moving to Declarative Kubernetes YAML \u00b6 Using kubectl apply \u00b6 create/update resources in a file kubectl apply -f myfile.yaml create/update a whole directory of yaml kubectl apply -f myyaml/ create/update from a URL kubectl apply -f https://bret.run/pod.yml Be careful, lets look at it first (browser or curl) # Using Shell curl -L https://bret.run/pod # Using Windows CMD Win PoSH? start https://bret.run/pod.yml Kubernetes Configuration YAML \u00b6 Kubernetes configuration file (YAML or JSON) Each file contains one or more manifests Each manifest describes an API object (deployment, job, secret) Each manifest needs four parts (root key:values in the file) apiVersion: kind: metadata: spec: Building Your YAML Files \u00b6 kind : We can get a list of resources the cluster supports kubectl api-resources Notice some resources have multiple API's (old vs. new) apiVersion : We can get the API versions the cluster supports kubectl api-versions metadata : only name is required spec : Where all the action is at! Building Your YAML spec - explain Command \u00b6 We can get all the keys each kind supports kubectl explain services \u2013recursive Oh boy! Let's slow down kubectl explain services.spec We can walk through the spec this way kubectl explain services.spec.type spec: can have sub spec: of other resources kubectl explain deployment.spec.template.spec.volumes.nfs.server Use kubectl api-versions or kubectl api-resources along with kubectl explain as documentation on explain could be old We can also use docs kubernetes.io/docs/reference/#api-reference Dry Runs With Apply YAML \u00b6 dry-run a create (client side only) kubectl apply -f app.yml \u2013dry-run dry-run a create/update on server kubectl apply -f app.yml \u2013server-dry-run see a diff visually kubectl diff -f app.yml Difference between dry-run and diff Labels and Label Selectors \u00b6 Labels goes under metadata: in your YAML Simple list of key: value for identifying your resource later by selecting, grouping, or filtering for it Common examples include tier: frontend, app: api, env: prod, customer: acme.co Not meant to hold complex, large, or non- identifying info, which is what annotations are for filter a get command kubectl get pods -l app=nginx apply only matching labels kubectl apply -f myfile.yaml -l app=nginx Label Recommendation Label Selectors (Use case for Labels) \u00b6 The \"glue\" telling Services and Deployments which pods are theirs Many resources use Label Selectors to \"link\" resource dependencies You'll see these match up in the Service and Deployment YAML Using Label selectors Use Labels and Selectors to control which pods go to which nodes Assigning Pods to Nodes Taints and Tolerations also control node placement Taints and Tolerations Your Next Steps, and The Future of Kubernetes \u00b6 Storage in Kubernetes \u00b6 Storage and stateful workloads are harder in all systems Containers make it both harder and easier than before StatefulSets is a new resource type, making Pods more sticky Recommendation : avoid stateful workloads for first few deployments until you're good at the basics Use db-as-a-service whenever you can Volumes in Kubernetes \u00b6 Creating and connecting Volumes: 2 types Volumes Tied to lifecycle of a Pod All containers in a single Pod can share them PersistentVolumes Created at the cluster level, outlives a Pod Separates storage config from Pod using it Multiple Pods can share them CSI plugins are the new way to connect to storage Ingress \u00b6 None of our Service types work at OSI Layer 7 (HTTP) How do we route outside connections based on hostname or URL? Example Usecase: app1.com and app2.com are 2 different deployments in the cluster and both listen on port 443. You will need Ingress to understand the DNS and route traffic to those apps Ingress Controllers (optional) do this with 3 rd party proxies Nginx is popular, but Traefik, HAProxy, F5, Envoy, Istio, etc. Recommendation: Check out Traefik Implementation is specific to Controller chosen Why Controller - To configure LB which is outside the cluster CRD's and The Operator Pattern \u00b6 You can add 3 rd party Resources and Controllers This extends Kubernetes API and CLI A pattern is starting to emerge of using these together Operator : automate deployment and management of complex apps e.g. Databases, monitoring tools, backups, and custom ingresses Higher Deployment Abstractions \u00b6 All our kubectl commands just talk to the Kubernetes API Kubernetes has limited built-in templating, versioning, tracking, and management of your apps Helm is the most popular Compose on Kubernetes comes with Docker Desktop Remember these are optional, and your distro may have a preference Most distros support Helm Templating YAML \u00b6 Many of the deployment tools have templating options You'll need a solution as the number of environments/apps grow Helm was the first \"winner\" in this space, but can be complex Official Kustomize feature works out-of-the-box (as of 1.14) docker app and compose-on-kubernetes are Docker's way Kubernetes Dashboard \u00b6 Default GUI for \"upstream\" Kubernetes Clouds don't have it by default Let's you view resources and upload YAML Safety first! Namespaces and Context \u00b6 Namespaces limit scope, aka \"virtual clusters\" Not related to Docker/Linux namespaces Won't need them in small clusters There are some built-in, to hide system stuff from kubectl \"users\" kubectl get namespaces kubectl get all --all-namespaces - Context changes kubectl cluster and namespace - See ~/.kube/config file kubectl config get-contexts # Selectively show output of Kube config kubectl config get-contexts -o name kubectl config set*","title":"Kubernetes"},{"location":"learning/k8s/k8s-notes/#why-kubernetes","text":"Orchestration: Next logical step in journey to faster DevOps First, understand why you may need orchestration Not every solution needs orchestration Servers + Change Rate = Benefit of orchestration","title":"Why Kubernetes"},{"location":"learning/k8s/k8s-notes/#k8s-learning-resources","text":"Play with k8s Katacoda","title":"K8s Learning Resources"},{"location":"learning/k8s/k8s-notes/#install-kubernetes","text":"Linux - Microk8s Install SNAP first using apt-get sudo snap install microk8s --classic --channel = 1 .17/stable # Install specific k8s version microk8s.enable dns # Enbale DNS microk8s.status # Check status Windows - Minikube minikube start --kubernetes-version = '1.17.4' # Install specific k8s version minikube ip # IP of the machine minikube status # Check status minikube stop # Stop minkube service","title":"Install Kubernetes"},{"location":"learning/k8s/k8s-notes/#kubernetes-container-abstractions","text":"Pod: one or more containers running together on one Node. Basic unit of deployment. Containers are always in pods Controller: For creating/updating pods and other objects. Many types of Controllers inc. Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob, etc. Service: network endpoint to connect to a pod Namespace: Filtered group of objects in cluster - Secrets, ConfigMaps, and more.","title":"Kubernetes Container Abstractions"},{"location":"learning/k8s/k8s-notes/#our-first-pod-with-kubectl-run","text":"Two ways to deploy Pods (containers): Via commands, or via YAML Object hieracrhy - Pods -> ReplicaSet -> Deployment kubectl run my-nginx --image nginx # Creates a single pod kubectl run nginx-pod --generator = run-pod/v1 -- image nginx # Another way to create pod kubectl get pods # list the pod kubectl create deployment nginx --image nginx # Creates a deployment kubectl deployment deployment nginx # Deletes a deployment kubectl create deployment nginx --image nginx --dry-run --port 80 -- expose # Using Dry run option","title":"Our First Pod With Kubectl run"},{"location":"learning/k8s/k8s-notes/#scaling-replicasets","text":"kubectl create deployment my-apache --image httpd kubectl scale deploy/my-apache --replicas 2 # Scale up by 2 kubectl scale deployment my-apache --replicas 2 # Scale up by 2 kubectl get all","title":"Scaling ReplicaSets"},{"location":"learning/k8s/k8s-notes/#inspecting-kubernetes-objects","text":"kubectl get deploy,pods # Get multiple resources in one line kubectl get pods -o wide # Get all pods, in wide format ( gives more info ) kubectl get pods --show-labels # Get all pods and show labels kubectl logs deployment/my-apache kubectl logs deployment/my-apache --follow --tail 1 # Show the last line kubectl logs -l run = my-apache # Show logs using label kubectl describe pod/my-apache-<pod id> # Shows the pod configuration including events kubectl get pods -w # Watches the pods in real time kubectl delete pod/my-apache-<pod id> # Deletes a single instance","title":"Inspecting Kubernetes Objects"},{"location":"learning/k8s/k8s-notes/#exposing-kubernetes-ports","text":"A service is a stable address for pod(s) If we want to connect to pod(s), we need a service CoreDNS allows us to resolve services by name There are different types of services ClusterIP NodePort LoadBalancer ExternalName ClusterIP and NodePort services are always available in Kubernetes kubectl expose creates a service for existing pods","title":"Exposing Kubernetes Ports"},{"location":"learning/k8s/k8s-notes/#basic-service-types","text":"ClusterIP (default) Single, internal virtual IP allocated Only reachable from within cluster (nodes and pods) Pods can reach service on apps port number NodePort High port allocated on each node Port is open on every node\u2019s IP Anyone can connect (if they can reach node) Other pods need to be updated to this port LoadBalancer Controls a LB endpoint external to the cluster Only available when infra provider gives you a LB (AWS ELB, etc) Creates NodePort+ClusterIP services, tells LB to send to NodePort ExternalName Adds CNAME DNS record to CoreDNS only Not used for Pods, but for giving pods a DNS name to use for something outside Kubernetes # To show how to reach a ClusterIP deployment which is only accessible from the cluster in a Laptop kubectl create deployment httpenv --image = bretfisher/httpenv # simple http server kubectl scale deployment/httpenv --replicas = 5 kubectl expose deployment/httpenv --port 8888 # Create a ClusterIP service ( default ) kubectl get service # Shows services # Uses Generator option and launches the pod and gives BASH terminal kubectl run --generator run-pod/v1 tmp-shell --rm -it --image bretfisher/netshoot -- bash # Launch another pod to run curl curl httpenv:8888 curl [ ip of service ] :8888 #","title":"Basic Service Types"},{"location":"learning/k8s/k8s-notes/#creating-a-nodeport-and-loadbalancer-service","text":"Nodeport Port Range: 30000 to 32767 Did you know that a NodePort service also creates a ClusterIP? These three service types are additive, each one creates the ones above it: ClusterIP NodePort LoadBalancer If you're on Docker Desktop, it provides a built-in LoadBalancer that publishes the \u2013port on localhost If you're on kubeadm, minikube, or microk8s No built-in LB You can still run the command, it'll just stay at LoadBalancer recieves the packet on 8888, then transfers it to the Nodeport of the Node and then to the ClusterIP of the service. \"pending\" (but its NodePort works) kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort kubectl get services curl localhost:<Node Port> # Get this from svc output kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer kubectl get services curl localhost:8888 # Pod Port kubectl delete service/httpenv service/httpenv-np kubectl delete service/httpenv-lb deployment/httpenv","title":"Creating a NodePort and LoadBalancer Service"},{"location":"learning/k8s/k8s-notes/#kubernetes-services-dns","text":"Internal DNS is provided by CoreDNS Services also have a FQDN curl <hostname>.<namespace>.svc.cluster.local curl <hostname> kubectl get namespaces curl <hostname>.<namespace>.svc.cluster.local","title":"Kubernetes Services DNS"},{"location":"learning/k8s/k8s-notes/#kubernetes-management-techniques","text":"","title":"Kubernetes Management Techniques"},{"location":"learning/k8s/k8s-notes/#run-expose-and-create-generators","text":"These commands use helper templates called \"generators\" Every resource in Kubernetes has a specification or \"spec\" You can output those templates with \u2013dry-run -o yaml kubectl create deployment sample \u2013image nginx \u2013dry-run -o yaml You can use those YAML defaults as a starting point Generators are \"opinionated defaults\"","title":"Run, Expose and Create Generators"},{"location":"learning/k8s/k8s-notes/#generator-examples","text":"\u2022 Using dry-run with yaml output we can see the generators kubectl create deployment test \u2013image nginx \u2013dry-run -o yaml kubectl create job test \u2013image nginx \u2013dry-run -o yaml kubectl expose deployment/test \u2013port 80 \u2013dry-run -o yaml - You need the deployment to exist before this works","title":"Generator Examples"},{"location":"learning/k8s/k8s-notes/#imperative-vs-declarative","text":"Imperative : Focus on how a program operates Declarative : Focus on what a program should accomplish - Example: \"I'd like a cup of coffee\" Imperative : I boil water, scoop out 42 grams of medium-fine grounds, pour over 700 grams of water, etc. Declarative : \"Barista, I'd like a a cup of coffee\". (Barista is the engine that works through the steps, including retrying to make a cup, and is only finished when I have a cup)","title":"Imperative vs. Declarative"},{"location":"learning/k8s/k8s-notes/#kubernetes-imperative","text":"Examples: kubectl run, kubectl create deployment, kubectl update We start with a state we know (no deployment exists) We ask kubectl run to create a deployment Different commands are required to change that deployment Different commands are required per object Imperative is easier when you know the state Imperative is easier to get started Imperative is easier for humans at the CLI Imperative is NOT easy to automate","title":"Kubernetes Imperative"},{"location":"learning/k8s/k8s-notes/#kubernetes-declarative","text":"Example: kubectl apply -f my-resources.yaml We don't know the current state We only know what we want the end result to be (yaml contents) Same command each time (tiny exception for delete) Resources can be all in a file, or many files (apply a whole dir) Requires understanding the YAML keys and values More work than kubectl run for just starting a pod The easiest way to automate The eventual path to GitOps happiness","title":"Kubernetes Declarative"},{"location":"learning/k8s/k8s-notes/#three-management-approaches","text":"Imperative commands: run, expose, scale, edit, create deployment Best for dev/learning/personal projects Easy to learn, hardest to manage over time Imperative Commands Imperative objects: create -f file.yml, replace -f file.yml, delete\u2026 Good for prod of small environments, single file per command Store your changes in git-based yaml files Hard to automate Imperative Config File Declarative objects: apply -f file.yml or dir, diff Best for prod, easier to automate Harder to understand and predict changes Declarative Config File","title":"Three Management Approaches"},{"location":"learning/k8s/k8s-notes/#recommendations","text":"Most Important Rule : Don't mix the three approaches Recommendations: Learn the Imperative CLI for easy control of local and test setups Move to apply -f file.yml and apply -f directory for prod Store yaml in git, git commit each change before you apply This trains you for later doing GitOps (where git commits are automatically applied to clusters)","title":"Recommendations"},{"location":"learning/k8s/k8s-notes/#moving-to-declarative-kubernetes-yaml","text":"","title":"Moving to Declarative Kubernetes YAML"},{"location":"learning/k8s/k8s-notes/#using-kubectl-apply","text":"create/update resources in a file kubectl apply -f myfile.yaml create/update a whole directory of yaml kubectl apply -f myyaml/ create/update from a URL kubectl apply -f https://bret.run/pod.yml Be careful, lets look at it first (browser or curl) # Using Shell curl -L https://bret.run/pod # Using Windows CMD Win PoSH? start https://bret.run/pod.yml","title":"Using kubectl apply"},{"location":"learning/k8s/k8s-notes/#kubernetes-configuration-yaml","text":"Kubernetes configuration file (YAML or JSON) Each file contains one or more manifests Each manifest describes an API object (deployment, job, secret) Each manifest needs four parts (root key:values in the file) apiVersion: kind: metadata: spec:","title":"Kubernetes Configuration YAML"},{"location":"learning/k8s/k8s-notes/#building-your-yaml-files","text":"kind : We can get a list of resources the cluster supports kubectl api-resources Notice some resources have multiple API's (old vs. new) apiVersion : We can get the API versions the cluster supports kubectl api-versions metadata : only name is required spec : Where all the action is at!","title":"Building Your YAML Files"},{"location":"learning/k8s/k8s-notes/#building-your-yaml-spec---explain-command","text":"We can get all the keys each kind supports kubectl explain services \u2013recursive Oh boy! Let's slow down kubectl explain services.spec We can walk through the spec this way kubectl explain services.spec.type spec: can have sub spec: of other resources kubectl explain deployment.spec.template.spec.volumes.nfs.server Use kubectl api-versions or kubectl api-resources along with kubectl explain as documentation on explain could be old We can also use docs kubernetes.io/docs/reference/#api-reference","title":"Building Your YAML spec - explain Command"},{"location":"learning/k8s/k8s-notes/#dry-runs-with-apply-yaml","text":"dry-run a create (client side only) kubectl apply -f app.yml \u2013dry-run dry-run a create/update on server kubectl apply -f app.yml \u2013server-dry-run see a diff visually kubectl diff -f app.yml Difference between dry-run and diff","title":"Dry Runs With Apply YAML"},{"location":"learning/k8s/k8s-notes/#labels-and-label-selectors","text":"Labels goes under metadata: in your YAML Simple list of key: value for identifying your resource later by selecting, grouping, or filtering for it Common examples include tier: frontend, app: api, env: prod, customer: acme.co Not meant to hold complex, large, or non- identifying info, which is what annotations are for filter a get command kubectl get pods -l app=nginx apply only matching labels kubectl apply -f myfile.yaml -l app=nginx Label Recommendation","title":"Labels and Label Selectors"},{"location":"learning/k8s/k8s-notes/#label-selectors-use-case-for-labels","text":"The \"glue\" telling Services and Deployments which pods are theirs Many resources use Label Selectors to \"link\" resource dependencies You'll see these match up in the Service and Deployment YAML Using Label selectors Use Labels and Selectors to control which pods go to which nodes Assigning Pods to Nodes Taints and Tolerations also control node placement Taints and Tolerations","title":"Label Selectors (Use case for Labels)"},{"location":"learning/k8s/k8s-notes/#your-next-steps-and-the-future-of-kubernetes","text":"","title":"Your Next Steps, and The Future of Kubernetes"},{"location":"learning/k8s/k8s-notes/#storage-in-kubernetes","text":"Storage and stateful workloads are harder in all systems Containers make it both harder and easier than before StatefulSets is a new resource type, making Pods more sticky Recommendation : avoid stateful workloads for first few deployments until you're good at the basics Use db-as-a-service whenever you can","title":"Storage in Kubernetes"},{"location":"learning/k8s/k8s-notes/#volumes-in-kubernetes","text":"Creating and connecting Volumes: 2 types Volumes Tied to lifecycle of a Pod All containers in a single Pod can share them PersistentVolumes Created at the cluster level, outlives a Pod Separates storage config from Pod using it Multiple Pods can share them CSI plugins are the new way to connect to storage","title":"Volumes in Kubernetes"},{"location":"learning/k8s/k8s-notes/#ingress","text":"None of our Service types work at OSI Layer 7 (HTTP) How do we route outside connections based on hostname or URL? Example Usecase: app1.com and app2.com are 2 different deployments in the cluster and both listen on port 443. You will need Ingress to understand the DNS and route traffic to those apps Ingress Controllers (optional) do this with 3 rd party proxies Nginx is popular, but Traefik, HAProxy, F5, Envoy, Istio, etc. Recommendation: Check out Traefik Implementation is specific to Controller chosen Why Controller - To configure LB which is outside the cluster","title":"Ingress"},{"location":"learning/k8s/k8s-notes/#crds-and-the-operator-pattern","text":"You can add 3 rd party Resources and Controllers This extends Kubernetes API and CLI A pattern is starting to emerge of using these together Operator : automate deployment and management of complex apps e.g. Databases, monitoring tools, backups, and custom ingresses","title":"CRD's and The Operator Pattern"},{"location":"learning/k8s/k8s-notes/#higher-deployment-abstractions","text":"All our kubectl commands just talk to the Kubernetes API Kubernetes has limited built-in templating, versioning, tracking, and management of your apps Helm is the most popular Compose on Kubernetes comes with Docker Desktop Remember these are optional, and your distro may have a preference Most distros support Helm","title":"Higher Deployment Abstractions"},{"location":"learning/k8s/k8s-notes/#templating-yaml","text":"Many of the deployment tools have templating options You'll need a solution as the number of environments/apps grow Helm was the first \"winner\" in this space, but can be complex Official Kustomize feature works out-of-the-box (as of 1.14) docker app and compose-on-kubernetes are Docker's way","title":"Templating YAML"},{"location":"learning/k8s/k8s-notes/#kubernetes-dashboard","text":"Default GUI for \"upstream\" Kubernetes Clouds don't have it by default Let's you view resources and upload YAML Safety first!","title":"Kubernetes Dashboard"},{"location":"learning/k8s/k8s-notes/#namespaces-and-context","text":"Namespaces limit scope, aka \"virtual clusters\" Not related to Docker/Linux namespaces Won't need them in small clusters There are some built-in, to hide system stuff from kubectl \"users\" kubectl get namespaces kubectl get all --all-namespaces - Context changes kubectl cluster and namespace - See ~/.kube/config file kubectl config get-contexts # Selectively show output of Kube config kubectl config get-contexts -o name kubectl config set*","title":"Namespaces and Context"},{"location":"learning/linux/linux/","text":"Introduction \u00b6 Understanding Linux Filesystem Templates folder CronTab Guru Basic commands \u00b6 Important Commands Terminal Ctrl+Alt+T # Open the terminal Ctrl+D # Close the terminal exit # Close the terminal Ctrl + L # Clear the screen, but will keep the current command Ctrl + Shift + # Increases font size of the terminal Utility cal # Calendar current month cal -3 # Current -1, Current , Current +1 month cal 5 1967 # Format is (Month and Year). Gives May 1967 date # Current date in BST (default) date -u # Current date in UTC date --date \u201c30 days\u201d # Gives current date + 30 days (future date) date --date \u201c30 days ago\u201d # Gives current date \u2013 30 days (past date) which echo # Shows where the command is stored in PATH hostname -I # Gives IP address echo $? # Gives the output 0/1 value stored after a command is run wc \u2013l file1 # Gives line count in file1 wc file1 # Give word count in file1 History history # List all the commands executed !! # Run the previous command !50 # Run the command that is on line 50 of history output history \u2013c ; history \u2013w ; # Clears history and writes back to the file Ctrl + r # reverse searches for your input. Press Esc to edit the matched command man Using the Manual There are 8 sections in the manual. Important are 1, 5 and 8 sections man \u2013k <search term> # Search the manual for pages matching <search term>. man -k tmux # example of searching for tmux in the manual pages man -k \"list directory contents\" # Double quote seraches complete words man 1 tmux # Opens section 1 of tmux manual page, 1 is default and can be ignored man ls # Shows section 1 of ls command help cd # Shows the help pages if man pages are not present Redirection of Streams echo \"Hello\" 1 > output.txt # Standard output is redirected to output.txt echo \"Hello\" > output.txt # Standard output is default echo \"World\" 1 >> output.txt # Standard output is appended to output.txt echo \"Error\" 2 > error.txt # Standard error is redirected to error.txt cat -k bla 2 >> error.txt # Program error is redirected and appended to error.txt echo \"Hello World\" 1 >> output.txt 2 >> error.txt # Use both std output and error cat 0 < input.txt # Standard input is read from a file and sent to cat command cat < input.txt # Standard input is default cat 0 < input.txt 1 >> output.txt 2 >> error.txt # Use all 3 data streams cat -k bla 1 >> output.txt 2 > & 1 # Redirect Standard error to standard output stream and write to file Redirection to Terminals tty # Current terminal connected to Linux, gives path cat < input.txt > /dev/pts/1 # In another terminal, Standard input is read from a file and sent to tty 1 terminal Ctrl + Alt + F1 / chvt 1 # Goes to physical terminal with no graphics. Similarly you can change to 2 to 6 tty terminals. Ctrl + Alt + F7 / chvt 7 # Comes back to Graphical terminal Piping date | cut --delimiter \" \" --fields 1 # Output of date is input to cut command - Tee command - Used to store intermediate output in a file and then stream passed horizontally through the pipeline - tee command takes a snapshot of the standard output and then passes it along date > date.txt | cut --delimiter \" \" --fields 1 # Output will not work and date will only be stored in file and not passed to cut command date | tee date.txt | cut --delimiter \" \" --fields 1 # Output of date is first stored in file, then passed to cut command for display to Standard Output date | tee date.txt | cut --delimiter \" \" --fields 1 | tee today.txt cat file1.txt file2.txt | tee unsorted.txt | sort -r > reversed.txt # Output chaining and storing intermediate data in files - XARGS command (Powerful pipeline command) - Allows piped data into command line arguments - date | echo # Output of date is passed to echo, but echo doesn't accept standard input, only commandline arguments date | xargs echo # xargs will convert standard output into command line arguments date | cut --delimiter \" \" --fields 1 | xargs echo # Prints the day of the week Alias Used to store reusable scripts in .bash_aliases file can be used in scripts alias # Shows all the alias setup for the user # Store an alias in the `.bash_aliases` file alias calmagic = 'xargs cal -A 1 -B 1 > /home/leslie/calOutput.txt' # In the terminal use if in a pipe command, STDOUT will be stored in a file echo \"12 2021\" | calmagic File System Navigation # File Listing pwd # Prints absolute path of current working directory(CWD) ls \u2013l # Long list of CWD ls \u2013a # Shows all files including hidden ls -F # Shows directories as ending with / along with other files stat <filename> # Detailed file information file <filename> # File type ls -ld # Detailed folder information # Change Directories cd - # Helps to switch directories. Like a Toggle (Alt + Tab) in windows cd / cd ~ # User Home directory from anywhere cd .. # Back to parent directory of CWD Wildcards Wildcards and How to use - The star wildcard has the broadest meaning of any of the wildcards, as it can represent zero characters, all single characters or any string. - The question mark (?) is used as a wildcard character in shell commands to represent exactly one character, which can be any single character. - The square wildcard can represent any of the characters enclosed in the brackets. ls *.txt # Matches all txt files ls ???.txt # Matches all 3 letter txt files ls file [ 123 ] .txt # Matches all files ending with 1 to 3 ls file [ A-Z ] .txt # Matches all files ending with A to Z ls file [ 0 -9 ][ A-Z ] .txt # Matches all files ending with 0A to 9Z File and Folders # Create Operations touch file1 # Creates a new file1 echo \"Hello\" > hello.txt # Creates and writes using redirection # -p is parent directory which is data and inside that 2 directories called sales & mkt is created mkdir \u2013p /data/ { sales,mkt } # Brace exapansion will allow to create folders. Sequence can be expressed as .. mkdir -p /tmp/ { jan,feb,mar } _ { 2020 ..2023 } # Brace expansion for files, it will create 10 files inside each folder touch { jan,feb,mar } _ { 2020 ..2023 } /file { 1 ..10 } # Delete Operations rm file1 # Deletes file1 rm *.txt # Deletes all txt files # Deletes all files and folders inside the main folder and the main folder as well # CAUTION: Use the recursive option with care rm -r /tmp/ { jan,feb,mar } _ { 2020 ..2023 } / # Deletes only empty directories rmdir /tmp/ # Skips folders which have files # Copy Operations cp /data/sales/file1 /data/mkt/file1 cp /data/sales/* . # Copy all files to CWD cp -r /data/sales /data/backup # Copy folder to backup folder # Move and Rename Operations mv file1 file2 # Rename file in the same folder mv /data/mkt/ /data/hr # Rename folder, Note the slash after first folder mv /data/sales/* /tmp/backup/ # Move files to new location mv /data/mkt/ /tmp/newFolder # Move and rename the folder Nano - Editing M Key can be Alt or Cmd depending on keyboard layout Enable spell checking on nano by editing /etc/nanorc and uncomment set speller in the file. Ctrl + O # Write data out to file Ctrl + R # Copy contents of one file into another Ctrl + K # Cuts entire line, also used as a delete Alt + 6 # Copy entire line Ctrl + U # Paste the line Ctrl + T # Spell check the file Alt + U # Undo changes Alt + E # Redo changes # File operations in vi > filename # Empties an existing file :x # Saves file changes instead of :wq Search Files find ~/projects # Find matches of files and folders from projects and below find . # Find from CWD and below find . -maxdepth 1 # Find from CWD and one level below find . -type f # Find files only find . -type d # Find folder only find . -maxdepth 1 -type d # Find folder only and one level below find . -name \"*.txt\" # Find files ending with matching patterns find . -maxdepth 3 -iname \"*.TXT\" # Find files with case insensitive matching patterns find . -type f -size +100k # Find files greater than 100 Kb # Find files greater than 100 Kb AND less than 5 Mb and count them find . -type f -size +100k -size -5M | wc -l # Find files less than 100 Kb OR greater than 5 Mb and count them find . -type f -size -100k -o -size +5M | wc -l - Find and Execute commands # Find and copy files to backup folder. `\\;` denotes end of exec command find . -type f -size +100k -size +5M -exec cp {} ~/Desktop/backup \\; ### # Find file called needle.txt inside haystack folder ### # Create 100 folders and inside each folder 100 files mkdir -p haystack/folder { 1 ..100 } touch haystack/folder { 1 ..100 } /file { 1 ..100 } # Create file in one random folder touch haystack/folder $( shuf -i 1 -100 -n 1 ) /needle.txt ### # Finding the file using name find haystack/ -type f -name \"needle.txt\" # Move the file to haystack folder find haystack/ -type f -name \"needle.txt\" -exec mv {} ~/tmp/haystack \\; ### View/Read File Contents cat cat file1 file2 > file3 # Concatenate 2 files and write into file3 cat \u2013vet file3 # displays special characters in the file e.g. EOL as $. Useful if sh files are created in windows tac - Flips the file contents vertically tac file3 # Reads the file in reverse rev - Reverses the contents of each line rev file3 # Reads the line in reverse less - Allows to page through big files less file3 # Shows one page at a time. Use Arrow keys to scroll # Output of find piped to less command for scrolling find . -type f -name \"*.txt\" | less head - Shows limited lines from top of output cat file3 | head -n 3 # Shows first 3 lines tail - Shows limited lines from bottom of output cat file3 | tail -n 3 # Shows last 3 lines tail \u2013f /var/log/messages # follows the file and continuously shows the 10 lines Sort sort words.txt > sorted.txt # Sorts in Asc order and redirects to sorted.txt sort -r word.txt > reverse.txt # Sorts in Des order sort -n numbers.txt # Sorts in Asc numeric order based on digit placement sort -nr numbers.txt # Sorts in Des numeric order based on digit placement sort -u numbers0-9.txt # Sorts and shows only unique values - Sorting data in tabular format # Sort on the basis of file size (5th column and its numeric) ls -l /etc | head -n 20 | sort -k 5n # Reverse (r) the output showing largest files first ls -l /etc | head -n 20 | sort -k 5nr # Sort on the basis of largest file size in human readable format ls -lh /etc | head -n 20 | sort -k 5hr # Sort on the basis of month ls -lh /etc | head -n 20 | sort -k 6M Search data - grep grep is case-sensitive search command # grep <search term> file-name grep e words.txt # Shows matching lines as STDOUT grep -c e words.txt # Counts the matching lines # Search in case insensitive manner grep -i gadsby gadsby_manuscript.txt # Search strings using quotes grep -ci \"our boys\" gadsby_manuscript.txt # Invert the search grep -v \"our boys\" gadsby_manuscript.txt # Searches for server and not servers. \\b is the word boundary grep \u2018 \\b server \\b \u2019/etc/ntp.conf # Searches for server beginning in the line. \\b is the word boundary grep \u2018^server \\b \u2019/etc/ntp.conf Filter data using grep ls -lF / | grep opt # Shows details for opt folder only ls -F /etc | grep -v / # Shows only files in etc folder - Remove Commented and Blank Lines # Empty lines can be shown as ^$. \u2013v reverses our search and \u2013e allows more than one expression. O/p is sent to std o/p grep \u2013ve \u2018^#\u2019 \u2013ve\u2019^$\u2019 /etc/ntp.conf # -v ^# says I don\u2019t want to see lines starting with #. ^$ says I don\u2019t want to see lines that begin with EOL marker Archival and Compression Two step process: Create the tar ball, then compress the tar Compression tool comparison # Create the tar ball tar -cvf backup.tar file [ 1 -3 ] .txt # Create, Verbose, Files to archive tar -tf backup.tar # Test for tar file without unzipping # Compress the tar ball ## 3 compression tools - gzip -> bzip2 -> xz (Compression and time increases from left to right) gzip backup.tar # Compresses the tar ball and adds .gz extension to tar ball gunzip backup.tar.gz # Decompress the gzip ### bzip2 backup.tar # Smaller file size than gzip and adds .bz2 extension to tar ball bunzip2 backup.tar.bz2 # Decompress the bzip. Best used for larger file sizes # Open the tar ball contents tar -xvf backup.tar # Extract, Verbose, Files to unarchive ### Create tar and compress in single command # Adding the z option for gzip and renaming the tar as .gz tar -cvzf backup.tar.gz file [ 1 -3 ] .txt tar -xvzf backup.tar.gz # Adding the j option for bzip2 and renaming the tar as .bz2 tar -cvjf backup.tar.bz2 file [ 1 -3 ] .txt tar -xvjf backup.tar.bz2 ### BASH #!/bin/bash # First line in the script \"SHEBANG\" tells type of script ### # To create an executable script, create a `bin` folder in your home. # Move all utility shell scripts to bin. Also remove .sh file extenstions # Make the file as executable `chmod +x data_backup` # Add the `~/bin` to the PATH variable # Edit `.bashrc` with PATH=\"$PATH:$HOME/bin\" # Now all scripts in bin folder are executable from command line ### # Set and unset variables export $VARIABLE # Sets the variable unset VARIABLE # Removes the variable, NOTE \u2013 No $ in variable Cron Scheduling crontab -e < select editor> # Opens the template crontab # Multiple options for each column of crontab using comma. # SPACE is used to delimit the columns of crontab # */<value> can divide the time intervals ### # min hours \"day of month\" month \"day of week (0-6)\" ### * * * * * bash ~/data_backup.sh Package Management apt-cache search docx # Searches apt for programs that can work with MS Word apt-cache show <package> | less # Gives software information # Apt cache information resides in /var/lib/apt/lists sudo apt-get update # Updates the apt lists sudo apt-get upgrade # Upgraded to the latest software versions from the list sudo apt-get install <pkg-name> # Install package sudo apt-get purge <pkg-name> # Remove & uninstall package. Recommended approach sudo apt-get autoremove # Removes any installed package dependecies # Package compressed archives are stored in `/var/cache/apt/archives` sudo apt-get clean # Removes all package compressed acrhives sudo apt-get autoclean # Removes only package compressed acrhives that cannot be downloaded - Source Code for apps OS uname # Shows kernal uname -o # Shows OS uname -m # Shows computer architecture x86_64 (64 bit), x86 (32 bit) lsb_release -a # Distro version Misc fdisk -l # Gives device wise memory details free / free -m # Gives amount of free memory lsblk # Lists all partitions swapon \u2013s # List all swap files ps # Process id of the current bash shell shutdown \u2013h now / poweroff / init 0 # Power downs the system restart / init 6 / reboot # Restarts the system shutdown \u2013r + 1 \u201cWe are restarting\u201d # Restarts the system give all logged in users 1 min to shut down all process su - # Login to root id / id bob # Shows the current user and group id sudo -i # Interactive shell for password of the current user, to get elevated access ssh localhost # ssh connection to same server. Type exit or Ctrl + D to logout of ssh. who / w # Gives the list of terminals that are connected and who has logged on to the server Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Tips \u00b6 Alt + F2 Gives the run command terminal and then type gnome-system-monitor is like task Manger in windows. Gives graphical overview of the system and can kill processes. Putting & after any commands runs it in the background. Run jobs to see all the background running jobs. Type fg to bring the background running jobs to the foreground. Ctrl + C to cancel the job then. !<anycharacter> will search for the last command in history starting with that character !?etc executes the last command that contains etc Filesystem \u00b6 Creating Partitions \u00b6 fdisk or gdisk utility to partition. If Mountpoint is /, that is primary partition. Creating Filesystems \u00b6 mkfs.ext4 \u2013b 4096 /dev/sdb1 # Creates 4MB block size file system mkfs.xfs \u2013b size = 64k /dev/sdb2 # Creates 64k block size file system. Xfs is specialized filesystem Mounting Data \u00b6 mkdir \u2013p /data/ { sales,mkt } mount /dev/sdb1 /data/sales # Mounts device to data/sales directory mount /dev/sdb2 /data/mkt Unmounting Data \u00b6 umount /dev/sdb1 or umount /dev/sdb { 1 ,2 } # Unmounts both the devices Virtual Memory or Swap Filesystem \u00b6 They are temporary space requirements Virtual memory in Linux can be a Disk Partition or Swap file. Use gdisk to create swap filesystem. Option L and then hex code 8200. To make the swap filesystem permanent, make an entry in /etc/fstab file, so changes are persistent even after system reboot. partprobe /dev/sdb # Sync saved partition in memory. Or it requires system reboot mkswap /dev/sdb3 # Select the right swap device to create the filesystem swapon /dev/sdb3 # Mount the filesystem Troubleshooting Linux filesystem \u00b6 df \u2013hT # list all filesystem with space details du \u2013hs /etc # gives diskusage of etc directory with memory dumpe2fs /dev/sdb1 | less # human readable details for the device dd if = /dev/sda of = /data/sales/file count = 1 bs = 512 # takes data backup of sda to sales/file of the first 512 bytes dd if = /data/sales/file of = /dev/sda # copies the data back in case of recovery tar \u2013cvf /data/sales/etc.tar /etc # backs up etc directory by creating a tar file umount /dev/sdb1 # unmounts sales directory tune2fs \u2013L \u201cDATA\u201d /dev/sdb1 # adding label to the file system debugfs /dev/sdb1 # enters debug of sdb1 directory. Type quit to exit File Permissions \u00b6 # Format for file permission : User-Group-Others # Symbolic Notation (Default permission) RWX \u2013 RW - R # Octal Notation 7 - 6 - 4 # So RWX is 111 i.e. 7, RW is 110 i.e. 6 and R is 100 i.e. 4 umask 2 # sets default permission to all the files in the directory chmod 777 file1 # Changes permission for a file1 chmod u = rwx,g = rw,o = rw file 2 # Verbose way to set permissions chmod +rx file3 # Sets read & write for User, group and others ls \u2013ld /data # Shows permission for a single directory chgrp users /data # Adds users group to the directory - Even if user does not have write access to a file, he has delete / add file access to a directory. - chmod o+t /data .Users can delete only their files and not other\u2019s. Root will not be able to delete files in this directory. - This permission is sent on the /tmp directory by default at installation. So only user\u2019s own file can be deleted, not of others. Links (Hard and Soft Links) \u00b6 Soft links are also called as Symbolic Links or symlinks . Here one file will be a pointer to the other file. If file has more than one name, it\u2019s called hard link. To find the number of sub directories , use stat dirname . Links number -2 is the total number of sub directories. Each directory has a minimum of 2 links, hence subtract 2. ln file2 file5 # Creates hard link between file2 and file5. # Shows the inode number which is same i.e. the same metadata is present for both. Cat on both the files shows the same data content ls \u2013li file2 file5 ln \u2013s file3 file4 # Creates a symlink between file 3 and file5. Cat on both the files shows the same data content ls \u2013li file3 file4 # Shows the symlink, but they are different files. Inode number is different. readlink file5 # shows where the link is Applying Quotas \u00b6 Quotas can be applied to Space/inodes, Group, User or File System. repquota \u2013auv # Give quota report per user space usage along with limits quotaon /dev/sdb1 # Checks quota limit # enable quotas and edit the hard and soft limits. Soft limit can be exceeded for 7 days, after which it is enforced. edquota \u2013u <username> # enables quota via command line. Soft limit is 21000 is 21MB, hard limit is 26MB setquota \u2013u <username> 21000 26000 0 0 /dev/sdb1 Directory Listing and Alias \u00b6 ls \u2013F /dir1 # shows directory with a / and symlink as @ at the end of the name ls \u2013-color = auto /dir1 # shows the same file types in color alias ls = \u2019ls \u2013-color = auto\u2019 # creates an alias for ls with color ls \u2013lh file1 # list in human readable format ls \u2013lt /etc # shows long listing with time modified in descending order ls \u2013ltr /etc | less # shows reverse listing, q to quit Synchronize Directories \u00b6 mkdir /backup rsync \u2013av /home/ /backup/ # archive home dir to backup dir. / after home and backup is important rsync \u2013av --delete /home/ /backup/ # sync deletions of data as well, otherwise rsync ignores it by default rsync \u2013ave ssh # sync data between servers using e option Process Management \u00b6 Monitor Process \u00b6 which ps # shows the installation directory for ps uptime # shows the uptime of the system along with the load average in the range of 1 min, 5 mins and 15 mins # Rule of Thumb for uptime --> Load average for single core value should be less than 1, for dual core less than 2 etc. which uptime # shows the installation directory for uptime cat /proc/uptime # shows uptime and idle time cat /proc/loadavg # shows load avg for 1,5 and 15 mins, active process running/total process, last process id that was issued Jobs \u00b6 sleep 180 # sleeps for 180 secs in foreground. Ctrl + Z to pause the job. Run bg to put the sleep command in background. jobs # shows running jobs fg 1 # puts the sleep command in foreground Managing Processes \u00b6 ps to display processes and kill to send signals. pgrep, pkill and killall are great shortcuts. The default kill signal is -15 which can also be written as \u2013term or \u2013sigterm . To really kill it is -9, -kill or \u2013sigkill . echo $$ # Shows current process ps \u2013l # long listing with the process ps \u2013ef # shows all the processes for all users ps \u2013eaf | grep processname pgrep nginx # shows process ids for nginx sleep 900 & pkill sleep # searches for sleep process and kills it killall sleep # searches for all running sleep process and kills it kill \u2013l # shows the multiple kill signals available kill -9 <process id> # forcefully terminates the process, also use kill \u2013kill <process id> top \u2192 kill, renice, sort and display processes Running top, you can toggle between the information displayed at the top lines. l \u2013 on/off load, t \u2013 on/off tasks, m \u2013 on/off memory Sorting of top is on %CPU, f \u2013 shows current fields being shown on output of top. Select the new field to sort and type s Type r for renice and put in the process id. Esc and Enter to quit the shell Type k for kill and put in the process id. Esc and Enter to quit the shell q to quit out of top top # shows all running processes, q to quit top \u2013n 1 # shows the running processes for 1 capture and quits top \u2013n 2 \u2013d 3 # shows 2 captures with a delay of 3 seconds and quits Editors \u00b6 Vi \u00b6 : # Last line mode q, q! # quit the file x, wq, wq! # save and exit the file i, I # insert from cursor position, I for inserting from start of the line a, A # append after the cursor, A for append from last character in the line o, O # insert line below the cursor, O for above the current cursor position dd # delete the line u # undo the changes Line Navigation \u00b6 <Linenumber>G # e.g. 7G, takes cursor to 7th line in the file G # only G takes cursor to end of file w , b # w takes cursor to next word, b takes cursor to one word before ^ , $ # ^takes cursor to start of line, $ to end of the line vi +127 /etc/file1 # opens the file and takes cursor to 127th line vi +/Document /etc/file1.conf # opens the file and takes cursor to first occurrence of \u201cDocument\u201d set number / set nonumber # from last line mode, it will show and stop line number display syntax on # highlighting on, e.g. xml highlighting etc. Read and Write \u00b6 r /etc/hosts # Open an existing file, use : and then you can get content from hosts file into current file w newfile # :, it will copy entire file contents into newfile in the same directory 3 ,7w newfile # it will copy line 3 to 7 into newfile Search and Replace \u00b6 %s/Hi/Hello # Open an existing file, use : and you can search Hi and Replace with Hello. %s signifies entire document search /Hello # searches for Hello in the document. Type n to get next occurrence, N will take cursor in reverse 1 ,20s/Hi/Hello # searches for 1st 20 lines for Hi and replaces with Hello 14 ,20s/^/ / # from 14th to 20th line, it will add 3 spaces from the start of the line, just like Tab BASH Scripting \u00b6 Understanding Variables \u00b6 Local variables \u2192 accessible only to the current shell, FRUIT=\u2019apple\u2019, echo $FRUIT Global variables \u2192 you need to set and then export it to make it global. export FRUIT=\u2019apple\u2019 Simple Script \u00b6 vi hello.sh #!/bin/bash # Path to the interpreter echo \u201cHello World\u201d exit 0 # return code, :wq chmod +x hello.sh hello.sh # execute the script as it\u2019s in the home directory /user/bin Getting user input \u00b6 vi hello.sh #!/bin/bash echo \u2013e \u201cEnter your name: \\c \u201d # -e is the escape sequence, -c is for the prompt read INPUT_NAME # read the input data into a variable echo \u201cHello $INPUT_NAME \u201d exit 0 User Input types \u00b6 $1 $2 # $1 is the 1st input parameter, 2nd Parameter and so on. $0 # is the script name itself $# # count of input parameters $* # is collection of all the arguments Multiple inputs using positional parameters \u00b6 vi hello.sh #!/bin/bash echo \u201cHello $1 $2 \u201d # $1 is the 1st input parameter, $0 is the script name itself, $2 is the 2nd input parameter and so on exit 0 Code Snippets \u00b6 Gedit \u2192 Gnome Editor \u2192 Add the Snippet Plugin (Applications \u2192 Accessories \u2192 gedit. Preferences in gedit tab \u2192 Plugins enable Snippet Plugin and restart gedit) Conditional Statement - IF \u00b6 if [[condition]] \u2192 testing for string condition if ((condition)) \u2192 testing for numeric condition e.g. if (( $# < 1 )) \u2192 if count of input parameter vi hello.sh #!/bin/bash if (( $# 1 )) then echo \u201cUsage: $0 <name>\u201d exit 1 fi echo \u201cHello $1 $2 \u201d exit 0 Case Statement \u00b6 vi hello.sh #!/bin/bash if [[ ! \u2013d $1 ]] # if the 1st argument is not a directory then echo \u201cUsage: $0 <directory>\u201d exit 1 fi case $2 in \u201cdirectory\u201d ) find $1 \u2013maxdepth 1 \u2013type d ;; # break \u201clink\u201d ) find $1 \u2013maxdepth 1 \u2013type l ;; # break * ) # default statement echo \u201cUsage: $0 <directory> directory | link\u201d ;; esac exit 0 For \u00b6 vi hello.sh #!/bin/bash for u in $* # $* is collection of arguments, u is temporary variable do # do block useradd $u # access to temp variable is via $ echo Password1 | passwd \u2013stdin $u # use the passwd command and get the user input from keyboard passwd \u2013e $u # expire the password, so they can change it at first login done echo \u201cFinished\u201d # at time of execution ./hello.sh fred mary john vi listsize.sh #!/bin/bash for file in $( ls ) # for each file, in the output of ls do [[ ! \u2013f ]] && continue # not a file then continue to next # use the stats to get statistics of the file, to get the last accessed date and then format the date LA = $( stat \u2013c %x $file | cut \u2013d \u201c \u201d \u2013f1 ) echo \u201c $file is $( du \u2013b $file ) bytes and was last accessed on $LA \u201d # use du to get file size done While \u00b6 vi loop.sh #!/bin/bash -x # -x is for debug mode COUNT = 10 while (( COUNT > 0 )) do echo \u2013e \u201c $COUNT \\c \u201d # \\c will suppress the line feed (enter) sleep 1 (( COUNT -- )) # round brackets to avoid using $ symbol done - Use the until when you want to stop the loop when the condition becomes true. User Management \u00b6 Managing Users: User Lifecycle ==> useradd, usermod, userdel Local databases ==> /etc/passwd, /etc/shadow (encrypted) passwd (to set the password) pwconv (move pass to encrypted) pwunconv (move back to unencrypted) # /etc/passwd file structure # It has 7 filed separated by : Login Name, Optional encrypted password or \u201cx\u201d, Numerical UID, Numerical GID, Username or comment, User home directory, Optional command interpreter # /etc/shadow file structure where the actual passwords are stored # It has 8 filed separated by : Login Name, encrypted password ( if it begins with ! the account is locked ) , Date of last password change, Minimum password age, Maximum password age, Password warning period, password inactivity, account expiry date # /etc/login.defs The password ageing defaults can be configured with this file useradd \u2013D # shows the default settings for a user that is added cat /etc/default/useradd # shows where the defaults are set useradd bob # only adds the user, no home directory is created. Once the user logs in, it will get created tail -3 /etc/passwd # shows that bob is added useradd \u2013m bob # also creates the home directory tail -3 /etc/shadow # shows the password for the user passwd bob # add the password for bob passwd \u2013l bob # locks the account passwd \u2013u bob # unlocks the account usermod bob \u2013c \u201cBob Smith\u201d # adding additional details for the user userdel \u2013r bob # removes the user and home directory Group Management \u00b6 Group Lifecycle \u2192 groupadd, groupmod, groupdel Local databases \u2192 /etc/group, /etc/gshadow (encrypted) gpasswd (to set password) newgrp (switch to new groups) # /etc/group structure # It has 4 fields: Group Name, Password, Numerical GID, User list that is comma separated # /etc/gshadow structure # It has 4 field: Group Name, Encrypted password, Admin list that is comma separated, # this can be managed used \u2013A cmd # Members, this can be managed using the \u2013M cmd Private groups are enabled by default. The user added is also added to the same group. If this is disabled, users will belong to the groups users. Use useradd \u2013N to overwrite private groups. This can be enabled or disabled by setting USERGROUPS_ENAB in /etc/login.defs useradd \u2013m \u2013g users jim # -g is Primary Group, -G is secondary groups. Secondary groups are more traditional groups id jim usermod \u2013G sudo,adm jim # added jim to secondary groups sudo, adm useradd \u2013N \u2013m sally # adds sally to the default group gpasswd \u2013M jim,sally sudo # adds 2 users to sudo group groupadd sales gpasswd sales # sets the new password for sales newgrp sales # add the user to the sales group temporarily. If the user logs out, he is removed from the group Automate System Tasks \u00b6 Regular Tasks \u2192 cron (more than once a day but misses job if turned off), anacron (run jobs missed on startup but jobs can run just once a day) Once Off \u2192 at (runs at specified time and date), batch (runs when load average drops below 0.8) System Cron Jobs \u00b6 # /etc/crontab, /etc/cron.d # cron files # /etc/cron.<time> # where time is hourly, daily, weekly and monthly, contains scripts that need to be executed # Adding a system cron job cd /etc/cron.d vi daily-backup # add a new file 30 20 * * 1 -5 root /root/back.sh # run back.sh from Mon to Fri at 20:30 # Adding a user cron job crontab \u2013e # edit the user crontab file */10 10 1 1 1 tail /etc/passwd # Runs once on 1st day if it\u2019s a Mon of Jan, at 10 am for every 10 mins crontab \u2013l # list all cron jobs crontab \u2013r # remove the cron job # anacron: /etc/anacrontab structure # It has 4 fields Period in days or macro ( Daily, Monthly ) , Delay ( minutes after system startup for job to run ) , Job Identifier ( used to name timestamp file indicating when job was last run ) Command ( that needs to be executed ) @weekly 120 weekly-backup ls /etc // weekly, 120 mins after startup it will run weekly-backup Batch \u00b6 at and batch commands at noon tomorrow # Enter the command line, Ctrl + D to save at> ls /etc # enter the command that needs to be executed atq # shows the jobs queue atrm # remove the job batch # Enter the command line, Ctrl + D to save at> ls /etc > /root/file1 # redirect the o/p to file1. It will run if the system load avg is less than 0.8. Security for Cron \u00b6 Everyone is allowed to run their own cron and at jobs, unless you add entries to /etc/cron.allow or /etc/at.allow. No one is denied unless you add entries to /etc/cron.deny or /etc/at.deny Networking Fundamentals \u00b6 Network Time Protocol (NTP) \u00b6 Configuring Network Time Protocol (NTP) \u00b6 # vi /etc/ntp.conf # prefixing i with date creates a backup of the original file. Removes commented and blank lines sed \u2013i. $( date +%F ) \u2018/^#d ; /^$/d\u2019 /etc/ntp.conf Implementing the configuration file changes \u00b6 vi /etc/ntp.conf # Add lines other can default just below the driftfile command statsdir /var/log/ntpstats # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server instead of on the internet debian servers Save the file and restart the service \u00b6 service ntp restart # sudo if no access # check if the ntpstats directory is accessible to the ntp service # The user should be ntp and it should have write access ls \u2013ld /var/log/ntpstats/ - Date \u2192 Current system date and time. This is the time in memory. - HwClock \u2192 Hardware date and time set by the BIOS. hwclock \u2013r # shows the hardware clock hwclock \u2013-systohc # sets the hardware clock from system clock Hwclock \u2013-hctosys # sets the system clock from hardware clock NTP Tools \u00b6 ntpdate (once off adjustment) ntpq (query the ntp server) ntpq \u2013p (shows peers) ntpstat (Shows status but not on debian. Try ntpdc \u2013c sysinfo) ntpq \u2013c \u201cassociations\u201d \u2192 shows associations # Configuring NTP on centos --> Install ntp ntpdate 192 .168.0.3 # one off update with a local machine in the network # vi /etc/ntp.conf # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server systemctl start ntpd # save and restart systemctl enable ntpd # enable to start at system startup Managing System Log Daemons \u00b6 Rocket-Fast System for Log Processing (rsyslogd) \u00b6 rsyslogd \u2013v # vi /etc/rsyslog.conf # Adding a simple log rule # For any log event greater than or equal to info make a log entry in local5 log. Local5 could be a simple application local5.info /var/log/local5 systemctl restart rsyslog.service # restart the service # to test this in working using command line logger \u2013p local5.info \u201cScript started\u201d # p is priority, if you see /var/log/local5 file, the log would be present /var/log/ folder structure \u00b6 messages (Nearly everything is logged here) secure (su and sudo events amongst others) dmesg (kernel ring buffer messages) Logrotate \u00b6 ls /etc/cron.daily # has the logrotate script which will rotate log files cd /etc/logrotate.d/ # folder where all apps rotation policy is set cp syslog local5 # copy existing app conf for local5 app # vi local5 # make edits to point to /var/log/local5 file /var/log/local5 { weekly # period for rotation size +10 # size of the file for rotation compress # use compression for the rotated log file rotate 4 # keep 4 weeks of logs before overwriting } # manually running the rotate logrotate /etc/logrotate.conf # on execution, all files mentioned will be interrogated and log backup will be created Journalctl \u00b6 Responsible for viewing and log management. Need to be a member of adm group to read this. By default journal is memory resident i.e. it will be lost on restart journalctl # view the journal journalctl \u2013n 10 # shows the last 10 entries journalctl \u2013n 10 \u2013p err # shows the last 10 entries with priority errors mkdir /var/log/journal # to make the journal data persistent systemctl restart system-journald systemctl status system-journald # shows that the journal data is persistent usermod \u2013a \u2013G adm username # adding user to adm group, -a is append chgrp \u2013R adm /var/log/journal # recursively give adm group access jornalctl \u2013-disk-usage # shows disk usage journalctl \u2013-verify # verify the journal integrity SSH \u00b6 Remote access using SSH \u00b6 Server Configuration /etc/ssh/sshd_config The public key of the server is used to authenticate to the client. The public key of the server is stored in /etc/ssh/ssh_host_rsa_key.pub It is down to the client to check the public key using: StrictHostkeyChecking Server public keys are stored centrally in /etc/ssh/ssh_known_host or locally under ~/.ssh/known_hosts SSH Server Configuration \u00b6 netstat \u2013antl # shows the open tcp ports of the server grep ssh /etc/services # shows the services using ssh lsof \u2013i # Also shows open ports # vi /etc/ssh/sshd_config # Uncomment AddressFamily line and change as below AddressFamily inet # Now ssh will only listen on IPv6 systemctl restart sshd # vi /etc/ssh/sshd_config # Uncomment below lines and modify LoginGraceTime 1m # To avoid denial of service attacks and freeing up your service quickly PermitRootLogin no # 2 level authentication, first as normal user and then root SyslogFacility AUTHPRIV ClientAliveInterval 300 ClientAliveCountMax 0 MaxSessions 10 systemctl restart sshd Client Configuration and Authentication \u00b6 Client Configuration /etc/ssh/ssh_config Generate Private and Public keypair using ssh_keygen Use ssh-copy-id to copy to host we want to authenticate with. To provide Single Sign On using ssh-agent Client/User public keys are stored in ~/.ssh/authorized_keys using ssh-copy-id. To connect to server using ssh \u00b6 cd # home directory ls \u2013a # to show all hidden files ssh pi@192.168.0.97 # ssh using user and ip address. Add the password of the user to authenticate cd .ssh cat known_hosts # shows the client ip and public keys exit or logout or Ctrl + D # to end the ssh session To generate keypairs \u00b6 cd .ssh # On the client home directory ssh-keygen \u2013t rsa # generate key pair. Private key is encrypted using a passphrase # This will copy the generated public key to the target server. To which user\u2019s directory at the server we will # connect as. Give the password of the server\u2019s account. ssh-copy-id \u2013i id_rsa.pub pi@192.168.0.97 ssh pi@192.168.0.97 # now connect to the server using passphrase of the private key # From another terminal say tty we can now add the private key once and don\u2019t need to authenticate to the target server ssh-agent bash # fire up another bash terminal ssh-add .ssh/id_rsa # add the private key from the home directory. Enter the passphrase ssh \u2013l or ssh \u2013L # list all identities added ssh pi@192.168.0.97 SSH Tunnels \u00b6 ssh \u2013f \u2013N \u2013L 80 :localhost:80 user@s1.com # -f = execute in background, -N = We are not running any commands on remote host # -L = listening on port 80, we are listening on localhost and forwarding to port 80 on the remote host # On the remote host it has to listen on ssh called s1.com. We connect as user called user. # Example # Webservice on 192.168.0.3 # on a different machine, login as standard user cd .ssh ssh \u2013f \u2013N \u2013L 9000 :localhost:80 andrew@192.168.0.3 # we are listening on port 9000 on the localhost and forwarding traffic to port 80 om 192.168.0.3 netstat \u2013antlp # we can see that localhost:9000 is listening on ssh # On the client machine open the browser and type in http://127.0.0.1:9000 we will see the webservice data kill <process id of ssh> # shutdown the ssh tunneling process after finishing the work Configuring Network Protocols in Linux \u00b6 /etc/services \u00b6 Network services are identified by a port address Common services and associated port address is listed in /etc/services netstat \u2013alt will list services listening via TCP this resolves address to name in /etc/services # To verify the above we can use strace to map the netstat data with the services that are running strace netstat \u2013alt 2 > & 1 | grep /etc/services grep http /etc/services # service and port mapping dig command \u00b6 which dig # get the path of dig rpm \u2013qf /usr/bin/dig # dig is not installed by default. Hence needs to be installed. dig \u2013t AAAA ipv6.bbc.co.uk # shows the IPv6 address Interface Configuration Files \u00b6 ifconfig # shows ip address details ifconfig eth0 192 .168.0.99 netmask 255 .255.255.0 up # sets ip address for Ethernet card ip address show # same as ifconfig # These settings are lost upon restart unless they are written to configuration files. /etc/sysconfig/network-scripts/ # centos /etc/network/interfaces # debian To make the IP address static \u00b6 cd /etc/sysconfig/network-scripts/ vi ifcfg-ens32 # open the config file # Replace and add the lines BOOTPROTO = \u201dstatic\u201d # change from dhcp IPADDR = 192 .168.0.240 # select a static ip address NETMASK = 255 .255.255.0 # add a class c subnet GATEWAY = 192 .168.0.1 # add the gateway address DNS1 = 8 .8.8.8 # add google as the DNS server # Restart the services systemctl restart network.service Networking Tools \u00b6 nmap \u00b6 nmap localhost # shows all open ports used by localhost nmap 192 .168.0.3 # shows open ports at remote host nmap \u2013v 192 .168.0.3 # verbose mode nmap \u2013iL ip.txt # input file containing all ip address to be scanned netstat \u00b6 netstat \u2013a # shows all connections netstat \u2013at # shows all tcp connections netstat \u2013alt # shows all listening tcp connections netstat \u2013altpe # shows all user and process ids and listening tcp connections netstat \u2013s # statistics netstat \u2013i # shows interfaces netstat \u2013g # multicast groups netstat \u2013nr # network route tables Show Sockets (ss) \u00b6 ss \u2013t \u2013a # shows all tcp connections ss \u2013o state established \u2018 ( dport = :ssh or sport = :ssh ) \u2019 # shows all ssh connections ss \u2013x src /tmp/.X11-unix/* # shows X11 connections using socket files lsof \u00b6 lsof \u2013i -4 # list all ipv4 connections lsof \u2013i :23, 24 # list all port 22 and 23 connections lsof \u2013p 1385 # list process id 1385 connections Testing Network Connectivity \u00b6 ping www.centos.org # test network connectivity ping \u2013c3 www.centos.org # sends only 3 pings traceroute www.centos.org # describes the route to destination from source tracepath www.centos.org # shows maximum transmission unit size Host Name Resolution Tools \u00b6 hostname # Confirms the hostname cat /etc/hostname # shows the current host name hostname myComputer # changes the hostname to myComputer. You need to login as root to change this dig www.centos.org # resolves to ip address dig \u2013t MX centos.org # mail servers associated with centos Managing Interfaces \u00b6 ip a s # Shows ip addresses ip n s # Shows Neighbor shows looking at ARP cache ip r s # Shows root table ip ma s # Shows Multicast groups ip l # Shows network cards ifdown eth0 # Brings down interface ifup eth0 # Brings up interface Securing Access to your Server \u00b6 # Temporary disables logins for users other than root. Just the existence of this file will prevent user logins and is controlled via PAM(Pluggable Authentication Modules) /etc/nologin # Create a blank file, but if the user tries to login via ssh username@localhost, the connection will be immediately closed. Only root can access the server then. touch /etc/nologin rm /etc/nologin # Now users can login. This can be used as a temporary measure cd /etc/pam.d/ # Config files for authentication modules grep nologin * # Shows instances where nologin file exists last # shows last user activity present in /var/log/wtmp file. lastlog # List the last login time for each user lastlog | grep \u2013v Never # Reverse the grep search to check for all user logins ulimit \u00b6 Puts restrictions on system resources ulimit \u2013a # Shows system limitations that can be applied ulimit \u2013u 8000 # -u is for avoiding fork bombs and is defaulted to 4096. A std user can set a new value to 8000. # It will remain for his profile till system restart. cd /etc/security # To set the limits which can be persisted even after restart cat limits.conf # Shows soft (can be changed by processes) and hard (can\u2019t be changed by processes) limits cd limits.d/ # ls to see the files inside this directory. Edit the file and add an entry for the user account with soft or hard limits and save the file. Avoiding fork bombs \u00b6 They are a potential Denial of Service Attack ulimit \u2013u ##### Do not run on Production Machines. Test only in laptop ##### ps \u2013u username | wc -l # Shows the number of processes running under the user and gives the count # On the command line create a function called foo foo (){ echo hello } # Execute the function by just calling it and pressing enter foo # Similarly to execute a fork bomb, instead of foo, call it : : (){ : | : & # Here the function calls itself and pipes the output to itself } ; : # End the function with a semicolon and then call the function : # BEWARE: Do this only at your own risk. Ensure ulimit is set to protect the resources on the server # xinetd \u00b6 Called the super daemon as it can manage a lot of smaller services, secures access to your server /etc/xinetd.d /etc/xinetd.conf tftp server (Trivial file transfer protocol) # Sample configuration service tftp { socket_type = dgram # data gram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /tftpboot # root directory of the server disable = no } Implementing TFTP using xinetd.d service \u00b6 ls /etc/xinetd.d/ # blank directory yum install tftp-server tftp # install client and server, also xinetd as it\u2019s a dependency vi /etc/xinetd.d/tftp # after installation, delete disable line from the configuration to enable tftp # if the server directory doent exist, create it mkdir \u2013p /var/lib/tftpboot systemctl enable xinetd systemctl start xinetd netstat \u2013aulpe | grep tftp # shows the port # As a root user, create a temp file inside var/lib/tftpboot directory with hello text vi var/lib/tftpboot/file1 # Logout and login as standard user. Use TFTP to transfer the file to standard user tftp 127 .0.0.1 # Press enter get file1 # Get the file1 created by root using tftp quit # At the same file location do a cat to see the contents of the file cat file1 TCP Wrappers \u00b6 Alternative to firewalling on the server # To check if service supports TCP wrappers. Once we can determine this, we can use hosts.allow or hosts.deny ldd </path to service name> | grep libwrap # To set this up for a service, 2 entries are made # This in /etc/hosts.allow file to allow access to 0.3 IP which is raspberry pi. tftpd is the name of the binary in .tftpd : 192 .168.0.3 # This in /etc/hosts.deny file to deny access to all other IP except for 0.3 IP in .tftpd : ALL # If the client appears in both files then allow takes precedence and access is granted ldd /usr/sbin/xinetd | grep libwrap tftp 192 .168.0.240 # login from raspberry pi and access the remote server, disable your firewall before trying this. Delegating Admin rights using sudoers \u00b6 id # check if the user is already an admin and part of wheel group in centos cd /etc grep wheel sudoers # check the current setup visudo # to edit sudoers file %wheel ALL =( root ) ALL # Uncomment the line for %wheel, change (ALL) to (root), so only root can change the sudoers. Save and exit Data Encryption \u00b6 Using GPG to encrypt data between users \u00b6 password = $( mkpassword \u2013m sha-512 Password1 ) # encrypt the password using sha and mkpassword and put in a variable echo $password for u in marta ivan ; do # take an input from marta and ivan for users sudo useradd \u2013m \u2013p $password $u # add the user and set the encrypted password done # Run the command and create 2 users ivan and marta # Install GPG if not present dkpg \u2013S $( which gpg ) # Login as ivan and generate private and public keys for gpg encryption su - ivan gpg --gen-key # Take the default settings gpg --list-key # List the keys gpg --export \u2013a <email from gpg gen-key step> > /tmp/ivankey # export the public key and place in tmp folder for marta to access exit su - marta vi secret.txt # create a plain txt file for encryption chmod 400 secret.txt # make the file writeable by marta only gpg --import /tmp/ivankey # import ivan public key gpg \u2013e \u2013r <ivan mailid> secret.txt # add the recipient and encrypt the file mv secret.txt.gpg /tmp/ # move the encrypted file to tmp and exit su \u2013 ivan gpg \u2013d /tmp/secret.txt.gpg # decrypt the file and enter the passphrase for the private key Implementing LUKS for Full Disk Encryption \u00b6 example Laptops, USB sudo apt-get install cryptsetup # Install LUKS tools lsblk # list the blocks sudo fdisk /dev/sdb # partition the sdb drive and create a new partition called sdb1 sudo cryptsetup \u2013y luksFormat /dev/sdb1 # Allows to store encrypted data in this partition. Give a passphrase sudo cryptsetup luksDump /dev/sdb1 # Shows details of the encrypted partition sudo cryptsetup luksOpen /dev/sdb1 private_data # Assign a mapper called private_data ls /dev/mapper # Shows the new mapper setup and that is a device sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted EFS for data encryption \u00b6 sudo apt-get install ecryptfs-utils # Install EFS tools su \u2013 ivan # Login as std user ecryptfs-setup-private # Create a private key with passphrase. logout and login back as ivan ls # Private directory is created by EFS echo hello > Private/data # Write to private directory cat Private/data # ivan can see the data ecryptfs-umount-private # unmount the private directory ls # Now ivan cannot see the data cd /.Private/ # Go to the ivan\u2019s hidden Private folder with a dot ls # You can see the encrypted data file ecryptfs-mount-private # Mount the directory again Compiling software from source \u00b6 Install gcc compiler sudo apt-get install gcc make # go to gnu.org/software/Downloads -> coreutils -> file ending with .xz # tar -xvJf coreutils-8.28.tar.xz # J is for xz, j is for bzip2 # cd into src folder and select ls.c file # Add a line in main function printf(\"Hello World\\n\"); # After changes, change directory one level up and run the configure script bash configure # This will check for any configuration changes to the src files and update the make file # Excute the make command make # After binary code is compiled, it needs to be updated in OS sudo make install # Close the terminal and restart. New software is working ;)","title":"Linux"},{"location":"learning/linux/linux/#introduction","text":"Understanding Linux Filesystem Templates folder CronTab Guru","title":"Introduction"},{"location":"learning/linux/linux/#basic-commands","text":"Important Commands Terminal Ctrl+Alt+T # Open the terminal Ctrl+D # Close the terminal exit # Close the terminal Ctrl + L # Clear the screen, but will keep the current command Ctrl + Shift + # Increases font size of the terminal Utility cal # Calendar current month cal -3 # Current -1, Current , Current +1 month cal 5 1967 # Format is (Month and Year). Gives May 1967 date # Current date in BST (default) date -u # Current date in UTC date --date \u201c30 days\u201d # Gives current date + 30 days (future date) date --date \u201c30 days ago\u201d # Gives current date \u2013 30 days (past date) which echo # Shows where the command is stored in PATH hostname -I # Gives IP address echo $? # Gives the output 0/1 value stored after a command is run wc \u2013l file1 # Gives line count in file1 wc file1 # Give word count in file1 History history # List all the commands executed !! # Run the previous command !50 # Run the command that is on line 50 of history output history \u2013c ; history \u2013w ; # Clears history and writes back to the file Ctrl + r # reverse searches for your input. Press Esc to edit the matched command man Using the Manual There are 8 sections in the manual. Important are 1, 5 and 8 sections man \u2013k <search term> # Search the manual for pages matching <search term>. man -k tmux # example of searching for tmux in the manual pages man -k \"list directory contents\" # Double quote seraches complete words man 1 tmux # Opens section 1 of tmux manual page, 1 is default and can be ignored man ls # Shows section 1 of ls command help cd # Shows the help pages if man pages are not present Redirection of Streams echo \"Hello\" 1 > output.txt # Standard output is redirected to output.txt echo \"Hello\" > output.txt # Standard output is default echo \"World\" 1 >> output.txt # Standard output is appended to output.txt echo \"Error\" 2 > error.txt # Standard error is redirected to error.txt cat -k bla 2 >> error.txt # Program error is redirected and appended to error.txt echo \"Hello World\" 1 >> output.txt 2 >> error.txt # Use both std output and error cat 0 < input.txt # Standard input is read from a file and sent to cat command cat < input.txt # Standard input is default cat 0 < input.txt 1 >> output.txt 2 >> error.txt # Use all 3 data streams cat -k bla 1 >> output.txt 2 > & 1 # Redirect Standard error to standard output stream and write to file Redirection to Terminals tty # Current terminal connected to Linux, gives path cat < input.txt > /dev/pts/1 # In another terminal, Standard input is read from a file and sent to tty 1 terminal Ctrl + Alt + F1 / chvt 1 # Goes to physical terminal with no graphics. Similarly you can change to 2 to 6 tty terminals. Ctrl + Alt + F7 / chvt 7 # Comes back to Graphical terminal Piping date | cut --delimiter \" \" --fields 1 # Output of date is input to cut command - Tee command - Used to store intermediate output in a file and then stream passed horizontally through the pipeline - tee command takes a snapshot of the standard output and then passes it along date > date.txt | cut --delimiter \" \" --fields 1 # Output will not work and date will only be stored in file and not passed to cut command date | tee date.txt | cut --delimiter \" \" --fields 1 # Output of date is first stored in file, then passed to cut command for display to Standard Output date | tee date.txt | cut --delimiter \" \" --fields 1 | tee today.txt cat file1.txt file2.txt | tee unsorted.txt | sort -r > reversed.txt # Output chaining and storing intermediate data in files - XARGS command (Powerful pipeline command) - Allows piped data into command line arguments - date | echo # Output of date is passed to echo, but echo doesn't accept standard input, only commandline arguments date | xargs echo # xargs will convert standard output into command line arguments date | cut --delimiter \" \" --fields 1 | xargs echo # Prints the day of the week Alias Used to store reusable scripts in .bash_aliases file can be used in scripts alias # Shows all the alias setup for the user # Store an alias in the `.bash_aliases` file alias calmagic = 'xargs cal -A 1 -B 1 > /home/leslie/calOutput.txt' # In the terminal use if in a pipe command, STDOUT will be stored in a file echo \"12 2021\" | calmagic File System Navigation # File Listing pwd # Prints absolute path of current working directory(CWD) ls \u2013l # Long list of CWD ls \u2013a # Shows all files including hidden ls -F # Shows directories as ending with / along with other files stat <filename> # Detailed file information file <filename> # File type ls -ld # Detailed folder information # Change Directories cd - # Helps to switch directories. Like a Toggle (Alt + Tab) in windows cd / cd ~ # User Home directory from anywhere cd .. # Back to parent directory of CWD Wildcards Wildcards and How to use - The star wildcard has the broadest meaning of any of the wildcards, as it can represent zero characters, all single characters or any string. - The question mark (?) is used as a wildcard character in shell commands to represent exactly one character, which can be any single character. - The square wildcard can represent any of the characters enclosed in the brackets. ls *.txt # Matches all txt files ls ???.txt # Matches all 3 letter txt files ls file [ 123 ] .txt # Matches all files ending with 1 to 3 ls file [ A-Z ] .txt # Matches all files ending with A to Z ls file [ 0 -9 ][ A-Z ] .txt # Matches all files ending with 0A to 9Z File and Folders # Create Operations touch file1 # Creates a new file1 echo \"Hello\" > hello.txt # Creates and writes using redirection # -p is parent directory which is data and inside that 2 directories called sales & mkt is created mkdir \u2013p /data/ { sales,mkt } # Brace exapansion will allow to create folders. Sequence can be expressed as .. mkdir -p /tmp/ { jan,feb,mar } _ { 2020 ..2023 } # Brace expansion for files, it will create 10 files inside each folder touch { jan,feb,mar } _ { 2020 ..2023 } /file { 1 ..10 } # Delete Operations rm file1 # Deletes file1 rm *.txt # Deletes all txt files # Deletes all files and folders inside the main folder and the main folder as well # CAUTION: Use the recursive option with care rm -r /tmp/ { jan,feb,mar } _ { 2020 ..2023 } / # Deletes only empty directories rmdir /tmp/ # Skips folders which have files # Copy Operations cp /data/sales/file1 /data/mkt/file1 cp /data/sales/* . # Copy all files to CWD cp -r /data/sales /data/backup # Copy folder to backup folder # Move and Rename Operations mv file1 file2 # Rename file in the same folder mv /data/mkt/ /data/hr # Rename folder, Note the slash after first folder mv /data/sales/* /tmp/backup/ # Move files to new location mv /data/mkt/ /tmp/newFolder # Move and rename the folder Nano - Editing M Key can be Alt or Cmd depending on keyboard layout Enable spell checking on nano by editing /etc/nanorc and uncomment set speller in the file. Ctrl + O # Write data out to file Ctrl + R # Copy contents of one file into another Ctrl + K # Cuts entire line, also used as a delete Alt + 6 # Copy entire line Ctrl + U # Paste the line Ctrl + T # Spell check the file Alt + U # Undo changes Alt + E # Redo changes # File operations in vi > filename # Empties an existing file :x # Saves file changes instead of :wq Search Files find ~/projects # Find matches of files and folders from projects and below find . # Find from CWD and below find . -maxdepth 1 # Find from CWD and one level below find . -type f # Find files only find . -type d # Find folder only find . -maxdepth 1 -type d # Find folder only and one level below find . -name \"*.txt\" # Find files ending with matching patterns find . -maxdepth 3 -iname \"*.TXT\" # Find files with case insensitive matching patterns find . -type f -size +100k # Find files greater than 100 Kb # Find files greater than 100 Kb AND less than 5 Mb and count them find . -type f -size +100k -size -5M | wc -l # Find files less than 100 Kb OR greater than 5 Mb and count them find . -type f -size -100k -o -size +5M | wc -l - Find and Execute commands # Find and copy files to backup folder. `\\;` denotes end of exec command find . -type f -size +100k -size +5M -exec cp {} ~/Desktop/backup \\; ### # Find file called needle.txt inside haystack folder ### # Create 100 folders and inside each folder 100 files mkdir -p haystack/folder { 1 ..100 } touch haystack/folder { 1 ..100 } /file { 1 ..100 } # Create file in one random folder touch haystack/folder $( shuf -i 1 -100 -n 1 ) /needle.txt ### # Finding the file using name find haystack/ -type f -name \"needle.txt\" # Move the file to haystack folder find haystack/ -type f -name \"needle.txt\" -exec mv {} ~/tmp/haystack \\; ### View/Read File Contents cat cat file1 file2 > file3 # Concatenate 2 files and write into file3 cat \u2013vet file3 # displays special characters in the file e.g. EOL as $. Useful if sh files are created in windows tac - Flips the file contents vertically tac file3 # Reads the file in reverse rev - Reverses the contents of each line rev file3 # Reads the line in reverse less - Allows to page through big files less file3 # Shows one page at a time. Use Arrow keys to scroll # Output of find piped to less command for scrolling find . -type f -name \"*.txt\" | less head - Shows limited lines from top of output cat file3 | head -n 3 # Shows first 3 lines tail - Shows limited lines from bottom of output cat file3 | tail -n 3 # Shows last 3 lines tail \u2013f /var/log/messages # follows the file and continuously shows the 10 lines Sort sort words.txt > sorted.txt # Sorts in Asc order and redirects to sorted.txt sort -r word.txt > reverse.txt # Sorts in Des order sort -n numbers.txt # Sorts in Asc numeric order based on digit placement sort -nr numbers.txt # Sorts in Des numeric order based on digit placement sort -u numbers0-9.txt # Sorts and shows only unique values - Sorting data in tabular format # Sort on the basis of file size (5th column and its numeric) ls -l /etc | head -n 20 | sort -k 5n # Reverse (r) the output showing largest files first ls -l /etc | head -n 20 | sort -k 5nr # Sort on the basis of largest file size in human readable format ls -lh /etc | head -n 20 | sort -k 5hr # Sort on the basis of month ls -lh /etc | head -n 20 | sort -k 6M Search data - grep grep is case-sensitive search command # grep <search term> file-name grep e words.txt # Shows matching lines as STDOUT grep -c e words.txt # Counts the matching lines # Search in case insensitive manner grep -i gadsby gadsby_manuscript.txt # Search strings using quotes grep -ci \"our boys\" gadsby_manuscript.txt # Invert the search grep -v \"our boys\" gadsby_manuscript.txt # Searches for server and not servers. \\b is the word boundary grep \u2018 \\b server \\b \u2019/etc/ntp.conf # Searches for server beginning in the line. \\b is the word boundary grep \u2018^server \\b \u2019/etc/ntp.conf Filter data using grep ls -lF / | grep opt # Shows details for opt folder only ls -F /etc | grep -v / # Shows only files in etc folder - Remove Commented and Blank Lines # Empty lines can be shown as ^$. \u2013v reverses our search and \u2013e allows more than one expression. O/p is sent to std o/p grep \u2013ve \u2018^#\u2019 \u2013ve\u2019^$\u2019 /etc/ntp.conf # -v ^# says I don\u2019t want to see lines starting with #. ^$ says I don\u2019t want to see lines that begin with EOL marker Archival and Compression Two step process: Create the tar ball, then compress the tar Compression tool comparison # Create the tar ball tar -cvf backup.tar file [ 1 -3 ] .txt # Create, Verbose, Files to archive tar -tf backup.tar # Test for tar file without unzipping # Compress the tar ball ## 3 compression tools - gzip -> bzip2 -> xz (Compression and time increases from left to right) gzip backup.tar # Compresses the tar ball and adds .gz extension to tar ball gunzip backup.tar.gz # Decompress the gzip ### bzip2 backup.tar # Smaller file size than gzip and adds .bz2 extension to tar ball bunzip2 backup.tar.bz2 # Decompress the bzip. Best used for larger file sizes # Open the tar ball contents tar -xvf backup.tar # Extract, Verbose, Files to unarchive ### Create tar and compress in single command # Adding the z option for gzip and renaming the tar as .gz tar -cvzf backup.tar.gz file [ 1 -3 ] .txt tar -xvzf backup.tar.gz # Adding the j option for bzip2 and renaming the tar as .bz2 tar -cvjf backup.tar.bz2 file [ 1 -3 ] .txt tar -xvjf backup.tar.bz2 ### BASH #!/bin/bash # First line in the script \"SHEBANG\" tells type of script ### # To create an executable script, create a `bin` folder in your home. # Move all utility shell scripts to bin. Also remove .sh file extenstions # Make the file as executable `chmod +x data_backup` # Add the `~/bin` to the PATH variable # Edit `.bashrc` with PATH=\"$PATH:$HOME/bin\" # Now all scripts in bin folder are executable from command line ### # Set and unset variables export $VARIABLE # Sets the variable unset VARIABLE # Removes the variable, NOTE \u2013 No $ in variable Cron Scheduling crontab -e < select editor> # Opens the template crontab # Multiple options for each column of crontab using comma. # SPACE is used to delimit the columns of crontab # */<value> can divide the time intervals ### # min hours \"day of month\" month \"day of week (0-6)\" ### * * * * * bash ~/data_backup.sh Package Management apt-cache search docx # Searches apt for programs that can work with MS Word apt-cache show <package> | less # Gives software information # Apt cache information resides in /var/lib/apt/lists sudo apt-get update # Updates the apt lists sudo apt-get upgrade # Upgraded to the latest software versions from the list sudo apt-get install <pkg-name> # Install package sudo apt-get purge <pkg-name> # Remove & uninstall package. Recommended approach sudo apt-get autoremove # Removes any installed package dependecies # Package compressed archives are stored in `/var/cache/apt/archives` sudo apt-get clean # Removes all package compressed acrhives sudo apt-get autoclean # Removes only package compressed acrhives that cannot be downloaded - Source Code for apps OS uname # Shows kernal uname -o # Shows OS uname -m # Shows computer architecture x86_64 (64 bit), x86 (32 bit) lsb_release -a # Distro version Misc fdisk -l # Gives device wise memory details free / free -m # Gives amount of free memory lsblk # Lists all partitions swapon \u2013s # List all swap files ps # Process id of the current bash shell shutdown \u2013h now / poweroff / init 0 # Power downs the system restart / init 6 / reboot # Restarts the system shutdown \u2013r + 1 \u201cWe are restarting\u201d # Restarts the system give all logged in users 1 min to shut down all process su - # Login to root id / id bob # Shows the current user and group id sudo -i # Interactive shell for password of the current user, to get elevated access ssh localhost # ssh connection to same server. Type exit or Ctrl + D to logout of ssh. who / w # Gives the list of terminals that are connected and who has logged on to the server Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal","title":"Basic commands"},{"location":"learning/linux/linux/#tips","text":"Alt + F2 Gives the run command terminal and then type gnome-system-monitor is like task Manger in windows. Gives graphical overview of the system and can kill processes. Putting & after any commands runs it in the background. Run jobs to see all the background running jobs. Type fg to bring the background running jobs to the foreground. Ctrl + C to cancel the job then. !<anycharacter> will search for the last command in history starting with that character !?etc executes the last command that contains etc","title":"Tips"},{"location":"learning/linux/linux/#filesystem","text":"","title":"Filesystem"},{"location":"learning/linux/linux/#creating-partitions","text":"fdisk or gdisk utility to partition. If Mountpoint is /, that is primary partition.","title":"Creating Partitions"},{"location":"learning/linux/linux/#creating-filesystems","text":"mkfs.ext4 \u2013b 4096 /dev/sdb1 # Creates 4MB block size file system mkfs.xfs \u2013b size = 64k /dev/sdb2 # Creates 64k block size file system. Xfs is specialized filesystem","title":"Creating Filesystems"},{"location":"learning/linux/linux/#mounting-data","text":"mkdir \u2013p /data/ { sales,mkt } mount /dev/sdb1 /data/sales # Mounts device to data/sales directory mount /dev/sdb2 /data/mkt","title":"Mounting Data"},{"location":"learning/linux/linux/#unmounting-data","text":"umount /dev/sdb1 or umount /dev/sdb { 1 ,2 } # Unmounts both the devices","title":"Unmounting Data"},{"location":"learning/linux/linux/#virtual-memory-or-swap-filesystem","text":"They are temporary space requirements Virtual memory in Linux can be a Disk Partition or Swap file. Use gdisk to create swap filesystem. Option L and then hex code 8200. To make the swap filesystem permanent, make an entry in /etc/fstab file, so changes are persistent even after system reboot. partprobe /dev/sdb # Sync saved partition in memory. Or it requires system reboot mkswap /dev/sdb3 # Select the right swap device to create the filesystem swapon /dev/sdb3 # Mount the filesystem","title":"Virtual Memory or Swap Filesystem"},{"location":"learning/linux/linux/#troubleshooting-linux-filesystem","text":"df \u2013hT # list all filesystem with space details du \u2013hs /etc # gives diskusage of etc directory with memory dumpe2fs /dev/sdb1 | less # human readable details for the device dd if = /dev/sda of = /data/sales/file count = 1 bs = 512 # takes data backup of sda to sales/file of the first 512 bytes dd if = /data/sales/file of = /dev/sda # copies the data back in case of recovery tar \u2013cvf /data/sales/etc.tar /etc # backs up etc directory by creating a tar file umount /dev/sdb1 # unmounts sales directory tune2fs \u2013L \u201cDATA\u201d /dev/sdb1 # adding label to the file system debugfs /dev/sdb1 # enters debug of sdb1 directory. Type quit to exit","title":"Troubleshooting Linux filesystem"},{"location":"learning/linux/linux/#file-permissions","text":"# Format for file permission : User-Group-Others # Symbolic Notation (Default permission) RWX \u2013 RW - R # Octal Notation 7 - 6 - 4 # So RWX is 111 i.e. 7, RW is 110 i.e. 6 and R is 100 i.e. 4 umask 2 # sets default permission to all the files in the directory chmod 777 file1 # Changes permission for a file1 chmod u = rwx,g = rw,o = rw file 2 # Verbose way to set permissions chmod +rx file3 # Sets read & write for User, group and others ls \u2013ld /data # Shows permission for a single directory chgrp users /data # Adds users group to the directory - Even if user does not have write access to a file, he has delete / add file access to a directory. - chmod o+t /data .Users can delete only their files and not other\u2019s. Root will not be able to delete files in this directory. - This permission is sent on the /tmp directory by default at installation. So only user\u2019s own file can be deleted, not of others.","title":"File Permissions"},{"location":"learning/linux/linux/#links-hard-and-soft-links","text":"Soft links are also called as Symbolic Links or symlinks . Here one file will be a pointer to the other file. If file has more than one name, it\u2019s called hard link. To find the number of sub directories , use stat dirname . Links number -2 is the total number of sub directories. Each directory has a minimum of 2 links, hence subtract 2. ln file2 file5 # Creates hard link between file2 and file5. # Shows the inode number which is same i.e. the same metadata is present for both. Cat on both the files shows the same data content ls \u2013li file2 file5 ln \u2013s file3 file4 # Creates a symlink between file 3 and file5. Cat on both the files shows the same data content ls \u2013li file3 file4 # Shows the symlink, but they are different files. Inode number is different. readlink file5 # shows where the link is","title":"Links (Hard and Soft Links)"},{"location":"learning/linux/linux/#applying-quotas","text":"Quotas can be applied to Space/inodes, Group, User or File System. repquota \u2013auv # Give quota report per user space usage along with limits quotaon /dev/sdb1 # Checks quota limit # enable quotas and edit the hard and soft limits. Soft limit can be exceeded for 7 days, after which it is enforced. edquota \u2013u <username> # enables quota via command line. Soft limit is 21000 is 21MB, hard limit is 26MB setquota \u2013u <username> 21000 26000 0 0 /dev/sdb1","title":"Applying Quotas"},{"location":"learning/linux/linux/#directory-listing-and-alias","text":"ls \u2013F /dir1 # shows directory with a / and symlink as @ at the end of the name ls \u2013-color = auto /dir1 # shows the same file types in color alias ls = \u2019ls \u2013-color = auto\u2019 # creates an alias for ls with color ls \u2013lh file1 # list in human readable format ls \u2013lt /etc # shows long listing with time modified in descending order ls \u2013ltr /etc | less # shows reverse listing, q to quit","title":"Directory Listing and Alias"},{"location":"learning/linux/linux/#synchronize-directories","text":"mkdir /backup rsync \u2013av /home/ /backup/ # archive home dir to backup dir. / after home and backup is important rsync \u2013av --delete /home/ /backup/ # sync deletions of data as well, otherwise rsync ignores it by default rsync \u2013ave ssh # sync data between servers using e option","title":"Synchronize Directories"},{"location":"learning/linux/linux/#process-management","text":"","title":"Process Management"},{"location":"learning/linux/linux/#monitor-process","text":"which ps # shows the installation directory for ps uptime # shows the uptime of the system along with the load average in the range of 1 min, 5 mins and 15 mins # Rule of Thumb for uptime --> Load average for single core value should be less than 1, for dual core less than 2 etc. which uptime # shows the installation directory for uptime cat /proc/uptime # shows uptime and idle time cat /proc/loadavg # shows load avg for 1,5 and 15 mins, active process running/total process, last process id that was issued","title":"Monitor Process"},{"location":"learning/linux/linux/#jobs","text":"sleep 180 # sleeps for 180 secs in foreground. Ctrl + Z to pause the job. Run bg to put the sleep command in background. jobs # shows running jobs fg 1 # puts the sleep command in foreground","title":"Jobs"},{"location":"learning/linux/linux/#managing-processes","text":"ps to display processes and kill to send signals. pgrep, pkill and killall are great shortcuts. The default kill signal is -15 which can also be written as \u2013term or \u2013sigterm . To really kill it is -9, -kill or \u2013sigkill . echo $$ # Shows current process ps \u2013l # long listing with the process ps \u2013ef # shows all the processes for all users ps \u2013eaf | grep processname pgrep nginx # shows process ids for nginx sleep 900 & pkill sleep # searches for sleep process and kills it killall sleep # searches for all running sleep process and kills it kill \u2013l # shows the multiple kill signals available kill -9 <process id> # forcefully terminates the process, also use kill \u2013kill <process id> top \u2192 kill, renice, sort and display processes Running top, you can toggle between the information displayed at the top lines. l \u2013 on/off load, t \u2013 on/off tasks, m \u2013 on/off memory Sorting of top is on %CPU, f \u2013 shows current fields being shown on output of top. Select the new field to sort and type s Type r for renice and put in the process id. Esc and Enter to quit the shell Type k for kill and put in the process id. Esc and Enter to quit the shell q to quit out of top top # shows all running processes, q to quit top \u2013n 1 # shows the running processes for 1 capture and quits top \u2013n 2 \u2013d 3 # shows 2 captures with a delay of 3 seconds and quits","title":"Managing Processes"},{"location":"learning/linux/linux/#editors","text":"","title":"Editors"},{"location":"learning/linux/linux/#vi","text":": # Last line mode q, q! # quit the file x, wq, wq! # save and exit the file i, I # insert from cursor position, I for inserting from start of the line a, A # append after the cursor, A for append from last character in the line o, O # insert line below the cursor, O for above the current cursor position dd # delete the line u # undo the changes","title":"Vi"},{"location":"learning/linux/linux/#line-navigation","text":"<Linenumber>G # e.g. 7G, takes cursor to 7th line in the file G # only G takes cursor to end of file w , b # w takes cursor to next word, b takes cursor to one word before ^ , $ # ^takes cursor to start of line, $ to end of the line vi +127 /etc/file1 # opens the file and takes cursor to 127th line vi +/Document /etc/file1.conf # opens the file and takes cursor to first occurrence of \u201cDocument\u201d set number / set nonumber # from last line mode, it will show and stop line number display syntax on # highlighting on, e.g. xml highlighting etc.","title":"Line Navigation"},{"location":"learning/linux/linux/#read-and-write","text":"r /etc/hosts # Open an existing file, use : and then you can get content from hosts file into current file w newfile # :, it will copy entire file contents into newfile in the same directory 3 ,7w newfile # it will copy line 3 to 7 into newfile","title":"Read and Write"},{"location":"learning/linux/linux/#search-and-replace","text":"%s/Hi/Hello # Open an existing file, use : and you can search Hi and Replace with Hello. %s signifies entire document search /Hello # searches for Hello in the document. Type n to get next occurrence, N will take cursor in reverse 1 ,20s/Hi/Hello # searches for 1st 20 lines for Hi and replaces with Hello 14 ,20s/^/ / # from 14th to 20th line, it will add 3 spaces from the start of the line, just like Tab","title":"Search and Replace"},{"location":"learning/linux/linux/#bash-scripting","text":"","title":"BASH Scripting"},{"location":"learning/linux/linux/#understanding-variables","text":"Local variables \u2192 accessible only to the current shell, FRUIT=\u2019apple\u2019, echo $FRUIT Global variables \u2192 you need to set and then export it to make it global. export FRUIT=\u2019apple\u2019","title":"Understanding Variables"},{"location":"learning/linux/linux/#simple-script","text":"vi hello.sh #!/bin/bash # Path to the interpreter echo \u201cHello World\u201d exit 0 # return code, :wq chmod +x hello.sh hello.sh # execute the script as it\u2019s in the home directory /user/bin","title":"Simple Script"},{"location":"learning/linux/linux/#getting-user-input","text":"vi hello.sh #!/bin/bash echo \u2013e \u201cEnter your name: \\c \u201d # -e is the escape sequence, -c is for the prompt read INPUT_NAME # read the input data into a variable echo \u201cHello $INPUT_NAME \u201d exit 0","title":"Getting user input"},{"location":"learning/linux/linux/#user-input-types","text":"$1 $2 # $1 is the 1st input parameter, 2nd Parameter and so on. $0 # is the script name itself $# # count of input parameters $* # is collection of all the arguments","title":"User Input types"},{"location":"learning/linux/linux/#multiple-inputs-using-positional-parameters","text":"vi hello.sh #!/bin/bash echo \u201cHello $1 $2 \u201d # $1 is the 1st input parameter, $0 is the script name itself, $2 is the 2nd input parameter and so on exit 0","title":"Multiple inputs using positional parameters"},{"location":"learning/linux/linux/#code-snippets","text":"Gedit \u2192 Gnome Editor \u2192 Add the Snippet Plugin (Applications \u2192 Accessories \u2192 gedit. Preferences in gedit tab \u2192 Plugins enable Snippet Plugin and restart gedit)","title":"Code Snippets"},{"location":"learning/linux/linux/#conditional-statement---if","text":"if [[condition]] \u2192 testing for string condition if ((condition)) \u2192 testing for numeric condition e.g. if (( $# < 1 )) \u2192 if count of input parameter vi hello.sh #!/bin/bash if (( $# 1 )) then echo \u201cUsage: $0 <name>\u201d exit 1 fi echo \u201cHello $1 $2 \u201d exit 0","title":"Conditional Statement - IF"},{"location":"learning/linux/linux/#case-statement","text":"vi hello.sh #!/bin/bash if [[ ! \u2013d $1 ]] # if the 1st argument is not a directory then echo \u201cUsage: $0 <directory>\u201d exit 1 fi case $2 in \u201cdirectory\u201d ) find $1 \u2013maxdepth 1 \u2013type d ;; # break \u201clink\u201d ) find $1 \u2013maxdepth 1 \u2013type l ;; # break * ) # default statement echo \u201cUsage: $0 <directory> directory | link\u201d ;; esac exit 0","title":"Case Statement"},{"location":"learning/linux/linux/#for","text":"vi hello.sh #!/bin/bash for u in $* # $* is collection of arguments, u is temporary variable do # do block useradd $u # access to temp variable is via $ echo Password1 | passwd \u2013stdin $u # use the passwd command and get the user input from keyboard passwd \u2013e $u # expire the password, so they can change it at first login done echo \u201cFinished\u201d # at time of execution ./hello.sh fred mary john vi listsize.sh #!/bin/bash for file in $( ls ) # for each file, in the output of ls do [[ ! \u2013f ]] && continue # not a file then continue to next # use the stats to get statistics of the file, to get the last accessed date and then format the date LA = $( stat \u2013c %x $file | cut \u2013d \u201c \u201d \u2013f1 ) echo \u201c $file is $( du \u2013b $file ) bytes and was last accessed on $LA \u201d # use du to get file size done","title":"For"},{"location":"learning/linux/linux/#while","text":"vi loop.sh #!/bin/bash -x # -x is for debug mode COUNT = 10 while (( COUNT > 0 )) do echo \u2013e \u201c $COUNT \\c \u201d # \\c will suppress the line feed (enter) sleep 1 (( COUNT -- )) # round brackets to avoid using $ symbol done - Use the until when you want to stop the loop when the condition becomes true.","title":"While"},{"location":"learning/linux/linux/#user-management","text":"Managing Users: User Lifecycle ==> useradd, usermod, userdel Local databases ==> /etc/passwd, /etc/shadow (encrypted) passwd (to set the password) pwconv (move pass to encrypted) pwunconv (move back to unencrypted) # /etc/passwd file structure # It has 7 filed separated by : Login Name, Optional encrypted password or \u201cx\u201d, Numerical UID, Numerical GID, Username or comment, User home directory, Optional command interpreter # /etc/shadow file structure where the actual passwords are stored # It has 8 filed separated by : Login Name, encrypted password ( if it begins with ! the account is locked ) , Date of last password change, Minimum password age, Maximum password age, Password warning period, password inactivity, account expiry date # /etc/login.defs The password ageing defaults can be configured with this file useradd \u2013D # shows the default settings for a user that is added cat /etc/default/useradd # shows where the defaults are set useradd bob # only adds the user, no home directory is created. Once the user logs in, it will get created tail -3 /etc/passwd # shows that bob is added useradd \u2013m bob # also creates the home directory tail -3 /etc/shadow # shows the password for the user passwd bob # add the password for bob passwd \u2013l bob # locks the account passwd \u2013u bob # unlocks the account usermod bob \u2013c \u201cBob Smith\u201d # adding additional details for the user userdel \u2013r bob # removes the user and home directory","title":"User Management"},{"location":"learning/linux/linux/#group-management","text":"Group Lifecycle \u2192 groupadd, groupmod, groupdel Local databases \u2192 /etc/group, /etc/gshadow (encrypted) gpasswd (to set password) newgrp (switch to new groups) # /etc/group structure # It has 4 fields: Group Name, Password, Numerical GID, User list that is comma separated # /etc/gshadow structure # It has 4 field: Group Name, Encrypted password, Admin list that is comma separated, # this can be managed used \u2013A cmd # Members, this can be managed using the \u2013M cmd Private groups are enabled by default. The user added is also added to the same group. If this is disabled, users will belong to the groups users. Use useradd \u2013N to overwrite private groups. This can be enabled or disabled by setting USERGROUPS_ENAB in /etc/login.defs useradd \u2013m \u2013g users jim # -g is Primary Group, -G is secondary groups. Secondary groups are more traditional groups id jim usermod \u2013G sudo,adm jim # added jim to secondary groups sudo, adm useradd \u2013N \u2013m sally # adds sally to the default group gpasswd \u2013M jim,sally sudo # adds 2 users to sudo group groupadd sales gpasswd sales # sets the new password for sales newgrp sales # add the user to the sales group temporarily. If the user logs out, he is removed from the group","title":"Group Management"},{"location":"learning/linux/linux/#automate-system-tasks","text":"Regular Tasks \u2192 cron (more than once a day but misses job if turned off), anacron (run jobs missed on startup but jobs can run just once a day) Once Off \u2192 at (runs at specified time and date), batch (runs when load average drops below 0.8)","title":"Automate System Tasks"},{"location":"learning/linux/linux/#system-cron-jobs","text":"# /etc/crontab, /etc/cron.d # cron files # /etc/cron.<time> # where time is hourly, daily, weekly and monthly, contains scripts that need to be executed # Adding a system cron job cd /etc/cron.d vi daily-backup # add a new file 30 20 * * 1 -5 root /root/back.sh # run back.sh from Mon to Fri at 20:30 # Adding a user cron job crontab \u2013e # edit the user crontab file */10 10 1 1 1 tail /etc/passwd # Runs once on 1st day if it\u2019s a Mon of Jan, at 10 am for every 10 mins crontab \u2013l # list all cron jobs crontab \u2013r # remove the cron job # anacron: /etc/anacrontab structure # It has 4 fields Period in days or macro ( Daily, Monthly ) , Delay ( minutes after system startup for job to run ) , Job Identifier ( used to name timestamp file indicating when job was last run ) Command ( that needs to be executed ) @weekly 120 weekly-backup ls /etc // weekly, 120 mins after startup it will run weekly-backup","title":"System Cron Jobs"},{"location":"learning/linux/linux/#batch","text":"at and batch commands at noon tomorrow # Enter the command line, Ctrl + D to save at> ls /etc # enter the command that needs to be executed atq # shows the jobs queue atrm # remove the job batch # Enter the command line, Ctrl + D to save at> ls /etc > /root/file1 # redirect the o/p to file1. It will run if the system load avg is less than 0.8.","title":"Batch"},{"location":"learning/linux/linux/#security-for-cron","text":"Everyone is allowed to run their own cron and at jobs, unless you add entries to /etc/cron.allow or /etc/at.allow. No one is denied unless you add entries to /etc/cron.deny or /etc/at.deny","title":"Security for Cron"},{"location":"learning/linux/linux/#networking-fundamentals","text":"","title":"Networking Fundamentals"},{"location":"learning/linux/linux/#network-time-protocol-ntp","text":"","title":"Network Time Protocol (NTP)"},{"location":"learning/linux/linux/#configuring-network-time-protocol-ntp","text":"# vi /etc/ntp.conf # prefixing i with date creates a backup of the original file. Removes commented and blank lines sed \u2013i. $( date +%F ) \u2018/^#d ; /^$/d\u2019 /etc/ntp.conf","title":"Configuring Network Time Protocol (NTP)"},{"location":"learning/linux/linux/#implementing-the-configuration-file-changes","text":"vi /etc/ntp.conf # Add lines other can default just below the driftfile command statsdir /var/log/ntpstats # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server instead of on the internet debian servers","title":"Implementing the configuration file changes"},{"location":"learning/linux/linux/#save-the-file-and-restart-the-service","text":"service ntp restart # sudo if no access # check if the ntpstats directory is accessible to the ntp service # The user should be ntp and it should have write access ls \u2013ld /var/log/ntpstats/ - Date \u2192 Current system date and time. This is the time in memory. - HwClock \u2192 Hardware date and time set by the BIOS. hwclock \u2013r # shows the hardware clock hwclock \u2013-systohc # sets the hardware clock from system clock Hwclock \u2013-hctosys # sets the system clock from hardware clock","title":"Save the file and restart the service"},{"location":"learning/linux/linux/#ntp-tools","text":"ntpdate (once off adjustment) ntpq (query the ntp server) ntpq \u2013p (shows peers) ntpstat (Shows status but not on debian. Try ntpdc \u2013c sysinfo) ntpq \u2013c \u201cassociations\u201d \u2192 shows associations # Configuring NTP on centos --> Install ntp ntpdate 192 .168.0.3 # one off update with a local machine in the network # vi /etc/ntp.conf # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server systemctl start ntpd # save and restart systemctl enable ntpd # enable to start at system startup","title":"NTP Tools"},{"location":"learning/linux/linux/#managing-system-log-daemons","text":"","title":"Managing System Log Daemons"},{"location":"learning/linux/linux/#rocket-fast-system-for-log-processing-rsyslogd","text":"rsyslogd \u2013v # vi /etc/rsyslog.conf # Adding a simple log rule # For any log event greater than or equal to info make a log entry in local5 log. Local5 could be a simple application local5.info /var/log/local5 systemctl restart rsyslog.service # restart the service # to test this in working using command line logger \u2013p local5.info \u201cScript started\u201d # p is priority, if you see /var/log/local5 file, the log would be present","title":"Rocket-Fast System for Log Processing (rsyslogd)"},{"location":"learning/linux/linux/#varlog-folder-structure","text":"messages (Nearly everything is logged here) secure (su and sudo events amongst others) dmesg (kernel ring buffer messages)","title":"/var/log/ folder structure"},{"location":"learning/linux/linux/#logrotate","text":"ls /etc/cron.daily # has the logrotate script which will rotate log files cd /etc/logrotate.d/ # folder where all apps rotation policy is set cp syslog local5 # copy existing app conf for local5 app # vi local5 # make edits to point to /var/log/local5 file /var/log/local5 { weekly # period for rotation size +10 # size of the file for rotation compress # use compression for the rotated log file rotate 4 # keep 4 weeks of logs before overwriting } # manually running the rotate logrotate /etc/logrotate.conf # on execution, all files mentioned will be interrogated and log backup will be created","title":"Logrotate"},{"location":"learning/linux/linux/#journalctl","text":"Responsible for viewing and log management. Need to be a member of adm group to read this. By default journal is memory resident i.e. it will be lost on restart journalctl # view the journal journalctl \u2013n 10 # shows the last 10 entries journalctl \u2013n 10 \u2013p err # shows the last 10 entries with priority errors mkdir /var/log/journal # to make the journal data persistent systemctl restart system-journald systemctl status system-journald # shows that the journal data is persistent usermod \u2013a \u2013G adm username # adding user to adm group, -a is append chgrp \u2013R adm /var/log/journal # recursively give adm group access jornalctl \u2013-disk-usage # shows disk usage journalctl \u2013-verify # verify the journal integrity","title":"Journalctl"},{"location":"learning/linux/linux/#ssh","text":"","title":"SSH"},{"location":"learning/linux/linux/#remote-access-using-ssh","text":"Server Configuration /etc/ssh/sshd_config The public key of the server is used to authenticate to the client. The public key of the server is stored in /etc/ssh/ssh_host_rsa_key.pub It is down to the client to check the public key using: StrictHostkeyChecking Server public keys are stored centrally in /etc/ssh/ssh_known_host or locally under ~/.ssh/known_hosts","title":"Remote access using SSH"},{"location":"learning/linux/linux/#ssh-server-configuration","text":"netstat \u2013antl # shows the open tcp ports of the server grep ssh /etc/services # shows the services using ssh lsof \u2013i # Also shows open ports # vi /etc/ssh/sshd_config # Uncomment AddressFamily line and change as below AddressFamily inet # Now ssh will only listen on IPv6 systemctl restart sshd # vi /etc/ssh/sshd_config # Uncomment below lines and modify LoginGraceTime 1m # To avoid denial of service attacks and freeing up your service quickly PermitRootLogin no # 2 level authentication, first as normal user and then root SyslogFacility AUTHPRIV ClientAliveInterval 300 ClientAliveCountMax 0 MaxSessions 10 systemctl restart sshd","title":"SSH Server Configuration"},{"location":"learning/linux/linux/#client-configuration-and-authentication","text":"Client Configuration /etc/ssh/ssh_config Generate Private and Public keypair using ssh_keygen Use ssh-copy-id to copy to host we want to authenticate with. To provide Single Sign On using ssh-agent Client/User public keys are stored in ~/.ssh/authorized_keys using ssh-copy-id.","title":"Client Configuration and Authentication"},{"location":"learning/linux/linux/#to-connect-to-server-using-ssh","text":"cd # home directory ls \u2013a # to show all hidden files ssh pi@192.168.0.97 # ssh using user and ip address. Add the password of the user to authenticate cd .ssh cat known_hosts # shows the client ip and public keys exit or logout or Ctrl + D # to end the ssh session","title":"To connect to server using ssh"},{"location":"learning/linux/linux/#to-generate-keypairs","text":"cd .ssh # On the client home directory ssh-keygen \u2013t rsa # generate key pair. Private key is encrypted using a passphrase # This will copy the generated public key to the target server. To which user\u2019s directory at the server we will # connect as. Give the password of the server\u2019s account. ssh-copy-id \u2013i id_rsa.pub pi@192.168.0.97 ssh pi@192.168.0.97 # now connect to the server using passphrase of the private key # From another terminal say tty we can now add the private key once and don\u2019t need to authenticate to the target server ssh-agent bash # fire up another bash terminal ssh-add .ssh/id_rsa # add the private key from the home directory. Enter the passphrase ssh \u2013l or ssh \u2013L # list all identities added ssh pi@192.168.0.97","title":"To generate keypairs"},{"location":"learning/linux/linux/#ssh-tunnels","text":"ssh \u2013f \u2013N \u2013L 80 :localhost:80 user@s1.com # -f = execute in background, -N = We are not running any commands on remote host # -L = listening on port 80, we are listening on localhost and forwarding to port 80 on the remote host # On the remote host it has to listen on ssh called s1.com. We connect as user called user. # Example # Webservice on 192.168.0.3 # on a different machine, login as standard user cd .ssh ssh \u2013f \u2013N \u2013L 9000 :localhost:80 andrew@192.168.0.3 # we are listening on port 9000 on the localhost and forwarding traffic to port 80 om 192.168.0.3 netstat \u2013antlp # we can see that localhost:9000 is listening on ssh # On the client machine open the browser and type in http://127.0.0.1:9000 we will see the webservice data kill <process id of ssh> # shutdown the ssh tunneling process after finishing the work","title":"SSH Tunnels"},{"location":"learning/linux/linux/#configuring-network-protocols-in-linux","text":"","title":"Configuring Network Protocols in Linux"},{"location":"learning/linux/linux/#etcservices","text":"Network services are identified by a port address Common services and associated port address is listed in /etc/services netstat \u2013alt will list services listening via TCP this resolves address to name in /etc/services # To verify the above we can use strace to map the netstat data with the services that are running strace netstat \u2013alt 2 > & 1 | grep /etc/services grep http /etc/services # service and port mapping","title":"/etc/services"},{"location":"learning/linux/linux/#dig-command","text":"which dig # get the path of dig rpm \u2013qf /usr/bin/dig # dig is not installed by default. Hence needs to be installed. dig \u2013t AAAA ipv6.bbc.co.uk # shows the IPv6 address","title":"dig command"},{"location":"learning/linux/linux/#interface-configuration-files","text":"ifconfig # shows ip address details ifconfig eth0 192 .168.0.99 netmask 255 .255.255.0 up # sets ip address for Ethernet card ip address show # same as ifconfig # These settings are lost upon restart unless they are written to configuration files. /etc/sysconfig/network-scripts/ # centos /etc/network/interfaces # debian","title":"Interface Configuration Files"},{"location":"learning/linux/linux/#to-make-the-ip-address-static","text":"cd /etc/sysconfig/network-scripts/ vi ifcfg-ens32 # open the config file # Replace and add the lines BOOTPROTO = \u201dstatic\u201d # change from dhcp IPADDR = 192 .168.0.240 # select a static ip address NETMASK = 255 .255.255.0 # add a class c subnet GATEWAY = 192 .168.0.1 # add the gateway address DNS1 = 8 .8.8.8 # add google as the DNS server # Restart the services systemctl restart network.service","title":"To make the IP address static"},{"location":"learning/linux/linux/#networking-tools","text":"","title":"Networking Tools"},{"location":"learning/linux/linux/#nmap","text":"nmap localhost # shows all open ports used by localhost nmap 192 .168.0.3 # shows open ports at remote host nmap \u2013v 192 .168.0.3 # verbose mode nmap \u2013iL ip.txt # input file containing all ip address to be scanned","title":"nmap"},{"location":"learning/linux/linux/#netstat","text":"netstat \u2013a # shows all connections netstat \u2013at # shows all tcp connections netstat \u2013alt # shows all listening tcp connections netstat \u2013altpe # shows all user and process ids and listening tcp connections netstat \u2013s # statistics netstat \u2013i # shows interfaces netstat \u2013g # multicast groups netstat \u2013nr # network route tables","title":"netstat"},{"location":"learning/linux/linux/#show-sockets-ss","text":"ss \u2013t \u2013a # shows all tcp connections ss \u2013o state established \u2018 ( dport = :ssh or sport = :ssh ) \u2019 # shows all ssh connections ss \u2013x src /tmp/.X11-unix/* # shows X11 connections using socket files","title":"Show Sockets (ss)"},{"location":"learning/linux/linux/#lsof","text":"lsof \u2013i -4 # list all ipv4 connections lsof \u2013i :23, 24 # list all port 22 and 23 connections lsof \u2013p 1385 # list process id 1385 connections","title":"lsof"},{"location":"learning/linux/linux/#testing-network-connectivity","text":"ping www.centos.org # test network connectivity ping \u2013c3 www.centos.org # sends only 3 pings traceroute www.centos.org # describes the route to destination from source tracepath www.centos.org # shows maximum transmission unit size","title":"Testing Network Connectivity"},{"location":"learning/linux/linux/#host-name-resolution-tools","text":"hostname # Confirms the hostname cat /etc/hostname # shows the current host name hostname myComputer # changes the hostname to myComputer. You need to login as root to change this dig www.centos.org # resolves to ip address dig \u2013t MX centos.org # mail servers associated with centos","title":"Host Name Resolution Tools"},{"location":"learning/linux/linux/#managing-interfaces","text":"ip a s # Shows ip addresses ip n s # Shows Neighbor shows looking at ARP cache ip r s # Shows root table ip ma s # Shows Multicast groups ip l # Shows network cards ifdown eth0 # Brings down interface ifup eth0 # Brings up interface","title":"Managing Interfaces"},{"location":"learning/linux/linux/#securing-access-to-your-server","text":"# Temporary disables logins for users other than root. Just the existence of this file will prevent user logins and is controlled via PAM(Pluggable Authentication Modules) /etc/nologin # Create a blank file, but if the user tries to login via ssh username@localhost, the connection will be immediately closed. Only root can access the server then. touch /etc/nologin rm /etc/nologin # Now users can login. This can be used as a temporary measure cd /etc/pam.d/ # Config files for authentication modules grep nologin * # Shows instances where nologin file exists last # shows last user activity present in /var/log/wtmp file. lastlog # List the last login time for each user lastlog | grep \u2013v Never # Reverse the grep search to check for all user logins","title":"Securing Access to your Server"},{"location":"learning/linux/linux/#ulimit","text":"Puts restrictions on system resources ulimit \u2013a # Shows system limitations that can be applied ulimit \u2013u 8000 # -u is for avoiding fork bombs and is defaulted to 4096. A std user can set a new value to 8000. # It will remain for his profile till system restart. cd /etc/security # To set the limits which can be persisted even after restart cat limits.conf # Shows soft (can be changed by processes) and hard (can\u2019t be changed by processes) limits cd limits.d/ # ls to see the files inside this directory. Edit the file and add an entry for the user account with soft or hard limits and save the file.","title":"ulimit"},{"location":"learning/linux/linux/#avoiding-fork-bombs","text":"They are a potential Denial of Service Attack ulimit \u2013u ##### Do not run on Production Machines. Test only in laptop ##### ps \u2013u username | wc -l # Shows the number of processes running under the user and gives the count # On the command line create a function called foo foo (){ echo hello } # Execute the function by just calling it and pressing enter foo # Similarly to execute a fork bomb, instead of foo, call it : : (){ : | : & # Here the function calls itself and pipes the output to itself } ; : # End the function with a semicolon and then call the function : # BEWARE: Do this only at your own risk. Ensure ulimit is set to protect the resources on the server #","title":"Avoiding fork bombs"},{"location":"learning/linux/linux/#xinetd","text":"Called the super daemon as it can manage a lot of smaller services, secures access to your server /etc/xinetd.d /etc/xinetd.conf tftp server (Trivial file transfer protocol) # Sample configuration service tftp { socket_type = dgram # data gram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /tftpboot # root directory of the server disable = no }","title":"xinetd"},{"location":"learning/linux/linux/#implementing-tftp-using-xinetdd-service","text":"ls /etc/xinetd.d/ # blank directory yum install tftp-server tftp # install client and server, also xinetd as it\u2019s a dependency vi /etc/xinetd.d/tftp # after installation, delete disable line from the configuration to enable tftp # if the server directory doent exist, create it mkdir \u2013p /var/lib/tftpboot systemctl enable xinetd systemctl start xinetd netstat \u2013aulpe | grep tftp # shows the port # As a root user, create a temp file inside var/lib/tftpboot directory with hello text vi var/lib/tftpboot/file1 # Logout and login as standard user. Use TFTP to transfer the file to standard user tftp 127 .0.0.1 # Press enter get file1 # Get the file1 created by root using tftp quit # At the same file location do a cat to see the contents of the file cat file1","title":"Implementing TFTP using xinetd.d service"},{"location":"learning/linux/linux/#tcp-wrappers","text":"Alternative to firewalling on the server # To check if service supports TCP wrappers. Once we can determine this, we can use hosts.allow or hosts.deny ldd </path to service name> | grep libwrap # To set this up for a service, 2 entries are made # This in /etc/hosts.allow file to allow access to 0.3 IP which is raspberry pi. tftpd is the name of the binary in .tftpd : 192 .168.0.3 # This in /etc/hosts.deny file to deny access to all other IP except for 0.3 IP in .tftpd : ALL # If the client appears in both files then allow takes precedence and access is granted ldd /usr/sbin/xinetd | grep libwrap tftp 192 .168.0.240 # login from raspberry pi and access the remote server, disable your firewall before trying this.","title":"TCP Wrappers"},{"location":"learning/linux/linux/#delegating-admin-rights-using-sudoers","text":"id # check if the user is already an admin and part of wheel group in centos cd /etc grep wheel sudoers # check the current setup visudo # to edit sudoers file %wheel ALL =( root ) ALL # Uncomment the line for %wheel, change (ALL) to (root), so only root can change the sudoers. Save and exit","title":"Delegating Admin rights using sudoers"},{"location":"learning/linux/linux/#data-encryption","text":"","title":"Data Encryption"},{"location":"learning/linux/linux/#using-gpg-to-encrypt-data-between-users","text":"password = $( mkpassword \u2013m sha-512 Password1 ) # encrypt the password using sha and mkpassword and put in a variable echo $password for u in marta ivan ; do # take an input from marta and ivan for users sudo useradd \u2013m \u2013p $password $u # add the user and set the encrypted password done # Run the command and create 2 users ivan and marta # Install GPG if not present dkpg \u2013S $( which gpg ) # Login as ivan and generate private and public keys for gpg encryption su - ivan gpg --gen-key # Take the default settings gpg --list-key # List the keys gpg --export \u2013a <email from gpg gen-key step> > /tmp/ivankey # export the public key and place in tmp folder for marta to access exit su - marta vi secret.txt # create a plain txt file for encryption chmod 400 secret.txt # make the file writeable by marta only gpg --import /tmp/ivankey # import ivan public key gpg \u2013e \u2013r <ivan mailid> secret.txt # add the recipient and encrypt the file mv secret.txt.gpg /tmp/ # move the encrypted file to tmp and exit su \u2013 ivan gpg \u2013d /tmp/secret.txt.gpg # decrypt the file and enter the passphrase for the private key","title":"Using GPG to encrypt data between users"},{"location":"learning/linux/linux/#implementing-luks-for-full-disk-encryption","text":"example Laptops, USB sudo apt-get install cryptsetup # Install LUKS tools lsblk # list the blocks sudo fdisk /dev/sdb # partition the sdb drive and create a new partition called sdb1 sudo cryptsetup \u2013y luksFormat /dev/sdb1 # Allows to store encrypted data in this partition. Give a passphrase sudo cryptsetup luksDump /dev/sdb1 # Shows details of the encrypted partition sudo cryptsetup luksOpen /dev/sdb1 private_data # Assign a mapper called private_data ls /dev/mapper # Shows the new mapper setup and that is a device sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted","title":"Implementing LUKS for Full Disk Encryption"},{"location":"learning/linux/linux/#efs-for-data-encryption","text":"sudo apt-get install ecryptfs-utils # Install EFS tools su \u2013 ivan # Login as std user ecryptfs-setup-private # Create a private key with passphrase. logout and login back as ivan ls # Private directory is created by EFS echo hello > Private/data # Write to private directory cat Private/data # ivan can see the data ecryptfs-umount-private # unmount the private directory ls # Now ivan cannot see the data cd /.Private/ # Go to the ivan\u2019s hidden Private folder with a dot ls # You can see the encrypted data file ecryptfs-mount-private # Mount the directory again","title":"EFS for data encryption"},{"location":"learning/linux/linux/#compiling-software-from-source","text":"Install gcc compiler sudo apt-get install gcc make # go to gnu.org/software/Downloads -> coreutils -> file ending with .xz # tar -xvJf coreutils-8.28.tar.xz # J is for xz, j is for bzip2 # cd into src folder and select ls.c file # Add a line in main function printf(\"Hello World\\n\"); # After changes, change directory one level up and run the configure script bash configure # This will check for any configuration changes to the src files and update the make file # Excute the make command make # After binary code is compiled, it needs to be updated in OS sudo make install # Close the terminal and restart. New software is working ;)","title":"Compiling software from source"},{"location":"learning/linux/security/","text":"Introduction \u00b6 Linux Security Confernce Videos Hardening Guides from CIS, DISA, Ubuntu Add Ubuntu mailing list for security updates Linux is only as secure as you make it! \u00b6 Nothing is perfectly secure. Security is a series of trade-offs. convenience vs security # No passwords = easy to use, not secure. # System powered off = secure, not usable. Examples: Linux can be configured to be unsecure. Users may employ lax file permissions. System administration mistakes. Users could use easy to guess passwords. Data transmitted in the clear. Malicious software installed on the system. Lack of training or security awareness. Continous Improvement \u00b6 Just because you are using Linux, doesn\u2019t mean you are \u201csecure.\u201d Security is an ongoing process. Stay vigilant! Risk Assessment \u00b6 What is the severity of the risk? What is the probability of the risk occurring? What is the cost to mitigate the risk? What is the effectiveness of the countermeasure? Multiuser System \u00b6 Linux is a multiuser system. The superuser is the root account. root is all powerful. Required to install system-wide software, configure networking, manager users, etc. All other accounts are \u201cnormal\u201d accounts. Can be used by people or applications (services). Advantages to a Multiuser System. \u00b6 File permissions \u00b6 Every file has an owner. Permissions can be granted to other accounts and users as needed. Breaking into one account does not necessarily compromise the entire system. Process permissions. \u00b6 Every process has an owner. Each account can manage their processes. Exception to above rule: root can do anything Breaking into one account does not necessarily compromise the entire system. Security Guidelines \u00b6 Principle of Least Privilege Use encryption Shared accounts (Yes, root can be a shared account!) Multifactor authentication Firewall Monitoring logs Minimize Software and Services \u00b6 If you don\u2019t need a piece of software, don\u2019t install it. If you don\u2019t need a service, don\u2019t start it. If you no longer need the software or service, stop and uninstall it. Run Services on Separate Systems \u00b6 Minimizes the risk of one compromised service leading to other compromised services. Encrypt Data Transmissions \u00b6 Protect against eavesdropping and man-in-the middle attacks. Examples: Protocol \u2192 Replace with FTP \u2192 SFTP telnet \u2192 SSH SNMP v1/v2 \u2192 SNMP v3 HTTP \u2192 HTTPS Avoid Shared Accounts \u00b6 Each person should have their own account. Each service should have its own account. Shared accounts make security auditing difficult. Lack of accountability with shared accounts. Avoid Direct root Logins \u00b6 Do not allow direct login of shared accounts. Users must login to their personal accounts and then switch to the shared account. Control and monitor access with sudo. Maintain Accounts \u00b6 Create and use a process for removing access. Use Multifactor Authentication \u00b6 Something you know (password) + something you have (phone) or something you are(fingerprints). Examples: account password + phone to receive the one time password (OTP). account password + fingerprint The Principle of Least Privilege \u00b6 AKA, the Principle of Least Authority. Examples: Only use root privileges when required. Avoid running services as the root user. Use restrictive permissions that allow people and services enough access to do their jobs. Monitor System Activity \u00b6 Routinely review logs. Send logs to a central logging system. Use a Firewall \u00b6 Linux has a built-in firewall. Netfilters + iptables. Only allow network connections from desired sources. Encrypt Your Data \u00b6 Encryption protects your data while it is \u201cat rest\u201d (on disk). Physical Security \u00b6 Physical Security Is Linux Security \u00b6 Physical access poses a great security threat to your Linux system! Single user mode - Allows unrestricted access. Only allow physical access when necessary. Systems Not Under Your Control \u00b6 Data centers / colos - Like \u201cbanks\u201d of data. Possible targets for attackers Needs processes, procedures, and controls in place toprotect your valuable data. Cloud \u00b6 At some point the cloud is real equipment. Physical security is still important. Your data is on their storage systems. The provider has access to your virtual disks. If encryption is available, use it. Protecting Linux Against Physical Attacks \u00b6 Gaining Access to a Linux System: Single User Mode & Power Resets Changing the shell command from sushell to sulogin will prompt for root password when entrering into single user mode # Securing Single User Mode and blank passwords by having root password at logins # Login to root and gain access to shell echo $$ # Shows the current process id i.e shell ps -fp <pid> # Shows detailed information about shell process including command executed at login cd /lib/systemd/system grep sulogin emergency.target # No output should be visible grep sulogin emergency.service # Should have sulogin in ExecStart command head -1 /etc/shadow # Shows id root password is set or not. If not set (!) is present in output 2nd column passwd # Set root password Securing the Boot Loader \u00b6 To prevent a person who as physical access from passing arguments to the Linux kernel at boot time, you should password protect the boot loader. Check examples how to secure this. Disk Encryption \u00b6 dm-crypt \u00b6 device mapper crypt Provides transparent disk encryption. Creates a new device in /dev/mapper. Use like any other block device. Manage with cryptsetup LUKS \u00b6 Linux Unified Key Setup. Front-end for dm-crypt. Multiple passphrase support. Portable as LUKS stores setup information in the partition header. Great for removable media, too. Encrypt During Install \u00b6 PRO: easy, with sane defaults. CON: you give up some control. Setting up LUKS on a New Device \u00b6 Use this process for any block device presented to your system that you want to encrypt. Following this procedure will remove all data on the partition (device) that you are encrypting! Implementing LUKS for Full Disk Encryption \u00b6 example Laptops, USB sudo apt-get install cryptsetup # Install LUKS tools lsblk # list the blocks sudo shread -v -n 1 /dev/sdb # Writes random data to he device sdb sudo fdisk /dev/sdb # partition the sdb drive and create a new partition called sdb1 sudo cryptsetup \u2013y luksFormat /dev/sdb1 # Allows to store encrypted data in this partition. Give a passphrase sudo cryptsetup luksDump /dev/sdb1 # Shows details of the encrypted partition sudo cryptsetup luksOpen /dev/sdb1 private_data # Assign a mapper called private_data and opens the device ls /dev/mapper # Shows the new mapper setup and that is a device ls -arlt /dev/mapper | tail # Shows the virtual block devices private_data sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted # Make an entry in the /etc/fstab to mount private_data to /mnt on each boot # Make an entry in /etc/crypttab to mount private_data at boot time # To close the encrypted device sudo cryptsetup luksClose private_data Encrypting device in Cloud \u00b6 Sometimes cloud providers do not give block level access to volumes. For such cases, we will encrypt the files like we do for volumes sudo mkdir /data sudo fallocate -l 100M /data/private_data # Creates a non sparse file sudo strings /data/private_data # Shows any string data in the file. It mostly is blank # Write random data, if=input, of=output, bs=byte size<1 Mb>, count=<size of file i.e 100Mb> sudo dd if = /dev/urandom of = /data_private_data bs = 1M count = 100 sudo strings /data/private_data sudo cryptsetup \u2013y luksFormat /data/private_data # Allows to store encrypted data in this file. Give a passphrase sudo cryptsetup luksOpen /data/private_data private_data # Assign a mapper called private_data and opens the device ls /dev/mapper # Shows the new mapper setup and that is a device ls -arlt /dev/mapper | tail # Shows the virtual block devices private_data sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted sudo df -h /mnt # Shows the size # Make an entry in the /etc/fstab to mount private_data to /mnt on each boot # Make an entry in /etc/crypttab to mount private_data at boot time Converting an Existing Device to LUKS \u00b6 # Backup the data. /home lives on /dev/sda3 # for example. # Wipe the device. # use shred or dd if=/dev/urandom of=/dev/sda3 # Setup LUKS. cryptsetup luksFormat /dev/sda3 cryptsetup luksOpen /dev/sda3 home mkfs -t ext4 /dev/mapper/home mount /dev/mapper/home /home # & restore from backup Disabling Ctrl+Alt+Del (Systemd) \u00b6 Attackers can gain access to the virtual terminal and send command ctrl+alt+delete to reboot the system. # Disabling reboot using ctrl-alt-delete command over remote connection systemctl mask ctrl-alt-del.target systemctl daemon-reload Account Security \u00b6 It's easier to attack a system from the inside. Privilege escalation attacks are a threat. Mitigation \u00b6 Keep unwanted users out. Secure accounts. PAM \u00b6 Pluggable Authentication Modules Used to delegate / abstract authentication of services / programs like login or sshd PAM Configuration files \u00b6 Location: /etc/pam.d Configuration file for login is /etc/pam.d/login Configuration file for sshd is /etc/pam.d/sshd Format: module_interface control_flag module_name module_args PAM Module Interfaces \u00b6 auth - Authenticates users. account - Verifies if access is permitted. password - Changes a user\u2019s password. session - Manages user\u2019s sessions. PAM Control Flags \u00b6 required - Module result must be successful to continue. requisite - Like required, but no other modules are invoked. sufficient - Authenticates user if no required modules have failed, otherwise ignored. optional - Only used when no other modules reference the interface. include - Includes configuration from another file. complex control flags - attribute=value PAM Configuration Example \u00b6 The directives listed in the PAM module are executed in sequential order *.so extension stands for shared objects #%PAM-1.0 # Comment auth required pam_securetty.so # 3 auth modules which need to pass auth required pam_unix.so nullok auth required pam_nologin.so account required pam_unix.so # Checks if the user account is valid password required pam_pwquality.so retry = 3 # Checks for password quality if acount has expired and gives 3 tries to set password password required pam_unix.so shadow \\ # Allows to use shadow file nullok use_authtok session required pam_unix.so # Logs when user logs in and out of the system PAM Documentation \u00b6 Configuration: account required pam_nologin.so session required pam_unix.so Getting help, drop the .so extension and use the man page to get additional help: man pam_nologin man pam_unix Linux Account Types \u00b6 root, the superuser \u00b6 Root can do anything. Always has the UID of 0. System accounts \u00b6 UIDs < 1,000 Configured in /etc/login.defs useradd -r system_account_name Normal User Accounts \u00b6 UIDs >= 1,000 Intended for human (interactive) use Password Security \u00b6 Enforce, not hope for, strong passwords. Use pam_pwquality, based on pam_cracklib. Configuration File: /etc/security/pwquality.conf PAM Usage: password requisite pam_pwquality.so Module attributes: man pam_pwquality # /etc/login.defs format # PASS_MAX_DAYS 99999 # PASS_MIN_DAYS 0 # PASS_MIN_LEN 5 # PASS_WARN_AGE 7 Use Shadow Passwords \u00b6 /etc/passwd unencrypted: root:$6$L3ZSmlM1H5:0:0:root:/root:/bin/bash /etc/passwd with shadow passwords: root:x:0:0:root:/root:/bin/bash /etc/shadow: root:$6$L3ZSmlM1H5::0:99999:7::: Converting Passwords \u00b6 pwconv - convert to shadow passwords. pwunconv - convert from shadow passwords. /etc/shadow format \u00b6 Username Hashed password Days since epoch of last password change Days until change allowed Days before change required Days warning for expiration Days before account inactive Days since epoch when account expires Reserved Display user account expiry info with chage \u00b6 chage -l <account-name> # Show account aging info. $ chage -l jason # Last password change : Apr 01, 2016 # Password expires : never # Password inactive : never # Account expires : never # Minimum number of days between password change : 0 # Maximum number of days between password change : 99999 # Number of days of warning before password expires : 7 Change user account expiry info with chage \u00b6 -M MAX_DAYS - Set the maximum number of days during which a password is valid. -E EXPIRE_DATE - Date on which the user\u2019s account will no longer be accessible. -d LAST_DAY - Set the last day the password was changed. Demo to change normal account to root \u00b6 head -n 1 /etc/passwd # Shows the root entry, UID is 3rd field delimited by : sudo useradd jim # Create normal account sudo passwd jim # Change password su - jim # Login to account whoami # Show logged in user details exit sudo vi /etc/passwd # Edit the UID of jim to 0 su - jim id whoami # Now jim account shows root # Show how many users have UID of 0 awk -F: '($3 == ' 0 ')' { print } # Delimit Field by :, take the 3rd field and check if its 0 and print the line # This will show 2 entries, one for root and other for jim # Undo the change by editing `/etc/passwd` and updating jim's UID to original Controlling Account Access \u00b6 Locking and Unlocking accounts \u00b6 passwd -l account passwd -u account Disabling logins for system and root accounts \u00b6 Locking with nologin as the Shell # Example /etc/passwd entries: for apache and www-data system accounts apache:x:48:48:Apache:/usr/share/httpd:/sbin/nologin www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin # Using chsh : chsh -s SHELL ACCOUNT chsh -s /sbin/nologin jason # Does not allow jason user to login using password Centralized Authentication \u00b6 Easy to manage users system-wide - lock account everywhere Example authentication systems: freeIPA, LDAP (openLDAP) Has drawbacks too. Disable Logins \u00b6 pam_nologin module Looks for /etc/nologin or /var/run/nologin Disables logins and displays contents of nologin file. Monitoring Authentication Logs \u00b6 last # All login data lastb # Failed authentication lastlog # Last logins # Depends on syslog configuration, logs are stored in following files: /var/log/messages /var/log/syslog /var/log/secure /var/log/auth.log Intrusion Prevention with fail2ban \u00b6 Monitors log files. Blocks IP address of attacker. Automatic unban. Not just for Linux logins. Multifactor Authentication \u00b6 Google Authenticator PAM module DuoSecurity\u2019s pam_duo module RSA SecurID PAM module Security by Account Type \u00b6 Account Security - root \u00b6 Use a normal account for normal activities. Avoid logging in as root. Use sudo instead of su. Avoid using the same root password. Ensure only the root account has UID 0 - awk -F: '($3 == \"0\") {print}' /etc/passwd Disabling root Logins \u00b6 /etc/securetty - Controls root logins using terminals. Normal user logins don't use this file # pam_securetty module - /etc/pam.d/login auth [ user_unknown = ignore success = ok ignore = ignore default = bad ] pam_securetty.so # Shows pam_securetty module is used w # Shows the current terminal, assume tty1 vi /etc/securetty # Remove tty1 entry from this file and save # login to system as root and it fails as tty1 (first terminal) has been removed from logging # Alt+Ctrl+F2 (This will use tty2 to login to the system instead of tty1 as that is no longer valid) # Similarly F3 for tty3, F4 for tty4 and so on # Now login as root and it works as tty2 is present in /etc/securetty file # Empty the securetty file of all entries and save it. # Now there is no way root can login to this system. # Login using a normal account and that will work System / Application Accounts \u00b6 Use one account per service - web service (httpd), web service account (apache) Don\u2019t activate the account. Don\u2019t allow direct logins from the account - sshd_config: DenyUsers account1 accountN Use sudo for all access. sudo -u apache apachectl configtest User Accounts \u00b6 One account per person. Deleting Accounts \u00b6 Determine the UID - id ACCOUNT Delete their account and home directory - userdel -r Find other files that belong to them on the system. find / -user UID find / -nouser Using and Configuring Sudo \u00b6 sudo vs su \u00b6 \u201cSuperUser Do\u201d or \u201cSubstitute User Do\u201d Use instead of the su command. Complete shell access with su. With su you need to know the password of the other account. Breaks the Principle of Least Privilege. Vague audit trail with su. Sudo (Super User Do) \u00b6 Elevation of Privileges - Giving users temporary root priviledges Fine grain controls. No need to share passwords. Clear audit trail. Sudoers Format \u00b6 User Specification Format: user host=(run_as) command # Examples: jason webdev01 =( root ) /sbin/apachectl %web web* =( root ) /sbin/apachectl %wheel ALL =( ALL ) ALL Sudo Authentication \u00b6 Sudo requires a user to authenticate. Default 5 minute grace period (timeout). You may not want to use a password. apache web* =( root ) NOPASSWD:/sbin/backup-web, # No password required to run backup-web PASSWD:/sbin/apachectl # Password required for apache Sudo Aliases \u00b6 User_Alias Runas_Alias Host_Alias Cmnd_Alias Format: Alias_Type NAME = item1, item2, ... # Add normal users to group webteam User_Alias WEBTEAM = jason, bob # Give permission to group WEBTEAM web* =( root ) /sbin/apachectl WEBTEAM web* =( apache ) /sbin/apachebackup # Run permissions to system accounts Runas_Alias WEBUSERS = apache, httpd WEBTEAM web* =( WEBUSERS ) /sbin/apachectl # Host permissions to user accounts Host_Alias WEBHOSTS = web*, prodweb01 WEBTEAM WEBHOSTS =( WEBUSERS ) /sbin/apachectl # Command permissions Cmnd_Alias WEBCMNDS = /sbin/apachectl WEBTEAM WEBHOSTS =( WEBUSERS ) WEBCMNDS # Optimized sudoers configuration User_Alias WEBTEAM = jason, bob Runas_Alias WEBUSERS = apache, httpd Host_Alias WEBHOSTS = web*, prodweb01 Cmnd_Alias WEBCMNDS = /sbin/apachectl WEBTEAM WEBHOSTS =( root ) /sbin/apachebackup WEBTEAM WEBHOSTS =( WEBUSERS ) WEBCMNDS Displaying the Sudo Configuration \u00b6 List commands you are allowed to run: sudo -l Verbose listing of commands: sudo -ll List commands another USER is allowed: sudo -l -U user # Sudo configuration export EDITOR = nano visudo # Give Bob the rights to run yum command at the end of the sudoers file bob ALL =( root ) /usr/bin/yum # save and exit sudo -l -U bob # List sudo permissions for bob sudo -ll -U bob # More verbose output su - bob # Login as bob sudo -l # Shows current permissions sudo yum install dstat -y # It will work without password exit su - visudo -f /etc/sudoers.d/bob # Creates a new file inside sudoers.d bob ALL =( ALL ) /usr/bin/whoami # Give permission to run whoami. save and exit su - bob whoami # Gives bob as output sudo -u jason whoami # Pass user as jason who runs whoami and it works as well. # As one user can give another user access to run a command and this is dangerous. # All sudo operations are logged inside /var/log/secure # Allow the \u201cbob\u201d account to run the \u201creboot\u201d command only as the \u201croot\u201d user on the \u201clinuxsvr1\u201d system bob linuxsvr1 =( root ) /sbin/reboot Network Security \u00b6 Network Services \u00b6 Called -Network services, daemons, servers. Listen on network ports. Constantly running in the background. Output recorded in log files. Designed to perform a single task. Securing Network Services \u00b6 Use a dedicated user for each service. Take advantage of privilege separation. Ports below 1024 are privileged. Use root to open them, then drop privileges. Configuration controlled by each service. Stop and uninstall unused services. Avoid unsecure services. Use SSH instead of telnet, rlogin, rsh, and FTP Stay up to date with patches. Install services provided by your distro. Only listen on the required interfaces and addresses. Information Leakage: Avoid revealing information where possible. Examples: Web server banners or information in /etc/motd /etc/issue /etc/issue.net Stop and Disable Services systemctl stop SERVICE systemctl disable SERVICE # Example: systemctl stop httpd systemctl disable httpd List Listening Programs with netstat: netstat -nutlp Port Scanning: nmap HOSTNAME_OR_IP or lsof -i nmap localhost nmap 10 .11.12.13 Testing a Specific Port: telnet HOST_OR_ADDRESS PORT or nc -v HOST_OR_ADDRESS PORT Xinetd Controlled Services /etc/xinetd.d/SERVICE # To disable service: disable = yes # To disable xinetd: systemctl stop xinetd systemctl disable xinetd Securing SSH \u00b6 SSH = Secure SHell. Allows for key based authentication. Configuration: /etc/ssh/sshd_config # vi /etc/ssh/sshd_config PubkeyAuthentication yes PasswordAuthentication no # Force Key Authentication PermitRootLogin no # To disable root logins PermitRootLogin without-password # To only allow root to login with a key AllowUsers user1 user2 userN # Only Allow Certain Users SSH Access AllowGroups group1 group2 groupN # Only Allow Certain Groups SSH Access DenyUsers user1 user2 userN # Deny Certain Users SSH Access DenyGroups group1 group2 groupN # Deny Certain Groups SSH Access AllowTcpForwarding no # Disable TCP Port Forwarding GatewayPorts no Protocol 2 # Use SSHv2 instead of SSHv1 ListenAddress host_or_address1 # Bind SSH to a Specific Address ListenAddress host_or_addressN # Allow individual IP to connect to SSH, separate line for each IP Port 2222 # Change the Default Port. This masks port scanners Banner none # Disable the Banner, or update the banner in `/etc/issue.net` to remove identifiable data like versions # Reload the SSH Configuration systemctl reload sshd # Add the New Port to SELinux after changing ssh port number semanage port -a -t ssh_port_t -p tcp PORT semanage port -l | grep ssh # Additional information man ssh man sshd man sshd_config SSH Port Forwarding \u00b6 Expose service (database) to a client not in same network Forward traffic from client to a service running in remote # Client (Different network) ---> (SSH session on port 3306) ---> Server1 ---> (SSH session on port 22) ---> (Database on 127.0.0.0 and port 3306) ssh -L 3306 :127.0.0.1:3306 server1 # -L = forwarding # Similarly another application can connect to Client on port 3306 and access database on server 1 # Avoid this by having firewall block connections to client # Client (Different network) ---> (SSH session on port 8080) ---> Server1 ---> (SSH session on port 22) ---> Google.com (service on port 80) ssh -L 8080 :google.com:80 server1 # -L = forwarding # In this case google.com will understand that traffic is originating from server1 instead of client Dynamic Port Forwarding / SOCKS \u00b6 # Client (Different network) ---> (SSH session on port 8080) ---> Server1 ---> (SSH session on port 22) ---> (Internal Network) ssh -D 8080 server1 # -D = Dynamic forwarding # This allows users to connect to company network Reverse Port Forwarding \u00b6 Use a proxy to direct traffic to internal network Connect to a desktop inside corporate network to work from home # (Internal Network) (SSH session on port 22) <--- (Desktop having shell program) <--- (SSH session on port 22) (Port 2222 is open to internet) <--- Server1 <--- Client (Different network) ssh -R 2222 :127.0.0.0:22 # -R = Reverse forwarding Linux Firewall Fundamentals \u00b6 Firewalls control network access. Linux firewall = Netfilter + IPTables Netfilter is a kernel framework. IPTables is a packet selection system. Use the iptables command to control the firewall. Default Tables \u00b6 Filter - Most commonly used table. NAT - Network Address Translation. Mangle - Alter packets. Raw - Used to disable connection tracking. Security - Used by SELinux. Default Chains \u00b6 INPUT OUTPUT FORWARD PREROUTING POSTROUTING Rules \u00b6 Rules = Match + Target Match on: Protocol, Source/Dest IP or network, Source/Dest Port, Network Interface Example: protocol: TCP, source IP: 1.2.3.4, dest port: 80 Targets \u00b6 Built-in targets: ACCEPT DROP REJECT LOG RETURN iptables / ip6tables \u00b6 Command line interface to IPTables/netfilter. List / View iptables \u00b6 iptables -L # Display the filter table. iptables -t nat -L # Display the nat table. iptables -nL # Display using numeric output. iptables -vL # Display using verbose output. iptables --line-numbers -L # Use line nums. Chain Policy / Default Target \u00b6 Set the default TARGET for CHAIN: iptables -P CHAIN TARGET Example: iptables -P INPUT DROP # Appending Rules iptables -A CHAIN RULE-SPECIFICATION iptables [ -t TABLE ] -A CHAIN RULE-SPECIFICATION # Inserting Rules iptables -I CHAIN [ RULENUM ] RULE-SPECIFICATION # Deleting Rules iptables -D CHAIN RULE-SPECIFICATION iptables -D CHAIN RULENUM # Flushing rules or deleting tables iptables [ -t table ] -F [ chain ] Rule Specification Options \u00b6 -s 10 .11.12.13 # Source IP, network, or name -d 216 .58.192.0/19 # Destination IP, network, or name -p tcp / udp / icmp # Protocol -p tcp --dport 80 # Destination Port -p tcp --sport 8080 # Source Port -p icmp --icmp-type echo-reply # ICMP packet type, gives pong response -p icmp --icmp-type echo-request # Gives ping response # Rating limiting -m limit --limit rate [ /second/minute/hour/day ] # Match until a limit is reached. -m limit --limit-burst # --limit default is 3/hour and --limit-burst default is 5 -m limit --limit 5 /m --limit-burst 10 # /s = second, /m = minutes -m limit ! --limit 5 /s # ! = invert the match Target / Jump \u00b6 To specify a jump point or target: -j TARGET_OR_CHAIN -j ACCEPT # Built-in target. -j DROP # Built-in target. -j LOGNDROP # Custom chain. Rule Specification Example \u00b6 # To drop all traffic from given source ip iptables -A INPUT -s 216 .58.219.174 -j DROP # List the filter table, as the above rule didnt specify table name iptables -nL # Chain INPUT (policy ACCEPT) # target prot opt source destination # DROP all -- 216.58.219.174 0.0.0.0/0 # Accept SSH connection from only one network, drop all other SSH connections iptables -A INPUT -s 10 .0.0.0/24 -p tcp --dport 22 -j ACCEPT iptables -A INPUT -p tcp --dport 22 -j DROP # DOS protection by implementing Rate limiting iptables -I INPUT -p tcp --dport 80 \\ -m limit --limit 50 /min --limit-burst 200 -j REJECT # Apply the below rate limit rule only for new connections and ignore for established connections iptables -I INPUT -p tcp --dport 80 \\ -m limit --limit 50 /min --limit-burst 200 \\ -m state --state NEW -j REJECT Creating and Deleting a Chain \u00b6 Create CHAIN: iptables [-t table] -N CHAIN Delete CHAIN: iptables [-t table] -X CHAIN Saving Rules \u00b6 # Debian / Ubuntu: apt-get install iptables-persistent netfilter-persistent save Netfilter/iptable Front-Ends \u00b6 Uses iptables command on the back-end Firewalld - CentOS/RHEL UFW - Uncomplicated FireWall (Ubuntu) GUFW - Graphical interface to UFW system-configure-firewall - CentOS/RHEL IP Tables Examples \u00b6 # Allow anyone to connect to webserver, but only internal ips to connect via SSH # Block all other traffic iptables -L # List the existing rules, no rules # Accept all incoming TCP traffic on port 80 iptables -A INPUT -p tcp --dport 80 -j ACCEPT iptables -nL # List the current new rule # Accept all incoming SSH traffic on port 22 originating from the internal network iptables -A INPUT -p tcp --dport 22 -s 10 .0.0.0/24 -j ACCEPT # Drop all packets which dont match above 2 rules iptables -A INPUT -j DROP # Testing the filters nc -v 10 .0.0.8 80 # Net cat using an internal IP on port 80 nc -v 10 .0.0.8 22 # Test the SSH connection # Deleting existing rule iptables -D INPUT 1 # Deletes the Web server traffic on port 80 nc -w 2 -v 10 .0.0.8 80 # -w specifies timeout of 2 seconds # Reject the packets instead of dropping them iptables -D INPUT 2 # Deletes the drop rule iptables -A INPUT -J REJECT # Now rejects the packets nc -w 2 -v 10 .0.0.8 80 # Connection refused # Flush all existing rules iptables -F iptables -A INPUT -p tcp --dport 22 -s 10 .0.0.0/24 -j ACCEPT # Create a new custom table iptables -N LOGNDROP # Create a rule to log all incoming traffic other than internal network SSH connection iptables -A LOGNDROP -p tcp -m limit --limit 5 /min -j LOG --log-prefix \"iptables BLOCK \" iptables -A INPUT -j LOGNDROP # Accept and log before dropping the connection other than internal network # In another terminal, tail the syslog file tail -f /var/log/syslog # Other terminal nc -w 2 -v 10 .0.0.8 80 # This will log and drop the connection # Persist the iptables changes after reboot netfilter- persistent save TCP Wrappers \u00b6 Host-based networking ACL system. Controls access to \u201cwrapped\u201d services. A wrapped service is compiled with libwrap support. Check if a service supports wrappers use ldd . ldd /usr/sbin/sshd | grep libwrap # Prints required shared libraries. Can control access by IP address / networks. Can control access by hostname. Transparent to the client and service. Used with xinetd which is a super daemon as it can manage a lot of smaller services, secures access to your server. Centralized management for multiple network services. Runtime configuration Configuring TCP Wrappers \u00b6 /etc/hosts.allow is checked first. If a match is found, access is granted . /etc/hosts.deny is checked next. If a match is found, access is denied . Access Rules \u00b6 The rule format for hosts.allow and hosts.deny are the same. One rule per line Format: SERVICE(S) : CLIENT(S) [: ACTION(S) ] # Deny All configuration # /etc/hosts.deny: ALL : ALL # Deny all connections # /etc/hosts.allow: # Explicitly list allowed connections here. sshd : 10 .11.12.13 # Allow only ssh connection from this ip # Additional configuration possibilities sshd, imapd : 10 .11.12.13 # Multiple services allowed ALL : 10 .11.12.13 # Wild Card configuration sshd : 10 .11.12.13, 10 .5.6.7 # Multiple IPs sshd : jumpbox.example.com # Domain name sshd : .admin.example.com # Subdomain - server2.admin.example.com or webdev.admin.example.com sshd : jumpbox*.example.com # jumpbox4admins.example.com sshd : jumpbox0?.example.com # jumpbox03.example.com sshd : 10 .11.12. # Subnet range sshd : 10 . sshd : 10 .11.0.0/255.255.0.0 sshd : /etc/hosts.sshd # Allow all hosts in this file imapd : ALL # Wildcard in clients sshd : ALL EXCEPT .hackers.net # Conditions, except IP in hacker.net domains sshd : 10 .11.12.13 : severity emerg # Actions - Log emergency messages for that ssh connection coming from IP sshd : 10 .11.12.13 : severity local0.alert # Raise local alert # Raise custom message # /etc/hosts.deny: sshd : .hackers.net : spawn /usr/bin/wall \u201cAttack in progress.\u201d # Use expressions in message, %a logs client ip sshd : .hackers.net : spawn /usr/bin/wall \u201cAttack from %a.\u201d ## Possible expression expansions # %a (%A) The client (server) host address. # %c Client information. # %d The daemon process name. # %h (%H) The client (server) host name or address. # %n (%N) The client (server) host name. # %p The daemon process id. # %s Server information. # %u The client user name (or \"unknown\"). # %% Expands to a single `% \u0301 character. File Security \u00b6 File Permissions \u00b6 first letter in ls -l output Regular file: - Directory: d Symbolic link: l Read: r Write: w Execute: x Files vs Directory \u00b6 r - Allows a file to be read. Allows file names in a directory to be read w - Allows a file to be modified. Allows entries to be modified within the directory x - Allows a file to be executed. Allows access to the contents and metdadata for entries Groups \u00b6 Every user in linux is in atleast one group Users can belong to many groups group command shows a user's group. Can also use id -Gn # Changing file permissions ls -l sales.data # Long list file permissions chmod g+w sales.data # Add Write permission to group chmod g-w sales.data # Remove Write permission to group chmod u+rwx,g-x sales.data # Add user all permissions and remove execute permission for group chmod a = r sales.data # Give all groups read permissions chmod o = sales.data # removes all permissions for others Directory permissions \u00b6 Permissions on a directory can affect the files in the directory If the file permissions look correct, start checking the directory permissions Work your way up to the root Use chgrp to change group of a file # Changing directory permissions ls -l sales.data # Long list to show current group membership groups # Shows current user's groups (one is user and other is sales group) chgrp sales sales.data # Change file ownership from user to sales group chmod g+w sales.data # Give other members in sales group write access mkdir -p /usr/local/sales # Create a common directory for sales group mv sales.data /usr/local/sales # Now file and directory have the same permissions # Verify directory permissions mkdir -p my-cat # Create new directory touch my-cat/cat # Create new file chmod 755 my-cat # Modify permissions ls -ld my-cat # Shows permissions chmod 400 my-cat # Give only read permissions ls -ld my-cat # Gives permission error chmod 500 my-cat # Give write permission as well ls -ld my-cat # Gives output cat my-cat/cat # Shows file output File Creation Mask \u00b6 File creation mask are set by the admins This can be changed by per user basis File creation mask determines default permissions. If no mask were to be specified: deafault permissions would be 777 for directories and 666 for files umask command \u00b6 Sets the file creation mask to mode, if given Use -S for symbolic notation Format: umask [-S] [mode] mode of umask is opposite to chmod chmod - adds permissions. chmod 660 will give read and write permissions to file umask - subtracts permissions. umask 007 will give read an write permissions to file Common umask modes: 002, 022, 077, 007 # Test umask mkdir testumask cd testumask umask # Shows creation umask 0022 umask -S # Shows umask in symbolic mode mkdir newdir touch newfile ls -l # Shows dir permissions = 0755 and file permissions = 0644 rm newfile rmdir newdir umask 007 # Set new umask mkdir newdir touch newfile ls -l # Shows dir permissions = 0770 and file permissions = 0660 Special Modes \u00b6 Setuid \u00b6 When a process is started, it runs using the starting user's UID and GID. setuid = Set User ID upon execution. Examples of binaries running with root and setuid -rwsr-xr-x 1 root root /usr/bin/passwd # Requires root permissions to modify passwd file ping chsh setuid files are an attack surface. Not honored on shell scripts. Octal Permissions: setuid=4, setgid=2 and sticky=1 # Adding the Setuid Attribute chmod u+s /path/to/file chmod 4755 /path/to/file # Removing the Setuid Attribute chmod u-s /path/to/file chmod 0755 /path/to/file # Finding Setuid Files find / -perm /4000 -ls # ls also lists files # Only the Owner Should Edit Setuid Files Good: Symbolic Octal -rwsr-xr-x 4755 Really bad: -rwsrwxrwx 4777 Setgid \u00b6 setgid = Set Group ID upon execution. -rwxr-sr-x 1 root tty /usr/bin/wall # s - set in group field crw--w---- 1 bob tty /dev/pts/0 # Adding the Setgid Attribute chmod g+s /path/to/file chmod 2755 /path/to/file # Adding the Setuid & Setgid Attributes chmod ug+s /path/to/file chmod 6755 /path/to/file # Removing the Setgid Attribute chmod g-s /path/to/file chmod 0755 /path/to/file # Finding Setgid Files find / -perm /2000 -ls Setgid on Directories \u00b6 setgid on a directory causes new files to inherit the group of the directory. setgid causes directories to inherit the setgid bit. Is not retroactive. Does not apply on existing files, but new files in directories. Great for working with groups. Use an Integrity Checker \u00b6 Other options to find . Tripwire AIDE (Advanced Intrusion Detection Environment) OSSEC Samhain Package managers Sticky Bit \u00b6 Use on a directory to only allow the owner of the file/directory to delete it. Required on files or directories whcih are shared with multiple users or programs # Used on /tmp: drwxrwxrwt 10 root root 4096 Feb 1 09 :47 /tmp # Adding the Sticky Bit chmod o+s /path/to/directory chmod 1777 /path/to/directory # Removing the Sticky Bit chmod o-t /path/to/directory chmod 0777 /path/to/directory Reading ls Output \u00b6 A capitalized special permission means the underlying normal permission is not set. A lowercase special permission means the underlying normal permission set. # execute permission is not set for user -rwSr--r-- 1 root root 0 Feb 14 11 :21 test # execute permission is set for the user -rwsr--r-- 1 root root 0 Feb 14 # execute permission is not set for group -rwxrwSr-- 1 root root 0 Feb 14 11 :21 test # execute permission is not set for others drwxr-xr-T 2 root root 0 Feb 14 11 :30 testd File Attributes \u00b6 xattr: Supported by many file systems. ext2, ext3, ext4, XFS Attribute: i immutable \u00b6 The file cannot be: modified, deleted, renamed, hard linked to Unset the attribute in order to delete it. Attribute: a append \u00b6 Append only. Existing contents cannot be modified. Cannot be deleted while attribute is set. Use this attribute on log files. Safeguard the audit trail. Other Attributes \u00b6 Not every attribute is supported. man ext4, man xfs, man brtfs, etc. Example: s secure deletion Modifying Attributes \u00b6 Use the chattr command. adds attributes. removes attributes. = sets the exact attributes. # Making the hosts file immutable lsattr /etc/hosts # No attributes present chattr +i /etc/hosts # Add immutable attribute vi /etc/hosts # Not able to write and save rm /etc/hosts # Not able to delete the file chattr -i /etc/hosts # Remove immutable attribute lsattr /etc/hosts # No attributes present vi /etc/hosts # Now able to write and save chattr +i /etc/hosts # Add immutable attribute back again # Making the apache logs files append only cd /var/log/httpd chattr = a * # Adding append attribute to all the files inside lsattr echo test >> access_log # Data will be appended cat access_log vi access_log # Not able to write and save Access Control Lists \u00b6 ACL = Access Control List Provides additional control Example: Give one user access to a file. Traditional solution is to create another group. Increases management overhead of groups. Ensure file system mounted with ACL support XFS supports this by default # Mount files with ACL support to get this feature mount -o acl /path/to/dev /path/to/mount tune2fs -o acl /path/to/dev tune2fs -l /path/to/dev | grep options # Verify ACL Types of ACLs \u00b6 Access - Control access to a specific file or directory. Default - Used on directories only. Files without access rules use the default ACL rules. Not retroactive. Optional. ACLs Can Be Configured: \u00b6 Per user Per group For users not in the file\u2019s group Via the effective rights mask Creating ACLs \u00b6 Use the setfacl command. May need to install the ACL tools. Modify or add ACLs: setfacl -m ACL FILE_OR_DIRECTORY Detecting Files with ACLs: + at the end of ls -l permissions output # User ACLs / Rules # u:uid:perms Set the access ACL for a user. setfacl -m u:jason:rwx start.sh setfacl -m u:sam:xr start.sh # Group ACLs / Rules # g:gid:perms Sets the access ACL for a group. setfacl -m g:sales:rw sales.txt # Mask ACLs / Rules # m:perms Sets the effective rights mask. setfacl -m m:rx sales.txt # Other ACLs / Rules # o:perms Sets the access ACL for others. setfacl -m o:r sales.txt # Creating Multiple ACLs at Once setfacl -m u:bob:r,g:sales:rw sales.txt # Default ACLs # d:[ugo]:perms Sets the default ACL. setfacl -m d:g:sales:rw sales # Setting ACLs Recursively (-R) setfacl -R -m g:sales:rw sales # Removing ACLs for particular group or user setfacl -x ACL FILE_OR_DIRECTORY setfacl -x u:jason sales.txt # Note: Not supposed to give permissions when deleting ACL setfacl -x g:sales sales.txt # Removing All ACLs setfacl -b FILE_OR_DIRECTORY setfacl -b sales.txt # Viewing ACLs getfacl sales.txt # ACL for a shared project directory # Logged in as root user mkdir /usr/local/project cd /usr/local chgrp project project/ chmod 775 project/ # Give the project members access to the directory ls -ld project/ # Directory owned by root and project members have access su - sam # Switch user groups # same is in project group cd usr/local/project echo stuff > notes.txt ls -l notes.txt # sam is the owner of this file # Give bob who is not a member of project group access to this file setfacl -m u:bob:rw notes.txt # Modify access by sam to give read and write access getfacl notes.txt # List the permisions for bob su -bob # Switch user vi notes.txt # bob is able to write su -sam # Switch user setfacl -x u:bob notes.txt # Remove the rw permissions getfacl notes.txt # No permissions visble for bob su - # Switch to root cd project touch root-was-here # Create a new file owned by root in shared directory ls -l # No ACL is applied even though its in shared directory su sam # Switch to group user echo >> root-was-here # Permission denied su - # Switch to root cd /usr/local/project setfacl -m d:g:project:rw . # Set default ACL to project directory ls -ld . # Default ACL are added to directory denoted by + getfacl # Default ACL are added touch test # Create new file getfacl su sam vi test # Edit is possible setfacl -R -m g:project:rw . # Add recursively permissions to root-was-here file as well Rootkits \u00b6 Software used to gain root access and remain undetected. They attempt to hide from system administrators and antivirus software. User space rootkits replace common commands such as ls, ps, find, netstat, etc Kernel space rootkits add or replace parts of the core operating system. Rootkit Detection \u00b6 Use a file integrity checker for user space rootkits. (AIDE, tripwire, OSSEC, etc.) Identify inconsistent behavior of a system. High CPU utilization without corresponding processes. High network load or unusual connections. Kernel mode rootkits have to be running in order to hide themselves. Halt the system and examine the storage. Use a known good operating system to do the investigation. Use bootable media, for example. Rootkit Hunter / RKHunter \u00b6 Shell script that searches for rootkits. rkhunter --update # Update database rkhunter --propupd # Update RKHunter configurations rkhunter -c # Interactive Check Mode cat /var/log/rkhunter.log # Log file location rkhunter -c --rwo # Report only warnings rkhunter --cronjob # Run as cronjob # After a fresh installation of OS # Install RK Hunter and then update the properties and database # Configure it to run everyday and report any issues by configuring it to log all output in a central place # Configure Cronjob crontab -e # Create a new cronjob # At the end run at modnight everyday # Redirect the output to single file 0 0 * * * /usr/local/bin/rkhunter --cronjob --update > /var/tmp/rkhunter.cronlog 2 > & 1 OSSEC \u00b6 Host Intrusion Detection System (HIDS) More than just rookit detection: log analysis, file integrity checking, alerting. Syscheck module - user mode rootkit detection. Rootcheck module - both user mode and kernel mode rootkit detection. Rootkit Removal \u00b6 Keep a copy of the data if possible. Learn how to keep it from happening again. Reinstall core OS components and start investigating. Not recommended. Easy to make a mistake. Safest is to reinstall the OS from trusted media. Rootkit Prevention \u00b6 Use good security practices: Physical, Account Network Use file integrity monitoring: AIDE,Tripware, OSSEC Keep your systems patched.","title":"Linux_Security"},{"location":"learning/linux/security/#introduction","text":"Linux Security Confernce Videos Hardening Guides from CIS, DISA, Ubuntu Add Ubuntu mailing list for security updates","title":"Introduction"},{"location":"learning/linux/security/#linux-is-only-as-secure-as-you-make-it","text":"Nothing is perfectly secure. Security is a series of trade-offs. convenience vs security # No passwords = easy to use, not secure. # System powered off = secure, not usable. Examples: Linux can be configured to be unsecure. Users may employ lax file permissions. System administration mistakes. Users could use easy to guess passwords. Data transmitted in the clear. Malicious software installed on the system. Lack of training or security awareness.","title":"Linux is only as secure as you make it!"},{"location":"learning/linux/security/#continous-improvement","text":"Just because you are using Linux, doesn\u2019t mean you are \u201csecure.\u201d Security is an ongoing process. Stay vigilant!","title":"Continous Improvement"},{"location":"learning/linux/security/#risk-assessment","text":"What is the severity of the risk? What is the probability of the risk occurring? What is the cost to mitigate the risk? What is the effectiveness of the countermeasure?","title":"Risk Assessment"},{"location":"learning/linux/security/#multiuser-system","text":"Linux is a multiuser system. The superuser is the root account. root is all powerful. Required to install system-wide software, configure networking, manager users, etc. All other accounts are \u201cnormal\u201d accounts. Can be used by people or applications (services).","title":"Multiuser System"},{"location":"learning/linux/security/#advantages-to-a-multiuser-system","text":"","title":"Advantages to a Multiuser System."},{"location":"learning/linux/security/#file-permissions","text":"Every file has an owner. Permissions can be granted to other accounts and users as needed. Breaking into one account does not necessarily compromise the entire system.","title":"File permissions"},{"location":"learning/linux/security/#process-permissions","text":"Every process has an owner. Each account can manage their processes. Exception to above rule: root can do anything Breaking into one account does not necessarily compromise the entire system.","title":"Process permissions."},{"location":"learning/linux/security/#security-guidelines","text":"Principle of Least Privilege Use encryption Shared accounts (Yes, root can be a shared account!) Multifactor authentication Firewall Monitoring logs","title":"Security Guidelines"},{"location":"learning/linux/security/#minimize-software-and-services","text":"If you don\u2019t need a piece of software, don\u2019t install it. If you don\u2019t need a service, don\u2019t start it. If you no longer need the software or service, stop and uninstall it.","title":"Minimize Software and Services"},{"location":"learning/linux/security/#run-services-on-separate-systems","text":"Minimizes the risk of one compromised service leading to other compromised services.","title":"Run Services on Separate Systems"},{"location":"learning/linux/security/#encrypt-data-transmissions","text":"Protect against eavesdropping and man-in-the middle attacks. Examples: Protocol \u2192 Replace with FTP \u2192 SFTP telnet \u2192 SSH SNMP v1/v2 \u2192 SNMP v3 HTTP \u2192 HTTPS","title":"Encrypt Data Transmissions"},{"location":"learning/linux/security/#avoid-shared-accounts","text":"Each person should have their own account. Each service should have its own account. Shared accounts make security auditing difficult. Lack of accountability with shared accounts.","title":"Avoid Shared Accounts"},{"location":"learning/linux/security/#avoid-direct-root-logins","text":"Do not allow direct login of shared accounts. Users must login to their personal accounts and then switch to the shared account. Control and monitor access with sudo.","title":"Avoid Direct root Logins"},{"location":"learning/linux/security/#maintain-accounts","text":"Create and use a process for removing access.","title":"Maintain Accounts"},{"location":"learning/linux/security/#use-multifactor-authentication","text":"Something you know (password) + something you have (phone) or something you are(fingerprints). Examples: account password + phone to receive the one time password (OTP). account password + fingerprint","title":"Use Multifactor Authentication"},{"location":"learning/linux/security/#the-principle-of-least-privilege","text":"AKA, the Principle of Least Authority. Examples: Only use root privileges when required. Avoid running services as the root user. Use restrictive permissions that allow people and services enough access to do their jobs.","title":"The Principle of Least Privilege"},{"location":"learning/linux/security/#monitor-system-activity","text":"Routinely review logs. Send logs to a central logging system.","title":"Monitor System Activity"},{"location":"learning/linux/security/#use-a-firewall","text":"Linux has a built-in firewall. Netfilters + iptables. Only allow network connections from desired sources.","title":"Use a Firewall"},{"location":"learning/linux/security/#encrypt-your-data","text":"Encryption protects your data while it is \u201cat rest\u201d (on disk).","title":"Encrypt Your Data"},{"location":"learning/linux/security/#physical-security","text":"","title":"Physical Security"},{"location":"learning/linux/security/#physical-security-is-linux-security","text":"Physical access poses a great security threat to your Linux system! Single user mode - Allows unrestricted access. Only allow physical access when necessary.","title":"Physical Security Is Linux Security"},{"location":"learning/linux/security/#systems-not-under-your-control","text":"Data centers / colos - Like \u201cbanks\u201d of data. Possible targets for attackers Needs processes, procedures, and controls in place toprotect your valuable data.","title":"Systems Not Under Your Control"},{"location":"learning/linux/security/#cloud","text":"At some point the cloud is real equipment. Physical security is still important. Your data is on their storage systems. The provider has access to your virtual disks. If encryption is available, use it.","title":"Cloud"},{"location":"learning/linux/security/#protecting-linux-against-physical-attacks","text":"Gaining Access to a Linux System: Single User Mode & Power Resets Changing the shell command from sushell to sulogin will prompt for root password when entrering into single user mode # Securing Single User Mode and blank passwords by having root password at logins # Login to root and gain access to shell echo $$ # Shows the current process id i.e shell ps -fp <pid> # Shows detailed information about shell process including command executed at login cd /lib/systemd/system grep sulogin emergency.target # No output should be visible grep sulogin emergency.service # Should have sulogin in ExecStart command head -1 /etc/shadow # Shows id root password is set or not. If not set (!) is present in output 2nd column passwd # Set root password","title":"Protecting Linux Against Physical Attacks"},{"location":"learning/linux/security/#securing-the-boot-loader","text":"To prevent a person who as physical access from passing arguments to the Linux kernel at boot time, you should password protect the boot loader. Check examples how to secure this.","title":"Securing the Boot Loader"},{"location":"learning/linux/security/#disk-encryption","text":"","title":"Disk Encryption"},{"location":"learning/linux/security/#dm-crypt","text":"device mapper crypt Provides transparent disk encryption. Creates a new device in /dev/mapper. Use like any other block device. Manage with cryptsetup","title":"dm-crypt"},{"location":"learning/linux/security/#luks","text":"Linux Unified Key Setup. Front-end for dm-crypt. Multiple passphrase support. Portable as LUKS stores setup information in the partition header. Great for removable media, too.","title":"LUKS"},{"location":"learning/linux/security/#encrypt-during-install","text":"PRO: easy, with sane defaults. CON: you give up some control.","title":"Encrypt During Install"},{"location":"learning/linux/security/#setting-up-luks-on-a-new-device","text":"Use this process for any block device presented to your system that you want to encrypt. Following this procedure will remove all data on the partition (device) that you are encrypting!","title":"Setting up LUKS on a New Device"},{"location":"learning/linux/security/#implementing-luks-for-full-disk-encryption","text":"example Laptops, USB sudo apt-get install cryptsetup # Install LUKS tools lsblk # list the blocks sudo shread -v -n 1 /dev/sdb # Writes random data to he device sdb sudo fdisk /dev/sdb # partition the sdb drive and create a new partition called sdb1 sudo cryptsetup \u2013y luksFormat /dev/sdb1 # Allows to store encrypted data in this partition. Give a passphrase sudo cryptsetup luksDump /dev/sdb1 # Shows details of the encrypted partition sudo cryptsetup luksOpen /dev/sdb1 private_data # Assign a mapper called private_data and opens the device ls /dev/mapper # Shows the new mapper setup and that is a device ls -arlt /dev/mapper | tail # Shows the virtual block devices private_data sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted # Make an entry in the /etc/fstab to mount private_data to /mnt on each boot # Make an entry in /etc/crypttab to mount private_data at boot time # To close the encrypted device sudo cryptsetup luksClose private_data","title":"Implementing LUKS for Full Disk Encryption"},{"location":"learning/linux/security/#encrypting-device-in-cloud","text":"Sometimes cloud providers do not give block level access to volumes. For such cases, we will encrypt the files like we do for volumes sudo mkdir /data sudo fallocate -l 100M /data/private_data # Creates a non sparse file sudo strings /data/private_data # Shows any string data in the file. It mostly is blank # Write random data, if=input, of=output, bs=byte size<1 Mb>, count=<size of file i.e 100Mb> sudo dd if = /dev/urandom of = /data_private_data bs = 1M count = 100 sudo strings /data/private_data sudo cryptsetup \u2013y luksFormat /data/private_data # Allows to store encrypted data in this file. Give a passphrase sudo cryptsetup luksOpen /data/private_data private_data # Assign a mapper called private_data and opens the device ls /dev/mapper # Shows the new mapper setup and that is a device ls -arlt /dev/mapper | tail # Shows the virtual block devices private_data sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted sudo df -h /mnt # Shows the size # Make an entry in the /etc/fstab to mount private_data to /mnt on each boot # Make an entry in /etc/crypttab to mount private_data at boot time","title":"Encrypting device in Cloud"},{"location":"learning/linux/security/#converting-an-existing-device-to-luks","text":"# Backup the data. /home lives on /dev/sda3 # for example. # Wipe the device. # use shred or dd if=/dev/urandom of=/dev/sda3 # Setup LUKS. cryptsetup luksFormat /dev/sda3 cryptsetup luksOpen /dev/sda3 home mkfs -t ext4 /dev/mapper/home mount /dev/mapper/home /home # & restore from backup","title":"Converting an Existing Device to LUKS"},{"location":"learning/linux/security/#disabling-ctrlaltdel-systemd","text":"Attackers can gain access to the virtual terminal and send command ctrl+alt+delete to reboot the system. # Disabling reboot using ctrl-alt-delete command over remote connection systemctl mask ctrl-alt-del.target systemctl daemon-reload","title":"Disabling Ctrl+Alt+Del (Systemd)"},{"location":"learning/linux/security/#account-security","text":"It's easier to attack a system from the inside. Privilege escalation attacks are a threat.","title":"Account Security"},{"location":"learning/linux/security/#mitigation","text":"Keep unwanted users out. Secure accounts.","title":"Mitigation"},{"location":"learning/linux/security/#pam","text":"Pluggable Authentication Modules Used to delegate / abstract authentication of services / programs like login or sshd","title":"PAM"},{"location":"learning/linux/security/#pam-configuration-files","text":"Location: /etc/pam.d Configuration file for login is /etc/pam.d/login Configuration file for sshd is /etc/pam.d/sshd Format: module_interface control_flag module_name module_args","title":"PAM Configuration files"},{"location":"learning/linux/security/#pam-module-interfaces","text":"auth - Authenticates users. account - Verifies if access is permitted. password - Changes a user\u2019s password. session - Manages user\u2019s sessions.","title":"PAM Module Interfaces"},{"location":"learning/linux/security/#pam-control-flags","text":"required - Module result must be successful to continue. requisite - Like required, but no other modules are invoked. sufficient - Authenticates user if no required modules have failed, otherwise ignored. optional - Only used when no other modules reference the interface. include - Includes configuration from another file. complex control flags - attribute=value","title":"PAM Control Flags"},{"location":"learning/linux/security/#pam-configuration-example","text":"The directives listed in the PAM module are executed in sequential order *.so extension stands for shared objects #%PAM-1.0 # Comment auth required pam_securetty.so # 3 auth modules which need to pass auth required pam_unix.so nullok auth required pam_nologin.so account required pam_unix.so # Checks if the user account is valid password required pam_pwquality.so retry = 3 # Checks for password quality if acount has expired and gives 3 tries to set password password required pam_unix.so shadow \\ # Allows to use shadow file nullok use_authtok session required pam_unix.so # Logs when user logs in and out of the system","title":"PAM Configuration Example"},{"location":"learning/linux/security/#pam-documentation","text":"Configuration: account required pam_nologin.so session required pam_unix.so Getting help, drop the .so extension and use the man page to get additional help: man pam_nologin man pam_unix","title":"PAM Documentation"},{"location":"learning/linux/security/#linux-account-types","text":"","title":"Linux Account Types"},{"location":"learning/linux/security/#root-the-superuser","text":"Root can do anything. Always has the UID of 0.","title":"root, the superuser"},{"location":"learning/linux/security/#system-accounts","text":"UIDs < 1,000 Configured in /etc/login.defs useradd -r system_account_name","title":"System accounts"},{"location":"learning/linux/security/#normal-user-accounts","text":"UIDs >= 1,000 Intended for human (interactive) use","title":"Normal User Accounts"},{"location":"learning/linux/security/#password-security","text":"Enforce, not hope for, strong passwords. Use pam_pwquality, based on pam_cracklib. Configuration File: /etc/security/pwquality.conf PAM Usage: password requisite pam_pwquality.so Module attributes: man pam_pwquality # /etc/login.defs format # PASS_MAX_DAYS 99999 # PASS_MIN_DAYS 0 # PASS_MIN_LEN 5 # PASS_WARN_AGE 7","title":"Password Security"},{"location":"learning/linux/security/#use-shadow-passwords","text":"/etc/passwd unencrypted: root:$6$L3ZSmlM1H5:0:0:root:/root:/bin/bash /etc/passwd with shadow passwords: root:x:0:0:root:/root:/bin/bash /etc/shadow: root:$6$L3ZSmlM1H5::0:99999:7:::","title":"Use Shadow Passwords"},{"location":"learning/linux/security/#converting-passwords","text":"pwconv - convert to shadow passwords. pwunconv - convert from shadow passwords.","title":"Converting Passwords"},{"location":"learning/linux/security/#etcshadow-format","text":"Username Hashed password Days since epoch of last password change Days until change allowed Days before change required Days warning for expiration Days before account inactive Days since epoch when account expires Reserved","title":"/etc/shadow format"},{"location":"learning/linux/security/#display-user-account-expiry-info-with-chage","text":"chage -l <account-name> # Show account aging info. $ chage -l jason # Last password change : Apr 01, 2016 # Password expires : never # Password inactive : never # Account expires : never # Minimum number of days between password change : 0 # Maximum number of days between password change : 99999 # Number of days of warning before password expires : 7","title":"Display user account expiry info with chage"},{"location":"learning/linux/security/#change-user-account-expiry-info-with-chage","text":"-M MAX_DAYS - Set the maximum number of days during which a password is valid. -E EXPIRE_DATE - Date on which the user\u2019s account will no longer be accessible. -d LAST_DAY - Set the last day the password was changed.","title":"Change user account expiry info with chage"},{"location":"learning/linux/security/#demo-to-change-normal-account-to-root","text":"head -n 1 /etc/passwd # Shows the root entry, UID is 3rd field delimited by : sudo useradd jim # Create normal account sudo passwd jim # Change password su - jim # Login to account whoami # Show logged in user details exit sudo vi /etc/passwd # Edit the UID of jim to 0 su - jim id whoami # Now jim account shows root # Show how many users have UID of 0 awk -F: '($3 == ' 0 ')' { print } # Delimit Field by :, take the 3rd field and check if its 0 and print the line # This will show 2 entries, one for root and other for jim # Undo the change by editing `/etc/passwd` and updating jim's UID to original","title":"Demo to change normal account to root"},{"location":"learning/linux/security/#controlling-account-access","text":"","title":"Controlling Account Access"},{"location":"learning/linux/security/#locking-and-unlocking-accounts","text":"passwd -l account passwd -u account","title":"Locking and Unlocking accounts"},{"location":"learning/linux/security/#disabling-logins-for-system-and-root-accounts","text":"Locking with nologin as the Shell # Example /etc/passwd entries: for apache and www-data system accounts apache:x:48:48:Apache:/usr/share/httpd:/sbin/nologin www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin # Using chsh : chsh -s SHELL ACCOUNT chsh -s /sbin/nologin jason # Does not allow jason user to login using password","title":"Disabling logins for system and root accounts"},{"location":"learning/linux/security/#centralized-authentication","text":"Easy to manage users system-wide - lock account everywhere Example authentication systems: freeIPA, LDAP (openLDAP) Has drawbacks too.","title":"Centralized Authentication"},{"location":"learning/linux/security/#disable-logins","text":"pam_nologin module Looks for /etc/nologin or /var/run/nologin Disables logins and displays contents of nologin file.","title":"Disable Logins"},{"location":"learning/linux/security/#monitoring-authentication-logs","text":"last # All login data lastb # Failed authentication lastlog # Last logins # Depends on syslog configuration, logs are stored in following files: /var/log/messages /var/log/syslog /var/log/secure /var/log/auth.log","title":"Monitoring Authentication Logs"},{"location":"learning/linux/security/#intrusion-prevention-with-fail2ban","text":"Monitors log files. Blocks IP address of attacker. Automatic unban. Not just for Linux logins.","title":"Intrusion Prevention with fail2ban"},{"location":"learning/linux/security/#multifactor-authentication","text":"Google Authenticator PAM module DuoSecurity\u2019s pam_duo module RSA SecurID PAM module","title":"Multifactor Authentication"},{"location":"learning/linux/security/#security-by-account-type","text":"","title":"Security by Account Type"},{"location":"learning/linux/security/#account-security---root","text":"Use a normal account for normal activities. Avoid logging in as root. Use sudo instead of su. Avoid using the same root password. Ensure only the root account has UID 0 - awk -F: '($3 == \"0\") {print}' /etc/passwd","title":"Account Security - root"},{"location":"learning/linux/security/#disabling-root-logins","text":"/etc/securetty - Controls root logins using terminals. Normal user logins don't use this file # pam_securetty module - /etc/pam.d/login auth [ user_unknown = ignore success = ok ignore = ignore default = bad ] pam_securetty.so # Shows pam_securetty module is used w # Shows the current terminal, assume tty1 vi /etc/securetty # Remove tty1 entry from this file and save # login to system as root and it fails as tty1 (first terminal) has been removed from logging # Alt+Ctrl+F2 (This will use tty2 to login to the system instead of tty1 as that is no longer valid) # Similarly F3 for tty3, F4 for tty4 and so on # Now login as root and it works as tty2 is present in /etc/securetty file # Empty the securetty file of all entries and save it. # Now there is no way root can login to this system. # Login using a normal account and that will work","title":"Disabling root Logins"},{"location":"learning/linux/security/#system--application-accounts","text":"Use one account per service - web service (httpd), web service account (apache) Don\u2019t activate the account. Don\u2019t allow direct logins from the account - sshd_config: DenyUsers account1 accountN Use sudo for all access. sudo -u apache apachectl configtest","title":"System / Application Accounts"},{"location":"learning/linux/security/#user-accounts","text":"One account per person.","title":"User Accounts"},{"location":"learning/linux/security/#deleting-accounts","text":"Determine the UID - id ACCOUNT Delete their account and home directory - userdel -r Find other files that belong to them on the system. find / -user UID find / -nouser","title":"Deleting Accounts"},{"location":"learning/linux/security/#using-and-configuring-sudo","text":"","title":"Using and Configuring Sudo"},{"location":"learning/linux/security/#sudo-vs-su","text":"\u201cSuperUser Do\u201d or \u201cSubstitute User Do\u201d Use instead of the su command. Complete shell access with su. With su you need to know the password of the other account. Breaks the Principle of Least Privilege. Vague audit trail with su.","title":"sudo vs su"},{"location":"learning/linux/security/#sudo-super-user-do","text":"Elevation of Privileges - Giving users temporary root priviledges Fine grain controls. No need to share passwords. Clear audit trail.","title":"Sudo (Super User Do)"},{"location":"learning/linux/security/#sudoers-format","text":"User Specification Format: user host=(run_as) command # Examples: jason webdev01 =( root ) /sbin/apachectl %web web* =( root ) /sbin/apachectl %wheel ALL =( ALL ) ALL","title":"Sudoers Format"},{"location":"learning/linux/security/#sudo-authentication","text":"Sudo requires a user to authenticate. Default 5 minute grace period (timeout). You may not want to use a password. apache web* =( root ) NOPASSWD:/sbin/backup-web, # No password required to run backup-web PASSWD:/sbin/apachectl # Password required for apache","title":"Sudo Authentication"},{"location":"learning/linux/security/#sudo-aliases","text":"User_Alias Runas_Alias Host_Alias Cmnd_Alias Format: Alias_Type NAME = item1, item2, ... # Add normal users to group webteam User_Alias WEBTEAM = jason, bob # Give permission to group WEBTEAM web* =( root ) /sbin/apachectl WEBTEAM web* =( apache ) /sbin/apachebackup # Run permissions to system accounts Runas_Alias WEBUSERS = apache, httpd WEBTEAM web* =( WEBUSERS ) /sbin/apachectl # Host permissions to user accounts Host_Alias WEBHOSTS = web*, prodweb01 WEBTEAM WEBHOSTS =( WEBUSERS ) /sbin/apachectl # Command permissions Cmnd_Alias WEBCMNDS = /sbin/apachectl WEBTEAM WEBHOSTS =( WEBUSERS ) WEBCMNDS # Optimized sudoers configuration User_Alias WEBTEAM = jason, bob Runas_Alias WEBUSERS = apache, httpd Host_Alias WEBHOSTS = web*, prodweb01 Cmnd_Alias WEBCMNDS = /sbin/apachectl WEBTEAM WEBHOSTS =( root ) /sbin/apachebackup WEBTEAM WEBHOSTS =( WEBUSERS ) WEBCMNDS","title":"Sudo Aliases"},{"location":"learning/linux/security/#displaying-the-sudo-configuration","text":"List commands you are allowed to run: sudo -l Verbose listing of commands: sudo -ll List commands another USER is allowed: sudo -l -U user # Sudo configuration export EDITOR = nano visudo # Give Bob the rights to run yum command at the end of the sudoers file bob ALL =( root ) /usr/bin/yum # save and exit sudo -l -U bob # List sudo permissions for bob sudo -ll -U bob # More verbose output su - bob # Login as bob sudo -l # Shows current permissions sudo yum install dstat -y # It will work without password exit su - visudo -f /etc/sudoers.d/bob # Creates a new file inside sudoers.d bob ALL =( ALL ) /usr/bin/whoami # Give permission to run whoami. save and exit su - bob whoami # Gives bob as output sudo -u jason whoami # Pass user as jason who runs whoami and it works as well. # As one user can give another user access to run a command and this is dangerous. # All sudo operations are logged inside /var/log/secure # Allow the \u201cbob\u201d account to run the \u201creboot\u201d command only as the \u201croot\u201d user on the \u201clinuxsvr1\u201d system bob linuxsvr1 =( root ) /sbin/reboot","title":"Displaying the Sudo Configuration"},{"location":"learning/linux/security/#network-security","text":"","title":"Network Security"},{"location":"learning/linux/security/#network-services","text":"Called -Network services, daemons, servers. Listen on network ports. Constantly running in the background. Output recorded in log files. Designed to perform a single task.","title":"Network Services"},{"location":"learning/linux/security/#securing-network-services","text":"Use a dedicated user for each service. Take advantage of privilege separation. Ports below 1024 are privileged. Use root to open them, then drop privileges. Configuration controlled by each service. Stop and uninstall unused services. Avoid unsecure services. Use SSH instead of telnet, rlogin, rsh, and FTP Stay up to date with patches. Install services provided by your distro. Only listen on the required interfaces and addresses. Information Leakage: Avoid revealing information where possible. Examples: Web server banners or information in /etc/motd /etc/issue /etc/issue.net Stop and Disable Services systemctl stop SERVICE systemctl disable SERVICE # Example: systemctl stop httpd systemctl disable httpd List Listening Programs with netstat: netstat -nutlp Port Scanning: nmap HOSTNAME_OR_IP or lsof -i nmap localhost nmap 10 .11.12.13 Testing a Specific Port: telnet HOST_OR_ADDRESS PORT or nc -v HOST_OR_ADDRESS PORT Xinetd Controlled Services /etc/xinetd.d/SERVICE # To disable service: disable = yes # To disable xinetd: systemctl stop xinetd systemctl disable xinetd","title":"Securing Network Services"},{"location":"learning/linux/security/#securing-ssh","text":"SSH = Secure SHell. Allows for key based authentication. Configuration: /etc/ssh/sshd_config # vi /etc/ssh/sshd_config PubkeyAuthentication yes PasswordAuthentication no # Force Key Authentication PermitRootLogin no # To disable root logins PermitRootLogin without-password # To only allow root to login with a key AllowUsers user1 user2 userN # Only Allow Certain Users SSH Access AllowGroups group1 group2 groupN # Only Allow Certain Groups SSH Access DenyUsers user1 user2 userN # Deny Certain Users SSH Access DenyGroups group1 group2 groupN # Deny Certain Groups SSH Access AllowTcpForwarding no # Disable TCP Port Forwarding GatewayPorts no Protocol 2 # Use SSHv2 instead of SSHv1 ListenAddress host_or_address1 # Bind SSH to a Specific Address ListenAddress host_or_addressN # Allow individual IP to connect to SSH, separate line for each IP Port 2222 # Change the Default Port. This masks port scanners Banner none # Disable the Banner, or update the banner in `/etc/issue.net` to remove identifiable data like versions # Reload the SSH Configuration systemctl reload sshd # Add the New Port to SELinux after changing ssh port number semanage port -a -t ssh_port_t -p tcp PORT semanage port -l | grep ssh # Additional information man ssh man sshd man sshd_config","title":"Securing SSH"},{"location":"learning/linux/security/#ssh-port-forwarding","text":"Expose service (database) to a client not in same network Forward traffic from client to a service running in remote # Client (Different network) ---> (SSH session on port 3306) ---> Server1 ---> (SSH session on port 22) ---> (Database on 127.0.0.0 and port 3306) ssh -L 3306 :127.0.0.1:3306 server1 # -L = forwarding # Similarly another application can connect to Client on port 3306 and access database on server 1 # Avoid this by having firewall block connections to client # Client (Different network) ---> (SSH session on port 8080) ---> Server1 ---> (SSH session on port 22) ---> Google.com (service on port 80) ssh -L 8080 :google.com:80 server1 # -L = forwarding # In this case google.com will understand that traffic is originating from server1 instead of client","title":"SSH Port Forwarding"},{"location":"learning/linux/security/#dynamic-port-forwarding--socks","text":"# Client (Different network) ---> (SSH session on port 8080) ---> Server1 ---> (SSH session on port 22) ---> (Internal Network) ssh -D 8080 server1 # -D = Dynamic forwarding # This allows users to connect to company network","title":"Dynamic Port Forwarding / SOCKS"},{"location":"learning/linux/security/#reverse-port-forwarding","text":"Use a proxy to direct traffic to internal network Connect to a desktop inside corporate network to work from home # (Internal Network) (SSH session on port 22) <--- (Desktop having shell program) <--- (SSH session on port 22) (Port 2222 is open to internet) <--- Server1 <--- Client (Different network) ssh -R 2222 :127.0.0.0:22 # -R = Reverse forwarding","title":"Reverse Port Forwarding"},{"location":"learning/linux/security/#linux-firewall-fundamentals","text":"Firewalls control network access. Linux firewall = Netfilter + IPTables Netfilter is a kernel framework. IPTables is a packet selection system. Use the iptables command to control the firewall.","title":"Linux Firewall Fundamentals"},{"location":"learning/linux/security/#default-tables","text":"Filter - Most commonly used table. NAT - Network Address Translation. Mangle - Alter packets. Raw - Used to disable connection tracking. Security - Used by SELinux.","title":"Default Tables"},{"location":"learning/linux/security/#default-chains","text":"INPUT OUTPUT FORWARD PREROUTING POSTROUTING","title":"Default Chains"},{"location":"learning/linux/security/#rules","text":"Rules = Match + Target Match on: Protocol, Source/Dest IP or network, Source/Dest Port, Network Interface Example: protocol: TCP, source IP: 1.2.3.4, dest port: 80","title":"Rules"},{"location":"learning/linux/security/#targets","text":"Built-in targets: ACCEPT DROP REJECT LOG RETURN","title":"Targets"},{"location":"learning/linux/security/#iptables--ip6tables","text":"Command line interface to IPTables/netfilter.","title":"iptables / ip6tables"},{"location":"learning/linux/security/#list--view-iptables","text":"iptables -L # Display the filter table. iptables -t nat -L # Display the nat table. iptables -nL # Display using numeric output. iptables -vL # Display using verbose output. iptables --line-numbers -L # Use line nums.","title":"List / View iptables"},{"location":"learning/linux/security/#chain-policy--default-target","text":"Set the default TARGET for CHAIN: iptables -P CHAIN TARGET Example: iptables -P INPUT DROP # Appending Rules iptables -A CHAIN RULE-SPECIFICATION iptables [ -t TABLE ] -A CHAIN RULE-SPECIFICATION # Inserting Rules iptables -I CHAIN [ RULENUM ] RULE-SPECIFICATION # Deleting Rules iptables -D CHAIN RULE-SPECIFICATION iptables -D CHAIN RULENUM # Flushing rules or deleting tables iptables [ -t table ] -F [ chain ]","title":"Chain Policy / Default Target"},{"location":"learning/linux/security/#rule-specification-options","text":"-s 10 .11.12.13 # Source IP, network, or name -d 216 .58.192.0/19 # Destination IP, network, or name -p tcp / udp / icmp # Protocol -p tcp --dport 80 # Destination Port -p tcp --sport 8080 # Source Port -p icmp --icmp-type echo-reply # ICMP packet type, gives pong response -p icmp --icmp-type echo-request # Gives ping response # Rating limiting -m limit --limit rate [ /second/minute/hour/day ] # Match until a limit is reached. -m limit --limit-burst # --limit default is 3/hour and --limit-burst default is 5 -m limit --limit 5 /m --limit-burst 10 # /s = second, /m = minutes -m limit ! --limit 5 /s # ! = invert the match","title":"Rule Specification Options"},{"location":"learning/linux/security/#target--jump","text":"To specify a jump point or target: -j TARGET_OR_CHAIN -j ACCEPT # Built-in target. -j DROP # Built-in target. -j LOGNDROP # Custom chain.","title":"Target / Jump"},{"location":"learning/linux/security/#rule-specification-example","text":"# To drop all traffic from given source ip iptables -A INPUT -s 216 .58.219.174 -j DROP # List the filter table, as the above rule didnt specify table name iptables -nL # Chain INPUT (policy ACCEPT) # target prot opt source destination # DROP all -- 216.58.219.174 0.0.0.0/0 # Accept SSH connection from only one network, drop all other SSH connections iptables -A INPUT -s 10 .0.0.0/24 -p tcp --dport 22 -j ACCEPT iptables -A INPUT -p tcp --dport 22 -j DROP # DOS protection by implementing Rate limiting iptables -I INPUT -p tcp --dport 80 \\ -m limit --limit 50 /min --limit-burst 200 -j REJECT # Apply the below rate limit rule only for new connections and ignore for established connections iptables -I INPUT -p tcp --dport 80 \\ -m limit --limit 50 /min --limit-burst 200 \\ -m state --state NEW -j REJECT","title":"Rule Specification Example"},{"location":"learning/linux/security/#creating-and-deleting-a-chain","text":"Create CHAIN: iptables [-t table] -N CHAIN Delete CHAIN: iptables [-t table] -X CHAIN","title":"Creating and Deleting a Chain"},{"location":"learning/linux/security/#saving-rules","text":"# Debian / Ubuntu: apt-get install iptables-persistent netfilter-persistent save","title":"Saving Rules"},{"location":"learning/linux/security/#netfilteriptable-front-ends","text":"Uses iptables command on the back-end Firewalld - CentOS/RHEL UFW - Uncomplicated FireWall (Ubuntu) GUFW - Graphical interface to UFW system-configure-firewall - CentOS/RHEL","title":"Netfilter/iptable Front-Ends"},{"location":"learning/linux/security/#ip-tables-examples","text":"# Allow anyone to connect to webserver, but only internal ips to connect via SSH # Block all other traffic iptables -L # List the existing rules, no rules # Accept all incoming TCP traffic on port 80 iptables -A INPUT -p tcp --dport 80 -j ACCEPT iptables -nL # List the current new rule # Accept all incoming SSH traffic on port 22 originating from the internal network iptables -A INPUT -p tcp --dport 22 -s 10 .0.0.0/24 -j ACCEPT # Drop all packets which dont match above 2 rules iptables -A INPUT -j DROP # Testing the filters nc -v 10 .0.0.8 80 # Net cat using an internal IP on port 80 nc -v 10 .0.0.8 22 # Test the SSH connection # Deleting existing rule iptables -D INPUT 1 # Deletes the Web server traffic on port 80 nc -w 2 -v 10 .0.0.8 80 # -w specifies timeout of 2 seconds # Reject the packets instead of dropping them iptables -D INPUT 2 # Deletes the drop rule iptables -A INPUT -J REJECT # Now rejects the packets nc -w 2 -v 10 .0.0.8 80 # Connection refused # Flush all existing rules iptables -F iptables -A INPUT -p tcp --dport 22 -s 10 .0.0.0/24 -j ACCEPT # Create a new custom table iptables -N LOGNDROP # Create a rule to log all incoming traffic other than internal network SSH connection iptables -A LOGNDROP -p tcp -m limit --limit 5 /min -j LOG --log-prefix \"iptables BLOCK \" iptables -A INPUT -j LOGNDROP # Accept and log before dropping the connection other than internal network # In another terminal, tail the syslog file tail -f /var/log/syslog # Other terminal nc -w 2 -v 10 .0.0.8 80 # This will log and drop the connection # Persist the iptables changes after reboot netfilter- persistent save","title":"IP Tables Examples"},{"location":"learning/linux/security/#tcp-wrappers","text":"Host-based networking ACL system. Controls access to \u201cwrapped\u201d services. A wrapped service is compiled with libwrap support. Check if a service supports wrappers use ldd . ldd /usr/sbin/sshd | grep libwrap # Prints required shared libraries. Can control access by IP address / networks. Can control access by hostname. Transparent to the client and service. Used with xinetd which is a super daemon as it can manage a lot of smaller services, secures access to your server. Centralized management for multiple network services. Runtime configuration","title":"TCP Wrappers"},{"location":"learning/linux/security/#configuring-tcp-wrappers","text":"/etc/hosts.allow is checked first. If a match is found, access is granted . /etc/hosts.deny is checked next. If a match is found, access is denied .","title":"Configuring TCP Wrappers"},{"location":"learning/linux/security/#access-rules","text":"The rule format for hosts.allow and hosts.deny are the same. One rule per line Format: SERVICE(S) : CLIENT(S) [: ACTION(S) ] # Deny All configuration # /etc/hosts.deny: ALL : ALL # Deny all connections # /etc/hosts.allow: # Explicitly list allowed connections here. sshd : 10 .11.12.13 # Allow only ssh connection from this ip # Additional configuration possibilities sshd, imapd : 10 .11.12.13 # Multiple services allowed ALL : 10 .11.12.13 # Wild Card configuration sshd : 10 .11.12.13, 10 .5.6.7 # Multiple IPs sshd : jumpbox.example.com # Domain name sshd : .admin.example.com # Subdomain - server2.admin.example.com or webdev.admin.example.com sshd : jumpbox*.example.com # jumpbox4admins.example.com sshd : jumpbox0?.example.com # jumpbox03.example.com sshd : 10 .11.12. # Subnet range sshd : 10 . sshd : 10 .11.0.0/255.255.0.0 sshd : /etc/hosts.sshd # Allow all hosts in this file imapd : ALL # Wildcard in clients sshd : ALL EXCEPT .hackers.net # Conditions, except IP in hacker.net domains sshd : 10 .11.12.13 : severity emerg # Actions - Log emergency messages for that ssh connection coming from IP sshd : 10 .11.12.13 : severity local0.alert # Raise local alert # Raise custom message # /etc/hosts.deny: sshd : .hackers.net : spawn /usr/bin/wall \u201cAttack in progress.\u201d # Use expressions in message, %a logs client ip sshd : .hackers.net : spawn /usr/bin/wall \u201cAttack from %a.\u201d ## Possible expression expansions # %a (%A) The client (server) host address. # %c Client information. # %d The daemon process name. # %h (%H) The client (server) host name or address. # %n (%N) The client (server) host name. # %p The daemon process id. # %s Server information. # %u The client user name (or \"unknown\"). # %% Expands to a single `% \u0301 character.","title":"Access Rules"},{"location":"learning/linux/security/#file-security","text":"","title":"File Security"},{"location":"learning/linux/security/#file-permissions_1","text":"first letter in ls -l output Regular file: - Directory: d Symbolic link: l Read: r Write: w Execute: x","title":"File Permissions"},{"location":"learning/linux/security/#files-vs-directory","text":"r - Allows a file to be read. Allows file names in a directory to be read w - Allows a file to be modified. Allows entries to be modified within the directory x - Allows a file to be executed. Allows access to the contents and metdadata for entries","title":"Files vs Directory"},{"location":"learning/linux/security/#groups","text":"Every user in linux is in atleast one group Users can belong to many groups group command shows a user's group. Can also use id -Gn # Changing file permissions ls -l sales.data # Long list file permissions chmod g+w sales.data # Add Write permission to group chmod g-w sales.data # Remove Write permission to group chmod u+rwx,g-x sales.data # Add user all permissions and remove execute permission for group chmod a = r sales.data # Give all groups read permissions chmod o = sales.data # removes all permissions for others","title":"Groups"},{"location":"learning/linux/security/#directory-permissions","text":"Permissions on a directory can affect the files in the directory If the file permissions look correct, start checking the directory permissions Work your way up to the root Use chgrp to change group of a file # Changing directory permissions ls -l sales.data # Long list to show current group membership groups # Shows current user's groups (one is user and other is sales group) chgrp sales sales.data # Change file ownership from user to sales group chmod g+w sales.data # Give other members in sales group write access mkdir -p /usr/local/sales # Create a common directory for sales group mv sales.data /usr/local/sales # Now file and directory have the same permissions # Verify directory permissions mkdir -p my-cat # Create new directory touch my-cat/cat # Create new file chmod 755 my-cat # Modify permissions ls -ld my-cat # Shows permissions chmod 400 my-cat # Give only read permissions ls -ld my-cat # Gives permission error chmod 500 my-cat # Give write permission as well ls -ld my-cat # Gives output cat my-cat/cat # Shows file output","title":"Directory permissions"},{"location":"learning/linux/security/#file-creation-mask","text":"File creation mask are set by the admins This can be changed by per user basis File creation mask determines default permissions. If no mask were to be specified: deafault permissions would be 777 for directories and 666 for files","title":"File Creation Mask"},{"location":"learning/linux/security/#umask-command","text":"Sets the file creation mask to mode, if given Use -S for symbolic notation Format: umask [-S] [mode] mode of umask is opposite to chmod chmod - adds permissions. chmod 660 will give read and write permissions to file umask - subtracts permissions. umask 007 will give read an write permissions to file Common umask modes: 002, 022, 077, 007 # Test umask mkdir testumask cd testumask umask # Shows creation umask 0022 umask -S # Shows umask in symbolic mode mkdir newdir touch newfile ls -l # Shows dir permissions = 0755 and file permissions = 0644 rm newfile rmdir newdir umask 007 # Set new umask mkdir newdir touch newfile ls -l # Shows dir permissions = 0770 and file permissions = 0660","title":"umask command"},{"location":"learning/linux/security/#special-modes","text":"","title":"Special Modes"},{"location":"learning/linux/security/#setuid","text":"When a process is started, it runs using the starting user's UID and GID. setuid = Set User ID upon execution. Examples of binaries running with root and setuid -rwsr-xr-x 1 root root /usr/bin/passwd # Requires root permissions to modify passwd file ping chsh setuid files are an attack surface. Not honored on shell scripts. Octal Permissions: setuid=4, setgid=2 and sticky=1 # Adding the Setuid Attribute chmod u+s /path/to/file chmod 4755 /path/to/file # Removing the Setuid Attribute chmod u-s /path/to/file chmod 0755 /path/to/file # Finding Setuid Files find / -perm /4000 -ls # ls also lists files # Only the Owner Should Edit Setuid Files Good: Symbolic Octal -rwsr-xr-x 4755 Really bad: -rwsrwxrwx 4777","title":"Setuid"},{"location":"learning/linux/security/#setgid","text":"setgid = Set Group ID upon execution. -rwxr-sr-x 1 root tty /usr/bin/wall # s - set in group field crw--w---- 1 bob tty /dev/pts/0 # Adding the Setgid Attribute chmod g+s /path/to/file chmod 2755 /path/to/file # Adding the Setuid & Setgid Attributes chmod ug+s /path/to/file chmod 6755 /path/to/file # Removing the Setgid Attribute chmod g-s /path/to/file chmod 0755 /path/to/file # Finding Setgid Files find / -perm /2000 -ls","title":"Setgid"},{"location":"learning/linux/security/#setgid-on-directories","text":"setgid on a directory causes new files to inherit the group of the directory. setgid causes directories to inherit the setgid bit. Is not retroactive. Does not apply on existing files, but new files in directories. Great for working with groups.","title":"Setgid on Directories"},{"location":"learning/linux/security/#use-an-integrity-checker","text":"Other options to find . Tripwire AIDE (Advanced Intrusion Detection Environment) OSSEC Samhain Package managers","title":"Use an Integrity Checker"},{"location":"learning/linux/security/#sticky-bit","text":"Use on a directory to only allow the owner of the file/directory to delete it. Required on files or directories whcih are shared with multiple users or programs # Used on /tmp: drwxrwxrwt 10 root root 4096 Feb 1 09 :47 /tmp # Adding the Sticky Bit chmod o+s /path/to/directory chmod 1777 /path/to/directory # Removing the Sticky Bit chmod o-t /path/to/directory chmod 0777 /path/to/directory","title":"Sticky Bit"},{"location":"learning/linux/security/#reading-ls-output","text":"A capitalized special permission means the underlying normal permission is not set. A lowercase special permission means the underlying normal permission set. # execute permission is not set for user -rwSr--r-- 1 root root 0 Feb 14 11 :21 test # execute permission is set for the user -rwsr--r-- 1 root root 0 Feb 14 # execute permission is not set for group -rwxrwSr-- 1 root root 0 Feb 14 11 :21 test # execute permission is not set for others drwxr-xr-T 2 root root 0 Feb 14 11 :30 testd","title":"Reading ls Output"},{"location":"learning/linux/security/#file-attributes","text":"xattr: Supported by many file systems. ext2, ext3, ext4, XFS","title":"File Attributes"},{"location":"learning/linux/security/#attribute-i-immutable","text":"The file cannot be: modified, deleted, renamed, hard linked to Unset the attribute in order to delete it.","title":"Attribute: i immutable"},{"location":"learning/linux/security/#attribute-a-append","text":"Append only. Existing contents cannot be modified. Cannot be deleted while attribute is set. Use this attribute on log files. Safeguard the audit trail.","title":"Attribute: a append"},{"location":"learning/linux/security/#other-attributes","text":"Not every attribute is supported. man ext4, man xfs, man brtfs, etc. Example: s secure deletion","title":"Other Attributes"},{"location":"learning/linux/security/#modifying-attributes","text":"Use the chattr command. adds attributes. removes attributes. = sets the exact attributes. # Making the hosts file immutable lsattr /etc/hosts # No attributes present chattr +i /etc/hosts # Add immutable attribute vi /etc/hosts # Not able to write and save rm /etc/hosts # Not able to delete the file chattr -i /etc/hosts # Remove immutable attribute lsattr /etc/hosts # No attributes present vi /etc/hosts # Now able to write and save chattr +i /etc/hosts # Add immutable attribute back again # Making the apache logs files append only cd /var/log/httpd chattr = a * # Adding append attribute to all the files inside lsattr echo test >> access_log # Data will be appended cat access_log vi access_log # Not able to write and save","title":"Modifying Attributes"},{"location":"learning/linux/security/#access-control-lists","text":"ACL = Access Control List Provides additional control Example: Give one user access to a file. Traditional solution is to create another group. Increases management overhead of groups. Ensure file system mounted with ACL support XFS supports this by default # Mount files with ACL support to get this feature mount -o acl /path/to/dev /path/to/mount tune2fs -o acl /path/to/dev tune2fs -l /path/to/dev | grep options # Verify ACL","title":"Access Control Lists"},{"location":"learning/linux/security/#types-of-acls","text":"Access - Control access to a specific file or directory. Default - Used on directories only. Files without access rules use the default ACL rules. Not retroactive. Optional.","title":"Types of ACLs"},{"location":"learning/linux/security/#acls-can-be-configured","text":"Per user Per group For users not in the file\u2019s group Via the effective rights mask","title":"ACLs Can Be Configured:"},{"location":"learning/linux/security/#creating-acls","text":"Use the setfacl command. May need to install the ACL tools. Modify or add ACLs: setfacl -m ACL FILE_OR_DIRECTORY Detecting Files with ACLs: + at the end of ls -l permissions output # User ACLs / Rules # u:uid:perms Set the access ACL for a user. setfacl -m u:jason:rwx start.sh setfacl -m u:sam:xr start.sh # Group ACLs / Rules # g:gid:perms Sets the access ACL for a group. setfacl -m g:sales:rw sales.txt # Mask ACLs / Rules # m:perms Sets the effective rights mask. setfacl -m m:rx sales.txt # Other ACLs / Rules # o:perms Sets the access ACL for others. setfacl -m o:r sales.txt # Creating Multiple ACLs at Once setfacl -m u:bob:r,g:sales:rw sales.txt # Default ACLs # d:[ugo]:perms Sets the default ACL. setfacl -m d:g:sales:rw sales # Setting ACLs Recursively (-R) setfacl -R -m g:sales:rw sales # Removing ACLs for particular group or user setfacl -x ACL FILE_OR_DIRECTORY setfacl -x u:jason sales.txt # Note: Not supposed to give permissions when deleting ACL setfacl -x g:sales sales.txt # Removing All ACLs setfacl -b FILE_OR_DIRECTORY setfacl -b sales.txt # Viewing ACLs getfacl sales.txt # ACL for a shared project directory # Logged in as root user mkdir /usr/local/project cd /usr/local chgrp project project/ chmod 775 project/ # Give the project members access to the directory ls -ld project/ # Directory owned by root and project members have access su - sam # Switch user groups # same is in project group cd usr/local/project echo stuff > notes.txt ls -l notes.txt # sam is the owner of this file # Give bob who is not a member of project group access to this file setfacl -m u:bob:rw notes.txt # Modify access by sam to give read and write access getfacl notes.txt # List the permisions for bob su -bob # Switch user vi notes.txt # bob is able to write su -sam # Switch user setfacl -x u:bob notes.txt # Remove the rw permissions getfacl notes.txt # No permissions visble for bob su - # Switch to root cd project touch root-was-here # Create a new file owned by root in shared directory ls -l # No ACL is applied even though its in shared directory su sam # Switch to group user echo >> root-was-here # Permission denied su - # Switch to root cd /usr/local/project setfacl -m d:g:project:rw . # Set default ACL to project directory ls -ld . # Default ACL are added to directory denoted by + getfacl # Default ACL are added touch test # Create new file getfacl su sam vi test # Edit is possible setfacl -R -m g:project:rw . # Add recursively permissions to root-was-here file as well","title":"Creating ACLs"},{"location":"learning/linux/security/#rootkits","text":"Software used to gain root access and remain undetected. They attempt to hide from system administrators and antivirus software. User space rootkits replace common commands such as ls, ps, find, netstat, etc Kernel space rootkits add or replace parts of the core operating system.","title":"Rootkits"},{"location":"learning/linux/security/#rootkit-detection","text":"Use a file integrity checker for user space rootkits. (AIDE, tripwire, OSSEC, etc.) Identify inconsistent behavior of a system. High CPU utilization without corresponding processes. High network load or unusual connections. Kernel mode rootkits have to be running in order to hide themselves. Halt the system and examine the storage. Use a known good operating system to do the investigation. Use bootable media, for example.","title":"Rootkit Detection"},{"location":"learning/linux/security/#rootkit-hunter--rkhunter","text":"Shell script that searches for rootkits. rkhunter --update # Update database rkhunter --propupd # Update RKHunter configurations rkhunter -c # Interactive Check Mode cat /var/log/rkhunter.log # Log file location rkhunter -c --rwo # Report only warnings rkhunter --cronjob # Run as cronjob # After a fresh installation of OS # Install RK Hunter and then update the properties and database # Configure it to run everyday and report any issues by configuring it to log all output in a central place # Configure Cronjob crontab -e # Create a new cronjob # At the end run at modnight everyday # Redirect the output to single file 0 0 * * * /usr/local/bin/rkhunter --cronjob --update > /var/tmp/rkhunter.cronlog 2 > & 1","title":"Rootkit Hunter / RKHunter"},{"location":"learning/linux/security/#ossec","text":"Host Intrusion Detection System (HIDS) More than just rookit detection: log analysis, file integrity checking, alerting. Syscheck module - user mode rootkit detection. Rootcheck module - both user mode and kernel mode rootkit detection.","title":"OSSEC"},{"location":"learning/linux/security/#rootkit-removal","text":"Keep a copy of the data if possible. Learn how to keep it from happening again. Reinstall core OS components and start investigating. Not recommended. Easy to make a mistake. Safest is to reinstall the OS from trusted media.","title":"Rootkit Removal"},{"location":"learning/linux/security/#rootkit-prevention","text":"Use good security practices: Physical, Account Network Use file integrity monitoring: AIDE,Tripware, OSSEC Keep your systems patched.","title":"Rootkit Prevention"},{"location":"learning/linux/troubleshooting/","text":"Troubleshooting \u00b6 Scenarios Server Not reachable # Check server connectivity with outside world ping google.com # check for a connection nslookup google.com # Identify IP if DNS resolution is not working ping 172 .10.1.2 # IP address of google # Check DNS resolution issues of the server cat /etc/hosts # IP to DNS mapping ping localhost # Check connection to itself cat /etc/resolv.conf # Local DNS server details. Mostly router cat /etc/nsswitch.conf # Controls the flow of traffic # `hosts: files dns myhostname # This entry in `nsswitch.conf`, tells the first check in hosts file # If IP entry is not found, go to DNS Resolver i.e resolv.conf # Check if server has an IP address ifcongfig # Shows the Network configurations # Ping Gateway to check local connectivity netstat -rnv # Shows Gateway address # Netstat also shows the routing to the outside world. # This should show the destination 0.0.0.0 route to outside world Application / Webserver Not reachable # Server is reachable but application is not curl <IP>:<Port> # If no response, start the service # Login to the server and start the service ps -eaf | grep ntp # Shows if ntp process is running ps -eaf | grep -i network # Check if Network manager is running ps -eaf | grep httpd # If apache is down start it SSH Login Errors for root or normal users # Login to the server # Check Root can login using password. This would be off and should also be. # If Login is allowed, check if the password is wrong or account is locked # Check /var/log/secure more /var/log/secure # Check the error messages # Check if User does not exist or has nologin shell id bob # Shows user exists vi /etc/passwd # Shows nologin is set or not `/sbin/nologin` # Check if user acoount is disabled vi /etc/passwd # User information is not found or is commented # Check if ssh service is running ps -eaf | grep ssh systemctl restart sshd Firewall # Connection Refused errors service iptables status # Check if firewall service is running ps -eaf | grep iptables # Double check if service is up systemctl status ufw # Firewall service systemctl stop ufw # Temporray stop firewall to check connectivity cd chmod o+x secret/ # Executable permission is required to cd into a directory for any group su - sam # sam is in others group and has execute permission on dir cd secret # No error chmod o-x secret/ cd secret # Will fail # Read allows one to see the contents. ls -ltr # Will fail if read permission is not there for sam su - # Login to root chmod a+x /root # Give everyone permission to cd into root home directory su -sam cd /root # Allowed ls -ltr # Denied Read file or Execute a script Errors # Read permission is required to read a file chmod o-r README.md su - sam cat README.md # This will give permission denied # Reading directory will aslo throw an error chmod a+x script # Executable permission is required to execute a script for any group # 2 ways to execute this script # Give absolute path pwd /home/sam/script # This will execute # Give the current folder using dot notation ./script # This will execute # Error: If none of the above is done, \"Command not found\" error will be thrown as its not in path Finding Files or Directories Errors su - # Login as root find / -name \"hello.txt\" # Search for a file in the entire file system # Throws a permission denied error for /run/user filesystem which can be ignored Create Links Errors # Format: ln -s <source> <destination> # Important: source and directory has to be ABSOLUTE path ln -s hello.txt /tmp/greetings.txt # Link will get created cat /tmp/greetings.txt # No such file or directory. Why? Full path for hello.txt not given rm /tmp/greetings.txt # Remove the link pwd ln -s /home/sam/hello.txt /tmp/greetings.txt # Link will get created cat /tmp/greetings.txt # No Error and file is read # If you switch the destination and source position, it will throw an error ln -s /tmp/greetings.txt /home/sam/hello.txt # Error: File already exists # Source file should have read permissions # Owner of the source file can only create symbolic links # Destination where the file will land should also have read permissions for the group or others Write Errors Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility","title":"Troubleshooting"},{"location":"learning/linux/troubleshooting/#troubleshooting","text":"Scenarios Server Not reachable # Check server connectivity with outside world ping google.com # check for a connection nslookup google.com # Identify IP if DNS resolution is not working ping 172 .10.1.2 # IP address of google # Check DNS resolution issues of the server cat /etc/hosts # IP to DNS mapping ping localhost # Check connection to itself cat /etc/resolv.conf # Local DNS server details. Mostly router cat /etc/nsswitch.conf # Controls the flow of traffic # `hosts: files dns myhostname # This entry in `nsswitch.conf`, tells the first check in hosts file # If IP entry is not found, go to DNS Resolver i.e resolv.conf # Check if server has an IP address ifcongfig # Shows the Network configurations # Ping Gateway to check local connectivity netstat -rnv # Shows Gateway address # Netstat also shows the routing to the outside world. # This should show the destination 0.0.0.0 route to outside world Application / Webserver Not reachable # Server is reachable but application is not curl <IP>:<Port> # If no response, start the service # Login to the server and start the service ps -eaf | grep ntp # Shows if ntp process is running ps -eaf | grep -i network # Check if Network manager is running ps -eaf | grep httpd # If apache is down start it SSH Login Errors for root or normal users # Login to the server # Check Root can login using password. This would be off and should also be. # If Login is allowed, check if the password is wrong or account is locked # Check /var/log/secure more /var/log/secure # Check the error messages # Check if User does not exist or has nologin shell id bob # Shows user exists vi /etc/passwd # Shows nologin is set or not `/sbin/nologin` # Check if user acoount is disabled vi /etc/passwd # User information is not found or is commented # Check if ssh service is running ps -eaf | grep ssh systemctl restart sshd Firewall # Connection Refused errors service iptables status # Check if firewall service is running ps -eaf | grep iptables # Double check if service is up systemctl status ufw # Firewall service systemctl stop ufw # Temporray stop firewall to check connectivity cd chmod o+x secret/ # Executable permission is required to cd into a directory for any group su - sam # sam is in others group and has execute permission on dir cd secret # No error chmod o-x secret/ cd secret # Will fail # Read allows one to see the contents. ls -ltr # Will fail if read permission is not there for sam su - # Login to root chmod a+x /root # Give everyone permission to cd into root home directory su -sam cd /root # Allowed ls -ltr # Denied Read file or Execute a script Errors # Read permission is required to read a file chmod o-r README.md su - sam cat README.md # This will give permission denied # Reading directory will aslo throw an error chmod a+x script # Executable permission is required to execute a script for any group # 2 ways to execute this script # Give absolute path pwd /home/sam/script # This will execute # Give the current folder using dot notation ./script # This will execute # Error: If none of the above is done, \"Command not found\" error will be thrown as its not in path Finding Files or Directories Errors su - # Login as root find / -name \"hello.txt\" # Search for a file in the entire file system # Throws a permission denied error for /run/user filesystem which can be ignored Create Links Errors # Format: ln -s <source> <destination> # Important: source and directory has to be ABSOLUTE path ln -s hello.txt /tmp/greetings.txt # Link will get created cat /tmp/greetings.txt # No such file or directory. Why? Full path for hello.txt not given rm /tmp/greetings.txt # Remove the link pwd ln -s /home/sam/hello.txt /tmp/greetings.txt # Link will get created cat /tmp/greetings.txt # No Error and file is read # If you switch the destination and source position, it will throw an error ln -s /tmp/greetings.txt /home/sam/hello.txt # Error: File already exists # Source file should have read permissions # Owner of the source file can only create symbolic links # Destination where the file will land should also have read permissions for the group or others Write Errors Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility","title":"Troubleshooting"},{"location":"learning/linux/volumes/","text":"Why use LVM? \u00b6 Flexible Capacity \u00b6 One benefit of using LVM is that you can create file systems that extend across multiple storage devices. With LVM, you can aggregate multiple storage devices into a single logical volume. Easily Resize Storage While Online \u00b6 LVM allows you to expand or shrink filesystems in real-time while the data remains online and fully accessible. Without LVM you would have to reformat and repartition the underlying storage devices. Of course, you would have to take the file system offline to perform that work. LVM eliminates this problem. Online Data Relocation \u00b6 LVM also allows you to easily migrate data from one storage device to another while online. For example, if you want to deploy newer, faster, or more resilient storage, you can move your existing data from the current storage devices to the new ones while your system is active. Convenient Device Naming \u00b6 Instead of using abstract disk numbers, you can use human-readable device names of your choosing. Instead of wondering what data is on /dev/sdb, you can name your data with a descriptive name. Disk Striping \u00b6 With LVM, you can stripe data across two or more disks. This can dramatically increase throughput by allowing your system to read data in parallel. Data Redundancy / Data Mirroring \u00b6 If you want to increase fault tolerance and reliability, use LVM to mirror your data so that you always have more than one copy of your data. Using LVM mirroring prevents single points of failure. If one storage device fails, your data can be accessed via another storage device. You can then fix or replace the failed storage device to restore your mirror, without downtime. Snapshots \u00b6 LVM gives you the ability to create point-in-time snapshots of your filesystems. This can be perfect for when you need consistent backups. For example, you could pause writes to a database, take a snapshot of the logical volume where the database data resides, then resume writes to the database. That way you ensure your data is in a known-good state when you perform the backup of the snapshot. Layers of Abstraction in LVM \u00b6 The logical volume manager introduces extra layers of abstraction between the storage devices and the file systems placed on those storage devices. The first layer of abstraction is physical volumes. These are storage devices that are used by LVM. The name is a bit of a legacy name. To be clear, these storage devices do not have to be physical. They just have to be made available to the Linux operating system. In other words, as long as Linux sees the device as a block storage device, it can be used as a physical volume (PV). Physical hard drives, iSCSI devices, SAN disks, and so on can be PVs. You can allocate an entire storage device as a PV or you can partition a storage device and use just that one partition as a PV. The next layer of abstraction is the Volume Group. A volume group is made up of one or more PVs. You can think of a volume group as a pool of storage. If you want to increase the size of the pool, you simply add more PVs. Keep in mind that you can have different types of storage in the same volume group if you want. For example, you could have some PVs that are backed by hard drives and other PVs that are backed by san disks. The next layer of abstraction is the Logical Volume layer. Logical Volumes are created from a volume group. File systems are created on Logical Volumes. Without LVM you would create a file system on a disk partition, but with LVM you create a file system on a logical volume. As long as there is free space in the Volume Group, logical volumes can be extended. You can also shrink logical volumes to reclaim unused space if you want, but typically you'll find yourself extending logical volumes. Logical Volume Creation Process \u00b6 At a high level, the process for creating a logical volume is this: 1. Create one or more physical volumes. 2. Create a volume group from those one or more physical volumes. 3. Finally, you can create one or more logical volumes from the volume group. Creating Physical Volumes \u00b6 # Verify which devices are used su - lvmdiskscan # Shows all the storage devices that have the ability to be used with LVM. lsblk -p # Shows the partitions df -h # Display sizes in a human readable format. # Once verified which devices are not used # Create the physical volumes pvcreate /dev/sdb # initializes the disk for use by the logical volume manager. pvs # list of pvs Creating Volume Groups \u00b6 vgcreate vg_app /dev/sdb # Volume group naming convention of \"vg_\". vgs # view the volume groups # It shows that we have 1 physical volume. # The size of the volume group is 50GB and we have 50GB free in the volume group. # If we look at our pvs with the pvs command, we'll now see what VG our PV belongs to. Creating Logical Volumes \u00b6 lvcreate -L 20G -n lv_data vg_app # Logical Volume naming convention of \"lv_\". # Note captial L is to give human readable volume size lvs # view logical volumes lvdisplay # Also to view lv, but it provides different output. # For example, notice the LV Path: \u200b /dev/vg_app/lv_data\u200b # It's easy to tell that the \u200b lv_data\u200b logical volume belongs to the vg_app\u200b volume group. Creating File Systems \u00b6 Now that we have a logical volume, we can treat it like we would a normal disk partition. So, let's put a file system on our logical volume and mount it. mkfs -t ext4 /dev/vg_app/lv_data mkdir /data mount /dev/vg_app/lv_data /data df -h /data Creating Multiple Logical Volumes in Volume Groups \u00b6 lvcreate -L 5G -n lv_app vg_app # Now you can see we have two logical volumes in vg_app lvs # We can put a file system on this logical volume and mount it. mkfs -t ext4 /dev/vg_app/lv_app mkdir /app # Add Logical Volumes in the \u200b/etc/fstab\u200b so that it gets mounted at boot time. # vi /etc/fstab /dev/vg_app/lv_app /app ext4 defaults 0 0 # Save and quit mount /app df -h /app df -h # You also access your logical volume through the device mapper path as shown in the df output. # For example, \u200b /dev/vg_app/lv_app\u200b can be accessed via \u200b /dev/mapper/vg_app-lv_app\u200b . ls -l /dev/vg_app/lv_app ls -l /dev/mapper/vg_app-lv_app Logical Extents and Physical Extents \u00b6 There is yet another layer of abstraction that we haven't talked about. Well, two layers of abstraction, really. Each of our LVs is actually divided up into LEs, which stands for logical extents. Or if we look at it from the other direction, a collection of Logical Extents makes up a Logical Volume. This is how LVM allows us to expand or shrink a logical volume \u2013 the logical volume manager just changes the number of underlying Logical Extents for the logical volume. lvdisplay -m # Show a map of the logical volume. # This map view tells us that the logical extents for the \u200blv_app\u200b LV resize on the \u200b/dev/sdb\u200b disk. # Like an LV is divided into LE, a PV is divided into PE, physical extents. # There is a one-to-one mapping of LEs to PEs. pvdisplay -m # Shows a map from the view of the disk Create Logical Volumes using percentage \u00b6 lvcreate -l 100 %FREE -n lv_logs vg_app # Note small l is used with % or passing free extents # We can squeeze every possible bit of space out of your volume group and put it into a logical volume. Extending Volume Groups \u00b6 Let's say that the \u200blv_data\u200b logical volume is getting full and we need to extend it. If we look at our volume group, we'll see we've already allocated all our space to the existing logical volumes. vgs # Output shows VFREE = 0 # In this case, we need to extend the volume group itself # before we can extend the logical volume within that volume group. lvmdiskscan # Shows sdc is free pvcreate /dev/sdc vgextend vg_app /dev/sdc # add this PV to our VG vgs # Output shows VFREE = 50G pvs # We have one pv that is complete full and one that is completely free. # Before we extend our logical volume into this free space, # let's look at the space from the file system level. df -h /data Extending Logical Volumes \u00b6 Now, let's use the lvextend command to add 5GB of space to that logical volume. In addition to increasing the size of the logical volume, we also need to grow the file system on that logical volume to fill up this new space. That's what the \u200b -r\u200b option is for. lvextend -L +5G -r /dev/vg_app/lv_data df -h /data # Shows filesystem has increased by 5G # If you forget to use the \u200b-r\u200b option to lvextend to perform the resize, you'll have to do that after the fact. lvextend -L +5G /dev/vg_app/lv_data lvs # lv size has increased df -h /data # The file system is still the same size, and there is no 5G increase # To fix this you'll have to use a resize tool for the specific filesystem you're working with. # For ext file systems, that tool is \u200bresize2fs\u200b. # We give it the path the the underlying device, which is a logical volume in our case. resize2fs /dev/vg_app/lv_data df -h /data # Shows filesystem has increased by 5G lvdisplay -m /dev/vg_app/lv_data # We see that some of the extents live on \u200b/dev/sdb\u200b while other extents live on \u200b/dev/sdc\u200b . Creating Mirrored Logical Volumes \u00b6 Mirrored logical volume will ensure that an exact copy of the data will be stored on two different storage devices. lvmdiskscan # We have two more disks that we can use sdd and sde. pvcreate /dev/sdd /dev/sde vgcreate vg_safe /dev/sdd /dev/sde lvcreate -m 1 -L 5G -n lv_secrets vg_safe lvs # Copy%Sync column indicates if the mirror is synced. # When it finished syncing the data, it would be at 100%. lvs -a # The logical volume we created is actually RAID 1. So a mirror and raid 1 are the same thing. # Let's create a file system on that logical volume and mount it. mkfs -t ext4 /dev/vg_safe/lv_secrets mkdir /secrets mount /dev/vg_safe/lv_secrets /secrets df -h /secrets # So we write to \u200b /dev/vg_safe/lv_secrets\u200b and let the logical volume manager handle the mirroring. # We just use this file system like any other. Deleting Logical Volumes, Volume Groups, and Physical Volumes \u00b6 # unmount the file system that is mounted inside logical volumes. umount /secrets # we can remove the underlying logical volume. lvremove /dev/vg_safe/lv_secrets # If you want to remove a pv from a vg, use the vgreduce command. vgs vgreduce vg_safe /dev/sde pvs pvremove /dev/sde # Let's finish destroying the \u200b vg_safe\u200b volume group with \u200b vgremove\u200b . vgremove vg_safe vgs pvs pvremove /dev/sdd Migrating Data While Online \u00b6 Early I mentioned how easy it is to move data from one storage device to another with LVM. Let's say that the storage device attached to our system at \u200b /dev/sde \u200b is faster and has more space. Let's say we want to move the data that currently resides on \u200b /dev/sdb\u200b to that new disk. To do that, we'll just add \u200b /dev/sde \u200b to the volume group and migrate the data over. # we \u200bpvcreate\u200b the device. pvcreate /dev/sde # Now we add it to the volume group. vgextend vg_app /dev/sde pvs # Finally, we use the \u200bpvmove\u200b command to move all the data from \u200b/dev/sdb\u200b to \u200b/dev/sde\u200b pvmove /dev/sdb /dev/sde Once the \u200b pvmove\u200b command is complete, all the data the used to live on \u200b /dev/sdb\u200b now lives on \u200b /dev/sde\u200b . And the whole time this was happening, any logical volumes and file systems that where on \u200b /dev/sdb remained online and available throughout this entire process. There's no need to take an outage for this process. pvs # Now \u200b/dev/sdb\u200b is unused pvdisplay /dev/sdb # We see that \"Allocated PE\" is zero. # Now that we're done with this disk we can remove it from the volume group with \u200bvgreduce\u200b vgreduce vg_app /dev/sdb pvremove /dev/sdb pvs","title":"Why use LVM?"},{"location":"learning/linux/volumes/#why-use-lvm","text":"","title":"Why use LVM?"},{"location":"learning/linux/volumes/#flexible-capacity","text":"One benefit of using LVM is that you can create file systems that extend across multiple storage devices. With LVM, you can aggregate multiple storage devices into a single logical volume.","title":"Flexible Capacity"},{"location":"learning/linux/volumes/#easily-resize-storage-while-online","text":"LVM allows you to expand or shrink filesystems in real-time while the data remains online and fully accessible. Without LVM you would have to reformat and repartition the underlying storage devices. Of course, you would have to take the file system offline to perform that work. LVM eliminates this problem.","title":"Easily Resize Storage While Online"},{"location":"learning/linux/volumes/#online-data-relocation","text":"LVM also allows you to easily migrate data from one storage device to another while online. For example, if you want to deploy newer, faster, or more resilient storage, you can move your existing data from the current storage devices to the new ones while your system is active.","title":"Online Data Relocation"},{"location":"learning/linux/volumes/#convenient-device-naming","text":"Instead of using abstract disk numbers, you can use human-readable device names of your choosing. Instead of wondering what data is on /dev/sdb, you can name your data with a descriptive name.","title":"Convenient Device Naming"},{"location":"learning/linux/volumes/#disk-striping","text":"With LVM, you can stripe data across two or more disks. This can dramatically increase throughput by allowing your system to read data in parallel.","title":"Disk Striping"},{"location":"learning/linux/volumes/#data-redundancy--data-mirroring","text":"If you want to increase fault tolerance and reliability, use LVM to mirror your data so that you always have more than one copy of your data. Using LVM mirroring prevents single points of failure. If one storage device fails, your data can be accessed via another storage device. You can then fix or replace the failed storage device to restore your mirror, without downtime.","title":"Data Redundancy / Data Mirroring"},{"location":"learning/linux/volumes/#snapshots","text":"LVM gives you the ability to create point-in-time snapshots of your filesystems. This can be perfect for when you need consistent backups. For example, you could pause writes to a database, take a snapshot of the logical volume where the database data resides, then resume writes to the database. That way you ensure your data is in a known-good state when you perform the backup of the snapshot.","title":"Snapshots"},{"location":"learning/linux/volumes/#layers-of-abstraction-in-lvm","text":"The logical volume manager introduces extra layers of abstraction between the storage devices and the file systems placed on those storage devices. The first layer of abstraction is physical volumes. These are storage devices that are used by LVM. The name is a bit of a legacy name. To be clear, these storage devices do not have to be physical. They just have to be made available to the Linux operating system. In other words, as long as Linux sees the device as a block storage device, it can be used as a physical volume (PV). Physical hard drives, iSCSI devices, SAN disks, and so on can be PVs. You can allocate an entire storage device as a PV or you can partition a storage device and use just that one partition as a PV. The next layer of abstraction is the Volume Group. A volume group is made up of one or more PVs. You can think of a volume group as a pool of storage. If you want to increase the size of the pool, you simply add more PVs. Keep in mind that you can have different types of storage in the same volume group if you want. For example, you could have some PVs that are backed by hard drives and other PVs that are backed by san disks. The next layer of abstraction is the Logical Volume layer. Logical Volumes are created from a volume group. File systems are created on Logical Volumes. Without LVM you would create a file system on a disk partition, but with LVM you create a file system on a logical volume. As long as there is free space in the Volume Group, logical volumes can be extended. You can also shrink logical volumes to reclaim unused space if you want, but typically you'll find yourself extending logical volumes.","title":"Layers of Abstraction in LVM"},{"location":"learning/linux/volumes/#logical-volume-creation-process","text":"At a high level, the process for creating a logical volume is this: 1. Create one or more physical volumes. 2. Create a volume group from those one or more physical volumes. 3. Finally, you can create one or more logical volumes from the volume group.","title":"Logical Volume Creation Process"},{"location":"learning/linux/volumes/#creating-physical-volumes","text":"# Verify which devices are used su - lvmdiskscan # Shows all the storage devices that have the ability to be used with LVM. lsblk -p # Shows the partitions df -h # Display sizes in a human readable format. # Once verified which devices are not used # Create the physical volumes pvcreate /dev/sdb # initializes the disk for use by the logical volume manager. pvs # list of pvs","title":"Creating Physical Volumes"},{"location":"learning/linux/volumes/#creating-volume-groups","text":"vgcreate vg_app /dev/sdb # Volume group naming convention of \"vg_\". vgs # view the volume groups # It shows that we have 1 physical volume. # The size of the volume group is 50GB and we have 50GB free in the volume group. # If we look at our pvs with the pvs command, we'll now see what VG our PV belongs to.","title":"Creating Volume Groups"},{"location":"learning/linux/volumes/#creating-logical-volumes","text":"lvcreate -L 20G -n lv_data vg_app # Logical Volume naming convention of \"lv_\". # Note captial L is to give human readable volume size lvs # view logical volumes lvdisplay # Also to view lv, but it provides different output. # For example, notice the LV Path: \u200b /dev/vg_app/lv_data\u200b # It's easy to tell that the \u200b lv_data\u200b logical volume belongs to the vg_app\u200b volume group.","title":"Creating Logical Volumes"},{"location":"learning/linux/volumes/#creating-file-systems","text":"Now that we have a logical volume, we can treat it like we would a normal disk partition. So, let's put a file system on our logical volume and mount it. mkfs -t ext4 /dev/vg_app/lv_data mkdir /data mount /dev/vg_app/lv_data /data df -h /data","title":"Creating File Systems"},{"location":"learning/linux/volumes/#creating-multiple-logical-volumes-in-volume-groups","text":"lvcreate -L 5G -n lv_app vg_app # Now you can see we have two logical volumes in vg_app lvs # We can put a file system on this logical volume and mount it. mkfs -t ext4 /dev/vg_app/lv_app mkdir /app # Add Logical Volumes in the \u200b/etc/fstab\u200b so that it gets mounted at boot time. # vi /etc/fstab /dev/vg_app/lv_app /app ext4 defaults 0 0 # Save and quit mount /app df -h /app df -h # You also access your logical volume through the device mapper path as shown in the df output. # For example, \u200b /dev/vg_app/lv_app\u200b can be accessed via \u200b /dev/mapper/vg_app-lv_app\u200b . ls -l /dev/vg_app/lv_app ls -l /dev/mapper/vg_app-lv_app","title":"Creating Multiple Logical Volumes in Volume Groups"},{"location":"learning/linux/volumes/#logical-extents-and-physical-extents","text":"There is yet another layer of abstraction that we haven't talked about. Well, two layers of abstraction, really. Each of our LVs is actually divided up into LEs, which stands for logical extents. Or if we look at it from the other direction, a collection of Logical Extents makes up a Logical Volume. This is how LVM allows us to expand or shrink a logical volume \u2013 the logical volume manager just changes the number of underlying Logical Extents for the logical volume. lvdisplay -m # Show a map of the logical volume. # This map view tells us that the logical extents for the \u200blv_app\u200b LV resize on the \u200b/dev/sdb\u200b disk. # Like an LV is divided into LE, a PV is divided into PE, physical extents. # There is a one-to-one mapping of LEs to PEs. pvdisplay -m # Shows a map from the view of the disk","title":"Logical Extents and Physical Extents"},{"location":"learning/linux/volumes/#create-logical-volumes-using-percentage","text":"lvcreate -l 100 %FREE -n lv_logs vg_app # Note small l is used with % or passing free extents # We can squeeze every possible bit of space out of your volume group and put it into a logical volume.","title":"Create Logical Volumes using percentage"},{"location":"learning/linux/volumes/#extending-volume-groups","text":"Let's say that the \u200blv_data\u200b logical volume is getting full and we need to extend it. If we look at our volume group, we'll see we've already allocated all our space to the existing logical volumes. vgs # Output shows VFREE = 0 # In this case, we need to extend the volume group itself # before we can extend the logical volume within that volume group. lvmdiskscan # Shows sdc is free pvcreate /dev/sdc vgextend vg_app /dev/sdc # add this PV to our VG vgs # Output shows VFREE = 50G pvs # We have one pv that is complete full and one that is completely free. # Before we extend our logical volume into this free space, # let's look at the space from the file system level. df -h /data","title":"Extending Volume Groups"},{"location":"learning/linux/volumes/#extending-logical-volumes","text":"Now, let's use the lvextend command to add 5GB of space to that logical volume. In addition to increasing the size of the logical volume, we also need to grow the file system on that logical volume to fill up this new space. That's what the \u200b -r\u200b option is for. lvextend -L +5G -r /dev/vg_app/lv_data df -h /data # Shows filesystem has increased by 5G # If you forget to use the \u200b-r\u200b option to lvextend to perform the resize, you'll have to do that after the fact. lvextend -L +5G /dev/vg_app/lv_data lvs # lv size has increased df -h /data # The file system is still the same size, and there is no 5G increase # To fix this you'll have to use a resize tool for the specific filesystem you're working with. # For ext file systems, that tool is \u200bresize2fs\u200b. # We give it the path the the underlying device, which is a logical volume in our case. resize2fs /dev/vg_app/lv_data df -h /data # Shows filesystem has increased by 5G lvdisplay -m /dev/vg_app/lv_data # We see that some of the extents live on \u200b/dev/sdb\u200b while other extents live on \u200b/dev/sdc\u200b .","title":"Extending Logical Volumes"},{"location":"learning/linux/volumes/#creating-mirrored-logical-volumes","text":"Mirrored logical volume will ensure that an exact copy of the data will be stored on two different storage devices. lvmdiskscan # We have two more disks that we can use sdd and sde. pvcreate /dev/sdd /dev/sde vgcreate vg_safe /dev/sdd /dev/sde lvcreate -m 1 -L 5G -n lv_secrets vg_safe lvs # Copy%Sync column indicates if the mirror is synced. # When it finished syncing the data, it would be at 100%. lvs -a # The logical volume we created is actually RAID 1. So a mirror and raid 1 are the same thing. # Let's create a file system on that logical volume and mount it. mkfs -t ext4 /dev/vg_safe/lv_secrets mkdir /secrets mount /dev/vg_safe/lv_secrets /secrets df -h /secrets # So we write to \u200b /dev/vg_safe/lv_secrets\u200b and let the logical volume manager handle the mirroring. # We just use this file system like any other.","title":"Creating Mirrored Logical Volumes"},{"location":"learning/linux/volumes/#deleting-logical-volumes-volume-groups-and-physical-volumes","text":"# unmount the file system that is mounted inside logical volumes. umount /secrets # we can remove the underlying logical volume. lvremove /dev/vg_safe/lv_secrets # If you want to remove a pv from a vg, use the vgreduce command. vgs vgreduce vg_safe /dev/sde pvs pvremove /dev/sde # Let's finish destroying the \u200b vg_safe\u200b volume group with \u200b vgremove\u200b . vgremove vg_safe vgs pvs pvremove /dev/sdd","title":"Deleting Logical Volumes, Volume Groups, and Physical Volumes"},{"location":"learning/linux/volumes/#migrating-data-while-online","text":"Early I mentioned how easy it is to move data from one storage device to another with LVM. Let's say that the storage device attached to our system at \u200b /dev/sde \u200b is faster and has more space. Let's say we want to move the data that currently resides on \u200b /dev/sdb\u200b to that new disk. To do that, we'll just add \u200b /dev/sde \u200b to the volume group and migrate the data over. # we \u200bpvcreate\u200b the device. pvcreate /dev/sde # Now we add it to the volume group. vgextend vg_app /dev/sde pvs # Finally, we use the \u200bpvmove\u200b command to move all the data from \u200b/dev/sdb\u200b to \u200b/dev/sde\u200b pvmove /dev/sdb /dev/sde Once the \u200b pvmove\u200b command is complete, all the data the used to live on \u200b /dev/sdb\u200b now lives on \u200b /dev/sde\u200b . And the whole time this was happening, any logical volumes and file systems that where on \u200b /dev/sdb remained online and available throughout this entire process. There's no need to take an outage for this process. pvs # Now \u200b/dev/sdb\u200b is unused pvdisplay /dev/sdb # We see that \"Allocated PE\" is zero. # Now that we're done with this disk we can remove it from the volume group with \u200bvgreduce\u200b vgreduce vg_app /dev/sdb pvremove /dev/sdb pvs","title":"Migrating Data While Online"},{"location":"server/","text":"Server Details \u00b6 Linux Package Management Basics DNS Basics","title":"Server Details"},{"location":"server/#server-details","text":"Linux Package Management Basics DNS Basics","title":"Server Details"},{"location":"server/arm/","text":"Amlogic S905W Installation Download Ubuntu ARM 64 Bit OS check for focal current image.xz package On Windows use Etcher to update the image after extraction into SD Card. On Linux, go to /boot/dtb/meson-gxl-s905w-p281.dtb and copy the file name. Use the above file name and update file /boot/extlinux/extlinux.conf as per instruction Amlogic S905W Installation. Copy uboot file at root named u-boot-s905x-s912 as u-boot.ext Configure wireless network details # Use su to edit the below files. /etc/network/interfaces # Add or uncomment the lines allow-hotplug wlan0 iface wlan0 inet manual wpa-roam /etc/wpa_supplicant/wpa_supplicant.conf post-up ifdown eth0 iface default inet dhcp /etc/wpa_supplicant/wpa_supplicant.conf # Add below contents and update Wifi details ctrl_interface = DIR = /var/run/wpa_supplicant GROUP = netdev update_config = 1 network ={ ssid = \"YOUR_SSID_HERE\" psk = \"YOUR_SECRET_PASSPHRASE_HERE\" id_str = \"SOME_DESCRIPTIVE_NAME\" } Extract SD Card and insert inside Tx device to boot from new SD Card. After boot, check the IP address assigned in Router DHCP Client list SSH into Tx device IP as ssh root@IP , default passw0rd is 1234. Create new Sudo user and reset passwd. Update OS, type armbian-config in the terminal to configure the device. System \u2192 Firmware \u2192 Update System and then reboot sudo apt-update if that fails for libc-bin package, force pin the version and install it. sudo apt install libc6 = 2 .31-0ubuntu9.2 libc-bin = 2 .31-0ubuntu9.2 sudo apt-get update -y sudo apt-get upgrade -y sudo apt-get autoremove -y && sudo apt-get clean -y Update NAND frequency in case EEMC (internal 2GB RAM and 8GB ROM) is not detected. Use this guide . # You need to unpack your dtb file into dts via device-tree-compile tool. sudo apt-get install device-tree-compiler # decompile the dtb file dtc -I dtb -O dts -o meson-gxl-s905w-p281.dts meson-gxl-s905w-p281.dtb # Edit using sudo in BOOT/dtb/amlogic searching for \"mmc@74000\" block max-frequency = <0x5f5e100> ; 5f5e100 in hex = 100000000 in dec edit it to 0x2faf080, 50000000 in dec # compile with dtc -I dts -O dtb -o meson-gxl-s905w-p281.dtb meson-gxl-s905w-p281.dts # Delete the dts file # Remove SD card and inside into Tx device Get system diagonastics armbianmonitor -u , which is stored in html. Copy Boot directory to internal EEMC chip using ./install-aml.sh present in the root home directory. Remove the SD card and use the Tx device henceforth. After reboot usig EEMC, armbian-config \u2013 System and 3 rd Party Software, Install Full Firmware Packages. Use armbian-config to set static IP to Tx device nmcli con show sudo nmtui # Use UI to update IP Product Specs of Amlogic S905W Android TV Box \u00b6 Model NO. M18 CPU Amlogic S905W 64bits Quad-Core Cortex-A53@1.5GHz GPU Penta-core ARM\u00ae Mali\u2122-450 RAM 1G ROM 8G WIFI Wi-Fi 802.11 b/g/n Memory Card Support TF card, up to 32GB HOST USB 2.0 Operating System Android 7.1 Bluetooth No BT for 1G+8G Product Size 100 100 10mm","title":"Arm"},{"location":"server/arm/#product-specs-of-amlogic-s905w-android-tv-box","text":"Model NO. M18 CPU Amlogic S905W 64bits Quad-Core Cortex-A53@1.5GHz GPU Penta-core ARM\u00ae Mali\u2122-450 RAM 1G ROM 8G WIFI Wi-Fi 802.11 b/g/n Memory Card Support TF card, up to 32GB HOST USB 2.0 Operating System Android 7.1 Bluetooth No BT for 1G+8G Product Size 100 100 10mm","title":"Product Specs of Amlogic S905W Android TV Box"},{"location":"server/install/","text":"Installation \u00b6 Centos \u00b6 Identifying Version Download the latest version of Centos from centos.org Creating Live USB Format a USB as NTFS. Download Fedora LiveUSB Creator or read the supported software for Centos in the downloads page. See the instructions to write the ISO into the USB Setting Up PC Format the HDD (optional) Ensure - Advanced Option, Legacy USB Support is Enabled In Boot priority, USB is the first drive Ensure the Boot option has Boot from USB selected above HDD Configuring Centos Full Installation Steps Create Custom partitions for /boot, /, /var, /tmp, /home Initial Server Setup Update Centos with latest patches yum update -y && yum upgrade -y yum clean all # To clean downloaded packages in /var/cache/yum Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Adding Non Root User Add and Delete Users Creating Volume Groups Securing Server Security based on articles mentioned here or here or DigitalOcean -keygen -t rsa -b 4096 It will create two files: id_rsa and id_rsa.pub in the ~/.ssh directory. When you add a passphrase to your SSH key, it encrypts the private key using 128-bit AES so that the private key is useless without the passphrase to decrypt it. cat ~/.ssh/id_rsa.pub For example, if we had a Linux VM named myserver with a user azureuser, we could use the following command to install the public key file and authorize the user with the key: ssh-copy-id -i ~/.ssh/id_rsa.pub azureuser@myserver Connect with SSH ssh jim@137.117.101.249 Try executing a few Linux commands ls -la / to show the root of the disk ps -l to show all the running processes dmesg to list all the kernel messages lsblk to list all the block devices - here you will see your drives Initialize data disks Any additional drives you create from scratch need to be initialized and formatted. dmesg | grep SCSI - list all the messages from the kernel for SCSI devices. Use fdisk to initialize the drive We can use the following command to create a new primary partition: (echo n; echo p; echo 1; echo ; echo ; echo w) | sudo fdisk /dev/sdc We need to write a file system to the partition with the mkfs command. sudo mkfs -t ext4 /dev/sdc1 We need to mount the drive to the file system. sudo mkdir /data & sudo mount /dev/sdc1 /data Always make sure to lock down ports used for administrative access. An even better approach is to create a VPN to link the virtual network to your private network and only allow RDP or SSH requests from that address range. You can also change the port used by SSH to be something other than the default. Keep in mind that changing ports is not sufficient to stop attacks. It simply makes it a little harder to discover.","title":"Installation"},{"location":"server/install/#installation","text":"","title":"Installation"},{"location":"server/install/#centos","text":"Identifying Version Download the latest version of Centos from centos.org Creating Live USB Format a USB as NTFS. Download Fedora LiveUSB Creator or read the supported software for Centos in the downloads page. See the instructions to write the ISO into the USB Setting Up PC Format the HDD (optional) Ensure - Advanced Option, Legacy USB Support is Enabled In Boot priority, USB is the first drive Ensure the Boot option has Boot from USB selected above HDD Configuring Centos Full Installation Steps Create Custom partitions for /boot, /, /var, /tmp, /home Initial Server Setup Update Centos with latest patches yum update -y && yum upgrade -y yum clean all # To clean downloaded packages in /var/cache/yum Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Adding Non Root User Add and Delete Users Creating Volume Groups Securing Server Security based on articles mentioned here or here or DigitalOcean -keygen -t rsa -b 4096 It will create two files: id_rsa and id_rsa.pub in the ~/.ssh directory. When you add a passphrase to your SSH key, it encrypts the private key using 128-bit AES so that the private key is useless without the passphrase to decrypt it. cat ~/.ssh/id_rsa.pub For example, if we had a Linux VM named myserver with a user azureuser, we could use the following command to install the public key file and authorize the user with the key: ssh-copy-id -i ~/.ssh/id_rsa.pub azureuser@myserver Connect with SSH ssh jim@137.117.101.249 Try executing a few Linux commands ls -la / to show the root of the disk ps -l to show all the running processes dmesg to list all the kernel messages lsblk to list all the block devices - here you will see your drives Initialize data disks Any additional drives you create from scratch need to be initialized and formatted. dmesg | grep SCSI - list all the messages from the kernel for SCSI devices. Use fdisk to initialize the drive We can use the following command to create a new primary partition: (echo n; echo p; echo 1; echo ; echo ; echo w) | sudo fdisk /dev/sdc We need to write a file system to the partition with the mkfs command. sudo mkfs -t ext4 /dev/sdc1 We need to mount the drive to the file system. sudo mkdir /data & sudo mount /dev/sdc1 /data Always make sure to lock down ports used for administrative access. An even better approach is to create a VPN to link the virtual network to your private network and only allow RDP or SSH requests from that address range. You can also change the port used by SSH to be something other than the default. Keep in mind that changing ports is not sufficient to stop attacks. It simply makes it a little harder to discover.","title":"Centos"},{"location":"server/mobile/","text":"Converting Android Device Into Linux Server \u00b6 Centos \u00b6 Pre-requisites Identify an Android mobile phone Factory reset to remove any unwanted apps and data Remove any apps or services from the device to free up space and memory Install Termux and AnLinux from Playstore Installation Start AnLinux and search for the distro. Select Centos Copy the installation command Open Termux terminal and paste the command Begin installation Access Mobile Configure Static Ip to Mobile Add Ip address and remote-hostname to Laptop's hosts file. Install SSH on Termux apt install openssh Start the ssh server sshd . This will also generate the private and public keys. Test the service locally ssh localhost -p 8022 . Accept the public key. ssh-copy-id copies the local-host\u2019s public key to the remote-host\u2019s authorized_keys file. ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-hostname> -p 8022 . Enter remote user password to complete transaction. Login from Laptop ssh <hostname> -p 8022 . Enter the passphrase. Install Python Configuring Centos Update Centos with latest patches yum install update -y && yum install upgrade -y Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Init Centos yum -y install initscripts & yum clean all Change Root Password passwd root Install Sudo yum install sudo -y The difference between wheel and sudo . In CentOS, a user belonging to the wheel group can execute su and directly ascend to root. Meanwhile, a sudo user would have use the sudo su first. Essentially, there is no real difference except for the syntax used to become root, and users belonging to both groups can use the sudo command. * Adding Non Root User bmmnb Add and Delete Users Securing Server Security based on articles mentioned here or here or DigitalOcean Configures a Hostname Reconfigures the Timezone Updates the entire System Creates a New Admin user so you can manage your server safely without the need of doing remote connections with root. Helps user Generate Secure RSA Keys, so that remote access to your server is done exclusive from your local pc and no Conventional password Configures, Optimize and secures the SSH Server (Some Settings Following CIS Benchmark) Configures IPTABLES Rules to protect the server from common attacks Disables unused FileSystems and Network protocols Protects the server against Brute Force attacks by installing a configuring fail2ban Installs and Configure Artillery as a Honeypot, Monitoring, Blocking and Alerting tool Installs PortSentry Installs RootKit Hunter Secures Root Home and Grub Configuration Files Installs Unhide to help Detect Malicious Hidden Processes Installs Tiger, A Security Auditing and Intrusion Prevention system Restrict Access to Apache Config Files Disables Compilers Creates Daily Cron job for System Updates Kernel Hardening via sysctl configuration File (Tweaked) /tmp Directory Hardening PSAD IDS installation Enables Process Accounting Enables Unattended Upgrades MOTD and Banners for Unauthorized access Disables USB Support for Improved Security (Optional) Configures a Restrictive Default UMASK Configures and enables Auditd Configures Auditd rules following CIS Benchmark Sysstat install ArpWatch install Additional Hardening steps following CIS Benchmark Secures Cron Automates the process of setting a GRUB Bootloader Password Secures Boot Settings Sets Secure File Permissions for Critical System Files","title":"Mobile"},{"location":"server/mobile/#converting-android-device-into-linux-server","text":"","title":"Converting Android Device Into Linux Server"},{"location":"server/mobile/#centos","text":"Pre-requisites Identify an Android mobile phone Factory reset to remove any unwanted apps and data Remove any apps or services from the device to free up space and memory Install Termux and AnLinux from Playstore Installation Start AnLinux and search for the distro. Select Centos Copy the installation command Open Termux terminal and paste the command Begin installation Access Mobile Configure Static Ip to Mobile Add Ip address and remote-hostname to Laptop's hosts file. Install SSH on Termux apt install openssh Start the ssh server sshd . This will also generate the private and public keys. Test the service locally ssh localhost -p 8022 . Accept the public key. ssh-copy-id copies the local-host\u2019s public key to the remote-host\u2019s authorized_keys file. ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-hostname> -p 8022 . Enter remote user password to complete transaction. Login from Laptop ssh <hostname> -p 8022 . Enter the passphrase. Install Python Configuring Centos Update Centos with latest patches yum install update -y && yum install upgrade -y Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Init Centos yum -y install initscripts & yum clean all Change Root Password passwd root Install Sudo yum install sudo -y The difference between wheel and sudo . In CentOS, a user belonging to the wheel group can execute su and directly ascend to root. Meanwhile, a sudo user would have use the sudo su first. Essentially, there is no real difference except for the syntax used to become root, and users belonging to both groups can use the sudo command. * Adding Non Root User bmmnb Add and Delete Users Securing Server Security based on articles mentioned here or here or DigitalOcean Configures a Hostname Reconfigures the Timezone Updates the entire System Creates a New Admin user so you can manage your server safely without the need of doing remote connections with root. Helps user Generate Secure RSA Keys, so that remote access to your server is done exclusive from your local pc and no Conventional password Configures, Optimize and secures the SSH Server (Some Settings Following CIS Benchmark) Configures IPTABLES Rules to protect the server from common attacks Disables unused FileSystems and Network protocols Protects the server against Brute Force attacks by installing a configuring fail2ban Installs and Configure Artillery as a Honeypot, Monitoring, Blocking and Alerting tool Installs PortSentry Installs RootKit Hunter Secures Root Home and Grub Configuration Files Installs Unhide to help Detect Malicious Hidden Processes Installs Tiger, A Security Auditing and Intrusion Prevention system Restrict Access to Apache Config Files Disables Compilers Creates Daily Cron job for System Updates Kernel Hardening via sysctl configuration File (Tweaked) /tmp Directory Hardening PSAD IDS installation Enables Process Accounting Enables Unattended Upgrades MOTD and Banners for Unauthorized access Disables USB Support for Improved Security (Optional) Configures a Restrictive Default UMASK Configures and enables Auditd Configures Auditd rules following CIS Benchmark Sysstat install ArpWatch install Additional Hardening steps following CIS Benchmark Secures Cron Automates the process of setting a GRUB Bootloader Password Secures Boot Settings Sets Secure File Permissions for Critical System Files","title":"Centos"},{"location":"server/proxy/","text":"Nginx Reverse Proxy Setup","title":"Proxy"},{"location":"server/service/","text":"Create a systemd unit file for starting the application: \u00b6 Example service file can be found here: $ wget https://gist.githubusercontent.com/Artemmkin/ce82397cfc69d912df9cd648a8d69bec/raw/7193a36c9661c6b90e7e482d256865f085a853f2/raddit.service Move it to the systemd directory $ sudo mv raddit.service /etc/systemd/system/raddit.service Now start the application and enable autostart: $ sudo systemctl start raddit $ sudo systemctl enable raddit Verify that it's running: $ sudo systemctl status raddit","title":"Create a systemd unit file for starting the application:"},{"location":"server/service/#create-a-systemd-unit-file-for-starting-the-application","text":"Example service file can be found here: $ wget https://gist.githubusercontent.com/Artemmkin/ce82397cfc69d912df9cd648a8d69bec/raw/7193a36c9661c6b90e7e482d256865f085a853f2/raddit.service Move it to the systemd directory $ sudo mv raddit.service /etc/systemd/system/raddit.service Now start the application and enable autostart: $ sudo systemctl start raddit $ sudo systemctl enable raddit Verify that it's running: $ sudo systemctl status raddit","title":"Create a systemd unit file for starting the application:"},{"location":"server/volume-groups/","text":"Configuration \u00b6 Centos \u00b6 Prerequisites Install EPEL Packages yum install epel-release Install ntfs-3g and fuse packages to support NTFS drives yum install ntfs-3g fuse -y Format the new NTFS filestsystem to xfs mkfs.xfs -f /dev/sda Identifying Additional HDD Run blkid to verify if HDD is detected. On detection of HDD, follow the below steps to configure as XFS file system. * Run fdisk -l to list the devices, partitions and volume groups * Scan your hard drive for Bad Sectors or Bad Blocks 1. Partitioning HDD (Optional and for Primary HDD) * On identifying the device(HDD), say /dev/sda as the target device to partition, run fdisk /dev/sda * These steps below are not for secondary HDD as partitioning will not utilize the entire space * Option m to list the operations, Option p to print the current partitions * Note the start and end block ids of the current partitions * Option d to delete the current partitions, select the partition to delete 2 * Option n to create new partition, select only 1 partition for the HDD, select default start and end block id, it should match the one printed above * Option t to select the type of partition, followed by 8e to select as Linux LVM * Format the new partition to xfs mkfs.xfs -f /dev/sda1 as only one partition was created * Mount the new partition to test mount -t xfs /dev/sda1 /data * Make entry in /etc/fstab to make it permanent * Verify the mounted partition by running df -Th to see free and used space Securing Drive and adding Redundancy Software RAID and LVM For the partitions that are needed as Raid 1, use fdsik /dev/sdb3 ,Press \u2018t\u2019 to change the partition type. Press \u2018fd\u2019 to choose Linux Raid Auto. Install Raid 1 on /dev/sdb3 and /dev/sda1 using mdadm --create /dev/data --level=1 --raid-devices=2 /dev/sdb3 /dev/sda1 Encrypting Raid devices Volume Group Setup Logical Volume Manager (LVM) is a software-based RAID-like system that lets you create \"pools\" of storage and add hard drive space to those pools as needed. * LVM Basic * Create, expand, and encrypt storage pools Volume Group Lifecycle Migrate Data from one LV to another Identify the current Physical Volume and Volume Group mapping pvscan or pvs List the partitions and volumes in the Volume Groups vgdisplay centos-digital -v Logical volume and device mapping lvs -o+devices List all devices and device mapping lsscsi Before Reducing logical volume, move to another drive Format /dev/sdb3 to a physical volume: pvcreate /dev/sdb3 Give /dev/sdb3 to the volume group centos-digital: vgextend centos-digital /dev/sdb3 Migrate our logical volume Users from /dev/sdb2 to /dev/sdb3, pvmove -n Users /dev/sdb2 /dev/sdb3 Verify if the data is migrated lsblk Reduce your logical volume to make place for your new partition lvresize -L 25G /dev/centos-digital/home Use fdisk to reduce the size of /dev/sdb2 to 80G as total for volume group centos-digital was 75G (yes, a little bit bigger as we used in lvresize!) Use fdisk to create a new LVM partition, /dev/sdb3, with the new available space (it will be around 400G). Check if the kernel could automatically detect the partition change. See if /proc/partition matches the new state (thus, /dev/sdb3 is visible). If not, you need to reboot. (Probably it will.) Make /dev/sdb2 to be as big again pvresize /dev/sdb2","title":"Configuration"},{"location":"server/volume-groups/#configuration","text":"","title":"Configuration"},{"location":"server/volume-groups/#centos","text":"Prerequisites Install EPEL Packages yum install epel-release Install ntfs-3g and fuse packages to support NTFS drives yum install ntfs-3g fuse -y Format the new NTFS filestsystem to xfs mkfs.xfs -f /dev/sda Identifying Additional HDD Run blkid to verify if HDD is detected. On detection of HDD, follow the below steps to configure as XFS file system. * Run fdisk -l to list the devices, partitions and volume groups * Scan your hard drive for Bad Sectors or Bad Blocks 1. Partitioning HDD (Optional and for Primary HDD) * On identifying the device(HDD), say /dev/sda as the target device to partition, run fdisk /dev/sda * These steps below are not for secondary HDD as partitioning will not utilize the entire space * Option m to list the operations, Option p to print the current partitions * Note the start and end block ids of the current partitions * Option d to delete the current partitions, select the partition to delete 2 * Option n to create new partition, select only 1 partition for the HDD, select default start and end block id, it should match the one printed above * Option t to select the type of partition, followed by 8e to select as Linux LVM * Format the new partition to xfs mkfs.xfs -f /dev/sda1 as only one partition was created * Mount the new partition to test mount -t xfs /dev/sda1 /data * Make entry in /etc/fstab to make it permanent * Verify the mounted partition by running df -Th to see free and used space Securing Drive and adding Redundancy Software RAID and LVM For the partitions that are needed as Raid 1, use fdsik /dev/sdb3 ,Press \u2018t\u2019 to change the partition type. Press \u2018fd\u2019 to choose Linux Raid Auto. Install Raid 1 on /dev/sdb3 and /dev/sda1 using mdadm --create /dev/data --level=1 --raid-devices=2 /dev/sdb3 /dev/sda1 Encrypting Raid devices Volume Group Setup Logical Volume Manager (LVM) is a software-based RAID-like system that lets you create \"pools\" of storage and add hard drive space to those pools as needed. * LVM Basic * Create, expand, and encrypt storage pools Volume Group Lifecycle Migrate Data from one LV to another Identify the current Physical Volume and Volume Group mapping pvscan or pvs List the partitions and volumes in the Volume Groups vgdisplay centos-digital -v Logical volume and device mapping lvs -o+devices List all devices and device mapping lsscsi Before Reducing logical volume, move to another drive Format /dev/sdb3 to a physical volume: pvcreate /dev/sdb3 Give /dev/sdb3 to the volume group centos-digital: vgextend centos-digital /dev/sdb3 Migrate our logical volume Users from /dev/sdb2 to /dev/sdb3, pvmove -n Users /dev/sdb2 /dev/sdb3 Verify if the data is migrated lsblk Reduce your logical volume to make place for your new partition lvresize -L 25G /dev/centos-digital/home Use fdisk to reduce the size of /dev/sdb2 to 80G as total for volume group centos-digital was 75G (yes, a little bit bigger as we used in lvresize!) Use fdisk to create a new LVM partition, /dev/sdb3, with the new available space (it will be around 400G). Check if the kernel could automatically detect the partition change. See if /proc/partition matches the new state (thus, /dev/sdb3 is visible). If not, you need to reboot. (Probably it will.) Make /dev/sdb2 to be as big again pvresize /dev/sdb2","title":"Centos"}]}