{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Leslie's Notebook Install mkdocs using command pip install mkdocs Topics Server - Setup basic Linux server Kubernetes - Setup Kubernetes IDE - Tips and Tricks","title":"Welcome"},{"location":"#welcome-to-leslies-notebook","text":"Install mkdocs using command pip install mkdocs","title":"Welcome to Leslie's Notebook"},{"location":"#topics","text":"Server - Setup basic Linux server Kubernetes - Setup Kubernetes IDE - Tips and Tricks","title":"Topics"},{"location":"ide/","text":"IDE Tips and Tricks VS Code Intellij How to open multiple modules in the same workspace? Open the first module / project. Select File -> Project Structure -> Modules. Select Copy (icon) and open Browse your local directory and select the project you would like to add. Module name will resolve automatically. When you want to reopen to project with multiple sub-projects, in order to avoid re-doing steps as described above, just go to File -> Open Recent -> 'Your Big Project'. IDE Shortcuts Option Description Ctrl + Space Code Completion Ctrl + Shift + / Comments selected lines of code Ctrl + N Navigate to any class by typing the name in the editor Ctrl + B Jump to declaration","title":"Introduction"},{"location":"ide/#ide-tips-and-tricks","text":"","title":"IDE Tips and Tricks"},{"location":"ide/#vs-code","text":"","title":"VS Code"},{"location":"ide/#intellij","text":"How to open multiple modules in the same workspace? Open the first module / project. Select File -> Project Structure -> Modules. Select Copy (icon) and open Browse your local directory and select the project you would like to add. Module name will resolve automatically. When you want to reopen to project with multiple sub-projects, in order to avoid re-doing steps as described above, just go to File -> Open Recent -> 'Your Big Project'. IDE Shortcuts Option Description Ctrl + Space Code Completion Ctrl + Shift + / Comments selected lines of code Ctrl + N Navigate to any class by typing the name in the editor Ctrl + B Jump to declaration","title":"Intellij"},{"location":"ide/markdown/","text":"General Syntax MarkDown Cheat Sheet Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Horizontal Rule ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks Emphasis Bold **rendered as bold text** Italics _rendered as italicized text_ BlockQuote > Blockquotes Nested BlockQuote > First Level >> Second Level >>> Third Level List Unordered * or + or - Ordered 1. Code Block ` or [```html] (3 backticks and follwed by the language) Links Inline Links [Text](http://text.io) Link titles [Text](https://github.com/site/ \"Visit Site!\") Images ![Minion](http://octodex.github.com/images/minion.png)","title":"Markdown"},{"location":"ide/markdown/#general-syntax","text":"MarkDown Cheat Sheet Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Horizontal Rule ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks Emphasis Bold **rendered as bold text** Italics _rendered as italicized text_ BlockQuote > Blockquotes Nested BlockQuote > First Level >> Second Level >>> Third Level List Unordered * or + or - Ordered 1. Code Block ` or [```html] (3 backticks and follwed by the language) Links Inline Links [Text](http://text.io) Link titles [Text](https://github.com/site/ \"Visit Site!\") Images ![Minion](http://octodex.github.com/images/minion.png)","title":"General Syntax"},{"location":"k8s/","text":"Kubernetes","title":"Introduction"},{"location":"k8s/#kubernetes","text":"","title":"Kubernetes"},{"location":"k8s/install/","text":"Installing K8s Kubeadm Kops","title":"Installation"},{"location":"k8s/install/#installing-k8s","text":"","title":"Installing K8s"},{"location":"k8s/install/#kubeadm","text":"","title":"Kubeadm"},{"location":"k8s/install/#kops","text":"","title":"Kops"},{"location":"learning/ansible/","text":"Introduction Ansible 101 Ansible Cheat Sheet Launching situational commands To check the inventory file ansible-inventory --list -y Test Connection ansible all -m ping -u root Check the disk usage of all servers ansible all -a \"df -h\" -u root Check the time of uptime each host in a group servers ansible servers -a \"uptime\" -u root Specify multiple hosts by separating their names with colons ansible server1:server2 -m ping -u root - hosts : host01 --- become : true tasks : - name : ensure latest sysstat is installed apt : name : sysstat state : latest # ansible-playbook -i myhosts site.yml # ansible host01 -i myhosts -m copy -a \"src=test.txt dest=/tmp/\" # ansible host01 -i myhosts -m file -a \"dest=/tmp/test mode=644 state=directory\" # ansible host01 -i myhosts -m apt -a \"name=sudo state=latest\" # ansible host01 -i myhosts -m shell -a \"echo $TERM\" # ansible host01 -i myhosts -m command -a \"mkdir folder1\" --- - name : install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name : apache2 update_cache : yes state : latest --- - hosts : host01 tasks : - stat : path : /home/ubuntu/folder1/something.j2 register : st - name : Template to copy file template : src : ./something.j2 dest : /home/ubuntu/folder1/something.j2 mode : '0644' when : st.stat.exists == False # Looping - name : add several users user : name : \"{{ item }}\" state : present groups : \"developer\" with_items : - raj - david - john - lauren # show all the hosts in the inventory - debug : msg : \"{{ item }}\" with_items : - \"{{ groups['all'] }}\" # show all the hosts in the current play - debug : msg : \"{{ item }}\" with_items : - \"{{ play_hosts }}\" # This Playbook will add Java Packages to different systems (handling Ubuntu/Debian OS) - name : debian | ubuntu | add java ppa repo apt_repository : repo=ppa:webupd8team/java state=present become : yes when : ansible_distribution == 'Ubuntu' - name : debian | ensure the webupd8 launchpad apt repository is present apt_repository : repo=\"{{ item }} http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" update_cache=yes state=present with_items : - deb - deb-src become : yes when : ansible_distribution == 'Debian --- - name : install software hosts : host01 sudo : yes tasks : - name : Update and upgrade apt packages apt : upgrade : dist update_cache : yes cache_valid_time : 86400 - name : install software apt : name : \"{{item}}\" update_cache : yes state : installed with_items : - nginx - postgresql - postgresql-contrib - libpq-dev - python-psycopg2 - name : Ensure the Nginx service is running service : name : nginx state : started enabled : yes - name : Ensure the PostgreSQL service is running service : name : postgresql state : started enabled : yes #file: playbook.yml --- - hosts : all vars_files : - vars.yml tasks : - debug : msg=\"Variable 'var' is set to {{ var }}\" #file: vars.yml --- var : 20 #host variables - Variables are defined inline for individual host [ group1 ] host1 http_port=80 host2 http_port=303 #group variables - Variables are applied to entire group of hosts [ group1 : vars ] ntp_server= example.com proxy=proxy.example.com ### Whenever you run Playbook, Ansible by default collects information (facts) about each host ### like host IP address, CPU type, disk space, operating system information etc. ansible host01 -i myhosts -m setup # Consider you need the IP address of all the servers in you web group using 'group' variable {% for host in groups.web %} server {{ host.inventory_hostname }} {{ host.ansible_default_ipv4.address }}:8080 {% endfor %} # get a list of all the variables associated with the current host with the help of hostvars and inventory_hostname variables. --- - name : built-in variables hosts : all tasks : - debug : var=hostvars[inventory_hostname] # register variable stores the output, after executing command module, in contents variable # stdout is used to access string content of register variable --- - name : check registered variable for emptiness hosts : all tasks : - name : list contents of the directory in the host command : ls /home/ubuntu register : contents - name : check dir is empty debug : msg=\"Directory is empty\" when : contents.stdout == \"\" - name : check dir has contents debug : msg=\"Directory is not empty\" when : contents.stdout != \"\" # Variable Precedence => Command Line > Playbook > Facts > Roles # CLI: While running the playbook in Command Line redefine the variable ansible-playbook -i myhosts test.yml --extra-vars \"ansible_bios_version=Ansible\" # Tags are names pinned on individual tasks, roles or an entire play, that allows you to run or skip parts of your Playbook. # Tags can help you while testing certain parts of your Playbook. --- - name : Play1-install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name=apache2 update_cache=yes state=latest - name : displaying \"hello world\" debug : msg=\"hello world\" tags : - tag1 - name : Play2-install nginx hosts : all sudo : yes tags : - tag2 tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : debug module displays message in control machine debug : msg=\"have a good day\" tags : - mymessage - name : shell module displays message in host machine. shell : echo \"yet another task\" tags : - mymessage ansible-playbook -i myhosts tag.yml --list-tasks # displays the list of tasks in the Playbook ansible-playbook -i myhosts tag.yml --list-tags # displays only tags in your Playbook ansible-playbook -i myhosts tag.yml --tags \"tag1,mymessage\" # executes only certain tasks which are tagged as tag1 and mymessage # Ansible gives you the flexibility of organizing your tasks through include keyword, that introduces more abstraction and make your Playbook more easily maintainable, reusable and powerful. --- - name : testing includes hosts : all sudo : yes tasks : - include : apache.yml - include : content.yml - include : create_folder.yml - include : content.yml - include : nginx.yml # apache.yml will not hav hosts & tasks but nginx.yml as a separate play will have tasks and can run independently #nginx.yml --- - name : installing nginx hosts : all sudo : yes tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : displaying message debug : msg=\"yayy!! nginx installed\" # A Role is completely self contained or encapsulated and completely reusable # cat ansible.cfg [ defaults ] host_key_checking=False inventory = /etc/ansible/myhosts log_path = /home/scrapbook/tutorial/output.txt roles_path = /home/scrapbook/tutorial/roles/ # master-playbook.yml --- - name : my first role in ansible hosts : all sudo : yes roles : - sample_role - sample_role2 # sample_role/tasks/main.yml --- - include : nginx.yml - include : copy-template.yml - include : copy-static.yml # sample_role/tasks/nginx.yml --- - name : Installs nginx apt : pkg=nginx state=installed update_cache=true notify : - start nginx # sample_role/handlers/main.yml --- - name : start nginx service : name=nginx state=started # sample_role/tasks/copy-template.yml --- - name : sample template - x template : src : template-file.j2 dest : /home/ubuntu/copy-template-file.j2 with_items : var_x # sample_role/vars/main.yml var_x : - 'variable x' var_y : - 'variable y' # sample_role/tasks/copy-static.yml # Ensure some-file.txt is present under files folder of the role --- - name : Copy a file copy : src=some-file.txt dest=/home/ubuntu/file1.txt # Let us run the master_playbook and check the output: ansible-playbook -i myhosts master_playbook.yml # ansible-galaxy is command line tool for scaffolding the creation of directory structure needed for organizing your code ansible-galaxy init sample_role # ansible-galaxy useful commands # Install a Role from Ansible Galaxy To use others role, visit https://galaxy.ansible.com/ and search for the role that will achieve your goal. Goal : Install Apache in the host machines. # Ensure that roles_path are defined in ansible.cfg for a role to successfully install. ansible-galaxy install geerlingguy.apache # Here, apache is role name and geerlingguy is name of the user in GitHub who created the role. # Forcefully Recreate Role ansible-galaxy init geerlingguy.apache --force # Listing Installed Roles ansible-galaxy list # Remove an Installed Role ansible-galaxy remove geerlingguy.apache # Environment Variables # Ansible recommends maintaining inventory file for each environment, instead of keeping all your hosts in a single inventory. # Each environment directory has one inventory file (hosts) and group_vars directory.","title":"Ansible"},{"location":"learning/ansible/#introduction","text":"Ansible 101 Ansible Cheat Sheet","title":"Introduction"},{"location":"learning/ansible/#launching-situational-commands","text":"To check the inventory file ansible-inventory --list -y Test Connection ansible all -m ping -u root Check the disk usage of all servers ansible all -a \"df -h\" -u root Check the time of uptime each host in a group servers ansible servers -a \"uptime\" -u root Specify multiple hosts by separating their names with colons ansible server1:server2 -m ping -u root - hosts : host01 --- become : true tasks : - name : ensure latest sysstat is installed apt : name : sysstat state : latest # ansible-playbook -i myhosts site.yml # ansible host01 -i myhosts -m copy -a \"src=test.txt dest=/tmp/\" # ansible host01 -i myhosts -m file -a \"dest=/tmp/test mode=644 state=directory\" # ansible host01 -i myhosts -m apt -a \"name=sudo state=latest\" # ansible host01 -i myhosts -m shell -a \"echo $TERM\" # ansible host01 -i myhosts -m command -a \"mkdir folder1\" --- - name : install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name : apache2 update_cache : yes state : latest --- - hosts : host01 tasks : - stat : path : /home/ubuntu/folder1/something.j2 register : st - name : Template to copy file template : src : ./something.j2 dest : /home/ubuntu/folder1/something.j2 mode : '0644' when : st.stat.exists == False # Looping - name : add several users user : name : \"{{ item }}\" state : present groups : \"developer\" with_items : - raj - david - john - lauren # show all the hosts in the inventory - debug : msg : \"{{ item }}\" with_items : - \"{{ groups['all'] }}\" # show all the hosts in the current play - debug : msg : \"{{ item }}\" with_items : - \"{{ play_hosts }}\" # This Playbook will add Java Packages to different systems (handling Ubuntu/Debian OS) - name : debian | ubuntu | add java ppa repo apt_repository : repo=ppa:webupd8team/java state=present become : yes when : ansible_distribution == 'Ubuntu' - name : debian | ensure the webupd8 launchpad apt repository is present apt_repository : repo=\"{{ item }} http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" update_cache=yes state=present with_items : - deb - deb-src become : yes when : ansible_distribution == 'Debian --- - name : install software hosts : host01 sudo : yes tasks : - name : Update and upgrade apt packages apt : upgrade : dist update_cache : yes cache_valid_time : 86400 - name : install software apt : name : \"{{item}}\" update_cache : yes state : installed with_items : - nginx - postgresql - postgresql-contrib - libpq-dev - python-psycopg2 - name : Ensure the Nginx service is running service : name : nginx state : started enabled : yes - name : Ensure the PostgreSQL service is running service : name : postgresql state : started enabled : yes #file: playbook.yml --- - hosts : all vars_files : - vars.yml tasks : - debug : msg=\"Variable 'var' is set to {{ var }}\" #file: vars.yml --- var : 20 #host variables - Variables are defined inline for individual host [ group1 ] host1 http_port=80 host2 http_port=303 #group variables - Variables are applied to entire group of hosts [ group1 : vars ] ntp_server= example.com proxy=proxy.example.com ### Whenever you run Playbook, Ansible by default collects information (facts) about each host ### like host IP address, CPU type, disk space, operating system information etc. ansible host01 -i myhosts -m setup # Consider you need the IP address of all the servers in you web group using 'group' variable {% for host in groups.web %} server {{ host.inventory_hostname }} {{ host.ansible_default_ipv4.address }}:8080 {% endfor %} # get a list of all the variables associated with the current host with the help of hostvars and inventory_hostname variables. --- - name : built-in variables hosts : all tasks : - debug : var=hostvars[inventory_hostname] # register variable stores the output, after executing command module, in contents variable # stdout is used to access string content of register variable --- - name : check registered variable for emptiness hosts : all tasks : - name : list contents of the directory in the host command : ls /home/ubuntu register : contents - name : check dir is empty debug : msg=\"Directory is empty\" when : contents.stdout == \"\" - name : check dir has contents debug : msg=\"Directory is not empty\" when : contents.stdout != \"\" # Variable Precedence => Command Line > Playbook > Facts > Roles # CLI: While running the playbook in Command Line redefine the variable ansible-playbook -i myhosts test.yml --extra-vars \"ansible_bios_version=Ansible\" # Tags are names pinned on individual tasks, roles or an entire play, that allows you to run or skip parts of your Playbook. # Tags can help you while testing certain parts of your Playbook. --- - name : Play1-install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name=apache2 update_cache=yes state=latest - name : displaying \"hello world\" debug : msg=\"hello world\" tags : - tag1 - name : Play2-install nginx hosts : all sudo : yes tags : - tag2 tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : debug module displays message in control machine debug : msg=\"have a good day\" tags : - mymessage - name : shell module displays message in host machine. shell : echo \"yet another task\" tags : - mymessage ansible-playbook -i myhosts tag.yml --list-tasks # displays the list of tasks in the Playbook ansible-playbook -i myhosts tag.yml --list-tags # displays only tags in your Playbook ansible-playbook -i myhosts tag.yml --tags \"tag1,mymessage\" # executes only certain tasks which are tagged as tag1 and mymessage # Ansible gives you the flexibility of organizing your tasks through include keyword, that introduces more abstraction and make your Playbook more easily maintainable, reusable and powerful. --- - name : testing includes hosts : all sudo : yes tasks : - include : apache.yml - include : content.yml - include : create_folder.yml - include : content.yml - include : nginx.yml # apache.yml will not hav hosts & tasks but nginx.yml as a separate play will have tasks and can run independently #nginx.yml --- - name : installing nginx hosts : all sudo : yes tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : displaying message debug : msg=\"yayy!! nginx installed\" # A Role is completely self contained or encapsulated and completely reusable # cat ansible.cfg [ defaults ] host_key_checking=False inventory = /etc/ansible/myhosts log_path = /home/scrapbook/tutorial/output.txt roles_path = /home/scrapbook/tutorial/roles/ # master-playbook.yml --- - name : my first role in ansible hosts : all sudo : yes roles : - sample_role - sample_role2 # sample_role/tasks/main.yml --- - include : nginx.yml - include : copy-template.yml - include : copy-static.yml # sample_role/tasks/nginx.yml --- - name : Installs nginx apt : pkg=nginx state=installed update_cache=true notify : - start nginx # sample_role/handlers/main.yml --- - name : start nginx service : name=nginx state=started # sample_role/tasks/copy-template.yml --- - name : sample template - x template : src : template-file.j2 dest : /home/ubuntu/copy-template-file.j2 with_items : var_x # sample_role/vars/main.yml var_x : - 'variable x' var_y : - 'variable y' # sample_role/tasks/copy-static.yml # Ensure some-file.txt is present under files folder of the role --- - name : Copy a file copy : src=some-file.txt dest=/home/ubuntu/file1.txt # Let us run the master_playbook and check the output: ansible-playbook -i myhosts master_playbook.yml # ansible-galaxy is command line tool for scaffolding the creation of directory structure needed for organizing your code ansible-galaxy init sample_role # ansible-galaxy useful commands # Install a Role from Ansible Galaxy To use others role, visit https://galaxy.ansible.com/ and search for the role that will achieve your goal. Goal : Install Apache in the host machines. # Ensure that roles_path are defined in ansible.cfg for a role to successfully install. ansible-galaxy install geerlingguy.apache # Here, apache is role name and geerlingguy is name of the user in GitHub who created the role. # Forcefully Recreate Role ansible-galaxy init geerlingguy.apache --force # Listing Installed Roles ansible-galaxy list # Remove an Installed Role ansible-galaxy remove geerlingguy.apache # Environment Variables # Ansible recommends maintaining inventory file for each environment, instead of keeping all your hosts in a single inventory. # Each environment directory has one inventory file (hosts) and group_vars directory.","title":"Launching situational commands"},{"location":"learning/git/","text":"Create a new repository on the command line git init git add README.md git commit -m \"first commit\" git branch -M master git remote add origin https://github.com/leslieclif/dotfiles.git git push -u origin master Git Commands cd into git folder \u2192 ls -la \u2192 cd .git \u2192 ls -la Git help git help To quit help \u2192 q Best practise: Always do a pull before a push to merge changes from remote git pull origin master To git add and git commit for tracked files in a single comand use -a git commit -am \"Commit message\" Amend Commit message git commit --amend \"New commit message\" Check for tracked files in git git ls-files Back out changes that have been commited, but not pushed to remote. Once unstaged, you can remove the changes using checkout git reset HEAD git checkout -- Rename file-name. It also automatically stages the changes, so need to do git add git mv level3--file.txt level3.txt If file is renamed outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A git add -A Moving files and staging the changes git mv level2.txt new-folder If file is moved outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A mv level2.txt .. git add -A file renamed in OS. But say, git has identifed unwanted files during git status and you dont want to add those files, then don't use -A, use -u Individually add the new renamed file first then update git git add level1.txt git add -u Delete files tracked by git git rm doomed.txt If file is delete outside git, it will delete and is not staged. To add and stage the deleted file use -A git add -A Git History git log To quit help \u2192 q Git history in one line git log --oneline --graph --decorate Git history using duration git log --since=\"3 days ago\" Show all user actions git reflog Show commit history \u2192 do git log get commit id git show TODO: Get a git diff tool Show git config git config --global --list Compare with staging and current changes git diff Compare between current changes and remote last commit git diff HEAD Compare between staging and remote last commit git diff --staged HEAD Compare file changes with staging and current changes git diff -- Compare between commits (do git log to get commits) git diff Compare local and remote branches git diff master origin/master Compare local branches git diff master test-branch Branching List local and remote branches git branch -a Create new branch git branch Rename local branch git branch -m Delete a branch. Note: You have to be on another bracnh before you can delete the target branch git branch -d Create new branch and switch to it in single command git checkout -b Fash forward Merges \u2192 First switch to the target branches, do a git diff to review the changes. git merge Disable fast forward merge \u2192 Give tracing of merge by giving a custom merge message and also the commit history of the branch git merge --no-ff Automatic merge git merge -m \" \" Merge Conflict and Resolution Inside the merging workspace incase of conflict, open the conflicting file in editor or the merge-diff tool. Resolve conflict and close the file. Rebase feature branch from master git checkout feature-branch git rebase master Abort rebase git rebase --abort Rebase conflict resolution \u2192 Use merging tool to fix conflict, save and quit. Add file to git staging, then continue rebase git rebase --continue Pull with Rebase (Rebase local master with remote master) git fetch origin master (non destructive merge which only updates references) git pull --rebase origin master Stash git stash Stash + saving untracked files of git as well git stash -u Get the stash back to local git stash apply List the stash git stash list Drop the stash git stash drop Combination of apply and drop in one command. Brings the last saved state git stash pop Multiple Stashes git stash save \" \" Show any arbitary stash changes whithout popping. Do stash list first to get the stash ID git stash show stash@{1} Apply any arbitary stash changes. Do stash list first to get the stash ID git stash apply stash@{1} Drop any arbitary stash changes that was applied or not needed. git stash drop stash@{1} Stashing changes into a new branch. First see if you have any untracked files that also needs to be saved git stash -u git stash branch newbranchName Tagging Create Lightweight tag git tag mytag List existing tags git tag --list Delete tag git tag --delete mytag Create Annotated tags (It has additional information like release notes) git tag -a v1.0.0 -m \"Release 1.0.0\" Comparing tags git diff v1.0.0 v1.0.1 Tagging a specific commit ID git tag -a v0.0.9 -m \"Release 0.0.9\" Updating an existing tag with new commit id git tag -a v0.0.9 -f -m \"Correct Release 0.0.9\" Pushing tags to remote git push origin v1.0.0 Pushing all local tags to remote git push origin master --tags Deleting tags in remote (puting :before tag name will delete it from remote). Does not delete tag from local git push origin :v0.0.9 Reset HEAD position git reset Using Stash and Branch combination First stash the changes (WIP) in one brabch, then checkout a new test branch and then pop the changes into this test branch git stash git checkout -b test git stash pop Cherry Pick (Hot Fix scenario) git cherry-pick","title":"Git"},{"location":"learning/git/#create-a-new-repository-on-the-command-line","text":"git init git add README.md git commit -m \"first commit\" git branch -M master git remote add origin https://github.com/leslieclif/dotfiles.git git push -u origin master","title":"Create a new repository on the command line"},{"location":"learning/git/#git-commands","text":"cd into git folder \u2192 ls -la \u2192 cd .git \u2192 ls -la","title":"Git Commands"},{"location":"learning/git/#git-help","text":"git help","title":"Git help"},{"location":"learning/git/#to-quit-help-q","text":"","title":"To quit help --&gt; q"},{"location":"learning/git/#best-practise-always-do-a-pull-before-a-push-to-merge-changes-from-remote","text":"git pull origin master","title":"Best practise: Always do a pull before a push to merge changes from remote"},{"location":"learning/git/#to-git-add-and-git-commit-for-tracked-files-in-a-single-comand-use-a","text":"git commit -am \"Commit message\"","title":"To git add and git commit for tracked files in a single comand use -a"},{"location":"learning/git/#amend-commit-message","text":"git commit --amend \"New commit message\"","title":"Amend Commit message"},{"location":"learning/git/#check-for-tracked-files-in-git","text":"git ls-files","title":"Check for tracked files in git"},{"location":"learning/git/#back-out-changes-that-have-been-commited-but-not-pushed-to-remote-once-unstaged-you-can-remove-the-changes-using-checkout","text":"git reset HEAD git checkout --","title":"Back out changes that have been commited, but not pushed to remote. Once unstaged, you can remove the changes using checkout"},{"location":"learning/git/#rename-file-name-it-also-automatically-stages-the-changes-so-need-to-do-git-add","text":"git mv level3--file.txt level3.txt","title":"Rename file-name. It also automatically stages the changes, so need to do git add"},{"location":"learning/git/#if-file-is-renamed-outside-git-it-will-delete-and-add-the-files-in-git-history-and-is-not-staged-to-add-new-file-and-stage-the-renamed-files-use-a","text":"git add -A","title":"If file is renamed outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A"},{"location":"learning/git/#moving-files-and-staging-the-changes","text":"git mv level2.txt new-folder","title":"Moving files and staging the changes"},{"location":"learning/git/#if-file-is-moved-outside-git-it-will-delete-and-add-the-files-in-git-history-and-is-not-staged-to-add-new-file-and-stage-the-renamed-files-use-a","text":"mv level2.txt .. git add -A","title":"If file is moved outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A"},{"location":"learning/git/#file-renamed-in-os-but-say-git-has-identifed-unwanted-files-during-git-status-and-you-dont-want-to-add-those-files-then-dont-use-a-use-u","text":"","title":"file renamed in OS. But say, git has identifed unwanted files during git status and you dont want to add those files, then don't use -A, use -u"},{"location":"learning/git/#individually-add-the-new-renamed-file-first-then-update-git","text":"git add level1.txt git add -u","title":"Individually add the new renamed file first then update git"},{"location":"learning/git/#delete-files-tracked-by-git","text":"git rm doomed.txt","title":"Delete files tracked by git"},{"location":"learning/git/#if-file-is-delete-outside-git-it-will-delete-and-is-not-staged-to-add-and-stage-the-deleted-file-use-a","text":"git add -A","title":"If file is delete outside git, it will delete and is not staged. To add and stage the deleted file  use -A"},{"location":"learning/git/#git-history","text":"git log","title":"Git History"},{"location":"learning/git/#to-quit-help-q_1","text":"","title":"To quit help --&gt; q"},{"location":"learning/git/#git-history-in-one-line","text":"git log --oneline --graph --decorate","title":"Git history in one line"},{"location":"learning/git/#git-history-using-duration","text":"git log --since=\"3 days ago\"","title":"Git history using duration"},{"location":"learning/git/#show-all-user-actions","text":"git reflog","title":"Show all user actions"},{"location":"learning/git/#show-commit-history-do-git-log-get-commit-id","text":"git show","title":"Show commit history --&gt; do git log get commit id"},{"location":"learning/git/#todo-get-a-git-diff-tool","text":"","title":"TODO: Get a git diff tool"},{"location":"learning/git/#show-git-config","text":"git config --global --list","title":"Show git config"},{"location":"learning/git/#compare-with-staging-and-current-changes","text":"git diff","title":"Compare with staging and current changes"},{"location":"learning/git/#compare-between-current-changes-and-remote-last-commit","text":"git diff HEAD","title":"Compare between current changes and remote last commit"},{"location":"learning/git/#compare-between-staging-and-remote-last-commit","text":"git diff --staged HEAD","title":"Compare between staging and remote last commit"},{"location":"learning/git/#compare-file-changes-with-staging-and-current-changes","text":"git diff --","title":"Compare file changes with staging and current changes"},{"location":"learning/git/#compare-between-commits-do-git-log-to-get-commits","text":"git diff","title":"Compare between commits (do git log to get commits)"},{"location":"learning/git/#compare-local-and-remote-branches","text":"git diff master origin/master","title":"Compare local and remote branches"},{"location":"learning/git/#compare-local-branches","text":"git diff master test-branch","title":"Compare local branches"},{"location":"learning/git/#branching","text":"","title":"Branching"},{"location":"learning/git/#list-local-and-remote-branches","text":"git branch -a","title":"List local and remote branches"},{"location":"learning/git/#create-new-branch","text":"git branch","title":"Create new branch"},{"location":"learning/git/#rename-local-branch","text":"git branch -m","title":"Rename local branch"},{"location":"learning/git/#delete-a-branch-note-you-have-to-be-on-another-bracnh-before-you-can-delete-the-target-branch","text":"git branch -d","title":"Delete a branch. Note: You have to be on another bracnh before you can delete the target branch"},{"location":"learning/git/#create-new-branch-and-switch-to-it-in-single-command","text":"git checkout -b","title":"Create new branch and switch to it in single command"},{"location":"learning/git/#fash-forward-merges-first-switch-to-the-target-branches-do-a-git-diff-to-review-the-changes","text":"git merge","title":"Fash forward Merges  --&gt; First switch to the target branches, do a git diff to review the changes."},{"location":"learning/git/#disable-fast-forward-merge-give-tracing-of-merge-by-giving-a-custom-merge-message-and-also-the-commit-history-of-the-branch","text":"git merge --no-ff","title":"Disable fast forward merge --&gt; Give tracing of merge by giving a custom merge message and also the commit history of the branch"},{"location":"learning/git/#automatic-merge","text":"git merge -m \" \"","title":"Automatic merge"},{"location":"learning/git/#merge-conflict-and-resolution","text":"","title":"Merge Conflict and Resolution"},{"location":"learning/git/#inside-the-merging-workspace-incase-of-conflict-open-the-conflicting-file-in-editor-or-the-merge-diff-tool-resolve-conflict-and-close-the-file","text":"","title":"Inside the merging workspace incase of conflict, open the conflicting file in editor or the merge-diff tool. Resolve conflict and close the file."},{"location":"learning/git/#rebase-feature-branch-from-master","text":"git checkout feature-branch git rebase master","title":"Rebase feature branch from master"},{"location":"learning/git/#abort-rebase","text":"git rebase --abort","title":"Abort rebase"},{"location":"learning/git/#rebase-conflict-resolution-use-merging-tool-to-fix-conflict-save-and-quit-add-file-to-git-staging-then-continue-rebase","text":"git rebase --continue","title":"Rebase conflict resolution --&gt; Use merging tool to fix conflict, save and quit. Add file to git staging, then continue rebase"},{"location":"learning/git/#pull-with-rebase-rebase-local-master-with-remote-master","text":"git fetch origin master (non destructive merge which only updates references) git pull --rebase origin master","title":"Pull with Rebase (Rebase local master with remote master)"},{"location":"learning/git/#stash","text":"git stash","title":"Stash"},{"location":"learning/git/#stash-saving-untracked-files-of-git-as-well","text":"git stash -u","title":"Stash + saving untracked files of git as well"},{"location":"learning/git/#get-the-stash-back-to-local","text":"git stash apply","title":"Get the stash back to local"},{"location":"learning/git/#list-the-stash","text":"git stash list","title":"List the stash"},{"location":"learning/git/#drop-the-stash","text":"git stash drop","title":"Drop the stash"},{"location":"learning/git/#combination-of-apply-and-drop-in-one-command-brings-the-last-saved-state","text":"git stash pop","title":"Combination of apply and drop in one command. Brings the last saved state"},{"location":"learning/git/#multiple-stashes","text":"git stash save \" \"","title":"Multiple Stashes"},{"location":"learning/git/#show-any-arbitary-stash-changes-whithout-popping-do-stash-list-first-to-get-the-stash-id","text":"git stash show stash@{1}","title":"Show any arbitary stash changes whithout popping. Do stash list first to get the stash ID"},{"location":"learning/git/#apply-any-arbitary-stash-changes-do-stash-list-first-to-get-the-stash-id","text":"git stash apply stash@{1}","title":"Apply any arbitary stash changes. Do stash list first to get the stash ID"},{"location":"learning/git/#drop-any-arbitary-stash-changes-that-was-applied-or-not-needed","text":"git stash drop stash@{1}","title":"Drop any arbitary stash changes that was applied or not needed."},{"location":"learning/git/#stashing-changes-into-a-new-branch-first-see-if-you-have-any-untracked-files-that-also-needs-to-be-saved","text":"git stash -u git stash branch newbranchName","title":"Stashing changes into a new branch. First see if you have any untracked files that also needs to be saved"},{"location":"learning/git/#tagging","text":"","title":"Tagging"},{"location":"learning/git/#create-lightweight-tag","text":"git tag mytag","title":"Create Lightweight tag"},{"location":"learning/git/#list-existing-tags","text":"git tag --list","title":"List existing tags"},{"location":"learning/git/#delete-tag","text":"git tag --delete mytag","title":"Delete tag"},{"location":"learning/git/#create-annotated-tags-it-has-additional-information-like-release-notes","text":"git tag -a v1.0.0 -m \"Release 1.0.0\"","title":"Create Annotated tags (It has additional information like release notes)"},{"location":"learning/git/#comparing-tags","text":"git diff v1.0.0 v1.0.1","title":"Comparing tags"},{"location":"learning/git/#tagging-a-specific-commit-id","text":"git tag -a v0.0.9 -m \"Release 0.0.9\"","title":"Tagging a specific commit ID"},{"location":"learning/git/#updating-an-existing-tag-with-new-commit-id","text":"git tag -a v0.0.9 -f -m \"Correct Release 0.0.9\"","title":"Updating an existing tag with new commit id"},{"location":"learning/git/#pushing-tags-to-remote","text":"git push origin v1.0.0","title":"Pushing tags to remote"},{"location":"learning/git/#pushing-all-local-tags-to-remote","text":"git push origin master --tags","title":"Pushing all local tags to remote"},{"location":"learning/git/#deleting-tags-in-remote-puting-before-tag-name-will-delete-it-from-remote-does-not-delete-tag-from-local","text":"git push origin :v0.0.9","title":"Deleting tags in remote (puting :before tag name will delete it from remote). Does not delete tag from local"},{"location":"learning/git/#reset-head-position","text":"git reset","title":"Reset HEAD position"},{"location":"learning/git/#using-stash-and-branch-combination","text":"","title":"Using Stash and Branch combination"},{"location":"learning/git/#first-stash-the-changes-wip-in-one-brabch-then-checkout-a-new-test-branch-and-then-pop-the-changes-into-this-test-branch","text":"git stash git checkout -b test git stash pop","title":"First stash the changes (WIP) in one brabch, then checkout a new test branch and then pop the changes into this test branch"},{"location":"learning/git/#cherry-pick-hot-fix-scenario","text":"git cherry-pick","title":"Cherry Pick (Hot Fix scenario)"},{"location":"learning/python/","text":"# Range and For for index in range ( 6 ): print ( index ) # Range function is used generate a sequence of integers index = range ( 10 , - 1 , - 1 ) # start, stop and step, stops at 0 not including -1 # set class provides a mapping of unique immutable elements # One use of set is to remove duplicate elements dup_list = ( 'c' , 'd' , 'c' , 'e' ) beta = set ( dup_list ) uniq_list = list ( beta ) # dict class is an associative array of keys and values. keys must be unique immutable objects dict_syn = { 'k1' : 'v1' , 'k2' : 'v2' } dict_syn = dict ( k1 = 'v1' , k2 = 'v2' ) dict_syn [ 'k3' ] = 'v3' # adding new key value del ( dict_syn [ 'k3' ]) # delete key value print ( dict_syn . keys ()) # prints all keys print ( dict_syn . values ()) # prints all values # User Input name = input ( 'Name :' ) # Functions * A function is a piece of code , capable of performing a similar task repeatedly . * It is defined using ** def ** keyword in python . def < function_name > ( < parameter1 > , < parameter2 > , ... ): 'Function documentation' function_body return < value > * Parameters , return expression and documentation string are optional . def square ( n ): \"Returns Square of a given number\" return n ** 2 print ( square . __doc__ ) // prints the function documentation string * 4 types of arguments * Required Arguments : non - keyword arguments def showname ( name , age ) showname ( \"Jack\" , 40 ) // name = \"Jack\" , age = 40 showname ( 40 , \"Jack\" ) // name = 40 , age = \"Jack\" * Keyword Arguments : identified by paramater names def showname ( name , age ) showname ( age = 40 , name = \"Jack\" ) * Default Arguments : Assumes a default argument , if an arg is not passsed . def showname ( name , age = 50 ) showname ( \"Jack\" ) // name = \"Jack\" , age = 50 showname ( age = 40 , \"Jack\" ) // name = \"Jack\" , age = 40 showname ( name = \"Jack\" , age = 40 ) // name = \"Jack\" , age = 40 showname ( name = \"Jack\" , 40 ) // Python does not allow passing non - keyword after keyword arg . This will fail . * Variable Length Arguments : Function preocessed with more arguments than specified while defining the function def showname ( name , * vartuple , ** vardict ) # *vartuple = Variable non keyword argument which will be a tuple. Denoted by * # **vardict = Variable keyword argument which will be a dictionary. Denoted by ** showname ( \"Jack\" ) // name = \"Jack\" showname ( \"Jack\" , 35 , 'M' , 'Kansas' ) // name = \"Jack\" , * vartuple = ( 35 , 'M' , 'Kansas' ) showname ( \"Jack\" , 35 , city = 'Kansas' , sex = 'M' ) // name = \"Jack\" , * vartuple = ( 35 ), ** vardict = { city = 'Kansas' , sex = 'M' } # An Iterator is an object, which allows a programmer to traverse through all the elements of a collection, regardless of its specific implementation. x = [ 6 , 3 , 1 ] s = iter ( x ) print ( next ( s )) # -> 6 # List Comprehensions -> Alternative to for loops. * More concise , readable , efficient and mimic functional programming style . * Used to : Apply a method to all or specific elements of a list , and Filter elements of a list satisfying specific criteria . x = [ 6 , 3 , 1 ] y = [ i ** 2 for i in x ] # List Comprehension expression print ( y ) # -> [36, 9, 1] * Filter positive numbers ( using for and if ) vec = [ - 4 , - 2 , 0 , 2 , 4 ] pos_elm = [ x for x in vec if x >= 0 ] # Can be read as for every elem x in vec, filter x if x is greater than or equal to 0 print ( pos_elm ) # -> [0, 2, 4] * Applying a method to a list def add10 ( x ): return x + 10 n = [ 34 , 56 , 75 , 3 ] mod_n = [ add10 ( num ) for num in n ] print ( mod_n ) # A Generator is a function that produces a sequence of results instead of a single value def arithmatic_series ( a , r ): while a < 50 : yield a # yield is used in place of return which suspends processing a += r s = arithmatic_series ( 3 , 10 ) # Execution of further 'arithmetic series' can be resumed only by calling nextfunction again on generator 's' print ( s ) // Generator #output=3 print ( next ( s )) // Generator starts execution # output=13 print ( next ( s )) // resumed # output=23 # A Generator expresions are generator versions of list comprehensions. They return a generator instead of a list. x = [ 6 , 3 , 1 ] g = ( i ** 2 for i in x ) # generator expression print ( next ( g )) # -> 36 # Dictionary Comprehensions -> takes the form {key: value for (key, value) in iterable} myDict = { x : x ** 2 for x in [ 1 , 2 , 3 , 4 , 5 ]} print ( myDict ) # Output {1: 1, 2: 4, 3: 9, 4: 16, 5: 25} # Calculate the frequency of each identified unique word in the list words = [ 'Hello' , 'Hi' , 'Hello' ] freq = { w : words . count ( w ) for w in words } print ( freq ) # Output {'Hello': 2, 'Hi': 1} Create the dictionary frequent_words , which filter words having frequency greater than one words = [ 'Hello' , 'Hi' , 'Hello' ] freq = { w : words . count ( w ) for w in words if words . count ( w ) > 1 } print ( freq ) # Output {'Hello': 2} # Defining Classes * Syntax class < ClassName > ( < parent1 > , ... ): class_body # Creating Objects * An object is created by calling the class name followed by a pair of parenthesis . class Person : pass p1 = Person () # Creating the object 'p1' print ( p1 ) # -> '<__main__.Person object at 0x0A...>' # tells you what class it belongs to and hints on memory address it is referenced to. # initializer method -> __init__ * defined inside the class and called by default , during an object creation . * It also takes self as the first argument , which refers to the current object . class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname p1 = Person ( 'George' , 'Smith' ) print ( p1 . fname , '-' , p1 . lname ) # -> 'George - Smith' # Documenting a Class * Each class or a method definition can have an optional first line , known as docstring . class Person : 'Represents a person.' # Inheritance * Inheritance describes is a kind of relationship between two or more classes , abstracting common details into super class and storing specific ones in the subclass . * To create a child class , specify the parent class name inside the pair of parenthesis , followed by it 's name. class Child ( Parent ): pass * Every child class inherits all the behaviours exhibited by their parent class . * In Python , every class uses inheritance and is inherited from ** object ** by default . class MySubClass ( object ): # object is known as parent or super class. pass # Inheritance in Action class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname class Employee ( Person ): all_employees = [] def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) # Employee class utilizes __init __ method of the parent class Person to create its object. self . empid = empid Employee . all_employees . append ( self ) e1 = Employee ( 'Jack' , 'simmons' , 456342 ) print ( e1 . fname , '-' , e1 . empid ) # Output -> Jack - 456342 # Polymorphism * Polymorphism allows a subclass to override or change a specific behavior , exhibited by the parent class class Employee ( Person ): all_employees = EmployeesList () def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) self . empid = empid Employee . all_employees . append ( self ) def getSalary ( self ): return 'You get Monthly salary.' def getBonus ( self ): return 'You are eligible for Bonus.' * Definition of ContractEmployee class derived from Employee. It overrides functionality of getSalary and getBonus methods found in it 's parent class Employee. class ContractEmployee ( Employee ): def getSalary ( self ): return 'You will not get Salary from Organization.' def getBonus ( self ): return 'You are not eligible for Bonus.' e1 = Employee ( 'Jack' , 'simmons' , 456342 ) e2 = ContractEmployee ( 'John' , 'williams' , 123656 ) print ( e1 . getBonus ()) # Output - You are eligible for Bonus. print ( e2 . getBonus ()) # Output - You are not eligible for Bonus. # Abstraction * Abstraction means working with something you know how to use without knowing how it works internally . * It is hiding the defaults and sharing only necessary information . # Encapsulation * Encapsulation allows binding data and associated methods together in a unit i . e class . * Bringing related data and methods inside a class to avoid misuse outside . * These principles together allows a programmer to define an interface for applications , i . e . to define all tasks the program is capable to execute and their respective input and output data . * A good example is a television set . We don \u2019 t need to know the inner workings of a TV , in order to use it . All we need to know is how to use the remote control ( i . e the interface for the user to interact with the TV ) . # Abstracting Data * Direct access to data can be restricted by making required attributes or methods private , ** just by prefixing it 's name with one or two underscores.** * An attribute or a method starting with : + ** no underscores ** is a ** public ** one . + ** a single underscore ** is ** private ** , however , still accessible from outside. + ** double underscores ** is ** strongly private ** and not accessible from outside. # Abstraction and Encapsulation Example * ** empid ** attribute of Employee class is made private and is accessible outside the class only using the method ** getEmpid **. class Employee ( Person ): all_employees = EmployeesList () def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) self . __empid = empid Employee . all_employees . append ( self ) def getEmpid ( self ): return self . __empid e1 = Employee ( 'Jack' , 'simmons' , 456342 ) print ( e1 . fname , e1 . lname ) # Output -> Jack simmons print ( e1 . getEmpid ()) # Output -> 456342 print ( e1 . __empid ) # Output -> AttributeError: Employee instance has no attribute '__empid' # Exceptions * Python allows a programmer to handle such exceptions using ** try ... except ** clauses , thus avoiding the program to crash . * Some of the python expressions , though written correctly in syntax , result in error during execution . ** Such scenarios have to be handled .** * In Python , every error message has two parts . The first part tells what type of exception it is and second part explains the details of error . # Handling Exception * A try block is followed by one or more except clauses . * The code to be handled is written inside try clause and the code to be executed when an exception occurs is written inside except clause . try : a = pow ( 2 , 4 ) print ( \"Value of 'a' :\" , a ) b = pow ( 2 , 'hello' ) # results in exception print ( \"Value of 'b' :\" , b ) except TypeError as e : print ( 'oops!!!' ) print ( 'Out of try ... except.' ) Output -> Value of 'a' : 16 --> oops !!! --> Out of try ... except . # Raising Exceptions * ** raise ** keyword is used when a programmer wants a specific exception to occur . try : a = 2 ; b = 'hello' if not ( isinstance ( a , int ) and isinstance ( b , int )): raise TypeError ( 'Two inputs must be integers.' ) c = a ** b except TypeError as e : print ( e ) # User Defined Exception Functions * Python also allows a programmer to create custom exceptions , derived from base Exception class . class CustomError ( Exception ): def __init__ ( self , value ): self . value = value def __str__ ( self ): return str ( self . value ) try : a = 2 ; b = 'hello' if not ( isinstance ( a , int ) and isinstance ( b , int )): raise CustomError ( 'Two inputs must be integers.' ) # CustomError is raised in above example, instead of TypeError. c = a ** b except CustomError as e : print ( e ) # Using 'finally' clause * ** finally ** clause is an optional one that can be used with try ... except clauses . * All the statements under finally clause are executed irrespective of exception occurrence . def divide ( a , b ): try : result = a / b return result except ZeroDivisionError : print ( \"Dividing by Zero.\" ) finally : print ( \"In finally clause.\" ) # Statements inside finally clause are ALWAYS executed before the return back # Using 'else' clause * ** else ** clause is also an optional clause with try ... except clauses . * Statements under else clause are executed ** only when no exception occurs in try clause **. try : a = 14 / 7 except ZeroDivisionError : print ( 'oops!!!' ) else : print ( 'First ELSE' ) try : a = 14 / 0 except ZeroDivisionError : print ( 'oops!!!' ) else : print ( 'Second ELSE' ) Output : First ELSE --> oops !!! # Module * Any file containing logically organized Python code can be used as a module . * A module generally contains ** any of the defined functions , classes and variables **. A module can also include executable code . * Any Python source file can be used as a module by using an import statement in some other Python source file . # Packages * A package is a collection of modules present in a folder . * The name of the package is the name of the folder itself . * A package generally contains an empty file named ** __init__ . py ** in the same folder , which is required to treat the folder as a package . # Import Modules import math # Recommended method of importing a module import math as m from math import pi , tan from math import pi as pie , tan as tangent # Working with Files * Data from an opened file can be read using any of the methods : ** read , readline and readlines **. * Data can be written to a file using either ** write ** or ** writelines ** method . * A file ** must be opened ** , before it is used for reading or writing . fp = open ( 'temp.txt' , 'r' ) # opening ( operations 'r' & 'w') content = fp . read () # reading fp . close () # closing # read() -> Reads the entire contents of a file as bytes. # readline() -> Reads a single line at a time. # readlines() -> Reads a all the line & each line is stored as an element of a list. # write() -> Writes a single string to output file. # writelines() -> Writes multiple lines to output file & each string is stored as an element of a list. * Reading contents of file and storing as a dictionary fp = open ( 'emp_data.txt' , 'r' ) emps = fp . readlines () # Preprocessing data emps = [ emp . strip ( ' \\n ' ) for emp in emps ] emps = [ emp . split ( ';' ) for emp in emps ] header = emps . pop # remove header record separately emps = [ dict ( zip ( header , emp ) for emp in emps ] # header record is used to combine with data to form a dictionary print ( emps [: 2 ]) # prints first 2 records * Filtering data based on criteria fil_emps = [ emp [ 'Emp_name' ] for emp in emps if emp [ 'Emp_work_location' ] == 'HYD' ] * Filtering data based on pattern import re pattern = re . compile ( r 'oracle' , re . IGNORECASE ) # Regular Expression oracle_emps = [ emp [ 'Emp_name' ] for emp in emps if pattern . search ( emp [ 'Emp_skillset' ])] * Filter and Sort data in ascending order fil_emps = [ emp for emp in emps if emp [ 'Emp_designation' ] == 'ASE' ] fil_emps = sorted ( fil_emps , key = lambda k : k [ 'Emp_name' ]) print ( emp [ 'Emp_name' ] for emp in fil_emps ) * Sorting all employees based on custom sorting criteria order = { 'ASE' : 1 , 'ITA' : 2 , 'AST' : 3 } sorted_emp = sorted ( emp , key = lambda k : order [ k [ 'designation' ]]) * Filter data and write into files fil_emps = [ emp for emp in emps if emp [ 'Emp_Designation' ] == 'ITA' ] ofp = open ( outputtext . txt , 'w' ) keys = fil_emps [ 0 ] . keys () # Remove header from key name for key in keys : ofp . write ( key + \" \\t \" ) ofp . write ( \" \\n \" ) for emp in fil_emps : for key in keys : ofp . write ( emp [ key ] + \" \\t \" ) ofp . write ( \" \\n \" ) ofp . close () # Regular Expressions * Regex are useful to construct patterns that helps in filtering the text possessing the pattern . * ** re module ** is used to deal with regex . * ** search ** method takes pattern and text to scan and returns a Match object . Return None if not found . * Match object holds info on the nature of the match like ** original input string , Regular expression used , location within the original string ** match = re . search ( pattern , text ) start_index = match . start () # start location of match end_index = match . end () regex = match . re . pattern () print ( 'Found \" {} \" pattern in \" {} \" from {} to {} ' . format ( st , text , start_index , end_index )) # Compiling Expressions * In Python , its more efficient t compile the patterns that are frequently used . * ** compile ** function of re module converts an expression string into a ** RegexObject **. patterns = [ 'this' , 'that' ] regexes = [ re . compile ( p ) for p in patterns ] for regex in regexes : if regex . search ( text ): # pattern is not required print ( 'Match found' ) * search method only returns the first matching occurrence . # Finding Multiple Matches * findall method returns all the substrings of the pattern without overlapping pattern = 'ab' for match in re . findall ( pattern , text ): print ( 'match found - {} ' . format ( match )) # Grouping Matches * Adding groups to a pattern enables us to isolate parts of the matching text , expanding those capabilities to create a parser . * Groups are defined by enclosing patterns within parenthesis text = 'This is some text -- with punctuations.' for pattern in [ r '^(\\w+)' , # word at the start of the string r '(\\w+)\\S*$' , # word at the end of the string with punctuation r '(\\bt\\w+)\\W+(\\w+)' , # word staring with 't' and the next word r '(\\w+t)\\b' ]: # word ending with t regex = re . compile ( pattern ) match = regex . search ( text ) print ( match . groups ()) # Output -> ('This',) ('punctuations',) ('text','with') ('text',) # Naming Grouped Matches * Accessing the groups with defined names text = 'This is some text -- with punctuations.' for pattern in [ r '^(?P<first_word>\\w+)' , # word at the start of the string r '(?P<last_word>\\w+)\\S*$' , # word at the end of the string with punctuation r '(?P<t_word>\\bt\\w+)\\W+(?P<other_word>\\w+)' , # word staring with 't' and the next word r '(?P<ends_with_t>\\w+t)\\b' ]: # word ending with t regex = re . compile ( pattern ) match = regex . search ( text ) print ( \"Groups: \" , match . groups ()) # Output -> ('This',) ('punctuations',) ('text','with') ('text',) print ( \"Group Dictionary: \" , match . groupdict ()) # Output -> {'first_word':'This'} {'last_word': 'punctuations'} {'t_word':'text', 'other_word':'with'} {'ends_with_t':'text'} # Data Handling # Handling XML files * ** lxml ** 3 rd party module is a highly feature rich with ElementTree API and supports querying wthe xml content using XPATH . * In the ElementTree API , an element acts like a list . The items of the list are the elements children . * XML search is faster in lxml . < ? xml > < employee > < skill name = \"Python\" /> </ employee > from lxml import etree tree = etree . parse ( 'sample.xml' ) root = tree . getroot () # gets doc root <?xml> skills = tree . findall ( '//skill' ) # gets all skill tags for skill in skills : print ( \"Skills: \" , skill . attrib [ 'name' ]) # Adding new skill in the xml skill = etree . SubElement ( root , 'skill' , attrib = { 'name' : 'PHP' }) # Handling HTML files * ** lxml ** 3 rd party module is used for parsing HTML files as well . import urllib.request from lxml import etree def readURL ( url ): urlfile = urllib . request . urlopen ( url ) if urlfile . getcode () == 200 : contents = urlfile . read () return contents if __name__ == '__main__' : url = 'http://xkcd.com' html = readURL ( url ) # Data Serialization * Process of converting ** data types / objects ** into ** Transmittable / Storable ** format is called Data Serialization . * In python , ** pickle and json ** modules are used for Data Serialization . * Serialized data can then be written to file / Socket / Pipe . From these it can be de - serialized and stored into a new Object . json . dump ( data , file , indent = 2 ) # serialized data is written to file with indentation using dump method data_new = json . load ( file ) # de-serialized data is written to new object using load method # Database Connectivity * ** Python Database API ( DB - API ) ** is a standard interface to interact with various databases . * Different DB API \u2019 s are used for accessing different databases . Hence a programmer has to install DB API corresponding to the database one is working with . * Working with a database includes the following steps : + Importing the corresponding DB - API module . + Acquiring a connection with the database . + Executing SQL statements and stored procedures . + Closing the connection import sqlite3 # establishing a database connection con = sqlite3 . connect ( 'D: \\\\ TEST.db' ) # preparing a cursor object cursor = con . cursor () # preparing sql statements sql1 = 'DROP TABLE IF EXISTS EMPLOYEE' # closing the database connection con . close () # Inserting Data * Single rows are inserted using ** execute ** and multiple rows using ** executeMany ** method of created cursor object . # preparing sql statement rec = ( 456789 , 'Frodo' , 45 , 'M' , 100000.00 ) sql = ''' INSERT INTO EMPLOYEE VALUES ( ?, ?, ?, ?, ?) ''' # executing sql statement using try ... except blocks try : cursor . execute ( sql , rec ) con . commit () except Exception as e : print ( \"Error Message :\" , str ( e )) con . rollback () # Fetching Data * ** fetchone ** : It retrieves one record at a time in the form of a tuple . * ** fetchall ** : It retrieves all fetched records at a point in the form of tuple of tuples . # fetching the records records = cursor . fetchall () # Displaying the records for record in records : print ( record ) # Object Relational Mappers * An object - relational mapper ( ORM ) is a library that automates the transfer of data stored in relational database tables into objects that are adopted in application code . * ORMs offer a high - level abstraction upon a relational database , which permits a developer to write Python code rather than SQL to create , read , update and delete data and schemas in their database . * Such an ability to write Python code instead of SQL speeds up web application development . # Higher Order Functions * A ** Higher Order function ** is a function , which is capable of doing any one of the following things : + It can be functioned as a ** data ** and be assigned to a variable . + It can accept any other ** function as an argument **. + It can return a ** function as its result **. * The ability to build Higher order functions , ** allows a programmer to create Closures , which in turn are used to create Decorators **. # Function as a Data def greet (): return 'Hello Everyone!' print ( greet ()) wish = greet # 'greet' function assigned to variable 'wish' print ( type ( wish )) # Output -> <type 'function'> print ( wish ()) # Output -> Hello Everyone! # Function as an Argument def add ( x , y ): return x + y def sub ( x , y ): return x - y def prod ( x , y ): return x * y def do ( func , x , y ): return func ( x , y ) print ( do ( add , 12 , 4 )) # 'add' as arg # Output -> 16 print ( do ( sub , 12 , 4 )) # 'sub' as arg # Output -> 8 print ( do ( prod , 12 , 4 )) # 'prod' as arg # Output -> 48 # Returning a Function def outer (): def inner (): s = 'Hello world!' return s return inner () print ( outer ()) # Output -> Hello world! * You can observe from the output that the ** return value of 'outer' function is the return value of 'inner' function ** i . e 'Hello world!' . def outer (): def inner (): s = 'Hello world!' return s return inner # Removed '()' to return 'inner' function itself print ( outer ()) #returns 'inner' function # Output -> <function inner at 0xxxxxx> func = outer () print ( type ( func )) # Output -> <type 'function'> print ( func ()) # calling 'inner' function # Output -> Hello world! * Parenthesis after the ** inner ** function are removed so that the ** outer ** function returns ** inner function **. # Closures * A Closure is a ** function returned by a higher order function ** , whose return value depends on the data associated with the higher order function . def multiple_of ( x ): def multiple ( y ): return x * y return multiple c1 = multiple_of ( 5 ) # 'c1' is a closure c2 = multiple_of ( 6 ) # 'c2' is a closure print ( c1 ( 4 )) # Output -> 5 * 4 = 20 print ( c2 ( 4 )) # Output -> 6 * 4 = 24 * The first closure function , c1 binds the value 5 to argument x and when called with an argument 4 , it executes the body of multiple function and returns the product of 5 and 4. * Similarly c2 binds the value 6 to argument x and when called with argument 4 returns 24. # Decorators * Decorators are evolved from the concept of closures . * A decorator function is a higher order function that takes a function as an argument and returns the inner function . * A decorator is capable of adding extra functionality to an existing function , without altering it . * The decorator function is prefixed with **@ symbol ** and written above the function definition . + Shows the creation of closure function wish using the higher order function outer . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner def greet (): print ( 'Hello!' ) wish = outer ( greet ) # Output -> Accessing : greet wish () # Output -> Hello! - wish is the closure function obtained by calling an outer function with the argument greet . When wish function is called , inner function gets executed . + The second one shows the creation of decorator function outer , which is used to decorate function greet . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner def greet (): return 'Hello!' greet = outer ( greet ) # decorating 'greet' # Output -> No Output as return is used instead of print greet () # calling new 'greet' # Output -> Accessing : greet - The function returned by outer is assigned to greet i . e the function name passed as argument to outer . This makes outer a decorator to greet . + Third one displays decorating the greet function with decorator function , outer , using @ symbol . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner @outer # This is same as **greet = outer(greet)** def greet (): return 'Hello!' greet () # Output -> Accessing : greet # Descriptors * Python descriptors allow a programmer to create managed attributes . * In other object - oriented languages , you will find ** getter and setter ** methods to manage attributes . * However , Python allows a programmer to manage the attributes simply with the attribute name , without losing their protection . * This is achieved by defining a ** descriptor class ** , that implements any of ** __get__ , __set__ , __delete__ ** methods . class EmpNameDescriptor : def __get__ ( self , obj , owner ): return self . __empname def __set__ ( self , obj , value ): if not isinstance ( value , str ): raise TypeError ( \"'empname' must be a string.\" ) self . __empname = value * The descriptor , EmpNameDescriptor is defined to manage empname attribute . It checks if the value of empname attribute is a string or not . class EmpIdDescriptor : def __get__ ( self , obj , owner ): return self . __empid def __set__ ( self , obj , value ): if hasattr ( obj , 'empid' ): raise ValueError ( \"'empid' is read only attribute\" ) if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value * The descriptor , EmpIdDescriptor is defined to manage empid attribute . class Employee : empid = EmpIdDescriptor () empname = EmpNameDescriptor () def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name * Employee class is defined such that , it creates empid and empname attributes from descriptors EmpIdDescriptor and EmpNameDescriptor . e1 = Employee ( 123456 , 'John' ) print ( e1 . empid , '-' , e1 . empname ) # Output -> '123456 - John' e1 . empid = 76347322 # Output -> ValueError: 'empid' is read only attribute # Properties * Descriptors can also be created using property () type . + Syntax : property ( fget = None , fset = None , fdel = None , doc = None ) - where , fget : attribute get method fset : attribute set method fdel \u2013 attribute delete method doc \u2013 docstring class Employee : def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name def getEmpID ( self ): return self . __empid def setEmpID ( self , value ): if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value empid = property ( getEmpID , setEmpID ) # Property Decorators * Descriptors can also be created with property decorators . * While using property decorators , an attribute 's get method will be same as its name and will be decorated with property. * In a case of defining any set or delete methods , they will be decorated with respective setter and deleter methods . class Employee : def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name @property def empid ( self ): return self . __empid @empid . setter def empid ( self , value ): if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value e1 = Employee ( 123456 , 'John' ) print ( e1 . empid , '-' , e1 . empname ) # Output -> '123456 - John' # Introduction to Class and Static Methods Based on the ** scope ** , functions / methods are of two types . They are : * Class methods * Static methods # Class Methods * A method defined inside a class is bound to its object , by default . * However , if the method is bound to a Class , then it is known as ** classmethod **. class Circle ( object ): no_of_circles = 0 def __init__ ( self , radius ): self . __radius = radius Circle . no_of_circles += 1 def getCirclesCount ( self ): return Circle . no_of_circles c1 = Circle ( 3.5 ) c2 = Circle ( 5.2 ) c3 = Circle ( 4.8 ) print ( c1 . getCirclesCount ()) # -> 3 print ( Circle . getCirclesCount ( c3 )) # -> 3 print ( Circle . getCirclesCount ()) # -> TypeError: getCirclesCount() missing 1 required positional argument: 'self' class Circle ( object ): no_of_circles = 0 def __init__ ( self , radius ): self . __radius = radius Circle . no_of_circles += 1 @classmethod def getCirclesCount ( self ): return Circle . no_of_circles c1 = Circle ( 3.5 ) c2 = Circle ( 5.2 ) c3 = Circle ( 4.8 ) print ( c1 . getCirclesCount ()) # -> 3 print ( Circle . getCirclesCount ()) # -> 3 # Static Method * A method defined inside a class and not bound to either a class or an object is known as ** Static ** Method . * Decorating a method using ** @staticmethod ** decorator makes it a static method . def square ( x ): return x ** 2 class Circle ( object ): def __init__ ( self , radius ): self . __radius = radius def area ( self ): return 3.14 * square ( self . __radius ) c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 print ( square ( 10 )) # -> 100 * square function is not packaged properly and does not appear as integral part of class Circle . class Circle ( object ): def __init__ ( self , radius ): self . __radius = radius @staticmethod def square ( x ): return x ** 2 def area ( self ): return 3.14 * self . square ( self . __radius ) c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 print ( square ( 10 )) # -> NameError: name 'square' is not defined * square method is no longer accessible from outside the class Circle . * However , it is possible to access the static method using Class or the Object as shown below . print ( Circle . square ( 10 )) # -> 100 print ( c1 . square ( 10 )) # -> 100 # Abstract Base Classes * An ** Abstract Base Class ** or ** ABC ** mandates the derived classes to implement specific methods from the base class . * It is not possible to create an object from a defined ABC class . * Creating objects of derived classes is possible only when derived classes override existing functionality of all abstract methods defined in an ABC class . * In Python , an Abstract Base Class can be created using module abc . from abc import ABC , abstractmethod class Shape ( ABC ): @abstractmethod def area ( self ): pass @abstractmethod def perimeter ( self ): pass * Abstract base class Shape is defined with two abstract methods area and perimeter . class Circle ( Shape ): def __init__ ( self , radius ): self . __radius = radius @staticmethod def square ( x ): return x ** 2 def area ( self ): return 3.14 * self . square ( self . __radius ) def perimeter ( self ): return 2 * 3.14 * self . __radius c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 # Context Manager * A Context Manager allows a programmer to perform required activities , automatically , while entering or exiting a Context . * For example , opening a file , doing few file operations , and closing the file is manged using Context Manager as shown below . with open ( 'sample.txt' , 'w' ) as fp : content = fp . read () * The keyword ** with ** is used in Python to enable a context manager . It automatically takes care of closing the file . import sqlite3 class DbConnect ( object ): def __init__ ( self , dbname ): self . dbname = dbname def __enter__ ( self ): self . dbConnection = sqlite3 . connect ( self . dbname ) return self . dbConnection def __exit__ ( self , exc_type , exc_val , exc_tb ): self . dbConnection . close () with DbConnect ( 'TEST.db' ) as db : cursor = db . cursor () ''' Few db operations ... ''' * Example from contextlib import contextmanager @contextmanager def context (): print ( 'Entering Context' ) yield print ( \"Exiting Context\" ) with context (): print ( 'In Context' ) # Output -> Entering Context -> In Context -> Exiting Context # Coroutines * A Coroutine is ** generator ** which is capable of constantly receiving input data , process input data and may or may not return any output . * Coroutines are majorly used to build better ** Data Processing Pipelines **. * Similar to a generator , execution of a coroutine stops when it reaches ** yield ** statement . * A Coroutine uses ** send ** method to send any input value , which is captured by yield expression . def TokenIssuer (): tokenId = 0 while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) t = TokenIssuer () next ( t ) t . send ( 'George' ) # -> Token number of George: 1 t . send ( 'Rosy' ) # -> Token number of Rosy: 2 * ** TokenIssuer ** is a coroutine function , which uses yield to accept name as input . * Execution of coroutine function begins only when next is called on coroutine t . * This results in the execution of all the statements till a yield statement is encountered . * Further execution of function resumes when an input is passed using send , and processes all statements till next yield statement . def TokenIssuer ( tokenId = 0 ): try : while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) except GeneratorExit : print ( 'Last issued Token is :' , tokenId ) t = TokenIssuer ( 100 ) next ( t ) t . send ( 'George' ) # Token number of George: 101 t . send ( 'Rosy' ) # Token number of Rosy: 102 t . send ( 'Smith' ) # Token number of Smith: 103 t . close () # Last issued Token is: 103 * The coroutine function TokenIssuer takes an argument , which is used to set a starting number for tokens . * When coroutine t is closed , statements under GeneratorExit block are executed . * Many programmers may forget that passing input to coroutine is possible only after the first next function call , which results in error . * Such a scenario can be avoided using a decorator . def coroutine_decorator ( func ): def wrapper ( * args , ** kwdargs ): c = func ( * args , ** kwdargs ) next ( c ) return c return wrapper @coroutine_decorator def TokenIssuer ( tokenId = 0 ): try : while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) except GeneratorExit : print ( 'Last issued Token is :' , tokenId ) t = TokenIssuer ( 100 ) t . send ( 'George' ) t . send ( 'Rosy' ) t . send ( 'Smith' ) t . close () * coroutine_decorator takes care of calling next on the created coroutine t . def nameFeeder (): while True : fname = yield print ( 'First Name:' , fname ) lname = yield print ( 'Last Name:' , lname ) n = nameFeeder () next ( n ) n . send ( 'George' ) n . send ( 'Williams' ) n . send ( 'John' ) First Name : George Last Name : Williams First Name : John","title":"Python"},{"location":"learning/terraform/","text":"Infrastructure as Code IaC is an important DevOps cornerstone that enables you to define, automatically manage, and provision infrastructure through source code. Infrastructure is managed as a software system . IaC is frequently referred to as Programmable Infrastructure . Benefits of Iac In IaC, A tool will monitor the state of the infrastructure . A script will be sent automatically to fix the issue . A script can be written to add new instances, and it can be reused . Faster process , no/less human involvement. IaC Implemetation Approaches Declarative Focuses on the desired end state of infrastructure (Functional) . Tools perform the necessary actions to reach that state . Automatically takes care of the order and executes it . Examples are Terraform and CloudFormation. Imperative Focuses on how to achieve the desired state (Procedural) . Follows the order of tasks and it executes in that order . Examples are Chef and Ansible. Configuration Management It is a system that keeps the track of organization's hardware, software, and other related information. It includes the software updates and versions present in the system. Configuration management is also called as configuration control . E.g.: Chef and Ansible Orchestration: It automates the coordination, management, and configuration of systems and hardware. E.g.: Terraform and Nomad Installing Terraform sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip unzip terraform_0.11.11_linux_amd64.zip sudo mv terraform /usr/local/bin/ terraform --version Terraform Lifecycle Terraform init - Initializes the working directory having Terraform configuration files. Terraform plan - Creates an execution plan and it mentions the changes it is going to make. Terraform apply - When you are not amazed by seeing the results of the terraform plan, you can go ahead and run terraform apply to apply the changes. The main difference between plan and apply is - apply asks for a prompt like Do you want to perform these actions? and then implement the changes, whereas, plan shows the possible end state of your infrastructure without actually implementing them. Terraform destroy - Destroys the created infrastructure. Terraform Configuration A set of files that describe the infrastructure in Terraform is called as Terraform Configuration. The entire configuration file is saved with .tf extension. Make sure to have all the configurations maintained in a single .tf file instead of many. Terraform loads all the .tf files present in the working directory. Creating Virtual Network create a virtual network without using the Azure portal and azure CLI commands. resource \"azurerm_virtual_network\"\"myterraformnetwork\" { name = \"myvnet\" address_space = [\"10.0.0.0/16\"] location=\"East US\" resource_group_name=\"er-tyjy\" } Resource blocks define the infrastructure (resource \"azurerm_virtual_network\" \"myterraformnetwork\"). It contains the two strings - type of the resource(azurerm_virtualnetwork) and name of the resource(myterraformnetwork). Terraform uses plugin based architecture to support the various services providers and infrastructure available. When you run Terraform init command, it downloads and installs provider binary for the corresponding providers. In this case, it is Azure. Now, you can run the terraform plan command. It shows all the changes it is going to make. If the output is as you expected and there are no errors, then you can run Terraform apply. Terraform Validate You can run this command whenever you like to check whether the code is correct. Once you run this command, if there is no output, it means that there are no errors in the code. Variables Terraform provides the following variable types: Strings - The default data type is a string . List - A list is like simple arrays Map - A map value is nothing but a lookup table for string keys to get the string values. In terraform, to reduce the complexity of code and for easiness , all the variables are created in the variables.tf file. variables.tf variable \"rg\" { default = \"terraform-lab2\" } variable \"locations\" { default = [\"ap-southeast-1\" , \"ap-southeast-2\"] } variable \"tags\" { type = \"map\" default = { environment = \"training\" source=\"citadel\" } } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-east-2 = \"image-4321\" } } For example, you have defined resource_group name in the variable.tf file. you can call them in main.tf file like this \"${var.resource_group}\" Sensitive Parameters There may be sensitive parameters which should be shown only when running the terraform plan but not terraform apply. output \"sensitiveoutput\" { sensitive = true value = VALUE } When you run terraform apply, the output is labeled as sensitive. If there is any sensitive information, then you can protect it by using a sensitive parameter. terraform.tfvars File In terraform.tfvars file, we mention the \"key/value\" pair. The main.tf file takes the input from a terraform. tfvars file. If it is not present, it gets the input from variables.tf file. resource_group = \"user-ybje\" location = \"East US\" terraform fmt It rewrites the confguration files to canonical style and format. State File - terraform.tfstate It is a JSON file that maps the real world resources to the configuration files. In terraform, there are three states. Desired state - Represents how the infrastructure to be present. Actual state - Represents the current state of infrastructure. Known state - To bridge desired state and actual state, there is a known state. You can get the details of known state from a tfstate file. When you run terraform plan command , terraform performs a quick refresh and lists the actions needed to achieve the desired state. When terraform apply command is run, it applies the changes and updates the actual state. Modules A module is like a reusable blueprint of infrastructure . A module is nothing but a folder of Terraform files. In terraform the modules are categorized into two types: Root modules - The current directory of Terraform files on which you are running the commands. Child modules - The modules sourced by the root module. To create a child module, add resources to it. Module syntax: module \"child\" { source = \"../child\" } The difference between adding the resources and module is for adding the resource (Eg: resource \"azurerm_virtual_network\" 'test\") you need type and name but for adding a module (Eg: module \"child\") only the name is enough. The name of the module should be unique within configurations because it is used as a reference to the module and outputs. main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } Updates terraform get - This command is used to download and install the modules for that configuration. terraform get -update - It checks the downloaded modules and checks for the recent versions if any. Module Outputs If you need output from any module, you can get it by passing the required parameter. For example, the output of virtual network location can be found by following the below syntax. You can add this syntax to .tf file(main.tf or variables.tf) . output \"virtual_network_id\" { value = \"${azurerm_virtual_network.test.location}\" } Where virtual_network_id is a unique name and \"${azurerm_virtual_network.test.location}\" is the location of the virtual network using string interpolation. Benefits of Modules Code reuse : When there is a need to provision the group of resources on another resource at the same time, a module can be used instead of copying the same code. It helps in resolving the bugs easily. If you want to make changes, changing the code in one place is enough. Abstraction layer : It makes complex configurations easier to conceptualize. For example, if you like to add vault(Another harshicop's tool for managing secrets) cluster to the environment, it requires dozens of components. Instead of worrying about individual components, it gives ready-to-use vault cluster. Black box : To create a vault, you only need some configuration parameters without worrying about the complexity. You don't need any knowledge on how to install vault, configuring vault cluster and working of the vault. You can create a working cluster in a few minutes. Best practices in organization : When a module is created, you can give it to the other teams. It helps in easily developing the infrastructure. **Versioned artifacts: They are immutable artifacts and can be promoted from one environment to others. Introduction to Meta Parameters There are some difficulties with the declarative language. For example, since it is a declarative language, there is no concept of for loops. How do you repeat a piece of code without copy paste? * Terraform provides primitives like meta parameter called as count, life cycle blocks like create_before_destroy, ternary operator and a large number of interpolation functions . This helps in performing certain types of loops, if statements and zero downtime deployments. Count Count - It defines how many parameters you like to create. Modify main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } To increase Vnets to 5, update count = 5 . To decrease Vnets to 2, update count = 2 , last 3 Vnets will get deleted. Elements Suppose you like to create the three virtual networks with names vnet-A, vnet-B and vnet-C, you can do this easily with the help of list . Mention how many vnets you are going to create with the help of list and define it in variables.tf file variable \"name\" { type= \"list\" default = [\"A\",\"B\",\"C\"] } You can call this variable in the main.tf file in the following way count = \"${length(var.name)}\" - It returns number of elements present in the list. you should store it in meta parameter count. \"${element(var.name,count.index)}\" - It acts as a loop, it takes the input from list and repeats until there are no elements in list. variables.tf variable \"resource_group\" { default = \"user-nuqo\" } variable \"location\" { default = \"East US\" } variable \"name\" { type = \"list\" default = [\"A\",\"B\",\"C\"] } main.tf resource \"azurerm_virtual_network\"\"multiple\" { name = \"vnet-${element(var.name,count.index)}\" resource_group_name = \"${var.resource_group}\" location = \"${var.location}\" address_space=[\"10.0.0.0/16\"] count=\"${length(var.name)}\" } Conditions For example, a vnet has to be created only, if the variable number of vnets is 3 or else no. You can do this by using the ternary operator. variables.tf variable \"no_of_vnets\" { default= 3 } main.tf count = \"${var.no_of_vnets ==3 ?1 : 0}\" * First, it will execute the ternary operator. If it is true, it takes the output as 1 or else 0. * Now, in the above case, the output is 1, the count becomes one, and the vnet is created. Inheriting Variables Inheriting the variables between modules keeps your Terraform code DRY. Don't Repeat Yourself (DRY) It aims at reducing repeating patterns and code. There is no need to write the duplicate code for each environment you are deploying. You can write the minimum code and achieve the goal by having maximum re-usability. Module File Structure You will follow the below file structure to inherit the variables between modules. modules | |___mod | |__mod.tf | |___vnet |__vnet.tf |__vars.tf There is one vnet and you can configure the vnet by passing the parameters from module. The variables in the vnet are overriden by the values provided in mod.tf file. Create a folder with name Modules Create two subfolders of names vnet and mod in the nested_modules folder. In the vnet folder, create a virtual network using Terraform configuration. Maintain two files one for main configuration(vnet.tf) and other for declaring variables(vars.tf) . Now, create main.tf file like this resource \"azurerm_virtual_network\"\"vnet\" { name = \"${var.name}\" address_space = [\"10.0.0.0/16\"] resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" } Now, vars.tf file looks like this variable \"name\" { description = \"name of the vnet\" } variable \"resourcegroup\" { description = \"name of the resource group\" } variable \"location\" { description =\"name of the location\" } variable \"address\" { type =\"list\" description = \"specify the address\" } Create a folder with name mod in the modules directory. Create a file with name mod.tf and write the below code in it. We are passing the variables from the root module (mod) to the child module (vnet) using source path . module \"child\" { source = \"../vnet\" name = \"modvnet\" resourcegroup = \"user-vcom\" location = \"eastus\" address = \"10.0.1.0/16\" } Run terraform plan to visualize how the directories will be created. You can see the name of the vnet is taken from the module file instead of getting it from vars.tf Nested Modules For any module, root module should exist. Root module is basically a directory that holds the terraform configuration files for the desired infrastructure. They also provide the entry point to the nested modules you are going to utilize. For any module main.tf, vars.tf, and outputs.tf naming conventions are recommended. If you are using nested modules, create them under the subdirectory with name modules . checking_modules |__modules | |____main.tf | |______virtual_network | |_main.tf | |_vars.tf | |______storage_account |_main.tf |_vars.tf You will create three folders with names modules, virtual_network, and storage account. Make sure that you are following the terraform standards like writing the terraform configuration in the main file and defining the variables alone in the variables(vars.tf) file. stoacctmain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"${var.name}\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"${var.account_replication}\" account_tier = \"${var.account_tier}\" } stoacctvars.tf variable \"rg\" { default = \"user-wmle\" } variable \"loc\" { default = \"eastus\" } variable \"name\" { default =\"storagegh\" } variable \"account_replication\" { default = \"LRS\" } variable \"account_tier\" { default = \"Basic\" } Root Module (main.tf) file under checking_modules module \"vnet\" { source = \"../virtual_network\" name = \"mymodvnet\" } module \"stoacct\" { source = \"../storage_account\" name = \"mymodstorageaccount\" } Not only the parameters mentioned above, but you can also pass any parameter from root module to the child modules. You can pass the parameters from the module to avoid the duplicity between them. Remote Backends It is better to store the code in a central repository . However, is having some disadvantages of doing so: You have to maintain push, pull configurations . If one pushes the wrong configuration and commits it, it becomes a problem. State file consists of sensitive information . So state files are non-editable. Backend in terraform explains how the state file is loaded and operations are executed. Backend can get initialize only after running terraform init command. So, terraform init is required to be run every time when backend is configured when any changes made to the backend when the backend configuration is removed completely Terraform cannot perform auto-initialize because it may require additional info from the user, to perform state migrations, etc.. Below are the steps which you will be maintaining for creating remote backend in Azure. Create a storage account Create a storage container Create a backend Get the storage account access key and resource group name, give it as a parameter to the backend. Below is the complete file structure for sample backend. Backend |____stoacct.tf |____stocontainer.tf |____backend.tf |____vars.tf |____output.tf stoacct.tf resource \"azurerm_storage_account\" \"storageaccount\" { name = \"storageaccountname\" resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" account_tier = \"${var.accounttier}\" account_replication_type = \"GRS\" tags { environment = \"staging\" } } stocontainer.tf resource \"azurerm_storage_container\" \"storagecontainer\" { name = \"vhds\" resource_group_name = \"${var.resourcegroup}\" storage_account_name = \"${azurerm_storage_account.storageaccount.name}\" container_access_type = \"private\" } vars.tf variable \"resourcegroup\" { default = \"user-abcd\" } variable \"location\" { default = \"eastus\" } variable \"accounttier\" { default = \"Basic\" } output.tf output \"storageacctname\" { value = \"${azurerm_storage_account.storageaccount.name}\" } output \"storageacctcontainer\" { value = \"${azurerm_storage_account.storagecontainer.name}\" } output \"access_key\" { value = \"${azurerm_storage_account.storageaccount.primary_access_key\" } Before creating backend, run the command to get the storage account keys list( az storage account keys list --account-name storageacctname ) and copy one of the key somewhere. It is useful for configuring the backend. backend.tf terraform{ backend \"azurerm\" { storage_account_name = \"storageacct21\" container_name = \"mycontainer\" key = \"terraform.tfstate.change\" resource_group_name = \"user-okvt\" access_key = \"aJlf+XjZhxwRp4gsU4hkGrQJzO7xBzz7B9rSzMLB/ozwcM6k/1N.......==\" } } where Parameters: storage_account_name - name of the storage account. container_name - name of the container. key- name of the tfstate blob. resource_group_name - name of the resource group. access_key - Storage account access key(any one). Points to Remember You cannot use interpolation syntax to configure backends. After creating backend run terraform init, it will be in the locked state. By running terraform apply command automatically, the lease status is changed to locked. After it is applied, it will come to an unlocked state. This backend supports consistency checking and state locking using the native capabilities of the Azure storage account. Terragrunt Terragrunt is referred to as a thin wrapper for Terraform which provides extra tools to keep the terraform configurations DRY, manage remote state and to work with multiple terraform modules. Terragrunt Commands terragrunt get terragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt supports the following use cases: Keeps terraform code DRY. Remote state configuration DRY. CLI flags DRY. Executing terraform commands on multiple modules at a time. Advantages of using terragrunt: Provides locking mechanism. Allows you to use remote state always. Build-In Functions lookup This helps in performing a dynamic lookup into a map variable. The syntax for lookup variable is lookup(map,key, [default]) . map: The map parameter is a variable like var.name. key: The key parameter includes the key from where it should find the environment. If the key doesn't exist in the map, interpolation will fail, if it doesn't find a third argument (default). The lookup will not work on nested lists or maps. It only works on flat maps. Local Values Local values help in assigning names to the expressions , which can be used multiple times within a module without rewriting it. Local values are defined in the local blocks. It is recommended to group the logically related local values together as a single block, if there is a dependency between them. locals { service_name = \"Fresco\" owner = \"Team\" } Example locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } If a single value or a result is used in many places and it is likely to be changed in the future, then you can go with locals. It is recommended to not use many local values because it makes the read configurations hard to the future maintainers. Data Source Data source allows the data to be computed or fetched for use in Terraform configuration. Data sources allow the terraform configurations to build on the information defined outside the Terraform or by another Terraform configuration. Providers play a major role in implementing and defining data sources. Data source helps in two major ways: It provides a read-only view for the pre-existing data. It can compute new values on the fly. Every data source in the Terraform is mapped to the provider depending on the longest-prefix-matching . data \"azurerm_resource_group\" \"passed\" { name = \"${var.resource_group_name}\" } Concat and Contains concat(list1,list2) * It combines two or more lists into a single list. contains(list, element) * It returns true if the element is present in the list or else false. E.g. contains(var.list_of_strings, \"an_element\") Workspaces Every terraform configurations has associate backends which defines how the operations are executed and where the persistent data like terraform state are stored. Persistent data stored in backend belongs to a workspace. Previously, the backend has only one workspace which is called as default and there will be only one state file associated with the configuration. Some of the backends support multiple workspaces, i.e. It allows multiple state files to be stored within a single configuration. The configuration is still having only one backend, and the multiple distinct instances can be deployed without configuring to a new backend or by changing authentication credentials. default workspace is special because you can't delete the default workspace. You cannot delete your active workspace (the workspace in which you are currently working). If you haven't created the workspace before, then you are working on the default workspace. Workspace Commands terraform workspace new - It creates the workspace with specified name and switches to it. terraform workspace list - It lists the workspaces available. terraform workspace select - It switches to the specified workspace. terraform workspace delete - It deletes the mentioned workspace. Configuring Multiple Providers Below is the file structure that you will maintain for working with multiple providers. multiple_providers | |___awsmain.tf | |___azuremain.tf | |___vars.tf | |___out.tf Create an S3 bucket in Azure which helps in storing the file in AWS. awsmain.tf resource \"aws_s3_bucket\" \"b\" { bucket = \"my-tf-test-bucket-21\" acl = \"private\" tags = { Name = \"My bucket test\" Environment = \"Dev\" } } Resource has two parameters: name(aws_s3_bucket) and type(\"b\"). Name of the resource may vary from provider to provider. Type of the resource depends on the user. Bucket indicates the name of the bucket. If it is not assigned, terraform assigns a random unique name. acl (access control list) - It helps to manage access to the buckets and objects. tag - It is a label which is assigned to the AWS resource by you or AWS. Environment - On which environment do you like to deploy S3 bucket (Dev, Prod or Stage). azuremain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"storageacct21\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"LRS\" account_tier = \"Standard\" } Parameters name - It indicates the name of the storage account. resource_group_name and location- name of the resource group and location. account_replication_type - Type of the account replication. account_tier - Account tier type out.tf output \"account_tier\" { value = \"azurerm_storage_Account.storagecct.account_tier\" }","title":"Terraform"},{"location":"learning/terraform/#infrastructure-as-code","text":"IaC is an important DevOps cornerstone that enables you to define, automatically manage, and provision infrastructure through source code. Infrastructure is managed as a software system . IaC is frequently referred to as Programmable Infrastructure .","title":"Infrastructure as Code"},{"location":"learning/terraform/#benefits-of-iac","text":"In IaC, A tool will monitor the state of the infrastructure . A script will be sent automatically to fix the issue . A script can be written to add new instances, and it can be reused . Faster process , no/less human involvement.","title":"Benefits of Iac"},{"location":"learning/terraform/#iac-implemetation-approaches","text":"","title":"IaC Implemetation Approaches"},{"location":"learning/terraform/#declarative","text":"Focuses on the desired end state of infrastructure (Functional) . Tools perform the necessary actions to reach that state . Automatically takes care of the order and executes it . Examples are Terraform and CloudFormation.","title":"Declarative"},{"location":"learning/terraform/#imperative","text":"Focuses on how to achieve the desired state (Procedural) . Follows the order of tasks and it executes in that order . Examples are Chef and Ansible.","title":"Imperative"},{"location":"learning/terraform/#configuration-management","text":"It is a system that keeps the track of organization's hardware, software, and other related information. It includes the software updates and versions present in the system. Configuration management is also called as configuration control . E.g.: Chef and Ansible","title":"Configuration Management"},{"location":"learning/terraform/#orchestration","text":"It automates the coordination, management, and configuration of systems and hardware. E.g.: Terraform and Nomad","title":"Orchestration:"},{"location":"learning/terraform/#installing-terraform","text":"sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip unzip terraform_0.11.11_linux_amd64.zip sudo mv terraform /usr/local/bin/ terraform --version","title":"Installing Terraform"},{"location":"learning/terraform/#terraform-lifecycle","text":"Terraform init - Initializes the working directory having Terraform configuration files. Terraform plan - Creates an execution plan and it mentions the changes it is going to make. Terraform apply - When you are not amazed by seeing the results of the terraform plan, you can go ahead and run terraform apply to apply the changes. The main difference between plan and apply is - apply asks for a prompt like Do you want to perform these actions? and then implement the changes, whereas, plan shows the possible end state of your infrastructure without actually implementing them. Terraform destroy - Destroys the created infrastructure.","title":"Terraform Lifecycle"},{"location":"learning/terraform/#terraform-configuration","text":"A set of files that describe the infrastructure in Terraform is called as Terraform Configuration. The entire configuration file is saved with .tf extension. Make sure to have all the configurations maintained in a single .tf file instead of many. Terraform loads all the .tf files present in the working directory.","title":"Terraform Configuration"},{"location":"learning/terraform/#creating-virtual-network","text":"create a virtual network without using the Azure portal and azure CLI commands. resource \"azurerm_virtual_network\"\"myterraformnetwork\" { name = \"myvnet\" address_space = [\"10.0.0.0/16\"] location=\"East US\" resource_group_name=\"er-tyjy\" } Resource blocks define the infrastructure (resource \"azurerm_virtual_network\" \"myterraformnetwork\"). It contains the two strings - type of the resource(azurerm_virtualnetwork) and name of the resource(myterraformnetwork). Terraform uses plugin based architecture to support the various services providers and infrastructure available. When you run Terraform init command, it downloads and installs provider binary for the corresponding providers. In this case, it is Azure. Now, you can run the terraform plan command. It shows all the changes it is going to make. If the output is as you expected and there are no errors, then you can run Terraform apply.","title":"Creating Virtual Network"},{"location":"learning/terraform/#terraform-validate","text":"You can run this command whenever you like to check whether the code is correct. Once you run this command, if there is no output, it means that there are no errors in the code.","title":"Terraform Validate"},{"location":"learning/terraform/#variables","text":"Terraform provides the following variable types: Strings - The default data type is a string . List - A list is like simple arrays Map - A map value is nothing but a lookup table for string keys to get the string values. In terraform, to reduce the complexity of code and for easiness , all the variables are created in the variables.tf file. variables.tf variable \"rg\" { default = \"terraform-lab2\" } variable \"locations\" { default = [\"ap-southeast-1\" , \"ap-southeast-2\"] } variable \"tags\" { type = \"map\" default = { environment = \"training\" source=\"citadel\" } } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-east-2 = \"image-4321\" } } For example, you have defined resource_group name in the variable.tf file. you can call them in main.tf file like this \"${var.resource_group}\"","title":"Variables"},{"location":"learning/terraform/#sensitive-parameters","text":"There may be sensitive parameters which should be shown only when running the terraform plan but not terraform apply. output \"sensitiveoutput\" { sensitive = true value = VALUE } When you run terraform apply, the output is labeled as sensitive. If there is any sensitive information, then you can protect it by using a sensitive parameter.","title":"Sensitive Parameters"},{"location":"learning/terraform/#terraformtfvars-file","text":"In terraform.tfvars file, we mention the \"key/value\" pair. The main.tf file takes the input from a terraform. tfvars file. If it is not present, it gets the input from variables.tf file. resource_group = \"user-ybje\" location = \"East US\"","title":"terraform.tfvars File"},{"location":"learning/terraform/#terraform-fmt","text":"It rewrites the confguration files to canonical style and format.","title":"terraform fmt"},{"location":"learning/terraform/#state-file-terraformtfstate","text":"It is a JSON file that maps the real world resources to the configuration files. In terraform, there are three states. Desired state - Represents how the infrastructure to be present. Actual state - Represents the current state of infrastructure. Known state - To bridge desired state and actual state, there is a known state. You can get the details of known state from a tfstate file. When you run terraform plan command , terraform performs a quick refresh and lists the actions needed to achieve the desired state. When terraform apply command is run, it applies the changes and updates the actual state.","title":"State File - terraform.tfstate"},{"location":"learning/terraform/#modules","text":"A module is like a reusable blueprint of infrastructure . A module is nothing but a folder of Terraform files. In terraform the modules are categorized into two types: Root modules - The current directory of Terraform files on which you are running the commands. Child modules - The modules sourced by the root module. To create a child module, add resources to it. Module syntax: module \"child\" { source = \"../child\" } The difference between adding the resources and module is for adding the resource (Eg: resource \"azurerm_virtual_network\" 'test\") you need type and name but for adding a module (Eg: module \"child\") only the name is enough. The name of the module should be unique within configurations because it is used as a reference to the module and outputs. main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 }","title":"Modules"},{"location":"learning/terraform/#updates","text":"terraform get - This command is used to download and install the modules for that configuration. terraform get -update - It checks the downloaded modules and checks for the recent versions if any.","title":"Updates"},{"location":"learning/terraform/#module-outputs","text":"If you need output from any module, you can get it by passing the required parameter. For example, the output of virtual network location can be found by following the below syntax. You can add this syntax to .tf file(main.tf or variables.tf) . output \"virtual_network_id\" { value = \"${azurerm_virtual_network.test.location}\" } Where virtual_network_id is a unique name and \"${azurerm_virtual_network.test.location}\" is the location of the virtual network using string interpolation.","title":"Module Outputs"},{"location":"learning/terraform/#benefits-of-modules","text":"Code reuse : When there is a need to provision the group of resources on another resource at the same time, a module can be used instead of copying the same code. It helps in resolving the bugs easily. If you want to make changes, changing the code in one place is enough. Abstraction layer : It makes complex configurations easier to conceptualize. For example, if you like to add vault(Another harshicop's tool for managing secrets) cluster to the environment, it requires dozens of components. Instead of worrying about individual components, it gives ready-to-use vault cluster. Black box : To create a vault, you only need some configuration parameters without worrying about the complexity. You don't need any knowledge on how to install vault, configuring vault cluster and working of the vault. You can create a working cluster in a few minutes. Best practices in organization : When a module is created, you can give it to the other teams. It helps in easily developing the infrastructure. **Versioned artifacts: They are immutable artifacts and can be promoted from one environment to others.","title":"Benefits of Modules"},{"location":"learning/terraform/#introduction-to-meta-parameters","text":"There are some difficulties with the declarative language. For example, since it is a declarative language, there is no concept of for loops. How do you repeat a piece of code without copy paste? * Terraform provides primitives like meta parameter called as count, life cycle blocks like create_before_destroy, ternary operator and a large number of interpolation functions . This helps in performing certain types of loops, if statements and zero downtime deployments.","title":"Introduction to Meta Parameters"},{"location":"learning/terraform/#count","text":"Count - It defines how many parameters you like to create. Modify main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } To increase Vnets to 5, update count = 5 . To decrease Vnets to 2, update count = 2 , last 3 Vnets will get deleted.","title":"Count"},{"location":"learning/terraform/#elements","text":"Suppose you like to create the three virtual networks with names vnet-A, vnet-B and vnet-C, you can do this easily with the help of list . Mention how many vnets you are going to create with the help of list and define it in variables.tf file variable \"name\" { type= \"list\" default = [\"A\",\"B\",\"C\"] } You can call this variable in the main.tf file in the following way count = \"${length(var.name)}\" - It returns number of elements present in the list. you should store it in meta parameter count. \"${element(var.name,count.index)}\" - It acts as a loop, it takes the input from list and repeats until there are no elements in list. variables.tf variable \"resource_group\" { default = \"user-nuqo\" } variable \"location\" { default = \"East US\" } variable \"name\" { type = \"list\" default = [\"A\",\"B\",\"C\"] } main.tf resource \"azurerm_virtual_network\"\"multiple\" { name = \"vnet-${element(var.name,count.index)}\" resource_group_name = \"${var.resource_group}\" location = \"${var.location}\" address_space=[\"10.0.0.0/16\"] count=\"${length(var.name)}\" }","title":"Elements"},{"location":"learning/terraform/#conditions","text":"For example, a vnet has to be created only, if the variable number of vnets is 3 or else no. You can do this by using the ternary operator. variables.tf variable \"no_of_vnets\" { default= 3 } main.tf count = \"${var.no_of_vnets ==3 ?1 : 0}\" * First, it will execute the ternary operator. If it is true, it takes the output as 1 or else 0. * Now, in the above case, the output is 1, the count becomes one, and the vnet is created.","title":"Conditions"},{"location":"learning/terraform/#inheriting-variables","text":"Inheriting the variables between modules keeps your Terraform code DRY. Don't Repeat Yourself (DRY) It aims at reducing repeating patterns and code. There is no need to write the duplicate code for each environment you are deploying. You can write the minimum code and achieve the goal by having maximum re-usability.","title":"Inheriting Variables"},{"location":"learning/terraform/#module-file-structure","text":"You will follow the below file structure to inherit the variables between modules. modules | |___mod | |__mod.tf | |___vnet |__vnet.tf |__vars.tf There is one vnet and you can configure the vnet by passing the parameters from module. The variables in the vnet are overriden by the values provided in mod.tf file. Create a folder with name Modules Create two subfolders of names vnet and mod in the nested_modules folder. In the vnet folder, create a virtual network using Terraform configuration. Maintain two files one for main configuration(vnet.tf) and other for declaring variables(vars.tf) . Now, create main.tf file like this resource \"azurerm_virtual_network\"\"vnet\" { name = \"${var.name}\" address_space = [\"10.0.0.0/16\"] resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" } Now, vars.tf file looks like this variable \"name\" { description = \"name of the vnet\" } variable \"resourcegroup\" { description = \"name of the resource group\" } variable \"location\" { description =\"name of the location\" } variable \"address\" { type =\"list\" description = \"specify the address\" } Create a folder with name mod in the modules directory. Create a file with name mod.tf and write the below code in it. We are passing the variables from the root module (mod) to the child module (vnet) using source path . module \"child\" { source = \"../vnet\" name = \"modvnet\" resourcegroup = \"user-vcom\" location = \"eastus\" address = \"10.0.1.0/16\" } Run terraform plan to visualize how the directories will be created. You can see the name of the vnet is taken from the module file instead of getting it from vars.tf","title":"Module File Structure"},{"location":"learning/terraform/#nested-modules","text":"For any module, root module should exist. Root module is basically a directory that holds the terraform configuration files for the desired infrastructure. They also provide the entry point to the nested modules you are going to utilize. For any module main.tf, vars.tf, and outputs.tf naming conventions are recommended. If you are using nested modules, create them under the subdirectory with name modules . checking_modules |__modules | |____main.tf | |______virtual_network | |_main.tf | |_vars.tf | |______storage_account |_main.tf |_vars.tf You will create three folders with names modules, virtual_network, and storage account. Make sure that you are following the terraform standards like writing the terraform configuration in the main file and defining the variables alone in the variables(vars.tf) file. stoacctmain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"${var.name}\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"${var.account_replication}\" account_tier = \"${var.account_tier}\" } stoacctvars.tf variable \"rg\" { default = \"user-wmle\" } variable \"loc\" { default = \"eastus\" } variable \"name\" { default =\"storagegh\" } variable \"account_replication\" { default = \"LRS\" } variable \"account_tier\" { default = \"Basic\" } Root Module (main.tf) file under checking_modules module \"vnet\" { source = \"../virtual_network\" name = \"mymodvnet\" } module \"stoacct\" { source = \"../storage_account\" name = \"mymodstorageaccount\" } Not only the parameters mentioned above, but you can also pass any parameter from root module to the child modules. You can pass the parameters from the module to avoid the duplicity between them.","title":"Nested Modules"},{"location":"learning/terraform/#remote-backends","text":"It is better to store the code in a central repository . However, is having some disadvantages of doing so: You have to maintain push, pull configurations . If one pushes the wrong configuration and commits it, it becomes a problem. State file consists of sensitive information . So state files are non-editable. Backend in terraform explains how the state file is loaded and operations are executed. Backend can get initialize only after running terraform init command. So, terraform init is required to be run every time when backend is configured when any changes made to the backend when the backend configuration is removed completely Terraform cannot perform auto-initialize because it may require additional info from the user, to perform state migrations, etc.. Below are the steps which you will be maintaining for creating remote backend in Azure. Create a storage account Create a storage container Create a backend Get the storage account access key and resource group name, give it as a parameter to the backend. Below is the complete file structure for sample backend. Backend |____stoacct.tf |____stocontainer.tf |____backend.tf |____vars.tf |____output.tf stoacct.tf resource \"azurerm_storage_account\" \"storageaccount\" { name = \"storageaccountname\" resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" account_tier = \"${var.accounttier}\" account_replication_type = \"GRS\" tags { environment = \"staging\" } } stocontainer.tf resource \"azurerm_storage_container\" \"storagecontainer\" { name = \"vhds\" resource_group_name = \"${var.resourcegroup}\" storage_account_name = \"${azurerm_storage_account.storageaccount.name}\" container_access_type = \"private\" } vars.tf variable \"resourcegroup\" { default = \"user-abcd\" } variable \"location\" { default = \"eastus\" } variable \"accounttier\" { default = \"Basic\" } output.tf output \"storageacctname\" { value = \"${azurerm_storage_account.storageaccount.name}\" } output \"storageacctcontainer\" { value = \"${azurerm_storage_account.storagecontainer.name}\" } output \"access_key\" { value = \"${azurerm_storage_account.storageaccount.primary_access_key\" } Before creating backend, run the command to get the storage account keys list( az storage account keys list --account-name storageacctname ) and copy one of the key somewhere. It is useful for configuring the backend. backend.tf terraform{ backend \"azurerm\" { storage_account_name = \"storageacct21\" container_name = \"mycontainer\" key = \"terraform.tfstate.change\" resource_group_name = \"user-okvt\" access_key = \"aJlf+XjZhxwRp4gsU4hkGrQJzO7xBzz7B9rSzMLB/ozwcM6k/1N.......==\" } } where Parameters: storage_account_name - name of the storage account. container_name - name of the container. key- name of the tfstate blob. resource_group_name - name of the resource group. access_key - Storage account access key(any one).","title":"Remote Backends"},{"location":"learning/terraform/#points-to-remember","text":"You cannot use interpolation syntax to configure backends. After creating backend run terraform init, it will be in the locked state. By running terraform apply command automatically, the lease status is changed to locked. After it is applied, it will come to an unlocked state. This backend supports consistency checking and state locking using the native capabilities of the Azure storage account.","title":"Points to Remember"},{"location":"learning/terraform/#terragrunt","text":"Terragrunt is referred to as a thin wrapper for Terraform which provides extra tools to keep the terraform configurations DRY, manage remote state and to work with multiple terraform modules.","title":"Terragrunt"},{"location":"learning/terraform/#terragrunt-commands","text":"terragrunt get terragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt supports the following use cases: Keeps terraform code DRY. Remote state configuration DRY. CLI flags DRY. Executing terraform commands on multiple modules at a time. Advantages of using terragrunt: Provides locking mechanism. Allows you to use remote state always.","title":"Terragrunt Commands"},{"location":"learning/terraform/#build-in-functions","text":"","title":"Build-In Functions"},{"location":"learning/terraform/#lookup","text":"This helps in performing a dynamic lookup into a map variable. The syntax for lookup variable is lookup(map,key, [default]) . map: The map parameter is a variable like var.name. key: The key parameter includes the key from where it should find the environment. If the key doesn't exist in the map, interpolation will fail, if it doesn't find a third argument (default). The lookup will not work on nested lists or maps. It only works on flat maps.","title":"lookup"},{"location":"learning/terraform/#local-values","text":"Local values help in assigning names to the expressions , which can be used multiple times within a module without rewriting it. Local values are defined in the local blocks. It is recommended to group the logically related local values together as a single block, if there is a dependency between them. locals { service_name = \"Fresco\" owner = \"Team\" } Example locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } If a single value or a result is used in many places and it is likely to be changed in the future, then you can go with locals. It is recommended to not use many local values because it makes the read configurations hard to the future maintainers.","title":"Local Values"},{"location":"learning/terraform/#data-source","text":"Data source allows the data to be computed or fetched for use in Terraform configuration. Data sources allow the terraform configurations to build on the information defined outside the Terraform or by another Terraform configuration. Providers play a major role in implementing and defining data sources. Data source helps in two major ways: It provides a read-only view for the pre-existing data. It can compute new values on the fly. Every data source in the Terraform is mapped to the provider depending on the longest-prefix-matching . data \"azurerm_resource_group\" \"passed\" { name = \"${var.resource_group_name}\" }","title":"Data Source"},{"location":"learning/terraform/#concat-and-contains","text":"concat(list1,list2) * It combines two or more lists into a single list. contains(list, element) * It returns true if the element is present in the list or else false. E.g. contains(var.list_of_strings, \"an_element\")","title":"Concat and Contains"},{"location":"learning/terraform/#workspaces","text":"Every terraform configurations has associate backends which defines how the operations are executed and where the persistent data like terraform state are stored. Persistent data stored in backend belongs to a workspace. Previously, the backend has only one workspace which is called as default and there will be only one state file associated with the configuration. Some of the backends support multiple workspaces, i.e. It allows multiple state files to be stored within a single configuration. The configuration is still having only one backend, and the multiple distinct instances can be deployed without configuring to a new backend or by changing authentication credentials. default workspace is special because you can't delete the default workspace. You cannot delete your active workspace (the workspace in which you are currently working). If you haven't created the workspace before, then you are working on the default workspace.","title":"Workspaces"},{"location":"learning/terraform/#workspace-commands","text":"terraform workspace new - It creates the workspace with specified name and switches to it. terraform workspace list - It lists the workspaces available. terraform workspace select - It switches to the specified workspace. terraform workspace delete - It deletes the mentioned workspace.","title":"Workspace Commands"},{"location":"learning/terraform/#configuring-multiple-providers","text":"Below is the file structure that you will maintain for working with multiple providers. multiple_providers | |___awsmain.tf | |___azuremain.tf | |___vars.tf | |___out.tf Create an S3 bucket in Azure which helps in storing the file in AWS. awsmain.tf resource \"aws_s3_bucket\" \"b\" { bucket = \"my-tf-test-bucket-21\" acl = \"private\" tags = { Name = \"My bucket test\" Environment = \"Dev\" } } Resource has two parameters: name(aws_s3_bucket) and type(\"b\"). Name of the resource may vary from provider to provider. Type of the resource depends on the user. Bucket indicates the name of the bucket. If it is not assigned, terraform assigns a random unique name. acl (access control list) - It helps to manage access to the buckets and objects. tag - It is a label which is assigned to the AWS resource by you or AWS. Environment - On which environment do you like to deploy S3 bucket (Dev, Prod or Stage). azuremain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"storageacct21\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"LRS\" account_tier = \"Standard\" } Parameters name - It indicates the name of the storage account. resource_group_name and location- name of the resource group and location. account_replication_type - Type of the account replication. account_tier - Account tier type out.tf output \"account_tier\" { value = \"azurerm_storage_Account.storagecct.account_tier\" }","title":"Configuring Multiple Providers"},{"location":"learning/vagrant/","text":"Vagrant Init with a image in vagrant cloud vagrant init hashicorp/precise64 Start the vm vagrant up SSH into the vm vagrant ssh Hibernate the vm vagrant suspend Check the status of vagrant vm vagrant status Stop the vm vagrant halt Clean up the vm vagrant destroy","title":"Vagrant"},{"location":"learning/vagrant/#vagrant","text":"","title":"Vagrant"},{"location":"learning/vagrant/#init-with-a-image-in-vagrant-cloud","text":"vagrant init hashicorp/precise64","title":"Init with a image in vagrant cloud"},{"location":"learning/vagrant/#start-the-vm","text":"vagrant up","title":"Start the vm"},{"location":"learning/vagrant/#ssh-into-the-vm","text":"vagrant ssh","title":"SSH into the vm"},{"location":"learning/vagrant/#hibernate-the-vm","text":"vagrant suspend","title":"Hibernate the vm"},{"location":"learning/vagrant/#check-the-status-of-vagrant-vm","text":"vagrant status","title":"Check the status of vagrant vm"},{"location":"learning/vagrant/#stop-the-vm","text":"vagrant halt","title":"Stop the vm"},{"location":"learning/vagrant/#clean-up-the-vm","text":"vagrant destroy","title":"Clean up the vm"},{"location":"server/","text":"Server Details","title":"Introduction"},{"location":"server/#server-details","text":"","title":"Server Details"},{"location":"server/install/","text":"Installation Centos Identifying Version Download the latest version of Centos from centos.org Creating Live USB Format a USB as NTFS. Download Fedora LiveUSB Creator or read the supported software for Centos in the downloads page. See the instructions to write the ISO into the USB Setting Up PC Format the HDD (optional) Ensure - Advanced Option, Legacy USB Support is Enabled In Boot priority, USB is the first drive Ensure the Boot option has Boot from USB selected above HDD Configuring Centos Full Installation Steps Create Custom partitions for /boot, /, /var, /tmp, /home Initial Server Setup Update Centos with latest patches yum update -y && yum upgrade -y yum clean all # To clean downloaded packages in /var/cache/yum Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Adding Non Root User Add and Delete Users Creating Volume Groups Securing Server Security based on articles mentioned here or here or DigitalOcean ssh-keygen -t rsa -b 4096 It will create two files: id_rsa and id_rsa.pub in the ~/.ssh directory. When you add a passphrase to your SSH key, it encrypts the private key using 128-bit AES so that the private key is useless without the passphrase to decrypt it. cat ~/.ssh/id_rsa.pub For example, if we had a Linux VM named myserver with a user azureuser, we could use the following command to install the public key file and authorize the user with the key: ssh-copy-id -i ~/.ssh/id_rsa.pub azureuser@myserver Connect with SSH ssh jim@137.117.101.249 Try executing a few Linux commands ls -la / to show the root of the disk ps -l to show all the running processes dmesg to list all the kernel messages lsblk to list all the block devices - here you will see your drives Initialize data disks Any additional drives you create from scratch need to be initialized and formatted. dmesg | grep SCSI - list all the messages from the kernel for SCSI devices. Use fdisk to initialize the drive We can use the following command to create a new primary partition: (echo n; echo p; echo 1; echo ; echo ; echo w) | sudo fdisk /dev/sdc We need to write a file system to the partition with the mkfs command. sudo mkfs -t ext4 /dev/sdc1 We need to mount the drive to the file system. sudo mkdir /data & sudo mount /dev/sdc1 /data Always make sure to lock down ports used for administrative access. An even better approach is to create a VPN to link the virtual network to your private network and only allow RDP or SSH requests from that address range. You can also change the port used by SSH to be something other than the default. Keep in mind that changing ports is not sufficient to stop attacks. It simply makes it a little harder to discover.","title":"Installation"},{"location":"server/install/#installation","text":"","title":"Installation"},{"location":"server/install/#centos","text":"Identifying Version Download the latest version of Centos from centos.org Creating Live USB Format a USB as NTFS. Download Fedora LiveUSB Creator or read the supported software for Centos in the downloads page. See the instructions to write the ISO into the USB Setting Up PC Format the HDD (optional) Ensure - Advanced Option, Legacy USB Support is Enabled In Boot priority, USB is the first drive Ensure the Boot option has Boot from USB selected above HDD Configuring Centos Full Installation Steps Create Custom partitions for /boot, /, /var, /tmp, /home Initial Server Setup Update Centos with latest patches yum update -y && yum upgrade -y yum clean all # To clean downloaded packages in /var/cache/yum Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Adding Non Root User Add and Delete Users Creating Volume Groups Securing Server Security based on articles mentioned here or here or DigitalOcean ssh-keygen -t rsa -b 4096 It will create two files: id_rsa and id_rsa.pub in the ~/.ssh directory. When you add a passphrase to your SSH key, it encrypts the private key using 128-bit AES so that the private key is useless without the passphrase to decrypt it. cat ~/.ssh/id_rsa.pub For example, if we had a Linux VM named myserver with a user azureuser, we could use the following command to install the public key file and authorize the user with the key: ssh-copy-id -i ~/.ssh/id_rsa.pub azureuser@myserver Connect with SSH ssh jim@137.117.101.249 Try executing a few Linux commands ls -la / to show the root of the disk ps -l to show all the running processes dmesg to list all the kernel messages lsblk to list all the block devices - here you will see your drives Initialize data disks Any additional drives you create from scratch need to be initialized and formatted. dmesg | grep SCSI - list all the messages from the kernel for SCSI devices. Use fdisk to initialize the drive We can use the following command to create a new primary partition: (echo n; echo p; echo 1; echo ; echo ; echo w) | sudo fdisk /dev/sdc We need to write a file system to the partition with the mkfs command. sudo mkfs -t ext4 /dev/sdc1 We need to mount the drive to the file system. sudo mkdir /data & sudo mount /dev/sdc1 /data Always make sure to lock down ports used for administrative access. An even better approach is to create a VPN to link the virtual network to your private network and only allow RDP or SSH requests from that address range. You can also change the port used by SSH to be something other than the default. Keep in mind that changing ports is not sufficient to stop attacks. It simply makes it a little harder to discover.","title":"Centos"},{"location":"server/mobile/","text":"Converting Android Device Into Linux Server Centos Pre-requisites Identify an Android mobile phone Factory reset to remove any unwanted apps and data Remove any apps or services from the device to free up space and memory Install Termux and AnLinux from Playstore Installation Start AnLinux and search for the distro. Select Centos Copy the installation command Open Termux terminal and paste the command Begin installation Access Mobile Configure Static Ip to Mobile Add Ip address and remote-hostname to Laptop's hosts file. Install SSH on Termux apt install openssh Start the ssh server sshd . This will also generate the private and public keys. Test the service locally ssh localhost -p 8022 . Accept the public key. ssh-copy-id copies the local-host\u2019s public key to the remote-host\u2019s authorized_keys file. ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-hostname> -p 8022 . Enter remote user password to complete transaction. Login from Laptop ssh <hostname> -p 8022 . Enter the passphrase. Install Python Configuring Centos Update Centos with latest patches yum install update -y && yum install upgrade -y Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Init Centos yum -y install initscripts & yum clean all Change Root Password passwd root Install Sudo yum install sudo -y The difference between wheel and sudo . In CentOS, a user belonging to the wheel group can execute su and directly ascend to root. Meanwhile, a sudo user would have use the sudo su first. Essentially, there is no real difference except for the syntax used to become root, and users belonging to both groups can use the sudo command. * Adding Non Root User bmmnb Add and Delete Users Securing Server Security based on articles mentioned here or here or DigitalOcean Configures a Hostname Reconfigures the Timezone Updates the entire System Creates a New Admin user so you can manage your server safely without the need of doing remote connections with root. Helps user Generate Secure RSA Keys, so that remote access to your server is done exclusive from your local pc and no Conventional password Configures, Optimize and secures the SSH Server (Some Settings Following CIS Benchmark) Configures IPTABLES Rules to protect the server from common attacks Disables unused FileSystems and Network protocols Protects the server against Brute Force attacks by installing a configuring fail2ban Installs and Configure Artillery as a Honeypot, Monitoring, Blocking and Alerting tool Installs PortSentry Installs RootKit Hunter Secures Root Home and Grub Configuration Files Installs Unhide to help Detect Malicious Hidden Processes Installs Tiger, A Security Auditing and Intrusion Prevention system Restrict Access to Apache Config Files Disables Compilers Creates Daily Cron job for System Updates Kernel Hardening via sysctl configuration File (Tweaked) /tmp Directory Hardening PSAD IDS installation Enables Process Accounting Enables Unattended Upgrades MOTD and Banners for Unauthorized access Disables USB Support for Improved Security (Optional) Configures a Restrictive Default UMASK Configures and enables Auditd Configures Auditd rules following CIS Benchmark Sysstat install ArpWatch install Additional Hardening steps following CIS Benchmark Secures Cron Automates the process of setting a GRUB Bootloader Password Secures Boot Settings Sets Secure File Permissions for Critical System Files","title":"Mobile"},{"location":"server/mobile/#converting-android-device-into-linux-server","text":"","title":"Converting Android Device Into Linux Server"},{"location":"server/mobile/#centos","text":"Pre-requisites Identify an Android mobile phone Factory reset to remove any unwanted apps and data Remove any apps or services from the device to free up space and memory Install Termux and AnLinux from Playstore Installation Start AnLinux and search for the distro. Select Centos Copy the installation command Open Termux terminal and paste the command Begin installation Access Mobile Configure Static Ip to Mobile Add Ip address and remote-hostname to Laptop's hosts file. Install SSH on Termux apt install openssh Start the ssh server sshd . This will also generate the private and public keys. Test the service locally ssh localhost -p 8022 . Accept the public key. ssh-copy-id copies the local-host\u2019s public key to the remote-host\u2019s authorized_keys file. ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-hostname> -p 8022 . Enter remote user password to complete transaction. Login from Laptop ssh <hostname> -p 8022 . Enter the passphrase. Install Python Configuring Centos Update Centos with latest patches yum install update -y && yum install upgrade -y Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Init Centos yum -y install initscripts & yum clean all Change Root Password passwd root Install Sudo yum install sudo -y The difference between wheel and sudo . In CentOS, a user belonging to the wheel group can execute su and directly ascend to root. Meanwhile, a sudo user would have use the sudo su first. Essentially, there is no real difference except for the syntax used to become root, and users belonging to both groups can use the sudo command. * Adding Non Root User bmmnb Add and Delete Users Securing Server Security based on articles mentioned here or here or DigitalOcean Configures a Hostname Reconfigures the Timezone Updates the entire System Creates a New Admin user so you can manage your server safely without the need of doing remote connections with root. Helps user Generate Secure RSA Keys, so that remote access to your server is done exclusive from your local pc and no Conventional password Configures, Optimize and secures the SSH Server (Some Settings Following CIS Benchmark) Configures IPTABLES Rules to protect the server from common attacks Disables unused FileSystems and Network protocols Protects the server against Brute Force attacks by installing a configuring fail2ban Installs and Configure Artillery as a Honeypot, Monitoring, Blocking and Alerting tool Installs PortSentry Installs RootKit Hunter Secures Root Home and Grub Configuration Files Installs Unhide to help Detect Malicious Hidden Processes Installs Tiger, A Security Auditing and Intrusion Prevention system Restrict Access to Apache Config Files Disables Compilers Creates Daily Cron job for System Updates Kernel Hardening via sysctl configuration File (Tweaked) /tmp Directory Hardening PSAD IDS installation Enables Process Accounting Enables Unattended Upgrades MOTD and Banners for Unauthorized access Disables USB Support for Improved Security (Optional) Configures a Restrictive Default UMASK Configures and enables Auditd Configures Auditd rules following CIS Benchmark Sysstat install ArpWatch install Additional Hardening steps following CIS Benchmark Secures Cron Automates the process of setting a GRUB Bootloader Password Secures Boot Settings Sets Secure File Permissions for Critical System Files","title":"Centos"},{"location":"server/volume-groups/","text":"Configuration Centos Prerequisites Install EPEL Packages yum install epel-release Install ntfs-3g and fuse packages to support NTFS drives yum install ntfs-3g fuse -y Format the new NTFS filestsystem to xfs mkfs.xfs -f /dev/sda Identifying Additional HDD Run blkid to verify if HDD is detected. On detection of HDD, follow the below steps to configure as XFS file system. * Run fdisk -l to list the devices, partitions and volume groups * Scan your hard drive for Bad Sectors or Bad Blocks 1. Partitioning HDD (Optional and for Primary HDD) * On identifying the device(HDD), say /dev/sda as the target device to partition, run fdisk /dev/sda * These steps below are not for secondary HDD as partitioning will not utilize the entire space * Option m to list the operations, Option p to print the current partitions * Note the start and end block ids of the current partitions * Option d to delete the current partitions, select the partition to delete 2 * Option n to create new partition, select only 1 partition for the HDD, select default start and end block id, it should match the one printed above * Option t to select the type of partition, followed by 8e to select as Linux LVM * Format the new partition to xfs mkfs.xfs -f /dev/sda1 as only one partition was created * Mount the new partition to test mount -t xfs /dev/sda1 /data * Make entry in /etc/fstab to make it permanent * Verify the mounted partition by running df -Th to see free and used space Securing Drive and adding Redundancy Software RAID and LVM For the partitions that are needed as Raid 1, use fdsik /dev/sdb3 ,Press \u2018t\u2019 to change the partition type. Press \u2018fd\u2019 to choose Linux Raid Auto. Install Raid 1 on /dev/sdb3 and /dev/sda1 using mdadm --create /dev/data --level=1 --raid-devices=2 /dev/sdb3 /dev/sda1 Encrypting Raid devices Volume Group Setup Logical Volume Manager (LVM) is a software-based RAID-like system that lets you create \"pools\" of storage and add hard drive space to those pools as needed. * LVM Basic * Create, expand, and encrypt storage pools Volume Group Lifecycle Migrate Data from one LV to another Identify the current Physical Volume and Volume Group mapping pvscan or pvs List the partitions and volumes in the Volume Groups vgdisplay centos-digital -v Logical volume and device mapping lvs -o+devices List all devices and device mapping lsscsi Before Reducing logical volume, move to another drive Format /dev/sdb3 to a physical volume: pvcreate /dev/sdb3 Give /dev/sdb3 to the volume group centos-digital: vgextend centos-digital /dev/sdb3 Migrate our logical volume Users from /dev/sdb2 to /dev/sdb3, pvmove -n Users /dev/sdb2 /dev/sdb3 Verify if the data is migrated lsblk Reduce your logical volume to make place for your new partition lvresize -L 25G /dev/centos-digital/home Use fdisk to reduce the size of /dev/sdb2 to 80G as total for volume group centos-digital was 75G (yes, a little bit bigger as we used in lvresize!) Use fdisk to create a new LVM partition, /dev/sdb3, with the new available space (it will be around 400G). Check if the kernel could automatically detect the partition change. See if /proc/partition matches the new state (thus, /dev/sdb3 is visible). If not, you need to reboot. (Probably it will.) Make /dev/sdb2 to be as big again pvresize /dev/sdb2","title":"Configuration"},{"location":"server/volume-groups/#configuration","text":"","title":"Configuration"},{"location":"server/volume-groups/#centos","text":"Prerequisites Install EPEL Packages yum install epel-release Install ntfs-3g and fuse packages to support NTFS drives yum install ntfs-3g fuse -y Format the new NTFS filestsystem to xfs mkfs.xfs -f /dev/sda Identifying Additional HDD Run blkid to verify if HDD is detected. On detection of HDD, follow the below steps to configure as XFS file system. * Run fdisk -l to list the devices, partitions and volume groups * Scan your hard drive for Bad Sectors or Bad Blocks 1. Partitioning HDD (Optional and for Primary HDD) * On identifying the device(HDD), say /dev/sda as the target device to partition, run fdisk /dev/sda * These steps below are not for secondary HDD as partitioning will not utilize the entire space * Option m to list the operations, Option p to print the current partitions * Note the start and end block ids of the current partitions * Option d to delete the current partitions, select the partition to delete 2 * Option n to create new partition, select only 1 partition for the HDD, select default start and end block id, it should match the one printed above * Option t to select the type of partition, followed by 8e to select as Linux LVM * Format the new partition to xfs mkfs.xfs -f /dev/sda1 as only one partition was created * Mount the new partition to test mount -t xfs /dev/sda1 /data * Make entry in /etc/fstab to make it permanent * Verify the mounted partition by running df -Th to see free and used space Securing Drive and adding Redundancy Software RAID and LVM For the partitions that are needed as Raid 1, use fdsik /dev/sdb3 ,Press \u2018t\u2019 to change the partition type. Press \u2018fd\u2019 to choose Linux Raid Auto. Install Raid 1 on /dev/sdb3 and /dev/sda1 using mdadm --create /dev/data --level=1 --raid-devices=2 /dev/sdb3 /dev/sda1 Encrypting Raid devices Volume Group Setup Logical Volume Manager (LVM) is a software-based RAID-like system that lets you create \"pools\" of storage and add hard drive space to those pools as needed. * LVM Basic * Create, expand, and encrypt storage pools Volume Group Lifecycle Migrate Data from one LV to another Identify the current Physical Volume and Volume Group mapping pvscan or pvs List the partitions and volumes in the Volume Groups vgdisplay centos-digital -v Logical volume and device mapping lvs -o+devices List all devices and device mapping lsscsi Before Reducing logical volume, move to another drive Format /dev/sdb3 to a physical volume: pvcreate /dev/sdb3 Give /dev/sdb3 to the volume group centos-digital: vgextend centos-digital /dev/sdb3 Migrate our logical volume Users from /dev/sdb2 to /dev/sdb3, pvmove -n Users /dev/sdb2 /dev/sdb3 Verify if the data is migrated lsblk Reduce your logical volume to make place for your new partition lvresize -L 25G /dev/centos-digital/home Use fdisk to reduce the size of /dev/sdb2 to 80G as total for volume group centos-digital was 75G (yes, a little bit bigger as we used in lvresize!) Use fdisk to create a new LVM partition, /dev/sdb3, with the new available space (it will be around 400G). Check if the kernel could automatically detect the partition change. See if /proc/partition matches the new state (thus, /dev/sdb3 is visible). If not, you need to reboot. (Probably it will.) Make /dev/sdb2 to be as big again pvresize /dev/sdb2","title":"Centos"}]}