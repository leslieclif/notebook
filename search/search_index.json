{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Leslie's Notebook Install mkdocs using command pip install mkdocs Topics Server - Setup basic Linux server Kubernetes - Setup Kubernetes IDE - Tips and Tricks","title":"Welcome"},{"location":"#welcome-to-leslies-notebook","text":"Install mkdocs using command pip install mkdocs","title":"Welcome to Leslie's Notebook"},{"location":"#topics","text":"Server - Setup basic Linux server Kubernetes - Setup Kubernetes IDE - Tips and Tricks","title":"Topics"},{"location":"developer/","text":"Update VS Code in Windows from dotfiles/programs - run ./vsc.sh. update chmod +x to make it executable in windows using Git Bash This will install vscode extensions Update WSL2 first (by default WLS1 is enabled) Install Ubuntu from Microsoft Stores Install Visual Studio Code Update Linux packages sudo apt update sudo apt -y upgrade To find the home directory in Ubuntu explorer.exe . Install Windows Terminal for Miscrosoft Store Install Menlo font (from Powerlevel10k site) To test the terminal color output, run this code in the terminal for code in { 30 ..37 } ; do \\ echo -en \"\\e[ ${ code } m\" '\\\\e[' \" $code \" 'm' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;1m\" '\\\\e[' \" $code \" ';1m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;3m\" '\\\\e[' \" $code \" ';3m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;4m\" '\\\\e[' \" $code \" ';4m' \"\\e[0m\" ; \\ echo -e \" \\e[ $(( code+60 )) m\" '\\\\e[' \" $(( code+60 )) \" 'm' \"\\e[0m\" ; \\ done ssh-keygen -t rsa -b 4096 -f ~/.ssh/raddit-user -C raddit-user Using the .ssh config files (~/.ssh/config) Generate public & private ssh keys: ssh-keygen -t rsa Type in a name which will be put in ~/.ssh directory To bypass password prompt, you should add the foo.pub file to the authorized_keys file on the server's ~/.ssh directory. You can do a pipe via ssh: cat mykey.pub | ssh myuser@mysite.com -p 123 'cat >> .ssh/authorized_keys' Add the publickey name to the ~/.ssh/config file like this: Host bitbucket.org IdentityFile ~/.ssh/myprivatekeyfile # the leading spaces are important! Port 123 Verify and then SSH into the remote server. To check if your config is right type: ssh -T git@github.com ssh root@mysite.com or ssh mysite.com # if you setup the User setting in config Adding SSH Keys to servers SSH Client Config Edit setings on the new terminal to make Ubuntu as the default terminal. Also set the fontFace and https://www.the-digital-life.com/en/awesome-wsl-wsl2-terminal/ Switching remote URLs from HTTPS to SSH List your existing remotes in order to get the name of the remote you want to change. $ git remote -v > origin https://github.com/USERNAME/REPOSITORY.git ( fetch ) > origin https://github.com/USERNAME/REPOSITORY.git ( push ) Change your remote's URL from HTTPS to SSH with the git remote set-url command. $ git remote set-url origin git@github.com:USERNAME/REPOSITORY.git git remote set-url origin git@github.com :leslieclif/notebook.git Inspirational dotfile repos https://www.freecodecamp.org/news/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/ https://github.com/victoriadrake/dotfiles/tree/ubuntu-19.10 https://github.com/georgijd/dotfiles https://github.com/jieverson/dotfiles-win/blob/master/install.sh Bashrc Automation https://victoria.dev/blog/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/ https://victoria.dev/blog/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/ Vagrant setup https://www.techdrabble.com/ansible/36-install-ansible-molecule-vagrant-on-windows-wsl","title":"Setup"},{"location":"developer/#update-linux-packages","text":"sudo apt update sudo apt -y upgrade","title":"Update Linux packages"},{"location":"developer/#to-find-the-home-directory-in-ubuntu","text":"explorer.exe . Install Windows Terminal for Miscrosoft Store Install Menlo font (from Powerlevel10k site) To test the terminal color output, run this code in the terminal for code in { 30 ..37 } ; do \\ echo -en \"\\e[ ${ code } m\" '\\\\e[' \" $code \" 'm' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;1m\" '\\\\e[' \" $code \" ';1m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;3m\" '\\\\e[' \" $code \" ';3m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;4m\" '\\\\e[' \" $code \" ';4m' \"\\e[0m\" ; \\ echo -e \" \\e[ $(( code+60 )) m\" '\\\\e[' \" $(( code+60 )) \" 'm' \"\\e[0m\" ; \\ done ssh-keygen -t rsa -b 4096 -f ~/.ssh/raddit-user -C raddit-user","title":"To find the home directory in Ubuntu"},{"location":"developer/#using-the-ssh-config-files-sshconfig","text":"Generate public & private ssh keys: ssh-keygen -t rsa Type in a name which will be put in ~/.ssh directory To bypass password prompt, you should add the foo.pub file to the authorized_keys file on the server's ~/.ssh directory. You can do a pipe via ssh: cat mykey.pub | ssh myuser@mysite.com -p 123 'cat >> .ssh/authorized_keys' Add the publickey name to the ~/.ssh/config file like this: Host bitbucket.org IdentityFile ~/.ssh/myprivatekeyfile # the leading spaces are important! Port 123 Verify and then SSH into the remote server. To check if your config is right type: ssh -T git@github.com ssh root@mysite.com or ssh mysite.com # if you setup the User setting in config Adding SSH Keys to servers SSH Client Config Edit setings on the new terminal to make Ubuntu as the default terminal. Also set the fontFace and https://www.the-digital-life.com/en/awesome-wsl-wsl2-terminal/","title":"Using the .ssh config files (~/.ssh/config)"},{"location":"developer/#switching-remote-urls-from-https-to-ssh","text":"List your existing remotes in order to get the name of the remote you want to change. $ git remote -v > origin https://github.com/USERNAME/REPOSITORY.git ( fetch ) > origin https://github.com/USERNAME/REPOSITORY.git ( push ) Change your remote's URL from HTTPS to SSH with the git remote set-url command. $ git remote set-url origin git@github.com:USERNAME/REPOSITORY.git git remote set-url origin git@github.com :leslieclif/notebook.git","title":"Switching remote URLs from HTTPS to SSH"},{"location":"developer/#inspirational-dotfile-repos","text":"https://www.freecodecamp.org/news/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/ https://github.com/victoriadrake/dotfiles/tree/ubuntu-19.10 https://github.com/georgijd/dotfiles https://github.com/jieverson/dotfiles-win/blob/master/install.sh","title":"Inspirational dotfile repos"},{"location":"developer/#bashrc-automation","text":"https://victoria.dev/blog/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/ https://victoria.dev/blog/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/","title":"Bashrc Automation"},{"location":"developer/#vagrant-setup","text":"https://www.techdrabble.com/ansible/36-install-ansible-molecule-vagrant-on-windows-wsl","title":"Vagrant setup"},{"location":"devops/","text":"Search for Autopilot and Technology to find for automation scripts or patterns","title":"Introduction"},{"location":"ide/","text":"IDE Tips and Tricks VS Code Intellij How to open multiple modules in the same workspace? Open the first module / project. Select File -> Project Structure -> Modules. Select Copy (icon) and open Browse your local directory and select the project you would like to add. Module name will resolve automatically. When you want to reopen to project with multiple sub-projects, in order to avoid re-doing steps as described above, just go to File -> Open Recent -> 'Your Big Project'. IDE Shortcuts Option Description Ctrl + Space Code Completion Ctrl + Shift + / Comments selected lines of code Ctrl + N Navigate to any class by typing the name in the editor Ctrl + B Jump to declaration","title":"Introduction"},{"location":"ide/#ide-tips-and-tricks","text":"","title":"IDE Tips and Tricks"},{"location":"ide/#vs-code","text":"","title":"VS Code"},{"location":"ide/#intellij","text":"How to open multiple modules in the same workspace? Open the first module / project. Select File -> Project Structure -> Modules. Select Copy (icon) and open Browse your local directory and select the project you would like to add. Module name will resolve automatically. When you want to reopen to project with multiple sub-projects, in order to avoid re-doing steps as described above, just go to File -> Open Recent -> 'Your Big Project'. IDE Shortcuts Option Description Ctrl + Space Code Completion Ctrl + Shift + / Comments selected lines of code Ctrl + N Navigate to any class by typing the name in the editor Ctrl + B Jump to declaration","title":"Intellij"},{"location":"ide/markdown/","text":"General Syntax MarkDown Cheat Sheet Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Horizontal Rule ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks Emphasis Bold **rendered as bold text** Italics _rendered as italicized text_ BlockQuote > Blockquotes Nested BlockQuote > First Level >> Second Level >>> Third Level List Unordered * or + or - Ordered 1. Code Block ` or [```html] (3 backticks and follwed by the language) Links Inline Links [Text](http://text.io) Link titles [Text](https://github.com/site/ \"Visit Site!\") Images ![Minion](http://octodex.github.com/images/minion.png)","title":"Markdown"},{"location":"ide/markdown/#general-syntax","text":"MarkDown Cheat Sheet Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Horizontal Rule ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks Emphasis Bold **rendered as bold text** Italics _rendered as italicized text_ BlockQuote > Blockquotes Nested BlockQuote > First Level >> Second Level >>> Third Level List Unordered * or + or - Ordered 1. Code Block ` or [```html] (3 backticks and follwed by the language) Links Inline Links [Text](http://text.io) Link titles [Text](https://github.com/site/ \"Visit Site!\") Images ![Minion](http://octodex.github.com/images/minion.png)","title":"General Syntax"},{"location":"k8s/","text":"Kubernetes","title":"Introduction"},{"location":"k8s/#kubernetes","text":"","title":"Kubernetes"},{"location":"k8s/install/","text":"Installing K8s Kubeadm Kops","title":"Installation"},{"location":"k8s/install/#installing-k8s","text":"","title":"Installing K8s"},{"location":"k8s/install/#kubeadm","text":"","title":"Kubeadm"},{"location":"k8s/install/#kops","text":"","title":"Kops"},{"location":"learning/ansible/","text":"Introduction Ansible 101 Ansible Cheat Sheet Default Configuration ansible.cfg and hosts files are present inside /etc/ansible Testing ansible on Ubuntu WSL ansible localhost -m ping Enabling SSH on the VM If you need SSH enabled on the system, follow the below steps: Ensure the /etc/apt/sources.list file has been updated as per above Run the command: apt-get update Run the command: apt-get install openssh-server Run the command: service sshd start ssh-keygen -t rsa -C \"ansible\" OR Generate an SSH key pair for future connections to the VM instances ( run the command exactly as it is ) : $ ssh-keygen -t rsa -b 4096 -f ~/.ssh/raddit-user -C raddit-user -P \"\" Add the SSH private key to the ssh-agent: $ ssh-add ~/.ssh/raddit-user Verify that the key was added to the ssh-agent: $ ssh-add -l Access VM over SSH ssh vagrant@127.0.0.1 -p 2222 -i ~/.ssh/insecure_private_key Copy files recursively from local desktop to remote server scp -r ./scripts vagrant@127.0.0.1:/home/vagrant -p 2222 -i ~/.ssh/insecure_private_key Target Docker containers for Ansible controller The Docker file used to create the ubuntu-ssh-enabled Docker image is located here. https://github.com/mmumshad/ubuntu-ssh-enabled Issues installing Ansible and its dependencies Once the Debian VM is up and running make the following changes to the /etc/apt/sources.list file to get the Ansible installation working right. deb http://security.debian.org/ jessie/updates main contrib deb-src http://security.debian.org/ jessie/updates main contrib deb http://ftp.debian.org/debian/ jessie-updates main contrib deb-src http://ftp.debian.org/debian/ jessie-updates main contrib deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main deb http://ftp.de.debian.org/debian sid main Directory Structure as per Best Practises This is the directory layout of this repository with explanation. production.ini # inventory file for production stage development.ini # inventory file for development stage test.ini # inventory file for test stage vpass # ansible-vault password file # This file should not be committed into the repository # therefore file is in ignored by git group_vars/ all/ # variables under this directory belongs all the groups apt.yml # ansible-apt role variable file for all groups webservers/ # here we assign variables to webservers groups apt.yml # Each file will correspond to a role i.e. apt.yml nginx.yml # \"\" postgresql/ # here we assign variables to postgresql groups postgresql.yml # Each file will correspond to a role i.e. postgresql postgresql-password.yml # Encrypted password file plays/ ansible.cfg # Ansible.cfg file that holds all ansible config webservers.yml # playbook for webserver tier postgresql.yml # playbook for postgresql tier roles/ roles_requirements.yml# All the information about the roles external/ # All the roles that are in git or ansible galaxy # Roles that are in roles_requirements.yml file will be downloaded into this directory internal/ # All the roles that are not public scripts/ setup/ # All the setup files for updating roles and ansible dependencies WebApp Installation Instructions for Centos 7 Install Python Pip and dependencies on Centos 7 sudo yum install -y epel-release python python-pip sudo pip install flask flask-mysql If you come across a certification validation error while running the above command, please use the below command. sudo pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org flask flask-mysql Install MySQL Server on Centos 7 wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm sudo yum update sudo yum -y install mysql-server sudo service mysql start The complete playbook to get the same workin on CentOS is here: https://github.com/kodekloudhub/simple_web_application Launching situational commands To check the inventory file ansible-inventory --list -y Test Connection ansible all -m ping -u root Check the disk usage of all servers ansible all -a \"df -h\" -u root Check the time of uptime each host in a group servers ansible servers -a \"uptime\" -u root Specify multiple hosts by separating their names with colons ansible server1:server2 -m ping -u root Dynamic Inventory You will need to set an environment variable with your API Personal Access Token in the provisioning machine export DO_API_TOKEN=YOUR_API_TOKEN_HERE - hosts : host01 --- become : true tasks : - name : ensure latest sysstat is installed apt : name : sysstat state : latest # ansible-playbook -i myhosts site.yml # ansible host01 -i myhosts -m copy -a \"src=test.txt dest=/tmp/\" # ansible host01 -i myhosts -m file -a \"dest=/tmp/test mode=644 state=directory\" # ansible host01 -i myhosts -m apt -a \"name=sudo state=latest\" # ansible host01 -i myhosts -m shell -a \"echo $TERM\" # ansible host01 -i myhosts -m command -a \"mkdir folder1\" --- - name : install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name : apache2 update_cache : yes state : latest --- - hosts : host01 tasks : - stat : path : /home/ubuntu/folder1/something.j2 register : st - name : Template to copy file template : src : ./something.j2 dest : /home/ubuntu/folder1/something.j2 mode : '0644' when : st.stat.exists == False # Looping - name : add several users user : name : \"{{ item }}\" state : present groups : \"developer\" with_items : - raj - david - john - lauren # show all the hosts in the inventory - debug : msg : \"{{ item }}\" with_items : - \"{{ groups['all'] }}\" # show all the hosts in the current play - debug : msg : \"{{ item }}\" with_items : - \"{{ play_hosts }}\" # This Playbook will add Java Packages to different systems (handling Ubuntu/Debian OS) - name : debian | ubuntu | add java ppa repo apt_repository : repo=ppa:webupd8team/java state=present become : yes when : ansible_distribution == 'Ubuntu' - name : debian | ensure the webupd8 launchpad apt repository is present apt_repository : repo=\"{{ item }} http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" update_cache=yes state=present with_items : - deb - deb-src become : yes when : ansible_distribution == 'Debian --- - name : install software hosts : host01 sudo : yes tasks : - name : Update and upgrade apt packages apt : upgrade : dist update_cache : yes cache_valid_time : 86400 - name : install software apt : name : \"{{item}}\" update_cache : yes state : installed with_items : - nginx - postgresql - postgresql-contrib - libpq-dev - python-psycopg2 - name : Ensure the Nginx service is running service : name : nginx state : started enabled : yes - name : Ensure the PostgreSQL service is running service : name : postgresql state : started enabled : yes #file: playbook.yml --- - hosts : all vars_files : - vars.yml tasks : - debug : msg=\"Variable 'var' is set to {{ var }}\" #file: vars.yml --- var : 20 #host variables - Variables are defined inline for individual host [ group1 ] host1 http_port=80 host2 http_port=303 #group variables - Variables are applied to entire group of hosts [ group1 : vars ] ntp_server= example.com proxy=proxy.example.com ### Whenever you run Playbook, Ansible by default collects information (facts) about each host ### like host IP address, CPU type, disk space, operating system information etc. ansible host01 -i myhosts -m setup # Consider you need the IP address of all the servers in you web group using 'group' variable {% for host in groups.web %} server {{ host.inventory_hostname }} {{ host.ansible_default_ipv4.address }}:8080 {% endfor %} # get a list of all the variables associated with the current host with the help of hostvars and inventory_hostname variables. --- - name : built-in variables hosts : all tasks : - debug : var=hostvars[inventory_hostname] # register variable stores the output, after executing command module, in contents variable # stdout is used to access string content of register variable --- - name : check registered variable for emptiness hosts : all tasks : - name : list contents of the directory in the host command : ls /home/ubuntu register : contents - name : check dir is empty debug : msg=\"Directory is empty\" when : contents.stdout == \"\" - name : check dir has contents debug : msg=\"Directory is not empty\" when : contents.stdout != \"\" # Variable Precedence => Command Line > Playbook > Facts > Roles # CLI: While running the playbook in Command Line redefine the variable ansible-playbook -i myhosts test.yml --extra-vars \"ansible_bios_version=Ansible\" # Tags are names pinned on individual tasks, roles or an entire play, that allows you to run or skip parts of your Playbook. # Tags can help you while testing certain parts of your Playbook. --- - name : Play1-install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name=apache2 update_cache=yes state=latest - name : displaying \"hello world\" debug : msg=\"hello world\" tags : - tag1 - name : Play2-install nginx hosts : all sudo : yes tags : - tag2 tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : debug module displays message in control machine debug : msg=\"have a good day\" tags : - mymessage - name : shell module displays message in host machine. shell : echo \"yet another task\" tags : - mymessage ansible-playbook -i myhosts tag.yml --list-tasks # displays the list of tasks in the Playbook ansible-playbook -i myhosts tag.yml --list-tags # displays only tags in your Playbook ansible-playbook -i myhosts tag.yml --tags \"tag1,mymessage\" # executes only certain tasks which are tagged as tag1 and mymessage # Ansible gives you the flexibility of organizing your tasks through include keyword, that introduces more abstraction and make your Playbook more easily maintainable, reusable and powerful. --- - name : testing includes hosts : all sudo : yes tasks : - include : apache.yml - include : content.yml - include : create_folder.yml - include : content.yml - include : nginx.yml # apache.yml will not have hosts & tasks but nginx.yml as a separate play will have tasks and can run independently #nginx.yml --- - name : installing nginx hosts : all sudo : yes tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : displaying message debug : msg=\"yayy!! nginx installed\" # A Role is completely self contained or encapsulated and completely reusable # cat ansible.cfg [ defaults ] host_key_checking=False inventory = /etc/ansible/myhosts log_path = /home/scrapbook/tutorial/output.txt roles_path = /home/scrapbook/tutorial/roles/ # master-playbook.yml --- - name : my first role in ansible hosts : all sudo : yes roles : - sample_role - sample_role2 # sample_role/tasks/main.yml --- - include : nginx.yml - include : copy-template.yml - include : copy-static.yml # sample_role/tasks/nginx.yml --- - name : Installs nginx apt : pkg=nginx state=installed update_cache=true notify : - start nginx # sample_role/handlers/main.yml --- - name : start nginx service : name=nginx state=started # sample_role/tasks/copy-template.yml --- - name : sample template - x template : src : template-file.j2 dest : /home/ubuntu/copy-template-file.j2 with_items : var_x # sample_role/vars/main.yml var_x : - 'variable x' var_y : - 'variable y' # sample_role/tasks/copy-static.yml # Ensure some-file.txt is present under files folder of the role --- - name : Copy a file copy : src=some-file.txt dest=/home/ubuntu/file1.txt # Let us run the master_playbook and check the output: ansible-playbook -i myhosts master_playbook.yml # ansible-galaxy is command line tool for scaffolding the creation of directory structure needed for organizing your code ansible-galaxy init sample_role # ansible-galaxy useful commands # Install a Role from Ansible Galaxy To use others role, visit https://galaxy.ansible.com/ and search for the role that will achieve your goal. Goal : Install Apache in the host machines. # Ensure that roles_path are defined in ansible.cfg for a role to successfully install. ansible-galaxy install geerlingguy.apache # Here, apache is role name and geerlingguy is name of the user in GitHub who created the role. # Forcefully Recreate Role ansible-galaxy init geerlingguy.apache --force # Listing Installed Roles ansible-galaxy list # Remove an Installed Role ansible-galaxy remove geerlingguy.apache # Environment Variables # Ansible recommends maintaining inventory file for each environment, instead of keeping all your hosts in a single inventory. # Each environment directory has one inventory file (hosts) and group_vars directory.","title":"Ansible"},{"location":"learning/ansible/#introduction","text":"Ansible 101 Ansible Cheat Sheet","title":"Introduction"},{"location":"learning/ansible/#default-configuration","text":"ansible.cfg and hosts files are present inside /etc/ansible Testing ansible on Ubuntu WSL ansible localhost -m ping","title":"Default Configuration"},{"location":"learning/ansible/#enabling-ssh-on-the-vm","text":"If you need SSH enabled on the system, follow the below steps: Ensure the /etc/apt/sources.list file has been updated as per above Run the command: apt-get update Run the command: apt-get install openssh-server Run the command: service sshd start ssh-keygen -t rsa -C \"ansible\" OR Generate an SSH key pair for future connections to the VM instances ( run the command exactly as it is ) : $ ssh-keygen -t rsa -b 4096 -f ~/.ssh/raddit-user -C raddit-user -P \"\" Add the SSH private key to the ssh-agent: $ ssh-add ~/.ssh/raddit-user Verify that the key was added to the ssh-agent: $ ssh-add -l","title":"Enabling SSH on the VM"},{"location":"learning/ansible/#access-vm-over-ssh","text":"ssh vagrant@127.0.0.1 -p 2222 -i ~/.ssh/insecure_private_key","title":"Access VM over SSH"},{"location":"learning/ansible/#copy-files-recursively-from-local-desktop-to-remote-server","text":"scp -r ./scripts vagrant@127.0.0.1:/home/vagrant -p 2222 -i ~/.ssh/insecure_private_key","title":"Copy files recursively from local desktop to remote server"},{"location":"learning/ansible/#target-docker-containers-for-ansible-controller","text":"The Docker file used to create the ubuntu-ssh-enabled Docker image is located here. https://github.com/mmumshad/ubuntu-ssh-enabled","title":"Target Docker containers for Ansible controller"},{"location":"learning/ansible/#issues-installing-ansible-and-its-dependencies","text":"Once the Debian VM is up and running make the following changes to the /etc/apt/sources.list file to get the Ansible installation working right. deb http://security.debian.org/ jessie/updates main contrib deb-src http://security.debian.org/ jessie/updates main contrib deb http://ftp.debian.org/debian/ jessie-updates main contrib deb-src http://ftp.debian.org/debian/ jessie-updates main contrib deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main deb http://ftp.de.debian.org/debian sid main","title":"Issues installing Ansible and its dependencies"},{"location":"learning/ansible/#directory-structure-as-per-best-practises","text":"This is the directory layout of this repository with explanation. production.ini # inventory file for production stage development.ini # inventory file for development stage test.ini # inventory file for test stage vpass # ansible-vault password file # This file should not be committed into the repository # therefore file is in ignored by git group_vars/ all/ # variables under this directory belongs all the groups apt.yml # ansible-apt role variable file for all groups webservers/ # here we assign variables to webservers groups apt.yml # Each file will correspond to a role i.e. apt.yml nginx.yml # \"\" postgresql/ # here we assign variables to postgresql groups postgresql.yml # Each file will correspond to a role i.e. postgresql postgresql-password.yml # Encrypted password file plays/ ansible.cfg # Ansible.cfg file that holds all ansible config webservers.yml # playbook for webserver tier postgresql.yml # playbook for postgresql tier roles/ roles_requirements.yml# All the information about the roles external/ # All the roles that are in git or ansible galaxy # Roles that are in roles_requirements.yml file will be downloaded into this directory internal/ # All the roles that are not public scripts/ setup/ # All the setup files for updating roles and ansible dependencies","title":"Directory Structure as per Best Practises"},{"location":"learning/ansible/#webapp-installation-instructions-for-centos-7","text":"","title":"WebApp  Installation Instructions for Centos 7"},{"location":"learning/ansible/#install-python-pip-and-dependencies-on-centos-7","text":"sudo yum install -y epel-release python python-pip sudo pip install flask flask-mysql If you come across a certification validation error while running the above command, please use the below command. sudo pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org flask flask-mysql","title":"Install Python Pip and dependencies on Centos 7"},{"location":"learning/ansible/#install-mysql-server-on-centos-7","text":"wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm sudo yum update sudo yum -y install mysql-server sudo service mysql start The complete playbook to get the same workin on CentOS is here: https://github.com/kodekloudhub/simple_web_application","title":"Install MySQL Server on Centos 7"},{"location":"learning/ansible/#launching-situational-commands","text":"To check the inventory file ansible-inventory --list -y Test Connection ansible all -m ping -u root Check the disk usage of all servers ansible all -a \"df -h\" -u root Check the time of uptime each host in a group servers ansible servers -a \"uptime\" -u root Specify multiple hosts by separating their names with colons ansible server1:server2 -m ping -u root","title":"Launching situational commands"},{"location":"learning/ansible/#dynamic-inventory","text":"You will need to set an environment variable with your API Personal Access Token in the provisioning machine export DO_API_TOKEN=YOUR_API_TOKEN_HERE - hosts : host01 --- become : true tasks : - name : ensure latest sysstat is installed apt : name : sysstat state : latest # ansible-playbook -i myhosts site.yml # ansible host01 -i myhosts -m copy -a \"src=test.txt dest=/tmp/\" # ansible host01 -i myhosts -m file -a \"dest=/tmp/test mode=644 state=directory\" # ansible host01 -i myhosts -m apt -a \"name=sudo state=latest\" # ansible host01 -i myhosts -m shell -a \"echo $TERM\" # ansible host01 -i myhosts -m command -a \"mkdir folder1\" --- - name : install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name : apache2 update_cache : yes state : latest --- - hosts : host01 tasks : - stat : path : /home/ubuntu/folder1/something.j2 register : st - name : Template to copy file template : src : ./something.j2 dest : /home/ubuntu/folder1/something.j2 mode : '0644' when : st.stat.exists == False # Looping - name : add several users user : name : \"{{ item }}\" state : present groups : \"developer\" with_items : - raj - david - john - lauren # show all the hosts in the inventory - debug : msg : \"{{ item }}\" with_items : - \"{{ groups['all'] }}\" # show all the hosts in the current play - debug : msg : \"{{ item }}\" with_items : - \"{{ play_hosts }}\" # This Playbook will add Java Packages to different systems (handling Ubuntu/Debian OS) - name : debian | ubuntu | add java ppa repo apt_repository : repo=ppa:webupd8team/java state=present become : yes when : ansible_distribution == 'Ubuntu' - name : debian | ensure the webupd8 launchpad apt repository is present apt_repository : repo=\"{{ item }} http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" update_cache=yes state=present with_items : - deb - deb-src become : yes when : ansible_distribution == 'Debian --- - name : install software hosts : host01 sudo : yes tasks : - name : Update and upgrade apt packages apt : upgrade : dist update_cache : yes cache_valid_time : 86400 - name : install software apt : name : \"{{item}}\" update_cache : yes state : installed with_items : - nginx - postgresql - postgresql-contrib - libpq-dev - python-psycopg2 - name : Ensure the Nginx service is running service : name : nginx state : started enabled : yes - name : Ensure the PostgreSQL service is running service : name : postgresql state : started enabled : yes #file: playbook.yml --- - hosts : all vars_files : - vars.yml tasks : - debug : msg=\"Variable 'var' is set to {{ var }}\" #file: vars.yml --- var : 20 #host variables - Variables are defined inline for individual host [ group1 ] host1 http_port=80 host2 http_port=303 #group variables - Variables are applied to entire group of hosts [ group1 : vars ] ntp_server= example.com proxy=proxy.example.com ### Whenever you run Playbook, Ansible by default collects information (facts) about each host ### like host IP address, CPU type, disk space, operating system information etc. ansible host01 -i myhosts -m setup # Consider you need the IP address of all the servers in you web group using 'group' variable {% for host in groups.web %} server {{ host.inventory_hostname }} {{ host.ansible_default_ipv4.address }}:8080 {% endfor %} # get a list of all the variables associated with the current host with the help of hostvars and inventory_hostname variables. --- - name : built-in variables hosts : all tasks : - debug : var=hostvars[inventory_hostname] # register variable stores the output, after executing command module, in contents variable # stdout is used to access string content of register variable --- - name : check registered variable for emptiness hosts : all tasks : - name : list contents of the directory in the host command : ls /home/ubuntu register : contents - name : check dir is empty debug : msg=\"Directory is empty\" when : contents.stdout == \"\" - name : check dir has contents debug : msg=\"Directory is not empty\" when : contents.stdout != \"\" # Variable Precedence => Command Line > Playbook > Facts > Roles # CLI: While running the playbook in Command Line redefine the variable ansible-playbook -i myhosts test.yml --extra-vars \"ansible_bios_version=Ansible\" # Tags are names pinned on individual tasks, roles or an entire play, that allows you to run or skip parts of your Playbook. # Tags can help you while testing certain parts of your Playbook. --- - name : Play1-install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name=apache2 update_cache=yes state=latest - name : displaying \"hello world\" debug : msg=\"hello world\" tags : - tag1 - name : Play2-install nginx hosts : all sudo : yes tags : - tag2 tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : debug module displays message in control machine debug : msg=\"have a good day\" tags : - mymessage - name : shell module displays message in host machine. shell : echo \"yet another task\" tags : - mymessage ansible-playbook -i myhosts tag.yml --list-tasks # displays the list of tasks in the Playbook ansible-playbook -i myhosts tag.yml --list-tags # displays only tags in your Playbook ansible-playbook -i myhosts tag.yml --tags \"tag1,mymessage\" # executes only certain tasks which are tagged as tag1 and mymessage # Ansible gives you the flexibility of organizing your tasks through include keyword, that introduces more abstraction and make your Playbook more easily maintainable, reusable and powerful. --- - name : testing includes hosts : all sudo : yes tasks : - include : apache.yml - include : content.yml - include : create_folder.yml - include : content.yml - include : nginx.yml # apache.yml will not have hosts & tasks but nginx.yml as a separate play will have tasks and can run independently #nginx.yml --- - name : installing nginx hosts : all sudo : yes tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : displaying message debug : msg=\"yayy!! nginx installed\" # A Role is completely self contained or encapsulated and completely reusable # cat ansible.cfg [ defaults ] host_key_checking=False inventory = /etc/ansible/myhosts log_path = /home/scrapbook/tutorial/output.txt roles_path = /home/scrapbook/tutorial/roles/ # master-playbook.yml --- - name : my first role in ansible hosts : all sudo : yes roles : - sample_role - sample_role2 # sample_role/tasks/main.yml --- - include : nginx.yml - include : copy-template.yml - include : copy-static.yml # sample_role/tasks/nginx.yml --- - name : Installs nginx apt : pkg=nginx state=installed update_cache=true notify : - start nginx # sample_role/handlers/main.yml --- - name : start nginx service : name=nginx state=started # sample_role/tasks/copy-template.yml --- - name : sample template - x template : src : template-file.j2 dest : /home/ubuntu/copy-template-file.j2 with_items : var_x # sample_role/vars/main.yml var_x : - 'variable x' var_y : - 'variable y' # sample_role/tasks/copy-static.yml # Ensure some-file.txt is present under files folder of the role --- - name : Copy a file copy : src=some-file.txt dest=/home/ubuntu/file1.txt # Let us run the master_playbook and check the output: ansible-playbook -i myhosts master_playbook.yml # ansible-galaxy is command line tool for scaffolding the creation of directory structure needed for organizing your code ansible-galaxy init sample_role # ansible-galaxy useful commands # Install a Role from Ansible Galaxy To use others role, visit https://galaxy.ansible.com/ and search for the role that will achieve your goal. Goal : Install Apache in the host machines. # Ensure that roles_path are defined in ansible.cfg for a role to successfully install. ansible-galaxy install geerlingguy.apache # Here, apache is role name and geerlingguy is name of the user in GitHub who created the role. # Forcefully Recreate Role ansible-galaxy init geerlingguy.apache --force # Listing Installed Roles ansible-galaxy list # Remove an Installed Role ansible-galaxy remove geerlingguy.apache # Environment Variables # Ansible recommends maintaining inventory file for each environment, instead of keeping all your hosts in a single inventory. # Each environment directory has one inventory file (hosts) and group_vars directory.","title":"Dynamic Inventory"},{"location":"learning/docker/","text":"Running Docker and passing shell commands docker run -e TERM -e COLORTERM -it --rm alpine sh -uec ' apk update apk add git zsh nano vim git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ~/powerlevel10k echo \"source ~/powerlevel10k/powerlevel10k.zsh-theme\" >>~/.zshrc cd ~/powerlevel10k exec zsh'","title":"Running Docker and passing shell commands"},{"location":"learning/docker/#running-docker-and-passing-shell-commands","text":"docker run -e TERM -e COLORTERM -it --rm alpine sh -uec ' apk update apk add git zsh nano vim git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ~/powerlevel10k echo \"source ~/powerlevel10k/powerlevel10k.zsh-theme\" >>~/.zshrc cd ~/powerlevel10k exec zsh'","title":"Running Docker and passing shell commands"},{"location":"learning/git/","text":"Create a new repository on the command line git init git add README.md git commit -m \"first commit\" git branch -M master git remote add origin https://github.com/leslieclif/dotfiles.git git push -u origin master Git Commands cd into git folder \u2192 ls -la \u2192 cd .git \u2192 ls -la Git help git help To quit help \u2192 q Best practise: Always do a pull before a push to merge changes from remote git pull origin master To git add and git commit for tracked files in a single comand use -a git commit -am \"Commit message\" Amend Commit message git commit --amend \"New commit message\" Check for tracked files in git git ls-files Back out changes that have been commited, but not pushed to remote. Once unstaged, you can remove the changes using checkout git reset HEAD git checkout -- Rename file-name. It also automatically stages the changes, so need to do git add git mv level3--file.txt level3.txt If file is renamed outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A git add -A Moving files and staging the changes git mv level2.txt new-folder If file is moved outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A mv level2.txt .. git add -A file renamed in OS. But say, git has identifed unwanted files during git status and you dont want to add those files, then don't use -A, use -u Individually add the new renamed file first then update git git add level1.txt git add -u Delete files tracked by git git rm doomed.txt If file is delete outside git, it will delete and is not staged. To add and stage the deleted file use -A git add -A Git History git log To quit help \u2192 q Git history in one line git log --oneline --graph --decorate Git history using duration git log --since=\"3 days ago\" Show all user actions git reflog Show commit history \u2192 do git log get commit id git show TODO: Get a git diff tool Show git config git config --global --list Compare with staging and current changes git diff Compare between current changes and remote last commit git diff HEAD Compare between staging and remote last commit git diff --staged HEAD Compare file changes with staging and current changes git diff -- Compare between commits (do git log to get commits) git diff Compare local and remote branches git diff master origin/master Compare local branches git diff master test-branch Branching List local and remote branches git branch -a Create new branch git branch Rename local branch git branch -m Delete a branch. Note: You have to be on another bracnh before you can delete the target branch git branch -d Create new branch and switch to it in single command git checkout -b Fash forward Merges \u2192 First switch to the target branches, do a git diff to review the changes. git merge Disable fast forward merge \u2192 Give tracing of merge by giving a custom merge message and also the commit history of the branch git merge --no-ff Automatic merge git merge -m \" \" Merge Conflict and Resolution Inside the merging workspace incase of conflict, open the conflicting file in editor or the merge-diff tool. Resolve conflict and close the file. Rebase feature branch from master git checkout feature-branch git rebase master Abort rebase git rebase --abort Rebase conflict resolution \u2192 Use merging tool to fix conflict, save and quit. Add file to git staging, then continue rebase git rebase --continue Pull with Rebase (Rebase local master with remote master) git fetch origin master (non destructive merge which only updates references) git pull --rebase origin master Stash git stash Stash + saving untracked files of git as well git stash -u Get the stash back to local git stash apply List the stash git stash list Drop the stash git stash drop Combination of apply and drop in one command. Brings the last saved state git stash pop Multiple Stashes git stash save \" \" Show any arbitary stash changes whithout popping. Do stash list first to get the stash ID git stash show stash@{1} Apply any arbitary stash changes. Do stash list first to get the stash ID git stash apply stash@{1} Drop any arbitary stash changes that was applied or not needed. git stash drop stash@{1} Stashing changes into a new branch. First see if you have any untracked files that also needs to be saved git stash -u git stash branch newbranchName Tagging Create Lightweight tag git tag mytag List existing tags git tag --list Delete tag git tag --delete mytag Create Annotated tags (It has additional information like release notes) git tag -a v1.0.0 -m \"Release 1.0.0\" Comparing tags git diff v1.0.0 v1.0.1 Tagging a specific commit ID git tag -a v0.0.9 -m \"Release 0.0.9\" Updating an existing tag with new commit id git tag -a v0.0.9 -f -m \"Correct Release 0.0.9\" Pushing tags to remote git push origin v1.0.0 Pushing all local tags to remote git push origin master --tags Deleting tags in remote (puting :before tag name will delete it from remote). Does not delete tag from local git push origin :v0.0.9 Reset HEAD position git reset Using Stash and Branch combination First stash the changes (WIP) in one brabch, then checkout a new test branch and then pop the changes into this test branch git stash git checkout -b test git stash pop Cherry Pick (Hot Fix scenario) git cherry-pick","title":"Git"},{"location":"learning/git/#create-a-new-repository-on-the-command-line","text":"git init git add README.md git commit -m \"first commit\" git branch -M master git remote add origin https://github.com/leslieclif/dotfiles.git git push -u origin master","title":"Create a new repository on the command line"},{"location":"learning/git/#git-commands","text":"cd into git folder \u2192 ls -la \u2192 cd .git \u2192 ls -la","title":"Git Commands"},{"location":"learning/git/#git-help","text":"git help","title":"Git help"},{"location":"learning/git/#to-quit-help-q","text":"","title":"To quit help --&gt; q"},{"location":"learning/git/#best-practise-always-do-a-pull-before-a-push-to-merge-changes-from-remote","text":"git pull origin master","title":"Best practise: Always do a pull before a push to merge changes from remote"},{"location":"learning/git/#to-git-add-and-git-commit-for-tracked-files-in-a-single-comand-use-a","text":"git commit -am \"Commit message\"","title":"To git add and git commit for tracked files in a single comand use -a"},{"location":"learning/git/#amend-commit-message","text":"git commit --amend \"New commit message\"","title":"Amend Commit message"},{"location":"learning/git/#check-for-tracked-files-in-git","text":"git ls-files","title":"Check for tracked files in git"},{"location":"learning/git/#back-out-changes-that-have-been-commited-but-not-pushed-to-remote-once-unstaged-you-can-remove-the-changes-using-checkout","text":"git reset HEAD git checkout --","title":"Back out changes that have been commited, but not pushed to remote. Once unstaged, you can remove the changes using checkout"},{"location":"learning/git/#rename-file-name-it-also-automatically-stages-the-changes-so-need-to-do-git-add","text":"git mv level3--file.txt level3.txt","title":"Rename file-name. It also automatically stages the changes, so need to do git add"},{"location":"learning/git/#if-file-is-renamed-outside-git-it-will-delete-and-add-the-files-in-git-history-and-is-not-staged-to-add-new-file-and-stage-the-renamed-files-use-a","text":"git add -A","title":"If file is renamed outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A"},{"location":"learning/git/#moving-files-and-staging-the-changes","text":"git mv level2.txt new-folder","title":"Moving files and staging the changes"},{"location":"learning/git/#if-file-is-moved-outside-git-it-will-delete-and-add-the-files-in-git-history-and-is-not-staged-to-add-new-file-and-stage-the-renamed-files-use-a","text":"mv level2.txt .. git add -A","title":"If file is moved outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A"},{"location":"learning/git/#file-renamed-in-os-but-say-git-has-identifed-unwanted-files-during-git-status-and-you-dont-want-to-add-those-files-then-dont-use-a-use-u","text":"","title":"file renamed in OS. But say, git has identifed unwanted files during git status and you dont want to add those files, then don't use -A, use -u"},{"location":"learning/git/#individually-add-the-new-renamed-file-first-then-update-git","text":"git add level1.txt git add -u","title":"Individually add the new renamed file first then update git"},{"location":"learning/git/#delete-files-tracked-by-git","text":"git rm doomed.txt","title":"Delete files tracked by git"},{"location":"learning/git/#if-file-is-delete-outside-git-it-will-delete-and-is-not-staged-to-add-and-stage-the-deleted-file-use-a","text":"git add -A","title":"If file is delete outside git, it will delete and is not staged. To add and stage the deleted file  use -A"},{"location":"learning/git/#git-history","text":"git log","title":"Git History"},{"location":"learning/git/#to-quit-help-q_1","text":"","title":"To quit help --&gt; q"},{"location":"learning/git/#git-history-in-one-line","text":"git log --oneline --graph --decorate","title":"Git history in one line"},{"location":"learning/git/#git-history-using-duration","text":"git log --since=\"3 days ago\"","title":"Git history using duration"},{"location":"learning/git/#show-all-user-actions","text":"git reflog","title":"Show all user actions"},{"location":"learning/git/#show-commit-history-do-git-log-get-commit-id","text":"git show","title":"Show commit history --&gt; do git log get commit id"},{"location":"learning/git/#todo-get-a-git-diff-tool","text":"","title":"TODO: Get a git diff tool"},{"location":"learning/git/#show-git-config","text":"git config --global --list","title":"Show git config"},{"location":"learning/git/#compare-with-staging-and-current-changes","text":"git diff","title":"Compare with staging and current changes"},{"location":"learning/git/#compare-between-current-changes-and-remote-last-commit","text":"git diff HEAD","title":"Compare between current changes and remote last commit"},{"location":"learning/git/#compare-between-staging-and-remote-last-commit","text":"git diff --staged HEAD","title":"Compare between staging and remote last commit"},{"location":"learning/git/#compare-file-changes-with-staging-and-current-changes","text":"git diff --","title":"Compare file changes with staging and current changes"},{"location":"learning/git/#compare-between-commits-do-git-log-to-get-commits","text":"git diff","title":"Compare between commits (do git log to get commits)"},{"location":"learning/git/#compare-local-and-remote-branches","text":"git diff master origin/master","title":"Compare local and remote branches"},{"location":"learning/git/#compare-local-branches","text":"git diff master test-branch","title":"Compare local branches"},{"location":"learning/git/#branching","text":"","title":"Branching"},{"location":"learning/git/#list-local-and-remote-branches","text":"git branch -a","title":"List local and remote branches"},{"location":"learning/git/#create-new-branch","text":"git branch","title":"Create new branch"},{"location":"learning/git/#rename-local-branch","text":"git branch -m","title":"Rename local branch"},{"location":"learning/git/#delete-a-branch-note-you-have-to-be-on-another-bracnh-before-you-can-delete-the-target-branch","text":"git branch -d","title":"Delete a branch. Note: You have to be on another bracnh before you can delete the target branch"},{"location":"learning/git/#create-new-branch-and-switch-to-it-in-single-command","text":"git checkout -b","title":"Create new branch and switch to it in single command"},{"location":"learning/git/#fash-forward-merges-first-switch-to-the-target-branches-do-a-git-diff-to-review-the-changes","text":"git merge","title":"Fash forward Merges  --&gt; First switch to the target branches, do a git diff to review the changes."},{"location":"learning/git/#disable-fast-forward-merge-give-tracing-of-merge-by-giving-a-custom-merge-message-and-also-the-commit-history-of-the-branch","text":"git merge --no-ff","title":"Disable fast forward merge --&gt; Give tracing of merge by giving a custom merge message and also the commit history of the branch"},{"location":"learning/git/#automatic-merge","text":"git merge -m \" \"","title":"Automatic merge"},{"location":"learning/git/#merge-conflict-and-resolution","text":"","title":"Merge Conflict and Resolution"},{"location":"learning/git/#inside-the-merging-workspace-incase-of-conflict-open-the-conflicting-file-in-editor-or-the-merge-diff-tool-resolve-conflict-and-close-the-file","text":"","title":"Inside the merging workspace incase of conflict, open the conflicting file in editor or the merge-diff tool. Resolve conflict and close the file."},{"location":"learning/git/#rebase-feature-branch-from-master","text":"git checkout feature-branch git rebase master","title":"Rebase feature branch from master"},{"location":"learning/git/#abort-rebase","text":"git rebase --abort","title":"Abort rebase"},{"location":"learning/git/#rebase-conflict-resolution-use-merging-tool-to-fix-conflict-save-and-quit-add-file-to-git-staging-then-continue-rebase","text":"git rebase --continue","title":"Rebase conflict resolution --&gt; Use merging tool to fix conflict, save and quit. Add file to git staging, then continue rebase"},{"location":"learning/git/#pull-with-rebase-rebase-local-master-with-remote-master","text":"git fetch origin master (non destructive merge which only updates references) git pull --rebase origin master","title":"Pull with Rebase (Rebase local master with remote master)"},{"location":"learning/git/#stash","text":"git stash","title":"Stash"},{"location":"learning/git/#stash-saving-untracked-files-of-git-as-well","text":"git stash -u","title":"Stash + saving untracked files of git as well"},{"location":"learning/git/#get-the-stash-back-to-local","text":"git stash apply","title":"Get the stash back to local"},{"location":"learning/git/#list-the-stash","text":"git stash list","title":"List the stash"},{"location":"learning/git/#drop-the-stash","text":"git stash drop","title":"Drop the stash"},{"location":"learning/git/#combination-of-apply-and-drop-in-one-command-brings-the-last-saved-state","text":"git stash pop","title":"Combination of apply and drop in one command. Brings the last saved state"},{"location":"learning/git/#multiple-stashes","text":"git stash save \" \"","title":"Multiple Stashes"},{"location":"learning/git/#show-any-arbitary-stash-changes-whithout-popping-do-stash-list-first-to-get-the-stash-id","text":"git stash show stash@{1}","title":"Show any arbitary stash changes whithout popping. Do stash list first to get the stash ID"},{"location":"learning/git/#apply-any-arbitary-stash-changes-do-stash-list-first-to-get-the-stash-id","text":"git stash apply stash@{1}","title":"Apply any arbitary stash changes. Do stash list first to get the stash ID"},{"location":"learning/git/#drop-any-arbitary-stash-changes-that-was-applied-or-not-needed","text":"git stash drop stash@{1}","title":"Drop any arbitary stash changes that was applied or not needed."},{"location":"learning/git/#stashing-changes-into-a-new-branch-first-see-if-you-have-any-untracked-files-that-also-needs-to-be-saved","text":"git stash -u git stash branch newbranchName","title":"Stashing changes into a new branch. First see if you have any untracked files that also needs to be saved"},{"location":"learning/git/#tagging","text":"","title":"Tagging"},{"location":"learning/git/#create-lightweight-tag","text":"git tag mytag","title":"Create Lightweight tag"},{"location":"learning/git/#list-existing-tags","text":"git tag --list","title":"List existing tags"},{"location":"learning/git/#delete-tag","text":"git tag --delete mytag","title":"Delete tag"},{"location":"learning/git/#create-annotated-tags-it-has-additional-information-like-release-notes","text":"git tag -a v1.0.0 -m \"Release 1.0.0\"","title":"Create Annotated tags (It has additional information like release notes)"},{"location":"learning/git/#comparing-tags","text":"git diff v1.0.0 v1.0.1","title":"Comparing tags"},{"location":"learning/git/#tagging-a-specific-commit-id","text":"git tag -a v0.0.9 -m \"Release 0.0.9\"","title":"Tagging a specific commit ID"},{"location":"learning/git/#updating-an-existing-tag-with-new-commit-id","text":"git tag -a v0.0.9 -f -m \"Correct Release 0.0.9\"","title":"Updating an existing tag with new commit id"},{"location":"learning/git/#pushing-tags-to-remote","text":"git push origin v1.0.0","title":"Pushing tags to remote"},{"location":"learning/git/#pushing-all-local-tags-to-remote","text":"git push origin master --tags","title":"Pushing all local tags to remote"},{"location":"learning/git/#deleting-tags-in-remote-puting-before-tag-name-will-delete-it-from-remote-does-not-delete-tag-from-local","text":"git push origin :v0.0.9","title":"Deleting tags in remote (puting :before tag name will delete it from remote). Does not delete tag from local"},{"location":"learning/git/#reset-head-position","text":"git reset","title":"Reset HEAD position"},{"location":"learning/git/#using-stash-and-branch-combination","text":"","title":"Using Stash and Branch combination"},{"location":"learning/git/#first-stash-the-changes-wip-in-one-brabch-then-checkout-a-new-test-branch-and-then-pop-the-changes-into-this-test-branch","text":"git stash git checkout -b test git stash pop","title":"First stash the changes (WIP) in one brabch, then checkout a new test branch and then pop the changes into this test branch"},{"location":"learning/git/#cherry-pick-hot-fix-scenario","text":"git cherry-pick","title":"Cherry Pick (Hot Fix scenario)"},{"location":"learning/python/","text":"# Range and For for index in range ( 6 ): print ( index ) # Range function is used generate a sequence of integers index = range ( 10 , - 1 , - 1 ) # start, stop and step, stops at 0 not including -1 # set class provides a mapping of unique immutable elements # One use of set is to remove duplicate elements dup_list = ( 'c' , 'd' , 'c' , 'e' ) beta = set ( dup_list ) uniq_list = list ( beta ) # dict class is an associative array of keys and values. keys must be unique immutable objects dict_syn = { 'k1' : 'v1' , 'k2' : 'v2' } dict_syn = dict ( k1 = 'v1' , k2 = 'v2' ) dict_syn [ 'k3' ] = 'v3' # adding new key value del ( dict_syn [ 'k3' ]) # delete key value print ( dict_syn . keys ()) # prints all keys print ( dict_syn . values ()) # prints all values # User Input name = input ( 'Name :' ) # Functions * A function is a piece of code , capable of performing a similar task repeatedly . * It is defined using ** def ** keyword in python . def < function_name > ( < parameter1 > , < parameter2 > , ... ): 'Function documentation' function_body return < value > * Parameters , return expression and documentation string are optional . def square ( n ): \"Returns Square of a given number\" return n ** 2 print ( square . __doc__ ) // prints the function documentation string * 4 types of arguments * Required Arguments : non - keyword arguments def showname ( name , age ) showname ( \"Jack\" , 40 ) // name = \"Jack\" , age = 40 showname ( 40 , \"Jack\" ) // name = 40 , age = \"Jack\" * Keyword Arguments : identified by paramater names def showname ( name , age ) showname ( age = 40 , name = \"Jack\" ) * Default Arguments : Assumes a default argument , if an arg is not passsed . def showname ( name , age = 50 ) showname ( \"Jack\" ) // name = \"Jack\" , age = 50 showname ( age = 40 , \"Jack\" ) // name = \"Jack\" , age = 40 showname ( name = \"Jack\" , age = 40 ) // name = \"Jack\" , age = 40 showname ( name = \"Jack\" , 40 ) // Python does not allow passing non - keyword after keyword arg . This will fail . * Variable Length Arguments : Function preocessed with more arguments than specified while defining the function def showname ( name , * vartuple , ** vardict ) # *vartuple = Variable non keyword argument which will be a tuple. Denoted by * # **vardict = Variable keyword argument which will be a dictionary. Denoted by ** showname ( \"Jack\" ) // name = \"Jack\" showname ( \"Jack\" , 35 , 'M' , 'Kansas' ) // name = \"Jack\" , * vartuple = ( 35 , 'M' , 'Kansas' ) showname ( \"Jack\" , 35 , city = 'Kansas' , sex = 'M' ) // name = \"Jack\" , * vartuple = ( 35 ), ** vardict = { city = 'Kansas' , sex = 'M' } # An Iterator is an object, which allows a programmer to traverse through all the elements of a collection, regardless of its specific implementation. x = [ 6 , 3 , 1 ] s = iter ( x ) print ( next ( s )) # -> 6 # List Comprehensions -> Alternative to for loops. * More concise , readable , efficient and mimic functional programming style . * Used to : Apply a method to all or specific elements of a list , and Filter elements of a list satisfying specific criteria . x = [ 6 , 3 , 1 ] y = [ i ** 2 for i in x ] # List Comprehension expression print ( y ) # -> [36, 9, 1] * Filter positive numbers ( using for and if ) vec = [ - 4 , - 2 , 0 , 2 , 4 ] pos_elm = [ x for x in vec if x >= 0 ] # Can be read as for every elem x in vec, filter x if x is greater than or equal to 0 print ( pos_elm ) # -> [0, 2, 4] * Applying a method to a list def add10 ( x ): return x + 10 n = [ 34 , 56 , 75 , 3 ] mod_n = [ add10 ( num ) for num in n ] print ( mod_n ) # A Generator is a function that produces a sequence of results instead of a single value def arithmatic_series ( a , r ): while a < 50 : yield a # yield is used in place of return which suspends processing a += r s = arithmatic_series ( 3 , 10 ) # Execution of further 'arithmetic series' can be resumed only by calling nextfunction again on generator 's' print ( s ) // Generator #output=3 print ( next ( s )) // Generator starts execution # output=13 print ( next ( s )) // resumed # output=23 # A Generator expresions are generator versions of list comprehensions. They return a generator instead of a list. x = [ 6 , 3 , 1 ] g = ( i ** 2 for i in x ) # generator expression print ( next ( g )) # -> 36 # Dictionary Comprehensions -> takes the form {key: value for (key, value) in iterable} myDict = { x : x ** 2 for x in [ 1 , 2 , 3 , 4 , 5 ]} print ( myDict ) # Output {1: 1, 2: 4, 3: 9, 4: 16, 5: 25} # Calculate the frequency of each identified unique word in the list words = [ 'Hello' , 'Hi' , 'Hello' ] freq = { w : words . count ( w ) for w in words } print ( freq ) # Output {'Hello': 2, 'Hi': 1} Create the dictionary frequent_words , which filter words having frequency greater than one words = [ 'Hello' , 'Hi' , 'Hello' ] freq = { w : words . count ( w ) for w in words if words . count ( w ) > 1 } print ( freq ) # Output {'Hello': 2} # Defining Classes * Syntax class < ClassName > ( < parent1 > , ... ): class_body # Creating Objects * An object is created by calling the class name followed by a pair of parenthesis . class Person : pass p1 = Person () # Creating the object 'p1' print ( p1 ) # -> '<__main__.Person object at 0x0A...>' # tells you what class it belongs to and hints on memory address it is referenced to. # initializer method -> __init__ * defined inside the class and called by default , during an object creation . * It also takes self as the first argument , which refers to the current object . class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname p1 = Person ( 'George' , 'Smith' ) print ( p1 . fname , '-' , p1 . lname ) # -> 'George - Smith' # Documenting a Class * Each class or a method definition can have an optional first line , known as docstring . class Person : 'Represents a person.' # Inheritance * Inheritance describes is a kind of relationship between two or more classes , abstracting common details into super class and storing specific ones in the subclass . * To create a child class , specify the parent class name inside the pair of parenthesis , followed by it 's name. class Child ( Parent ): pass * Every child class inherits all the behaviours exhibited by their parent class . * In Python , every class uses inheritance and is inherited from ** object ** by default . class MySubClass ( object ): # object is known as parent or super class. pass # Inheritance in Action class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname class Employee ( Person ): all_employees = [] def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) # Employee class utilizes __init __ method of the parent class Person to create its object. self . empid = empid Employee . all_employees . append ( self ) e1 = Employee ( 'Jack' , 'simmons' , 456342 ) print ( e1 . fname , '-' , e1 . empid ) # Output -> Jack - 456342 # Polymorphism * Polymorphism allows a subclass to override or change a specific behavior , exhibited by the parent class class Employee ( Person ): all_employees = EmployeesList () def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) self . empid = empid Employee . all_employees . append ( self ) def getSalary ( self ): return 'You get Monthly salary.' def getBonus ( self ): return 'You are eligible for Bonus.' * Definition of ContractEmployee class derived from Employee. It overrides functionality of getSalary and getBonus methods found in it 's parent class Employee. class ContractEmployee ( Employee ): def getSalary ( self ): return 'You will not get Salary from Organization.' def getBonus ( self ): return 'You are not eligible for Bonus.' e1 = Employee ( 'Jack' , 'simmons' , 456342 ) e2 = ContractEmployee ( 'John' , 'williams' , 123656 ) print ( e1 . getBonus ()) # Output - You are eligible for Bonus. print ( e2 . getBonus ()) # Output - You are not eligible for Bonus. # Abstraction * Abstraction means working with something you know how to use without knowing how it works internally . * It is hiding the defaults and sharing only necessary information . # Encapsulation * Encapsulation allows binding data and associated methods together in a unit i . e class . * Bringing related data and methods inside a class to avoid misuse outside . * These principles together allows a programmer to define an interface for applications , i . e . to define all tasks the program is capable to execute and their respective input and output data . * A good example is a television set . We don \u2019 t need to know the inner workings of a TV , in order to use it . All we need to know is how to use the remote control ( i . e the interface for the user to interact with the TV ) . # Abstracting Data * Direct access to data can be restricted by making required attributes or methods private , ** just by prefixing it 's name with one or two underscores.** * An attribute or a method starting with : + ** no underscores ** is a ** public ** one . + ** a single underscore ** is ** private ** , however , still accessible from outside. + ** double underscores ** is ** strongly private ** and not accessible from outside. # Abstraction and Encapsulation Example * ** empid ** attribute of Employee class is made private and is accessible outside the class only using the method ** getEmpid **. class Employee ( Person ): all_employees = EmployeesList () def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) self . __empid = empid Employee . all_employees . append ( self ) def getEmpid ( self ): return self . __empid e1 = Employee ( 'Jack' , 'simmons' , 456342 ) print ( e1 . fname , e1 . lname ) # Output -> Jack simmons print ( e1 . getEmpid ()) # Output -> 456342 print ( e1 . __empid ) # Output -> AttributeError: Employee instance has no attribute '__empid' # Exceptions * Python allows a programmer to handle such exceptions using ** try ... except ** clauses , thus avoiding the program to crash . * Some of the python expressions , though written correctly in syntax , result in error during execution . ** Such scenarios have to be handled .** * In Python , every error message has two parts . The first part tells what type of exception it is and second part explains the details of error . # Handling Exception * A try block is followed by one or more except clauses . * The code to be handled is written inside try clause and the code to be executed when an exception occurs is written inside except clause . try : a = pow ( 2 , 4 ) print ( \"Value of 'a' :\" , a ) b = pow ( 2 , 'hello' ) # results in exception print ( \"Value of 'b' :\" , b ) except TypeError as e : print ( 'oops!!!' ) print ( 'Out of try ... except.' ) Output -> Value of 'a' : 16 --> oops !!! --> Out of try ... except . # Raising Exceptions * ** raise ** keyword is used when a programmer wants a specific exception to occur . try : a = 2 ; b = 'hello' if not ( isinstance ( a , int ) and isinstance ( b , int )): raise TypeError ( 'Two inputs must be integers.' ) c = a ** b except TypeError as e : print ( e ) # User Defined Exception Functions * Python also allows a programmer to create custom exceptions , derived from base Exception class . class CustomError ( Exception ): def __init__ ( self , value ): self . value = value def __str__ ( self ): return str ( self . value ) try : a = 2 ; b = 'hello' if not ( isinstance ( a , int ) and isinstance ( b , int )): raise CustomError ( 'Two inputs must be integers.' ) # CustomError is raised in above example, instead of TypeError. c = a ** b except CustomError as e : print ( e ) # Using 'finally' clause * ** finally ** clause is an optional one that can be used with try ... except clauses . * All the statements under finally clause are executed irrespective of exception occurrence . def divide ( a , b ): try : result = a / b return result except ZeroDivisionError : print ( \"Dividing by Zero.\" ) finally : print ( \"In finally clause.\" ) # Statements inside finally clause are ALWAYS executed before the return back # Using 'else' clause * ** else ** clause is also an optional clause with try ... except clauses . * Statements under else clause are executed ** only when no exception occurs in try clause **. try : a = 14 / 7 except ZeroDivisionError : print ( 'oops!!!' ) else : print ( 'First ELSE' ) try : a = 14 / 0 except ZeroDivisionError : print ( 'oops!!!' ) else : print ( 'Second ELSE' ) Output : First ELSE --> oops !!! # Module * Any file containing logically organized Python code can be used as a module . * A module generally contains ** any of the defined functions , classes and variables **. A module can also include executable code . * Any Python source file can be used as a module by using an import statement in some other Python source file . # Packages * A package is a collection of modules present in a folder . * The name of the package is the name of the folder itself . * A package generally contains an empty file named ** __init__ . py ** in the same folder , which is required to treat the folder as a package . # Import Modules import math # Recommended method of importing a module import math as m from math import pi , tan from math import pi as pie , tan as tangent # Working with Files * Data from an opened file can be read using any of the methods : ** read , readline and readlines **. * Data can be written to a file using either ** write ** or ** writelines ** method . * A file ** must be opened ** , before it is used for reading or writing . fp = open ( 'temp.txt' , 'r' ) # opening ( operations 'r' & 'w') content = fp . read () # reading fp . close () # closing # read() -> Reads the entire contents of a file as bytes. # readline() -> Reads a single line at a time. # readlines() -> Reads a all the line & each line is stored as an element of a list. # write() -> Writes a single string to output file. # writelines() -> Writes multiple lines to output file & each string is stored as an element of a list. * Reading contents of file and storing as a dictionary fp = open ( 'emp_data.txt' , 'r' ) emps = fp . readlines () # Preprocessing data emps = [ emp . strip ( ' \\n ' ) for emp in emps ] emps = [ emp . split ( ';' ) for emp in emps ] header = emps . pop # remove header record separately emps = [ dict ( zip ( header , emp ) for emp in emps ] # header record is used to combine with data to form a dictionary print ( emps [: 2 ]) # prints first 2 records * Filtering data based on criteria fil_emps = [ emp [ 'Emp_name' ] for emp in emps if emp [ 'Emp_work_location' ] == 'HYD' ] * Filtering data based on pattern import re pattern = re . compile ( r 'oracle' , re . IGNORECASE ) # Regular Expression oracle_emps = [ emp [ 'Emp_name' ] for emp in emps if pattern . search ( emp [ 'Emp_skillset' ])] * Filter and Sort data in ascending order fil_emps = [ emp for emp in emps if emp [ 'Emp_designation' ] == 'ASE' ] fil_emps = sorted ( fil_emps , key = lambda k : k [ 'Emp_name' ]) print ( emp [ 'Emp_name' ] for emp in fil_emps ) * Sorting all employees based on custom sorting criteria order = { 'ASE' : 1 , 'ITA' : 2 , 'AST' : 3 } sorted_emp = sorted ( emp , key = lambda k : order [ k [ 'designation' ]]) * Filter data and write into files fil_emps = [ emp for emp in emps if emp [ 'Emp_Designation' ] == 'ITA' ] ofp = open ( outputtext . txt , 'w' ) keys = fil_emps [ 0 ] . keys () # Remove header from key name for key in keys : ofp . write ( key + \" \\t \" ) ofp . write ( \" \\n \" ) for emp in fil_emps : for key in keys : ofp . write ( emp [ key ] + \" \\t \" ) ofp . write ( \" \\n \" ) ofp . close () # Regular Expressions * Regex are useful to construct patterns that helps in filtering the text possessing the pattern . * ** re module ** is used to deal with regex . * ** search ** method takes pattern and text to scan and returns a Match object . Return None if not found . * Match object holds info on the nature of the match like ** original input string , Regular expression used , location within the original string ** match = re . search ( pattern , text ) start_index = match . start () # start location of match end_index = match . end () regex = match . re . pattern () print ( 'Found \" {} \" pattern in \" {} \" from {} to {} ' . format ( st , text , start_index , end_index )) # Compiling Expressions * In Python , its more efficient t compile the patterns that are frequently used . * ** compile ** function of re module converts an expression string into a ** RegexObject **. patterns = [ 'this' , 'that' ] regexes = [ re . compile ( p ) for p in patterns ] for regex in regexes : if regex . search ( text ): # pattern is not required print ( 'Match found' ) * search method only returns the first matching occurrence . # Finding Multiple Matches * findall method returns all the substrings of the pattern without overlapping pattern = 'ab' for match in re . findall ( pattern , text ): print ( 'match found - {} ' . format ( match )) # Grouping Matches * Adding groups to a pattern enables us to isolate parts of the matching text , expanding those capabilities to create a parser . * Groups are defined by enclosing patterns within parenthesis text = 'This is some text -- with punctuations.' for pattern in [ r '^(\\w+)' , # word at the start of the string r '(\\w+)\\S*$' , # word at the end of the string with punctuation r '(\\bt\\w+)\\W+(\\w+)' , # word staring with 't' and the next word r '(\\w+t)\\b' ]: # word ending with t regex = re . compile ( pattern ) match = regex . search ( text ) print ( match . groups ()) # Output -> ('This',) ('punctuations',) ('text','with') ('text',) # Naming Grouped Matches * Accessing the groups with defined names text = 'This is some text -- with punctuations.' for pattern in [ r '^(?P<first_word>\\w+)' , # word at the start of the string r '(?P<last_word>\\w+)\\S*$' , # word at the end of the string with punctuation r '(?P<t_word>\\bt\\w+)\\W+(?P<other_word>\\w+)' , # word staring with 't' and the next word r '(?P<ends_with_t>\\w+t)\\b' ]: # word ending with t regex = re . compile ( pattern ) match = regex . search ( text ) print ( \"Groups: \" , match . groups ()) # Output -> ('This',) ('punctuations',) ('text','with') ('text',) print ( \"Group Dictionary: \" , match . groupdict ()) # Output -> {'first_word':'This'} {'last_word': 'punctuations'} {'t_word':'text', 'other_word':'with'} {'ends_with_t':'text'} # Data Handling # Handling XML files * ** lxml ** 3 rd party module is a highly feature rich with ElementTree API and supports querying wthe xml content using XPATH . * In the ElementTree API , an element acts like a list . The items of the list are the elements children . * XML search is faster in lxml . < ? xml > < employee > < skill name = \"Python\" /> </ employee > from lxml import etree tree = etree . parse ( 'sample.xml' ) root = tree . getroot () # gets doc root <?xml> skills = tree . findall ( '//skill' ) # gets all skill tags for skill in skills : print ( \"Skills: \" , skill . attrib [ 'name' ]) # Adding new skill in the xml skill = etree . SubElement ( root , 'skill' , attrib = { 'name' : 'PHP' }) # Handling HTML files * ** lxml ** 3 rd party module is used for parsing HTML files as well . import urllib.request from lxml import etree def readURL ( url ): urlfile = urllib . request . urlopen ( url ) if urlfile . getcode () == 200 : contents = urlfile . read () return contents if __name__ == '__main__' : url = 'http://xkcd.com' html = readURL ( url ) # Data Serialization * Process of converting ** data types / objects ** into ** Transmittable / Storable ** format is called Data Serialization . * In python , ** pickle and json ** modules are used for Data Serialization . * Serialized data can then be written to file / Socket / Pipe . From these it can be de - serialized and stored into a new Object . json . dump ( data , file , indent = 2 ) # serialized data is written to file with indentation using dump method data_new = json . load ( file ) # de-serialized data is written to new object using load method # Database Connectivity * ** Python Database API ( DB - API ) ** is a standard interface to interact with various databases . * Different DB API \u2019 s are used for accessing different databases . Hence a programmer has to install DB API corresponding to the database one is working with . * Working with a database includes the following steps : + Importing the corresponding DB - API module . + Acquiring a connection with the database . + Executing SQL statements and stored procedures . + Closing the connection import sqlite3 # establishing a database connection con = sqlite3 . connect ( 'D: \\\\ TEST.db' ) # preparing a cursor object cursor = con . cursor () # preparing sql statements sql1 = 'DROP TABLE IF EXISTS EMPLOYEE' # closing the database connection con . close () # Inserting Data * Single rows are inserted using ** execute ** and multiple rows using ** executeMany ** method of created cursor object . # preparing sql statement rec = ( 456789 , 'Frodo' , 45 , 'M' , 100000.00 ) sql = ''' INSERT INTO EMPLOYEE VALUES ( ?, ?, ?, ?, ?) ''' # executing sql statement using try ... except blocks try : cursor . execute ( sql , rec ) con . commit () except Exception as e : print ( \"Error Message :\" , str ( e )) con . rollback () # Fetching Data * ** fetchone ** : It retrieves one record at a time in the form of a tuple . * ** fetchall ** : It retrieves all fetched records at a point in the form of tuple of tuples . # fetching the records records = cursor . fetchall () # Displaying the records for record in records : print ( record ) # Object Relational Mappers * An object - relational mapper ( ORM ) is a library that automates the transfer of data stored in relational database tables into objects that are adopted in application code . * ORMs offer a high - level abstraction upon a relational database , which permits a developer to write Python code rather than SQL to create , read , update and delete data and schemas in their database . * Such an ability to write Python code instead of SQL speeds up web application development . # Higher Order Functions * A ** Higher Order function ** is a function , which is capable of doing any one of the following things : + It can be functioned as a ** data ** and be assigned to a variable . + It can accept any other ** function as an argument **. + It can return a ** function as its result **. * The ability to build Higher order functions , ** allows a programmer to create Closures , which in turn are used to create Decorators **. # Function as a Data def greet (): return 'Hello Everyone!' print ( greet ()) wish = greet # 'greet' function assigned to variable 'wish' print ( type ( wish )) # Output -> <type 'function'> print ( wish ()) # Output -> Hello Everyone! # Function as an Argument def add ( x , y ): return x + y def sub ( x , y ): return x - y def prod ( x , y ): return x * y def do ( func , x , y ): return func ( x , y ) print ( do ( add , 12 , 4 )) # 'add' as arg # Output -> 16 print ( do ( sub , 12 , 4 )) # 'sub' as arg # Output -> 8 print ( do ( prod , 12 , 4 )) # 'prod' as arg # Output -> 48 # Returning a Function def outer (): def inner (): s = 'Hello world!' return s return inner () print ( outer ()) # Output -> Hello world! * You can observe from the output that the ** return value of 'outer' function is the return value of 'inner' function ** i . e 'Hello world!' . def outer (): def inner (): s = 'Hello world!' return s return inner # Removed '()' to return 'inner' function itself print ( outer ()) #returns 'inner' function # Output -> <function inner at 0xxxxxx> func = outer () print ( type ( func )) # Output -> <type 'function'> print ( func ()) # calling 'inner' function # Output -> Hello world! * Parenthesis after the ** inner ** function are removed so that the ** outer ** function returns ** inner function **. # Closures * A Closure is a ** function returned by a higher order function ** , whose return value depends on the data associated with the higher order function . def multiple_of ( x ): def multiple ( y ): return x * y return multiple c1 = multiple_of ( 5 ) # 'c1' is a closure c2 = multiple_of ( 6 ) # 'c2' is a closure print ( c1 ( 4 )) # Output -> 5 * 4 = 20 print ( c2 ( 4 )) # Output -> 6 * 4 = 24 * The first closure function , c1 binds the value 5 to argument x and when called with an argument 4 , it executes the body of multiple function and returns the product of 5 and 4. * Similarly c2 binds the value 6 to argument x and when called with argument 4 returns 24. # Decorators * Decorators are evolved from the concept of closures . * A decorator function is a higher order function that takes a function as an argument and returns the inner function . * A decorator is capable of adding extra functionality to an existing function , without altering it . * The decorator function is prefixed with **@ symbol ** and written above the function definition . + Shows the creation of closure function wish using the higher order function outer . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner def greet (): print ( 'Hello!' ) wish = outer ( greet ) # Output -> Accessing : greet wish () # Output -> Hello! - wish is the closure function obtained by calling an outer function with the argument greet . When wish function is called , inner function gets executed . + The second one shows the creation of decorator function outer , which is used to decorate function greet . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner def greet (): return 'Hello!' greet = outer ( greet ) # decorating 'greet' # Output -> No Output as return is used instead of print greet () # calling new 'greet' # Output -> Accessing : greet - The function returned by outer is assigned to greet i . e the function name passed as argument to outer . This makes outer a decorator to greet . + Third one displays decorating the greet function with decorator function , outer , using @ symbol . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner @outer # This is same as **greet = outer(greet)** def greet (): return 'Hello!' greet () # Output -> Accessing : greet # Descriptors * Python descriptors allow a programmer to create managed attributes . * In other object - oriented languages , you will find ** getter and setter ** methods to manage attributes . * However , Python allows a programmer to manage the attributes simply with the attribute name , without losing their protection . * This is achieved by defining a ** descriptor class ** , that implements any of ** __get__ , __set__ , __delete__ ** methods . class EmpNameDescriptor : def __get__ ( self , obj , owner ): return self . __empname def __set__ ( self , obj , value ): if not isinstance ( value , str ): raise TypeError ( \"'empname' must be a string.\" ) self . __empname = value * The descriptor , EmpNameDescriptor is defined to manage empname attribute . It checks if the value of empname attribute is a string or not . class EmpIdDescriptor : def __get__ ( self , obj , owner ): return self . __empid def __set__ ( self , obj , value ): if hasattr ( obj , 'empid' ): raise ValueError ( \"'empid' is read only attribute\" ) if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value * The descriptor , EmpIdDescriptor is defined to manage empid attribute . class Employee : empid = EmpIdDescriptor () empname = EmpNameDescriptor () def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name * Employee class is defined such that , it creates empid and empname attributes from descriptors EmpIdDescriptor and EmpNameDescriptor . e1 = Employee ( 123456 , 'John' ) print ( e1 . empid , '-' , e1 . empname ) # Output -> '123456 - John' e1 . empid = 76347322 # Output -> ValueError: 'empid' is read only attribute # Properties * Descriptors can also be created using property () type . + Syntax : property ( fget = None , fset = None , fdel = None , doc = None ) - where , fget : attribute get method fset : attribute set method fdel \u2013 attribute delete method doc \u2013 docstring class Employee : def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name def getEmpID ( self ): return self . __empid def setEmpID ( self , value ): if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value empid = property ( getEmpID , setEmpID ) # Property Decorators * Descriptors can also be created with property decorators . * While using property decorators , an attribute 's get method will be same as its name and will be decorated with property. * In a case of defining any set or delete methods , they will be decorated with respective setter and deleter methods . class Employee : def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name @property def empid ( self ): return self . __empid @empid . setter def empid ( self , value ): if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value e1 = Employee ( 123456 , 'John' ) print ( e1 . empid , '-' , e1 . empname ) # Output -> '123456 - John' # Introduction to Class and Static Methods Based on the ** scope ** , functions / methods are of two types . They are : * Class methods * Static methods # Class Methods * A method defined inside a class is bound to its object , by default . * However , if the method is bound to a Class , then it is known as ** classmethod **. class Circle ( object ): no_of_circles = 0 def __init__ ( self , radius ): self . __radius = radius Circle . no_of_circles += 1 def getCirclesCount ( self ): return Circle . no_of_circles c1 = Circle ( 3.5 ) c2 = Circle ( 5.2 ) c3 = Circle ( 4.8 ) print ( c1 . getCirclesCount ()) # -> 3 print ( Circle . getCirclesCount ( c3 )) # -> 3 print ( Circle . getCirclesCount ()) # -> TypeError: getCirclesCount() missing 1 required positional argument: 'self' class Circle ( object ): no_of_circles = 0 def __init__ ( self , radius ): self . __radius = radius Circle . no_of_circles += 1 @classmethod def getCirclesCount ( self ): return Circle . no_of_circles c1 = Circle ( 3.5 ) c2 = Circle ( 5.2 ) c3 = Circle ( 4.8 ) print ( c1 . getCirclesCount ()) # -> 3 print ( Circle . getCirclesCount ()) # -> 3 # Static Method * A method defined inside a class and not bound to either a class or an object is known as ** Static ** Method . * Decorating a method using ** @staticmethod ** decorator makes it a static method . def square ( x ): return x ** 2 class Circle ( object ): def __init__ ( self , radius ): self . __radius = radius def area ( self ): return 3.14 * square ( self . __radius ) c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 print ( square ( 10 )) # -> 100 * square function is not packaged properly and does not appear as integral part of class Circle . class Circle ( object ): def __init__ ( self , radius ): self . __radius = radius @staticmethod def square ( x ): return x ** 2 def area ( self ): return 3.14 * self . square ( self . __radius ) c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 print ( square ( 10 )) # -> NameError: name 'square' is not defined * square method is no longer accessible from outside the class Circle . * However , it is possible to access the static method using Class or the Object as shown below . print ( Circle . square ( 10 )) # -> 100 print ( c1 . square ( 10 )) # -> 100 # Abstract Base Classes * An ** Abstract Base Class ** or ** ABC ** mandates the derived classes to implement specific methods from the base class . * It is not possible to create an object from a defined ABC class . * Creating objects of derived classes is possible only when derived classes override existing functionality of all abstract methods defined in an ABC class . * In Python , an Abstract Base Class can be created using module abc . from abc import ABC , abstractmethod class Shape ( ABC ): @abstractmethod def area ( self ): pass @abstractmethod def perimeter ( self ): pass * Abstract base class Shape is defined with two abstract methods area and perimeter . class Circle ( Shape ): def __init__ ( self , radius ): self . __radius = radius @staticmethod def square ( x ): return x ** 2 def area ( self ): return 3.14 * self . square ( self . __radius ) def perimeter ( self ): return 2 * 3.14 * self . __radius c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 # Context Manager * A Context Manager allows a programmer to perform required activities , automatically , while entering or exiting a Context . * For example , opening a file , doing few file operations , and closing the file is manged using Context Manager as shown below . with open ( 'sample.txt' , 'w' ) as fp : content = fp . read () * The keyword ** with ** is used in Python to enable a context manager . It automatically takes care of closing the file . import sqlite3 class DbConnect ( object ): def __init__ ( self , dbname ): self . dbname = dbname def __enter__ ( self ): self . dbConnection = sqlite3 . connect ( self . dbname ) return self . dbConnection def __exit__ ( self , exc_type , exc_val , exc_tb ): self . dbConnection . close () with DbConnect ( 'TEST.db' ) as db : cursor = db . cursor () ''' Few db operations ... ''' * Example from contextlib import contextmanager @contextmanager def context (): print ( 'Entering Context' ) yield print ( \"Exiting Context\" ) with context (): print ( 'In Context' ) # Output -> Entering Context -> In Context -> Exiting Context # Coroutines * A Coroutine is ** generator ** which is capable of constantly receiving input data , process input data and may or may not return any output . * Coroutines are majorly used to build better ** Data Processing Pipelines **. * Similar to a generator , execution of a coroutine stops when it reaches ** yield ** statement . * A Coroutine uses ** send ** method to send any input value , which is captured by yield expression . def TokenIssuer (): tokenId = 0 while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) t = TokenIssuer () next ( t ) t . send ( 'George' ) # -> Token number of George: 1 t . send ( 'Rosy' ) # -> Token number of Rosy: 2 * ** TokenIssuer ** is a coroutine function , which uses yield to accept name as input . * Execution of coroutine function begins only when next is called on coroutine t . * This results in the execution of all the statements till a yield statement is encountered . * Further execution of function resumes when an input is passed using send , and processes all statements till next yield statement . def TokenIssuer ( tokenId = 0 ): try : while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) except GeneratorExit : print ( 'Last issued Token is :' , tokenId ) t = TokenIssuer ( 100 ) next ( t ) t . send ( 'George' ) # Token number of George: 101 t . send ( 'Rosy' ) # Token number of Rosy: 102 t . send ( 'Smith' ) # Token number of Smith: 103 t . close () # Last issued Token is: 103 * The coroutine function TokenIssuer takes an argument , which is used to set a starting number for tokens . * When coroutine t is closed , statements under GeneratorExit block are executed . * Many programmers may forget that passing input to coroutine is possible only after the first next function call , which results in error . * Such a scenario can be avoided using a decorator . def coroutine_decorator ( func ): def wrapper ( * args , ** kwdargs ): c = func ( * args , ** kwdargs ) next ( c ) return c return wrapper @coroutine_decorator def TokenIssuer ( tokenId = 0 ): try : while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) except GeneratorExit : print ( 'Last issued Token is :' , tokenId ) t = TokenIssuer ( 100 ) t . send ( 'George' ) t . send ( 'Rosy' ) t . send ( 'Smith' ) t . close () * coroutine_decorator takes care of calling next on the created coroutine t . def nameFeeder (): while True : fname = yield print ( 'First Name:' , fname ) lname = yield print ( 'Last Name:' , lname ) n = nameFeeder () next ( n ) n . send ( 'George' ) n . send ( 'Williams' ) n . send ( 'John' ) First Name : George Last Name : Williams First Name : John","title":"Python"},{"location":"learning/terraform/","text":"Example using Docker images // Main . tf resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image . nginx . latest name = \"webserver\" ports { internal = 80 external = 8050 } } Infrastructure as Code IaC is an important DevOps cornerstone that enables you to define, automatically manage, and provision infrastructure through source code. Infrastructure is managed as a software system . IaC is frequently referred to as Programmable Infrastructure . Benefits of Iac In IaC, A tool will monitor the state of the infrastructure . A script will be sent automatically to fix the issue . A script can be written to add new instances, and it can be reused . Faster process , no/less human involvement. IaC Implemetation Approaches Declarative Focuses on the desired end state of infrastructure (Functional) . Tools perform the necessary actions to reach that state . Automatically takes care of the order and executes it . Examples are Terraform and CloudFormation. Imperative Focuses on how to achieve the desired state (Procedural) . Follows the order of tasks and it executes in that order . Examples are Chef and Ansible. Configuration Management It is a system that keeps the track of organization's hardware, software, and other related information. It includes the software updates and versions present in the system. Configuration management is also called as configuration control . E.g.: Chef and Ansible Orchestration: It automates the coordination, management, and configuration of systems and hardware. E.g.: Terraform and Nomad Installing Terraform sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip unzip terraform_0.11.11_linux_amd64.zip sudo mv terraform /usr/local/bin/ terraform --version Terraform Lifecycle Terraform init - Initializes the working directory having Terraform configuration files. Terraform plan - Creates an execution plan and it mentions the changes it is going to make. Terraform apply - When you are not amazed by seeing the results of the terraform plan, you can go ahead and run terraform apply to apply the changes. The main difference between plan and apply is - apply asks for a prompt like Do you want to perform these actions? and then implement the changes, whereas, plan shows the possible end state of your infrastructure without actually implementing them. Terraform destroy - Destroys the created infrastructure. Terraform Configuration A set of files that describe the infrastructure in Terraform is called as Terraform Configuration. The entire configuration file is saved with .tf extension. Make sure to have all the configurations maintained in a single .tf file instead of many. Terraform loads all the .tf files present in the working directory. Creating Virtual Network create a virtual network without using the Azure portal and azure CLI commands. resource \"azurerm_virtual_network\"\"myterraformnetwork\" { name = \"myvnet\" address_space = [\"10.0.0.0/16\"] location=\"East US\" resource_group_name=\"er-tyjy\" } Resource blocks define the infrastructure (resource \"azurerm_virtual_network\" \"myterraformnetwork\"). It contains the two strings - type of the resource(azurerm_virtualnetwork) and name of the resource(myterraformnetwork). Terraform uses plugin based architecture to support the various services providers and infrastructure available. When you run Terraform init command, it downloads and installs provider binary for the corresponding providers. In this case, it is Azure. Now, you can run the terraform plan command. It shows all the changes it is going to make. If the output is as you expected and there are no errors, then you can run Terraform apply. Terraform Validate You can run this command whenever you like to check whether the code is correct. Once you run this command, if there is no output, it means that there are no errors in the code. Variables Terraform provides the following variable types: Strings - The default data type is a string . List - A list is like simple arrays Map - A map value is nothing but a lookup table for string keys to get the string values. In terraform, to reduce the complexity of code and for easiness , all the variables are created in the variables.tf file. variables.tf variable \"rg\" { default = \"terraform-lab2\" } variable \"locations\" { default = [\"ap-southeast-1\" , \"ap-southeast-2\"] } variable \"tags\" { type = \"map\" default = { environment = \"training\" source=\"citadel\" } } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-east-2 = \"image-4321\" } } For example, you have defined resource_group name in the variable.tf file. you can call them in main.tf file like this \"${var.resource_group}\" Sensitive Parameters There may be sensitive parameters which should be shown only when running the terraform plan but not terraform apply. output \"sensitiveoutput\" { sensitive = true value = VALUE } When you run terraform apply, the output is labeled as sensitive. If there is any sensitive information, then you can protect it by using a sensitive parameter. terraform.tfvars File In terraform.tfvars file, we mention the \"key/value\" pair. The main.tf file takes the input from a terraform. tfvars file. If it is not present, it gets the input from variables.tf file. resource_group = \"user-ybje\" location = \"East US\" terraform fmt It rewrites the confguration files to canonical style and format. State File - terraform.tfstate It is a JSON file that maps the real world resources to the configuration files. In terraform, there are three states. Desired state - Represents how the infrastructure to be present. Actual state - Represents the current state of infrastructure. Known state - To bridge desired state and actual state, there is a known state. You can get the details of known state from a tfstate file. When you run terraform plan command , terraform performs a quick refresh and lists the actions needed to achieve the desired state. When terraform apply command is run, it applies the changes and updates the actual state. Modules A module is like a reusable blueprint of infrastructure . A module is nothing but a folder of Terraform files. In terraform the modules are categorized into two types: Root modules - The current directory of Terraform files on which you are running the commands. Child modules - The modules sourced by the root module. To create a child module, add resources to it. Module syntax: module \"child\" { source = \"../child\" } The difference between adding the resources and module is for adding the resource (Eg: resource \"azurerm_virtual_network\" 'test\") you need type and name but for adding a module (Eg: module \"child\") only the name is enough. The name of the module should be unique within configurations because it is used as a reference to the module and outputs. main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } Updates terraform get - This command is used to download and install the modules for that configuration. terraform get -update - It checks the downloaded modules and checks for the recent versions if any. Module Outputs If you need output from any module, you can get it by passing the required parameter. For example, the output of virtual network location can be found by following the below syntax. You can add this syntax to .tf file(main.tf or variables.tf) . output \"virtual_network_id\" { value = \"${azurerm_virtual_network.test.location}\" } Where virtual_network_id is a unique name and \"${azurerm_virtual_network.test.location}\" is the location of the virtual network using string interpolation. Benefits of Modules Code reuse : When there is a need to provision the group of resources on another resource at the same time, a module can be used instead of copying the same code. It helps in resolving the bugs easily. If you want to make changes, changing the code in one place is enough. Abstraction layer : It makes complex configurations easier to conceptualize. For example, if you like to add vault(Another harshicop's tool for managing secrets) cluster to the environment, it requires dozens of components. Instead of worrying about individual components, it gives ready-to-use vault cluster. Black box : To create a vault, you only need some configuration parameters without worrying about the complexity. You don't need any knowledge on how to install vault, configuring vault cluster and working of the vault. You can create a working cluster in a few minutes. Best practices in organization : When a module is created, you can give it to the other teams. It helps in easily developing the infrastructure. **Versioned artifacts: They are immutable artifacts and can be promoted from one environment to others. Introduction to Meta Parameters There are some difficulties with the declarative language. For example, since it is a declarative language, there is no concept of for loops. How do you repeat a piece of code without copy paste? * Terraform provides primitives like meta parameter called as count, life cycle blocks like create_before_destroy, ternary operator and a large number of interpolation functions . This helps in performing certain types of loops, if statements and zero downtime deployments. Count Count - It defines how many parameters you like to create. Modify main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } To increase Vnets to 5, update count = 5 . To decrease Vnets to 2, update count = 2 , last 3 Vnets will get deleted. Elements Suppose you like to create the three virtual networks with names vnet-A, vnet-B and vnet-C, you can do this easily with the help of list . Mention how many vnets you are going to create with the help of list and define it in variables.tf file variable \"name\" { type= \"list\" default = [\"A\",\"B\",\"C\"] } You can call this variable in the main.tf file in the following way count = \"${length(var.name)}\" - It returns number of elements present in the list. you should store it in meta parameter count. \"${element(var.name,count.index)}\" - It acts as a loop, it takes the input from list and repeats until there are no elements in list. variables.tf variable \"resource_group\" { default = \"user-nuqo\" } variable \"location\" { default = \"East US\" } variable \"name\" { type = \"list\" default = [\"A\",\"B\",\"C\"] } main.tf resource \"azurerm_virtual_network\"\"multiple\" { name = \"vnet-${element(var.name,count.index)}\" resource_group_name = \"${var.resource_group}\" location = \"${var.location}\" address_space=[\"10.0.0.0/16\"] count=\"${length(var.name)}\" } Conditions For example, a vnet has to be created only, if the variable number of vnets is 3 or else no. You can do this by using the ternary operator. variables.tf variable \"no_of_vnets\" { default= 3 } main.tf count = \"${var.no_of_vnets ==3 ?1 : 0}\" * First, it will execute the ternary operator. If it is true, it takes the output as 1 or else 0. * Now, in the above case, the output is 1, the count becomes one, and the vnet is created. Inheriting Variables Inheriting the variables between modules keeps your Terraform code DRY. Don't Repeat Yourself (DRY) It aims at reducing repeating patterns and code. There is no need to write the duplicate code for each environment you are deploying. You can write the minimum code and achieve the goal by having maximum re-usability. Module File Structure You will follow the below file structure to inherit the variables between modules. modules | |___mod | |__mod.tf | |___vnet |__vnet.tf |__vars.tf There is one vnet and you can configure the vnet by passing the parameters from module. The variables in the vnet are overriden by the values provided in mod.tf file. Create a folder with name Modules Create two subfolders of names vnet and mod in the nested_modules folder. In the vnet folder, create a virtual network using Terraform configuration. Maintain two files one for main configuration(vnet.tf) and other for declaring variables(vars.tf) . Now, create main.tf file like this resource \"azurerm_virtual_network\"\"vnet\" { name = \"${var.name}\" address_space = [\"10.0.0.0/16\"] resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" } Now, vars.tf file looks like this variable \"name\" { description = \"name of the vnet\" } variable \"resourcegroup\" { description = \"name of the resource group\" } variable \"location\" { description =\"name of the location\" } variable \"address\" { type =\"list\" description = \"specify the address\" } Create a folder with name mod in the modules directory. Create a file with name mod.tf and write the below code in it. We are passing the variables from the root module (mod) to the child module (vnet) using source path . module \"child\" { source = \"../vnet\" name = \"modvnet\" resourcegroup = \"user-vcom\" location = \"eastus\" address = \"10.0.1.0/16\" } Run terraform plan to visualize how the directories will be created. You can see the name of the vnet is taken from the module file instead of getting it from vars.tf Nested Modules For any module, root module should exist. Root module is basically a directory that holds the terraform configuration files for the desired infrastructure. They also provide the entry point to the nested modules you are going to utilize. For any module main.tf, vars.tf, and outputs.tf naming conventions are recommended. If you are using nested modules, create them under the subdirectory with name modules . checking_modules |__modules | |____main.tf | |______virtual_network | |_main.tf | |_vars.tf | |______storage_account |_main.tf |_vars.tf You will create three folders with names modules, virtual_network, and storage account. Make sure that you are following the terraform standards like writing the terraform configuration in the main file and defining the variables alone in the variables(vars.tf) file. stoacctmain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"${var.name}\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"${var.account_replication}\" account_tier = \"${var.account_tier}\" } stoacctvars.tf variable \"rg\" { default = \"user-wmle\" } variable \"loc\" { default = \"eastus\" } variable \"name\" { default =\"storagegh\" } variable \"account_replication\" { default = \"LRS\" } variable \"account_tier\" { default = \"Basic\" } Root Module (main.tf) file under checking_modules module \"vnet\" { source = \"../virtual_network\" name = \"mymodvnet\" } module \"stoacct\" { source = \"../storage_account\" name = \"mymodstorageaccount\" } Not only the parameters mentioned above, but you can also pass any parameter from root module to the child modules. You can pass the parameters from the module to avoid the duplicity between them. Remote Backends It is better to store the code in a central repository . However, is having some disadvantages of doing so: You have to maintain push, pull configurations . If one pushes the wrong configuration and commits it, it becomes a problem. State file consists of sensitive information . So state files are non-editable. Backend in terraform explains how the state file is loaded and operations are executed. Backend can get initialize only after running terraform init command. So, terraform init is required to be run every time when backend is configured when any changes made to the backend when the backend configuration is removed completely Terraform cannot perform auto-initialize because it may require additional info from the user, to perform state migrations, etc.. Below are the steps which you will be maintaining for creating remote backend in Azure. Create a storage account Create a storage container Create a backend Get the storage account access key and resource group name, give it as a parameter to the backend. Below is the complete file structure for sample backend. Backend |____stoacct.tf |____stocontainer.tf |____backend.tf |____vars.tf |____output.tf stoacct.tf resource \"azurerm_storage_account\" \"storageaccount\" { name = \"storageaccountname\" resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" account_tier = \"${var.accounttier}\" account_replication_type = \"GRS\" tags { environment = \"staging\" } } stocontainer.tf resource \"azurerm_storage_container\" \"storagecontainer\" { name = \"vhds\" resource_group_name = \"${var.resourcegroup}\" storage_account_name = \"${azurerm_storage_account.storageaccount.name}\" container_access_type = \"private\" } vars.tf variable \"resourcegroup\" { default = \"user-abcd\" } variable \"location\" { default = \"eastus\" } variable \"accounttier\" { default = \"Basic\" } output.tf output \"storageacctname\" { value = \"${azurerm_storage_account.storageaccount.name}\" } output \"storageacctcontainer\" { value = \"${azurerm_storage_account.storagecontainer.name}\" } output \"access_key\" { value = \"${azurerm_storage_account.storageaccount.primary_access_key\" } Before creating backend, run the command to get the storage account keys list( az storage account keys list --account-name storageacctname ) and copy one of the key somewhere. It is useful for configuring the backend. backend.tf terraform{ backend \"azurerm\" { storage_account_name = \"storageacct21\" container_name = \"mycontainer\" key = \"terraform.tfstate.change\" resource_group_name = \"user-okvt\" access_key = \"aJlf+XjZhxwRp4gsU4hkGrQJzO7xBzz7B9rSzMLB/ozwcM6k/1N.......==\" } } where Parameters: storage_account_name - name of the storage account. container_name - name of the container. key- name of the tfstate blob. resource_group_name - name of the resource group. access_key - Storage account access key(any one). Points to Remember You cannot use interpolation syntax to configure backends. After creating backend run terraform init, it will be in the locked state. By running terraform apply command automatically, the lease status is changed to locked. After it is applied, it will come to an unlocked state. This backend supports consistency checking and state locking using the native capabilities of the Azure storage account. Terragrunt Terragrunt is referred to as a thin wrapper for Terraform which provides extra tools to keep the terraform configurations DRY, manage remote state and to work with multiple terraform modules. Terragrunt Commands terragrunt get terragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt supports the following use cases: Keeps terraform code DRY. Remote state configuration DRY. CLI flags DRY. Executing terraform commands on multiple modules at a time. Advantages of using terragrunt: Provides locking mechanism. Allows you to use remote state always. Build-In Functions lookup This helps in performing a dynamic lookup into a map variable. The syntax for lookup variable is lookup(map,key, [default]) . map: The map parameter is a variable like var.name. key: The key parameter includes the key from where it should find the environment. If the key doesn't exist in the map, interpolation will fail, if it doesn't find a third argument (default). The lookup will not work on nested lists or maps. It only works on flat maps. Local Values Local values help in assigning names to the expressions , which can be used multiple times within a module without rewriting it. Local values are defined in the local blocks. It is recommended to group the logically related local values together as a single block, if there is a dependency between them. locals { service_name = \"Fresco\" owner = \"Team\" } Example locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } If a single value or a result is used in many places and it is likely to be changed in the future, then you can go with locals. It is recommended to not use many local values because it makes the read configurations hard to the future maintainers. Data Source Data source allows the data to be computed or fetched for use in Terraform configuration. Data sources allow the terraform configurations to build on the information defined outside the Terraform or by another Terraform configuration. Providers play a major role in implementing and defining data sources. Data source helps in two major ways: It provides a read-only view for the pre-existing data. It can compute new values on the fly. Every data source in the Terraform is mapped to the provider depending on the longest-prefix-matching . data \"azurerm_resource_group\" \"passed\" { name = \"${var.resource_group_name}\" } Concat and Contains concat(list1,list2) * It combines two or more lists into a single list. contains(list, element) * It returns true if the element is present in the list or else false. E.g. contains(var.list_of_strings, \"an_element\") Workspaces Every terraform configurations has associate backends which defines how the operations are executed and where the persistent data like terraform state are stored. Persistent data stored in backend belongs to a workspace. Previously, the backend has only one workspace which is called as default and there will be only one state file associated with the configuration. Some of the backends support multiple workspaces, i.e. It allows multiple state files to be stored within a single configuration. The configuration is still having only one backend, and the multiple distinct instances can be deployed without configuring to a new backend or by changing authentication credentials. default workspace is special because you can't delete the default workspace. You cannot delete your active workspace (the workspace in which you are currently working). If you haven't created the workspace before, then you are working on the default workspace. Workspace Commands terraform workspace new - It creates the workspace with specified name and switches to it. terraform workspace list - It lists the workspaces available. terraform workspace select - It switches to the specified workspace. terraform workspace delete - It deletes the mentioned workspace. Configuring Multiple Providers Below is the file structure that you will maintain for working with multiple providers. multiple_providers | |___awsmain.tf | |___azuremain.tf | |___vars.tf | |___out.tf Create an S3 bucket in Azure which helps in storing the file in AWS. awsmain.tf resource \"aws_s3_bucket\" \"b\" { bucket = \"my-tf-test-bucket-21\" acl = \"private\" tags = { Name = \"My bucket test\" Environment = \"Dev\" } } Resource has two parameters: name(aws_s3_bucket) and type(\"b\"). Name of the resource may vary from provider to provider. Type of the resource depends on the user. Bucket indicates the name of the bucket. If it is not assigned, terraform assigns a random unique name. acl (access control list) - It helps to manage access to the buckets and objects. tag - It is a label which is assigned to the AWS resource by you or AWS. Environment - On which environment do you like to deploy S3 bucket (Dev, Prod or Stage). azuremain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"storageacct21\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"LRS\" account_tier = \"Standard\" } Parameters name - It indicates the name of the storage account. resource_group_name and location- name of the resource group and location. account_replication_type - Type of the account replication. account_tier - Account tier type out.tf output \"account_tier\" { value = \"azurerm_storage_Account.storagecct.account_tier\" }","title":"Terraform"},{"location":"learning/terraform/#example-using-docker-images","text":"// Main . tf resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image . nginx . latest name = \"webserver\" ports { internal = 80 external = 8050 } }","title":"Example using Docker images"},{"location":"learning/terraform/#infrastructure-as-code","text":"IaC is an important DevOps cornerstone that enables you to define, automatically manage, and provision infrastructure through source code. Infrastructure is managed as a software system . IaC is frequently referred to as Programmable Infrastructure .","title":"Infrastructure as Code"},{"location":"learning/terraform/#benefits-of-iac","text":"In IaC, A tool will monitor the state of the infrastructure . A script will be sent automatically to fix the issue . A script can be written to add new instances, and it can be reused . Faster process , no/less human involvement.","title":"Benefits of Iac"},{"location":"learning/terraform/#iac-implemetation-approaches","text":"","title":"IaC Implemetation Approaches"},{"location":"learning/terraform/#declarative","text":"Focuses on the desired end state of infrastructure (Functional) . Tools perform the necessary actions to reach that state . Automatically takes care of the order and executes it . Examples are Terraform and CloudFormation.","title":"Declarative"},{"location":"learning/terraform/#imperative","text":"Focuses on how to achieve the desired state (Procedural) . Follows the order of tasks and it executes in that order . Examples are Chef and Ansible.","title":"Imperative"},{"location":"learning/terraform/#configuration-management","text":"It is a system that keeps the track of organization's hardware, software, and other related information. It includes the software updates and versions present in the system. Configuration management is also called as configuration control . E.g.: Chef and Ansible","title":"Configuration Management"},{"location":"learning/terraform/#orchestration","text":"It automates the coordination, management, and configuration of systems and hardware. E.g.: Terraform and Nomad","title":"Orchestration:"},{"location":"learning/terraform/#installing-terraform","text":"sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip unzip terraform_0.11.11_linux_amd64.zip sudo mv terraform /usr/local/bin/ terraform --version","title":"Installing Terraform"},{"location":"learning/terraform/#terraform-lifecycle","text":"Terraform init - Initializes the working directory having Terraform configuration files. Terraform plan - Creates an execution plan and it mentions the changes it is going to make. Terraform apply - When you are not amazed by seeing the results of the terraform plan, you can go ahead and run terraform apply to apply the changes. The main difference between plan and apply is - apply asks for a prompt like Do you want to perform these actions? and then implement the changes, whereas, plan shows the possible end state of your infrastructure without actually implementing them. Terraform destroy - Destroys the created infrastructure.","title":"Terraform Lifecycle"},{"location":"learning/terraform/#terraform-configuration","text":"A set of files that describe the infrastructure in Terraform is called as Terraform Configuration. The entire configuration file is saved with .tf extension. Make sure to have all the configurations maintained in a single .tf file instead of many. Terraform loads all the .tf files present in the working directory.","title":"Terraform Configuration"},{"location":"learning/terraform/#creating-virtual-network","text":"create a virtual network without using the Azure portal and azure CLI commands. resource \"azurerm_virtual_network\"\"myterraformnetwork\" { name = \"myvnet\" address_space = [\"10.0.0.0/16\"] location=\"East US\" resource_group_name=\"er-tyjy\" } Resource blocks define the infrastructure (resource \"azurerm_virtual_network\" \"myterraformnetwork\"). It contains the two strings - type of the resource(azurerm_virtualnetwork) and name of the resource(myterraformnetwork). Terraform uses plugin based architecture to support the various services providers and infrastructure available. When you run Terraform init command, it downloads and installs provider binary for the corresponding providers. In this case, it is Azure. Now, you can run the terraform plan command. It shows all the changes it is going to make. If the output is as you expected and there are no errors, then you can run Terraform apply.","title":"Creating Virtual Network"},{"location":"learning/terraform/#terraform-validate","text":"You can run this command whenever you like to check whether the code is correct. Once you run this command, if there is no output, it means that there are no errors in the code.","title":"Terraform Validate"},{"location":"learning/terraform/#variables","text":"Terraform provides the following variable types: Strings - The default data type is a string . List - A list is like simple arrays Map - A map value is nothing but a lookup table for string keys to get the string values. In terraform, to reduce the complexity of code and for easiness , all the variables are created in the variables.tf file. variables.tf variable \"rg\" { default = \"terraform-lab2\" } variable \"locations\" { default = [\"ap-southeast-1\" , \"ap-southeast-2\"] } variable \"tags\" { type = \"map\" default = { environment = \"training\" source=\"citadel\" } } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-east-2 = \"image-4321\" } } For example, you have defined resource_group name in the variable.tf file. you can call them in main.tf file like this \"${var.resource_group}\"","title":"Variables"},{"location":"learning/terraform/#sensitive-parameters","text":"There may be sensitive parameters which should be shown only when running the terraform plan but not terraform apply. output \"sensitiveoutput\" { sensitive = true value = VALUE } When you run terraform apply, the output is labeled as sensitive. If there is any sensitive information, then you can protect it by using a sensitive parameter.","title":"Sensitive Parameters"},{"location":"learning/terraform/#terraformtfvars-file","text":"In terraform.tfvars file, we mention the \"key/value\" pair. The main.tf file takes the input from a terraform. tfvars file. If it is not present, it gets the input from variables.tf file. resource_group = \"user-ybje\" location = \"East US\"","title":"terraform.tfvars File"},{"location":"learning/terraform/#terraform-fmt","text":"It rewrites the confguration files to canonical style and format.","title":"terraform fmt"},{"location":"learning/terraform/#state-file-terraformtfstate","text":"It is a JSON file that maps the real world resources to the configuration files. In terraform, there are three states. Desired state - Represents how the infrastructure to be present. Actual state - Represents the current state of infrastructure. Known state - To bridge desired state and actual state, there is a known state. You can get the details of known state from a tfstate file. When you run terraform plan command , terraform performs a quick refresh and lists the actions needed to achieve the desired state. When terraform apply command is run, it applies the changes and updates the actual state.","title":"State File - terraform.tfstate"},{"location":"learning/terraform/#modules","text":"A module is like a reusable blueprint of infrastructure . A module is nothing but a folder of Terraform files. In terraform the modules are categorized into two types: Root modules - The current directory of Terraform files on which you are running the commands. Child modules - The modules sourced by the root module. To create a child module, add resources to it. Module syntax: module \"child\" { source = \"../child\" } The difference between adding the resources and module is for adding the resource (Eg: resource \"azurerm_virtual_network\" 'test\") you need type and name but for adding a module (Eg: module \"child\") only the name is enough. The name of the module should be unique within configurations because it is used as a reference to the module and outputs. main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 }","title":"Modules"},{"location":"learning/terraform/#updates","text":"terraform get - This command is used to download and install the modules for that configuration. terraform get -update - It checks the downloaded modules and checks for the recent versions if any.","title":"Updates"},{"location":"learning/terraform/#module-outputs","text":"If you need output from any module, you can get it by passing the required parameter. For example, the output of virtual network location can be found by following the below syntax. You can add this syntax to .tf file(main.tf or variables.tf) . output \"virtual_network_id\" { value = \"${azurerm_virtual_network.test.location}\" } Where virtual_network_id is a unique name and \"${azurerm_virtual_network.test.location}\" is the location of the virtual network using string interpolation.","title":"Module Outputs"},{"location":"learning/terraform/#benefits-of-modules","text":"Code reuse : When there is a need to provision the group of resources on another resource at the same time, a module can be used instead of copying the same code. It helps in resolving the bugs easily. If you want to make changes, changing the code in one place is enough. Abstraction layer : It makes complex configurations easier to conceptualize. For example, if you like to add vault(Another harshicop's tool for managing secrets) cluster to the environment, it requires dozens of components. Instead of worrying about individual components, it gives ready-to-use vault cluster. Black box : To create a vault, you only need some configuration parameters without worrying about the complexity. You don't need any knowledge on how to install vault, configuring vault cluster and working of the vault. You can create a working cluster in a few minutes. Best practices in organization : When a module is created, you can give it to the other teams. It helps in easily developing the infrastructure. **Versioned artifacts: They are immutable artifacts and can be promoted from one environment to others.","title":"Benefits of Modules"},{"location":"learning/terraform/#introduction-to-meta-parameters","text":"There are some difficulties with the declarative language. For example, since it is a declarative language, there is no concept of for loops. How do you repeat a piece of code without copy paste? * Terraform provides primitives like meta parameter called as count, life cycle blocks like create_before_destroy, ternary operator and a large number of interpolation functions . This helps in performing certain types of loops, if statements and zero downtime deployments.","title":"Introduction to Meta Parameters"},{"location":"learning/terraform/#count","text":"Count - It defines how many parameters you like to create. Modify main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } To increase Vnets to 5, update count = 5 . To decrease Vnets to 2, update count = 2 , last 3 Vnets will get deleted.","title":"Count"},{"location":"learning/terraform/#elements","text":"Suppose you like to create the three virtual networks with names vnet-A, vnet-B and vnet-C, you can do this easily with the help of list . Mention how many vnets you are going to create with the help of list and define it in variables.tf file variable \"name\" { type= \"list\" default = [\"A\",\"B\",\"C\"] } You can call this variable in the main.tf file in the following way count = \"${length(var.name)}\" - It returns number of elements present in the list. you should store it in meta parameter count. \"${element(var.name,count.index)}\" - It acts as a loop, it takes the input from list and repeats until there are no elements in list. variables.tf variable \"resource_group\" { default = \"user-nuqo\" } variable \"location\" { default = \"East US\" } variable \"name\" { type = \"list\" default = [\"A\",\"B\",\"C\"] } main.tf resource \"azurerm_virtual_network\"\"multiple\" { name = \"vnet-${element(var.name,count.index)}\" resource_group_name = \"${var.resource_group}\" location = \"${var.location}\" address_space=[\"10.0.0.0/16\"] count=\"${length(var.name)}\" }","title":"Elements"},{"location":"learning/terraform/#conditions","text":"For example, a vnet has to be created only, if the variable number of vnets is 3 or else no. You can do this by using the ternary operator. variables.tf variable \"no_of_vnets\" { default= 3 } main.tf count = \"${var.no_of_vnets ==3 ?1 : 0}\" * First, it will execute the ternary operator. If it is true, it takes the output as 1 or else 0. * Now, in the above case, the output is 1, the count becomes one, and the vnet is created.","title":"Conditions"},{"location":"learning/terraform/#inheriting-variables","text":"Inheriting the variables between modules keeps your Terraform code DRY. Don't Repeat Yourself (DRY) It aims at reducing repeating patterns and code. There is no need to write the duplicate code for each environment you are deploying. You can write the minimum code and achieve the goal by having maximum re-usability.","title":"Inheriting Variables"},{"location":"learning/terraform/#module-file-structure","text":"You will follow the below file structure to inherit the variables between modules. modules | |___mod | |__mod.tf | |___vnet |__vnet.tf |__vars.tf There is one vnet and you can configure the vnet by passing the parameters from module. The variables in the vnet are overriden by the values provided in mod.tf file. Create a folder with name Modules Create two subfolders of names vnet and mod in the nested_modules folder. In the vnet folder, create a virtual network using Terraform configuration. Maintain two files one for main configuration(vnet.tf) and other for declaring variables(vars.tf) . Now, create main.tf file like this resource \"azurerm_virtual_network\"\"vnet\" { name = \"${var.name}\" address_space = [\"10.0.0.0/16\"] resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" } Now, vars.tf file looks like this variable \"name\" { description = \"name of the vnet\" } variable \"resourcegroup\" { description = \"name of the resource group\" } variable \"location\" { description =\"name of the location\" } variable \"address\" { type =\"list\" description = \"specify the address\" } Create a folder with name mod in the modules directory. Create a file with name mod.tf and write the below code in it. We are passing the variables from the root module (mod) to the child module (vnet) using source path . module \"child\" { source = \"../vnet\" name = \"modvnet\" resourcegroup = \"user-vcom\" location = \"eastus\" address = \"10.0.1.0/16\" } Run terraform plan to visualize how the directories will be created. You can see the name of the vnet is taken from the module file instead of getting it from vars.tf","title":"Module File Structure"},{"location":"learning/terraform/#nested-modules","text":"For any module, root module should exist. Root module is basically a directory that holds the terraform configuration files for the desired infrastructure. They also provide the entry point to the nested modules you are going to utilize. For any module main.tf, vars.tf, and outputs.tf naming conventions are recommended. If you are using nested modules, create them under the subdirectory with name modules . checking_modules |__modules | |____main.tf | |______virtual_network | |_main.tf | |_vars.tf | |______storage_account |_main.tf |_vars.tf You will create three folders with names modules, virtual_network, and storage account. Make sure that you are following the terraform standards like writing the terraform configuration in the main file and defining the variables alone in the variables(vars.tf) file. stoacctmain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"${var.name}\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"${var.account_replication}\" account_tier = \"${var.account_tier}\" } stoacctvars.tf variable \"rg\" { default = \"user-wmle\" } variable \"loc\" { default = \"eastus\" } variable \"name\" { default =\"storagegh\" } variable \"account_replication\" { default = \"LRS\" } variable \"account_tier\" { default = \"Basic\" } Root Module (main.tf) file under checking_modules module \"vnet\" { source = \"../virtual_network\" name = \"mymodvnet\" } module \"stoacct\" { source = \"../storage_account\" name = \"mymodstorageaccount\" } Not only the parameters mentioned above, but you can also pass any parameter from root module to the child modules. You can pass the parameters from the module to avoid the duplicity between them.","title":"Nested Modules"},{"location":"learning/terraform/#remote-backends","text":"It is better to store the code in a central repository . However, is having some disadvantages of doing so: You have to maintain push, pull configurations . If one pushes the wrong configuration and commits it, it becomes a problem. State file consists of sensitive information . So state files are non-editable. Backend in terraform explains how the state file is loaded and operations are executed. Backend can get initialize only after running terraform init command. So, terraform init is required to be run every time when backend is configured when any changes made to the backend when the backend configuration is removed completely Terraform cannot perform auto-initialize because it may require additional info from the user, to perform state migrations, etc.. Below are the steps which you will be maintaining for creating remote backend in Azure. Create a storage account Create a storage container Create a backend Get the storage account access key and resource group name, give it as a parameter to the backend. Below is the complete file structure for sample backend. Backend |____stoacct.tf |____stocontainer.tf |____backend.tf |____vars.tf |____output.tf stoacct.tf resource \"azurerm_storage_account\" \"storageaccount\" { name = \"storageaccountname\" resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" account_tier = \"${var.accounttier}\" account_replication_type = \"GRS\" tags { environment = \"staging\" } } stocontainer.tf resource \"azurerm_storage_container\" \"storagecontainer\" { name = \"vhds\" resource_group_name = \"${var.resourcegroup}\" storage_account_name = \"${azurerm_storage_account.storageaccount.name}\" container_access_type = \"private\" } vars.tf variable \"resourcegroup\" { default = \"user-abcd\" } variable \"location\" { default = \"eastus\" } variable \"accounttier\" { default = \"Basic\" } output.tf output \"storageacctname\" { value = \"${azurerm_storage_account.storageaccount.name}\" } output \"storageacctcontainer\" { value = \"${azurerm_storage_account.storagecontainer.name}\" } output \"access_key\" { value = \"${azurerm_storage_account.storageaccount.primary_access_key\" } Before creating backend, run the command to get the storage account keys list( az storage account keys list --account-name storageacctname ) and copy one of the key somewhere. It is useful for configuring the backend. backend.tf terraform{ backend \"azurerm\" { storage_account_name = \"storageacct21\" container_name = \"mycontainer\" key = \"terraform.tfstate.change\" resource_group_name = \"user-okvt\" access_key = \"aJlf+XjZhxwRp4gsU4hkGrQJzO7xBzz7B9rSzMLB/ozwcM6k/1N.......==\" } } where Parameters: storage_account_name - name of the storage account. container_name - name of the container. key- name of the tfstate blob. resource_group_name - name of the resource group. access_key - Storage account access key(any one).","title":"Remote Backends"},{"location":"learning/terraform/#points-to-remember","text":"You cannot use interpolation syntax to configure backends. After creating backend run terraform init, it will be in the locked state. By running terraform apply command automatically, the lease status is changed to locked. After it is applied, it will come to an unlocked state. This backend supports consistency checking and state locking using the native capabilities of the Azure storage account.","title":"Points to Remember"},{"location":"learning/terraform/#terragrunt","text":"Terragrunt is referred to as a thin wrapper for Terraform which provides extra tools to keep the terraform configurations DRY, manage remote state and to work with multiple terraform modules.","title":"Terragrunt"},{"location":"learning/terraform/#terragrunt-commands","text":"terragrunt get terragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt supports the following use cases: Keeps terraform code DRY. Remote state configuration DRY. CLI flags DRY. Executing terraform commands on multiple modules at a time. Advantages of using terragrunt: Provides locking mechanism. Allows you to use remote state always.","title":"Terragrunt Commands"},{"location":"learning/terraform/#build-in-functions","text":"","title":"Build-In Functions"},{"location":"learning/terraform/#lookup","text":"This helps in performing a dynamic lookup into a map variable. The syntax for lookup variable is lookup(map,key, [default]) . map: The map parameter is a variable like var.name. key: The key parameter includes the key from where it should find the environment. If the key doesn't exist in the map, interpolation will fail, if it doesn't find a third argument (default). The lookup will not work on nested lists or maps. It only works on flat maps.","title":"lookup"},{"location":"learning/terraform/#local-values","text":"Local values help in assigning names to the expressions , which can be used multiple times within a module without rewriting it. Local values are defined in the local blocks. It is recommended to group the logically related local values together as a single block, if there is a dependency between them. locals { service_name = \"Fresco\" owner = \"Team\" } Example locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } If a single value or a result is used in many places and it is likely to be changed in the future, then you can go with locals. It is recommended to not use many local values because it makes the read configurations hard to the future maintainers.","title":"Local Values"},{"location":"learning/terraform/#data-source","text":"Data source allows the data to be computed or fetched for use in Terraform configuration. Data sources allow the terraform configurations to build on the information defined outside the Terraform or by another Terraform configuration. Providers play a major role in implementing and defining data sources. Data source helps in two major ways: It provides a read-only view for the pre-existing data. It can compute new values on the fly. Every data source in the Terraform is mapped to the provider depending on the longest-prefix-matching . data \"azurerm_resource_group\" \"passed\" { name = \"${var.resource_group_name}\" }","title":"Data Source"},{"location":"learning/terraform/#concat-and-contains","text":"concat(list1,list2) * It combines two or more lists into a single list. contains(list, element) * It returns true if the element is present in the list or else false. E.g. contains(var.list_of_strings, \"an_element\")","title":"Concat and Contains"},{"location":"learning/terraform/#workspaces","text":"Every terraform configurations has associate backends which defines how the operations are executed and where the persistent data like terraform state are stored. Persistent data stored in backend belongs to a workspace. Previously, the backend has only one workspace which is called as default and there will be only one state file associated with the configuration. Some of the backends support multiple workspaces, i.e. It allows multiple state files to be stored within a single configuration. The configuration is still having only one backend, and the multiple distinct instances can be deployed without configuring to a new backend or by changing authentication credentials. default workspace is special because you can't delete the default workspace. You cannot delete your active workspace (the workspace in which you are currently working). If you haven't created the workspace before, then you are working on the default workspace.","title":"Workspaces"},{"location":"learning/terraform/#workspace-commands","text":"terraform workspace new - It creates the workspace with specified name and switches to it. terraform workspace list - It lists the workspaces available. terraform workspace select - It switches to the specified workspace. terraform workspace delete - It deletes the mentioned workspace.","title":"Workspace Commands"},{"location":"learning/terraform/#configuring-multiple-providers","text":"Below is the file structure that you will maintain for working with multiple providers. multiple_providers | |___awsmain.tf | |___azuremain.tf | |___vars.tf | |___out.tf Create an S3 bucket in Azure which helps in storing the file in AWS. awsmain.tf resource \"aws_s3_bucket\" \"b\" { bucket = \"my-tf-test-bucket-21\" acl = \"private\" tags = { Name = \"My bucket test\" Environment = \"Dev\" } } Resource has two parameters: name(aws_s3_bucket) and type(\"b\"). Name of the resource may vary from provider to provider. Type of the resource depends on the user. Bucket indicates the name of the bucket. If it is not assigned, terraform assigns a random unique name. acl (access control list) - It helps to manage access to the buckets and objects. tag - It is a label which is assigned to the AWS resource by you or AWS. Environment - On which environment do you like to deploy S3 bucket (Dev, Prod or Stage). azuremain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"storageacct21\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"LRS\" account_tier = \"Standard\" } Parameters name - It indicates the name of the storage account. resource_group_name and location- name of the resource group and location. account_replication_type - Type of the account replication. account_tier - Account tier type out.tf output \"account_tier\" { value = \"azurerm_storage_Account.storagecct.account_tier\" }","title":"Configuring Multiple Providers"},{"location":"learning/vagrant/","text":"Vagrant is not able to run on Windows 10 + WLS2 Windows setup vagrant plugin install vagrant-vbguest Image should have guest additions or it will not share folder from windows inside the VM Internal Network Go to VirtualBox \u2192 File \u2192 Host Network Manager \u2192 Check the enabled network DHCP address Windows Features Turn On Off Disable \"virtual machine platform\" and \"windows hypervisor platform Installation Install same version of Vagrant in Windows and WSL Verify vagrant --version in both to match In windows try downloading a box and start vagrant up --provider=virtualbox Vagrant Init with a image in vagrant cloud vagrant init hashicorp/precise64 Start the vm vagrant up SSH into the vm vagrant ssh Hibernate the vm vagrant suspend Check the status of vagrant vm vagrant status Stop the vm vagrant halt Clean up the vm vagrant destroy Get Status of Vagrant Machines on host vagrant global-status Get SSH Settings vagrant ssh-config Reload Virtual Machine vagrant reload Make sure the ssh key you created is stored parallel to your Vagrantfile before you execute the vagrant up command. Vagrant commands Managing Vagrant boxes Download a box to a machine vagrant box add ubuntu/trusty64 vagrant box add centos/8 List boxes on machine vagrant box list Update an existing box on a machine vagrant box outdated vagrant box update Run a downloaded box \u2192 cd into a folder vagrant init ubuntu/trusty64 vagrant up Remove a downloaded box from a machine vagrant box remove ubuntu/trusty64 Finding boxes vagrantboxes.es & vagrantcloud \u2192 find a box and copy the url vagrant box add vagrant init vagrant up Using Plugins List existing plugins vagrant plugin list Install Plugins vagrant plugin install vagrant-vbguest Update Plugin version vagrant plugin update vagrant-vbguest Update all plugins vagrant plugin update Remove Plugins vagrant plugin uninstall vagrant-vbguest Adding services to startup boot sudo chkconfig --add httpd sudo chkconfig httpd on sudo service httpd stop Create symbolic link which will serve file from local on vagrant machine, ensure index html file is there in local root cd /var/www/html cd .. && rm -rf html sudo ln -s /vagrant /var/www/html sudo service httpd start Packaging Vagrant after baking Imp that VM is running, check status vagrant status vagrant package --output .box vagrant box add .box Custom base box packaging after customization / hardening vagrant package --base Switching of guest additions checks if the plugin is available in local Add line in config config.vbguest.auto_update = false Adding a file from local machine not in the project folder to the vm config.vm.provision \"file\", source: \"~/vagrant/files/git-files\", destination: \"~/.gitconfig\" If VM is running when above provisioning is done, it is not reflected vagrant provision Adding software at provisioning config.vm.provision \"shell\", inline: \"yum install -y git nano\" Adding custom scripts not in the project folder to the vm config.vm.provision \"shell\", path: \"~/vagrant/scripts/provision.sh\" To restart vm sudo shutdown -r now Restart service sudo systemctl restart sshd.service Update centos kernal sudo yum update kernel* Check and delete old kernels rpm -qa kernel sudo package-cleanup --old-kernels --count=2 Debugging Vagrant During Vagrant Up your Windows system tries to connect to SSH. If you type on your command line: set VAGRANT_LOG=INFO Debug SSH set VAGRANT_PREFER_SYSTEM_BIN=0 vagrant ssh --debug","title":"Vagrant"},{"location":"learning/vagrant/#windows-setup","text":"vagrant plugin install vagrant-vbguest Image should have guest additions or it will not share folder from windows inside the VM","title":"Windows setup"},{"location":"learning/vagrant/#internal-network","text":"Go to VirtualBox \u2192 File \u2192 Host Network Manager \u2192 Check the enabled network DHCP address","title":"Internal Network"},{"location":"learning/vagrant/#windows-features-turn-on-off","text":"Disable \"virtual machine platform\" and \"windows hypervisor platform","title":"Windows Features Turn On Off"},{"location":"learning/vagrant/#installation","text":"Install same version of Vagrant in Windows and WSL Verify vagrant --version in both to match In windows try downloading a box and start vagrant up --provider=virtualbox","title":"Installation"},{"location":"learning/vagrant/#vagrant","text":"","title":"Vagrant"},{"location":"learning/vagrant/#init-with-a-image-in-vagrant-cloud","text":"vagrant init hashicorp/precise64","title":"Init with a image in vagrant cloud"},{"location":"learning/vagrant/#start-the-vm","text":"vagrant up","title":"Start the vm"},{"location":"learning/vagrant/#ssh-into-the-vm","text":"vagrant ssh","title":"SSH into the vm"},{"location":"learning/vagrant/#hibernate-the-vm","text":"vagrant suspend","title":"Hibernate the vm"},{"location":"learning/vagrant/#check-the-status-of-vagrant-vm","text":"vagrant status","title":"Check the status of vagrant vm"},{"location":"learning/vagrant/#stop-the-vm","text":"vagrant halt","title":"Stop the vm"},{"location":"learning/vagrant/#clean-up-the-vm","text":"vagrant destroy Get Status of Vagrant Machines on host vagrant global-status Get SSH Settings vagrant ssh-config Reload Virtual Machine vagrant reload Make sure the ssh key you created is stored parallel to your Vagrantfile before you execute the vagrant up command. Vagrant commands","title":"Clean up the vm"},{"location":"learning/vagrant/#managing-vagrant-boxes","text":"","title":"Managing Vagrant boxes"},{"location":"learning/vagrant/#download-a-box-to-a-machine","text":"vagrant box add ubuntu/trusty64 vagrant box add centos/8","title":"Download a box to a machine"},{"location":"learning/vagrant/#list-boxes-on-machine","text":"vagrant box list","title":"List boxes on machine"},{"location":"learning/vagrant/#update-an-existing-box-on-a-machine","text":"vagrant box outdated vagrant box update","title":"Update an existing box on a machine"},{"location":"learning/vagrant/#run-a-downloaded-box-cd-into-a-folder","text":"vagrant init ubuntu/trusty64 vagrant up","title":"Run a downloaded box --&gt; cd into a folder"},{"location":"learning/vagrant/#remove-a-downloaded-box-from-a-machine","text":"vagrant box remove ubuntu/trusty64","title":"Remove a downloaded box  from a machine"},{"location":"learning/vagrant/#finding-boxes","text":"vagrantboxes.es & vagrantcloud \u2192 find a box and copy the url vagrant box add vagrant init vagrant up","title":"Finding boxes"},{"location":"learning/vagrant/#using-plugins","text":"","title":"Using Plugins"},{"location":"learning/vagrant/#list-existing-plugins","text":"vagrant plugin list","title":"List existing plugins"},{"location":"learning/vagrant/#install-plugins","text":"vagrant plugin install vagrant-vbguest","title":"Install Plugins"},{"location":"learning/vagrant/#update-plugin-version","text":"vagrant plugin update vagrant-vbguest","title":"Update Plugin version"},{"location":"learning/vagrant/#update-all-plugins","text":"vagrant plugin update","title":"Update all plugins"},{"location":"learning/vagrant/#remove-plugins","text":"vagrant plugin uninstall vagrant-vbguest","title":"Remove Plugins"},{"location":"learning/vagrant/#adding-services-to-startup-boot","text":"sudo chkconfig --add httpd sudo chkconfig httpd on sudo service httpd stop","title":"Adding services to startup boot"},{"location":"learning/vagrant/#create-symbolic-link-which-will-serve-file-from-local-on-vagrant-machine-ensure-index-html-file-is-there-in-local-root","text":"cd /var/www/html cd .. && rm -rf html sudo ln -s /vagrant /var/www/html sudo service httpd start","title":"Create symbolic link which will serve file from local on vagrant machine, ensure index html file is there in local root"},{"location":"learning/vagrant/#packaging-vagrant-after-baking","text":"","title":"Packaging Vagrant after baking"},{"location":"learning/vagrant/#imp-that-vm-is-running-check-status","text":"vagrant status vagrant package --output .box vagrant box add .box","title":"Imp that VM is running, check status"},{"location":"learning/vagrant/#custom-base-box-packaging-after-customization-hardening","text":"vagrant package --base","title":"Custom base box packaging after customization / hardening"},{"location":"learning/vagrant/#switching-of-guest-additions-checks-if-the-plugin-is-available-in-local","text":"","title":"Switching of guest additions checks if the plugin is available in local"},{"location":"learning/vagrant/#add-line-in-config","text":"config.vbguest.auto_update = false","title":"Add line in config"},{"location":"learning/vagrant/#adding-a-file-from-local-machine-not-in-the-project-folder-to-the-vm","text":"config.vm.provision \"file\", source: \"~/vagrant/files/git-files\", destination: \"~/.gitconfig\"","title":"Adding a file from local machine not in the project folder to the vm"},{"location":"learning/vagrant/#if-vm-is-running-when-above-provisioning-is-done-it-is-not-reflected","text":"vagrant provision","title":"If VM is running when above provisioning is done, it is not reflected"},{"location":"learning/vagrant/#adding-software-at-provisioning","text":"config.vm.provision \"shell\", inline: \"yum install -y git nano\"","title":"Adding software at provisioning"},{"location":"learning/vagrant/#adding-custom-scripts-not-in-the-project-folder-to-the-vm","text":"config.vm.provision \"shell\", path: \"~/vagrant/scripts/provision.sh\"","title":"Adding custom scripts not in the project folder to the vm"},{"location":"learning/vagrant/#to-restart-vm","text":"sudo shutdown -r now","title":"To restart vm"},{"location":"learning/vagrant/#restart-service","text":"sudo systemctl restart sshd.service","title":"Restart service"},{"location":"learning/vagrant/#update-centos-kernal","text":"sudo yum update kernel*","title":"Update centos kernal"},{"location":"learning/vagrant/#check-and-delete-old-kernels","text":"rpm -qa kernel sudo package-cleanup --old-kernels --count=2","title":"Check and delete old kernels"},{"location":"learning/vagrant/#debugging-vagrant","text":"","title":"Debugging Vagrant"},{"location":"learning/vagrant/#during-vagrant-up-your-windows-system-tries-to-connect-to-ssh-if-you-type-on-your-command-line","text":"set VAGRANT_LOG=INFO","title":"During Vagrant Up your Windows system tries to connect to SSH. If you type on your command line:"},{"location":"learning/vagrant/#debug-ssh","text":"set VAGRANT_PREFER_SYSTEM_BIN=0 vagrant ssh --debug","title":"Debug SSH"},{"location":"learning/docker/docker-notes/","text":"Master list of Docker Resources and Projects Play With Docker , a great resource for web-based docker testing and also has a library of labs built by Docker Captains and others, and supported by Docker Inc. Play With Docker Labs Docker Cloud: CI/CD and Server Ops Docker Mastery Bret's Podcast Bret's Youtube Dockerfile Best Practice Docker Devops Docker Best practise inside a code repo Docker Certificated Associate Docker Shell Config Containers vs VM ebook Docker Cgroup and Namespaces Linux Package Management Basics Formatting Docker CLI Output Dockerfile Reference DNS Basics Docker Image Immutable Software 12 factor App 12 Fractured Apps Devops Roadmap Docker Storage YAML Spec Ref Card Docker secrets Healthcheck in Dockerfile Docker Registry Config Docker Registry Garbage Collection Docker Registry as cache Docker CLI to kubectl Docker and Pi Projects Starting container process caused \"exec: \\\"ping\\\": executable file not found in $PATH \" : unknown apt-get update && apt-get install -y iputils-ping Starting mysql container and running ps causes \"ps: command not found\" apt-get update && apt-get install procps Dockerfile Reference Creating and Using Containers Like a Boss Check Our Docker Install and Config docker version - verified cli can talk to engine docker info - most config values of engine Image vs. Container An Image is the application we want to run A Container is an instance of that image running as a process You can have many containers running off the same image Docker's default image \"registry\" is called Docker Hub docker container run --publish 80 :80 --detach --name webhost nginx docker container run -it - start new container interactively docker container exec -it - run additional command in existing container docker container ls -a docker container logs webhost Container VS. VM: It's Just a Process docker run --name mongo -d mongo docker container top - process list in one container docker stop mongo docker ps docker start mongo docker container inspect - details of one container config docker container stats - performance stats for all containers The Mighty Hub: Using Docker Hub Registry Images docker pull nginx docker image ls Images and Their Layers: Discover the Image Cache docker history nginx:latest docker image inspect nginx Image Tagging and Pushing to Docker Hub docker pull nginx:latest docker image ls docker image tag nginx bretfisher/nginx docker login cat .docker/config.json docker image push bretfisher/nginx docker image push bretfisher/nginx bretfisher/nginx:testing Getting a Shell Inside Containers: No Need for SSH docker container exec -it mysql -- bash docker container run -it alpine -- bash docker container run -it alpine -- sh Cleaning Docker images Use docker system df to see space usage. docker image prune to clean up just \"dangling\" images. The big one is usually docker image prune -a which will remove all images you're not using. docker volume prune to remove unused volumes docker system prune will clean up everything (Nuke everything that is not used currently). docker system prune -a wipe everything. Docker Networks: Concepts for Private and Public Comms in Containers Each container connected to a private virtual network \"bridge\" Each virtual network routes through NAT firewall on host IP All containers on a virtual network can talk to each other without -p Best practice is to create a new virtual network for each app: network \"my_web_app\" for mysql and php/apache containers network \"my_api\" for mongo and nodejs containers docker container run -p 80 :80 --name webhost -d nginx docker container port webhost docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost Docker Networks: CLI Management of Virtual Networks Show networks docker network ls Inspect a network docker network inspect Create a network docker network create --driver Attach a network to container docker network connect Detach a network from container docker network disconnect docker network ls docker network inspect bridge docker network create my_app_net docker container run -d --name new_nginx --network my_app_net nginx docker network inspect my_app_net docker network connect <new network id> <container id> docker container disconnect <new network id> <container id> Docker Networks: DNS and How Containers Find Each Other Create your apps so frontend/backend sit on same Docker network Their inter-communication never leaves host All externally exposed ports closed by default You must manually expose via -p , which is better default security! Containers shouldn't rely on IP's for inter-communication DNS for friendly names is built-in if you use custom networks docker container run -d --name my_nginx --network my_app_net nginx docker container exec -it my_nginx ping new_nginx docker container exec -it new_nginx ping my_nginx DNS Round Robin Testing docker network create dude docker container run -d --net dude --net-alias search elasticsearch:2 docker container ls docker container run --rm -- net dude alpine nslookup search docker container run --rm --net dude centos curl -s search:9200 Container Lifetime & Persistent Data: Volumes, Volumes, Volumes Persistent Data: Data Volumes Containers are usually immutable and ephemeral \"immutable infrastructure\": only re-deploy containers, never change This is the ideal scenario, but what about databases, or unique data? Docker gives us features to ensure these \"separation of concerns\" This is known as \"persistent data\" Two ways: Volumes and Bind Mounts Volumes : make special location outside of container UFS Bind Mounts : link container path to host path docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD = True -v mysql-db:/var/lib/mysql mysql docker volume ls docker volume inspect mysql-db Persistent Data: Bind Mounting Used for local development Usecase: When you are changing files on laptop which you want to serve in the app It can be run only during docker run as there is no explicit volume command in the dockerfile the volume will be mounted in the working directory of the container docker container run -d --name nginx -p 80 :80 -v $( pwd ) :/usr/share/nginx/html nginx docker container exec -it nginx -- bash cd /usr/share/nginx/html && ls -la docker log streaming docker container logs -f <container name> Database Passwords in Containers When running postgres now, you'll need to either set a password, or tell it to allow any connection (which was the default before this change). -you need to either set a password with the environment variable: POSTGRES_PASSWORD=mypasswd Or tell it to ignore passwords with the environment variable: POSTGRES_HOST_AUTH_METHOD=trust Making It Easier with Docker Compose: The Multi-Container Tool Why: configure relationships between containers Why: save our docker container run settings in easy-to-read file Why: create one-liner developer environment startups YAML-formatted file that describes our solution options for: containers, networks, volumes A CLI tool docker-compose used for local dev/test automation with those YAML files docker-compose.yml is default filename, but any can be used with docker-compose -f Not a production-grade tool but ideal for local development and test Two most common commands are: docker-compose up # setup volumes/networks and start all containers docker-compose down # stop all containers and remove cont/vol/net Trying Out Basic Compose Commands docker-compose up docker-compose up -d # Running compose in bacground docker-compose down docker-compose down -v --rmi local/all # Removes images and volumes # Compose operations docker-compose logs docker-compose ps docker-compose top docker-compose build # Build images or docker-compose up --build Swarm Intro and Creating a 3-Node Swarm Cluster Swarm Mode is a clustering solution built inside Docker Not enabled by default docker swarm init: What Just Happened? Lots of PKI and security automation Root Signing Certificate created for our Swarm Certificate is issued for first Manager node Join tokens are created Raft database created to store root CA, configs and secrets Encrypted by default on disk (1.13+) No need for another key/value system to hold orchestration/secrets Replicates logs amongst Managers via mutual TLS in \"control plane\" Create Your First Service and Scale it Locally docker info # swarm is down by default docker swarm init # start swarm docker node ls docker service create alpine ping 8 .8.8.8 # creates service frosty_newton docker service ls docker service ps frosty_newton docker container ls docker service update frosty_newton --replicas 3 # creates 3 replicas docker service ls docker service rm frosty_newton # deletes the service docker service ls docker container ls Creating a 3-Node Swarm Cluster docker-machine + VirtualBox - Free and runs locally, but requires a machine with 8GB memory docker-machine create node1 docker-machine ssh node1 docker-machine env node1 docker swarm init docker swarm init --advertise-addr node1 docker node ls docker node update --role manager node2 # Update role to existing node docker swarm join-token manager # Shows join token for manager role docker service create --replicas 3 alpine ping 8 .8.8.8 # Creates service with 3 replicas and starts ping process docker service ls docker service ps <service name> docker node ps docker node ps node2 Scaling Out with Overlay Networking # Create Backend network docker network create --driver overlay mydrupal docker network ls docker service create --name psql --netowrk mydrupal -e POSTGRES_PASSWORD = mypass postgres docker service ls docker service ps psql docker container logs psql <container name> # Create Frontend network docker service create --name drupal --network mydrupal -p 80 :80 drupal docker service inspect drupal Scaling Out with Routing Mesh docker service create --name search --replicas 3 -p 9200 :9200 elasticsearch:2 docker service ps search Create a Multi-Service Multi-Node Web App docker network create -d overlay backend docker network create -d overlay frontend docker service create --name vote -p 80 :80 --network frontend \\ -- replica 2 dockersamples/examplevotingapp_vote:before docker service create --name redis --network frontend \\ --replica 1 redis:3.2 docker service create --name worker --network frontend --network backend dockersamples/examplevotingapp_worker docker service create --name db --network backend \\ --mount type = volume,source = db-data,target = /var/lib/postgresql/data postgres:9.4 docker service create --name result --network backend -p 5001 :80 COPY INFO docker service ls docker service logs worker Swarm Stacks and Production Grade Compose Docker adds a new layer of abstraction to Swarm called Stacks Stacks accept Compose files as their declarative definition for services, networks, and volumes We use docker stack deploy rather then docker service create Stacks manages all those objects for us, including overlay network per stack. Adds stack name to start of their name Compose now ignores deploy: , Swarm ignores build: docker stack deploy -c example-voting-app-stack.yml voteapp docker stack ls docker stack services voteapp docker stack ps voteapp Using Secrets in Swarm Services What is a Secret? - Usernames and passwords - TLS certificates and keys - SSH keys - Any data you would prefer not be \"on front page of news\" docker secret create psql_usr psql_usr.txt echo \"myDBpassWORD\" | docker secret create psql_pass - TAB COMPLETION docker secret inspect psql_usr docker service create --name psql --secret psql_user \\ --secret psql_pass -e POSTGRES_PASSWORD_FILE = /run/secrets/psql_pass \\ -e POSTGRES_USER_FILE = /run/secrets/psql_user postgres docker exec -it <container name> bash cat /run/secrets/psql_user Swarm App Lifecycle Full App Lifecycle: Dev, Build and Deploy With a Single Compose Design Single set of Compose files for: - Local docker-compose up development environment - Remote docker-compose up CI environment - Remote docker stack deploy production environment docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d docker-compose -f docker-compose.yml -f docker-compose.prod.yml config Service Updates: Changing Things In Flight Provides rolling replacement of tasks/containers in a service Limits downtime (be careful with \"prevents\" downtime) Will replace containers for most changes Has many, many cli options to control the update Create options will usually change, adding -add or -rm to them Includes rollback and healthcheck options Also has scale & rollback subcommand for quicker access docker service scale web=4 and docker service rollback web Just update the image used to a newer version docker service update --image myapp:1.2.1 <servicename> Adding an environment variable and remove a port docker service update --env-add NODE_ENV=production --publish-rm 8080 Change number of replicas of two services docker service scale web=8 api=6 docker service create -p 8088 :80 --name web nginx:1.13.7 docker service scale web = 5 docker service update --image nginx:1.13.6 web docker service update --publish-rm 8088 --publish-add 9090 :80 docker service update --force web # forces rebalancing of the service without changing anything docker service rm web Healthchecks in Dockerfiles HEALTHCHECK was added in 1.12 Supported in Dockerfile, Compose YAML, docker run, and Swarm Services Docker engine will exec's the command in the container (e.g. curl localhost) It expects exit 0 (OK) or exit 1 (Error) Three container states: starting, healthy, unhealthy Much better then \"is binary still running?\" Options for healthcheck command --interval = DURATION ( default: 30s ) --timeout = DURATION ( default: 30s ) --start-period = DURATION ( default: 0s ) ( 17 .09+ ) --retries = N ( default: 3 ) docker container run --name p2 -d --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres docker service create --name p2 --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres Container Registries: Image Storage and Distribution Run a Private Docker Registry Secure your Registry with TLS Storage cleanup via Garbage Collection Enable Hub caching via \"--registry-mirror\" # Run the registry image docker container run -d -p 5000 :5000 --name registry registry # Re-tag an existing image and push it to your new registry docker pull hello-world docker run hello-world docker tag hello-world 127 .0.0.1:5000/hello-world docker push 127 .0.0.1:5000/hello-world # Remove that image from local cache and pull it from new registry docker image remove hello-world docker image remove 127 .0.0.1:5000/hello-world docker pull 127 .0.0.1:5000/hello-world:latest # Re-create registry using a bind mount and see how it stores data docker container kill registry docker container rm registry docker container run -d -p 5000 :5000 --name registry -v $( pwd ) /registry-data:/var/lib/registry registry Using Docker Registry With Swarm docker node ls docker service create --name registry --publish 5000 :5000 registry docker service ps registry docker pull nginx docker tag nginx 127 .0.0.1:5000/nginx docker push 127 .0.0.1:5000/nginx docker service create --name nginx -p 80 :80 --replicas 5 --detach = false 127 .0.0.1:5000/nginx docker service ps nginx Using Docker in Production Focus on Dockerfiles first. Study ENTRYPOINT of Hub official images. Use it for config of images before CMD is executed. use ENTRYPOINT to set default values for all environments and then overide using ENV values. EntryPoint vs CMD FROM official distros. Make it == start, log all things in stdout/stderr, documented in file, lean and scale. Using SaaS for - Image Registry, Logging, Monitoring, Look at CNCF Landscape Using Layer 7 Reverse Proxy if port 80 and 443 are used by multiple apps Docker Security Docker Security Checklist Docker Engine Security Docker Security Tools Seccomp App Armor Docker Bench CIS Docker checklist Running Docker as non root user # Creating non root user in alpine RUN addgroup -g 1000 node \\ && adduser -u 1000 -G node -s /bin/sh -D node # Creating non root user in stretch RUN groupadd --gid 1000 node \\ && useradd --uid 1000 --gid node --shell /bin/bash --create-home node Sample Dockerfile with USER User Namespaces Shift Left Security Trivy - Image Scanning Sysdig Falco Appamror Profiles Seccomp Profile Docker Context Start a node on paly with Docker Copy the IP of the node Set the Docker Context with the Host Name of the node and port 2375 Contexts are created in the home folder of user called .docker/context docker context create --docker \"host=tcp://<Host Name>:2375\" <context-name> docker context ls docker context use <context-name> docker ps # Should show the new context of play with docker # Overriding Context to default in commandline docker -c default ps docker -c <context-name> ps # Looping through all the context and executing ps for c in ` docker context ls -q ` ; do ` docker -c $c ps ` ; done # Creates the image in all context for c in ` docker context ls -q ` ; do ` docker -c $c run hello-world ` ; done Recommendations To change permissions on file system (chown or chmod) use a Entrypoint script. Look up to official images for examples for Entrypoint One App or Website use one container, specially if using an orchestrator like K8s or Docker Swarm. Scaling is also a benefit due to one-one relationship. Changing Docker IP range Use Cloud DB as service instead of in containers Run one process per container Strict Separation of Config from Code. Use Env variables to achieve this. Using Development workflow in Compose Write all the ENV variables at the top of Dockerfile Using Env variables in Dockerfile Override Env variables in Docker Compose file say for Dev testing Using Env variables in Docker Entrypoint to write into Application config files during start up. Secrets and Application specific config goes into specific ENV var blocks. Tis can be changed. Defaults or data specific to SERVER or LANGUAGE goes to another ENV block and can be kept static. This avoids them being set for each ENV. Encrypting traffic for local development use Lets Encrypt ad store them in .cert folder in Home Directory. Encrypting traffic for production use Lets Encrypt and maybe Traefik as Front proxy. See example using Swarm COPY vs ADD. Use COPY to copy artefacts in the same repo to the image. Use ADD when you want to download something from the Internet or to untar or unzip. You can also replace using wget statements with ADD. Combine multiple RUN into a single statement. Delete packages which are downloaded and installed also in a single command to save image size. No secrets like configs, certificates should be saved in Image. Pass them during runtime. Always have a CMD in the image, even if its inheriting it from BASE image Version apt packages and BASE images Use multistage Dokcer builds to have Dev dependencies and Prod dependencies separate. Have healthchecks in K8s instead of Dockerfile Use DNS RoundRobin for Database inside Compose file so it switches of Virtual IP on the Overlay network and gives direct access from FrontEnd Service to Backend container. Setting resource limits inside Compose file DRY your compose files using templates","title":"Docker"},{"location":"learning/docker/docker-notes/#creating-and-using-containers-like-a-boss","text":"","title":"Creating and Using Containers Like a Boss"},{"location":"learning/docker/docker-notes/#check-our-docker-install-and-config","text":"docker version - verified cli can talk to engine docker info - most config values of engine","title":"Check Our Docker Install and Config"},{"location":"learning/docker/docker-notes/#image-vs-container","text":"An Image is the application we want to run A Container is an instance of that image running as a process You can have many containers running off the same image Docker's default image \"registry\" is called Docker Hub docker container run --publish 80 :80 --detach --name webhost nginx docker container run -it - start new container interactively docker container exec -it - run additional command in existing container docker container ls -a docker container logs webhost","title":"Image vs. Container"},{"location":"learning/docker/docker-notes/#container-vs-vm-its-just-a-process","text":"docker run --name mongo -d mongo docker container top - process list in one container docker stop mongo docker ps docker start mongo docker container inspect - details of one container config docker container stats - performance stats for all containers","title":"Container VS. VM: It's Just a Process"},{"location":"learning/docker/docker-notes/#the-mighty-hub-using-docker-hub-registry-images","text":"docker pull nginx docker image ls","title":"The Mighty Hub: Using Docker Hub Registry Images"},{"location":"learning/docker/docker-notes/#images-and-their-layers-discover-the-image-cache","text":"docker history nginx:latest docker image inspect nginx","title":"Images and Their Layers: Discover the Image Cache"},{"location":"learning/docker/docker-notes/#image-tagging-and-pushing-to-docker-hub","text":"docker pull nginx:latest docker image ls docker image tag nginx bretfisher/nginx docker login cat .docker/config.json docker image push bretfisher/nginx docker image push bretfisher/nginx bretfisher/nginx:testing","title":"Image Tagging and Pushing to Docker Hub"},{"location":"learning/docker/docker-notes/#getting-a-shell-inside-containers-no-need-for-ssh","text":"docker container exec -it mysql -- bash docker container run -it alpine -- bash docker container run -it alpine -- sh","title":"Getting a Shell Inside Containers: No Need for SSH"},{"location":"learning/docker/docker-notes/#cleaning-docker-images","text":"Use docker system df to see space usage. docker image prune to clean up just \"dangling\" images. The big one is usually docker image prune -a which will remove all images you're not using. docker volume prune to remove unused volumes docker system prune will clean up everything (Nuke everything that is not used currently). docker system prune -a wipe everything.","title":"Cleaning Docker images"},{"location":"learning/docker/docker-notes/#docker-networks-concepts-for-private-and-public-comms-in-containers","text":"Each container connected to a private virtual network \"bridge\" Each virtual network routes through NAT firewall on host IP All containers on a virtual network can talk to each other without -p Best practice is to create a new virtual network for each app: network \"my_web_app\" for mysql and php/apache containers network \"my_api\" for mongo and nodejs containers docker container run -p 80 :80 --name webhost -d nginx docker container port webhost docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost","title":"Docker Networks: Concepts for Private and Public Comms in Containers"},{"location":"learning/docker/docker-notes/#docker-networks-cli-management-of-virtual-networks","text":"Show networks docker network ls Inspect a network docker network inspect Create a network docker network create --driver Attach a network to container docker network connect Detach a network from container docker network disconnect docker network ls docker network inspect bridge docker network create my_app_net docker container run -d --name new_nginx --network my_app_net nginx docker network inspect my_app_net docker network connect <new network id> <container id> docker container disconnect <new network id> <container id>","title":"Docker Networks: CLI Management of Virtual Networks"},{"location":"learning/docker/docker-notes/#docker-networks-dns-and-how-containers-find-each-other","text":"Create your apps so frontend/backend sit on same Docker network Their inter-communication never leaves host All externally exposed ports closed by default You must manually expose via -p , which is better default security! Containers shouldn't rely on IP's for inter-communication DNS for friendly names is built-in if you use custom networks docker container run -d --name my_nginx --network my_app_net nginx docker container exec -it my_nginx ping new_nginx docker container exec -it new_nginx ping my_nginx","title":"Docker Networks: DNS and How Containers Find Each Other"},{"location":"learning/docker/docker-notes/#dns-round-robin-testing","text":"docker network create dude docker container run -d --net dude --net-alias search elasticsearch:2 docker container ls docker container run --rm -- net dude alpine nslookup search docker container run --rm --net dude centos curl -s search:9200","title":"DNS Round Robin Testing"},{"location":"learning/docker/docker-notes/#container-lifetime-persistent-data-volumes-volumes-volumes","text":"","title":"Container Lifetime &amp; Persistent Data: Volumes, Volumes, Volumes"},{"location":"learning/docker/docker-notes/#persistent-data-data-volumes","text":"Containers are usually immutable and ephemeral \"immutable infrastructure\": only re-deploy containers, never change This is the ideal scenario, but what about databases, or unique data? Docker gives us features to ensure these \"separation of concerns\" This is known as \"persistent data\" Two ways: Volumes and Bind Mounts Volumes : make special location outside of container UFS Bind Mounts : link container path to host path docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD = True -v mysql-db:/var/lib/mysql mysql docker volume ls docker volume inspect mysql-db","title":"Persistent Data: Data Volumes"},{"location":"learning/docker/docker-notes/#persistent-data-bind-mounting","text":"Used for local development Usecase: When you are changing files on laptop which you want to serve in the app It can be run only during docker run as there is no explicit volume command in the dockerfile the volume will be mounted in the working directory of the container docker container run -d --name nginx -p 80 :80 -v $( pwd ) :/usr/share/nginx/html nginx docker container exec -it nginx -- bash cd /usr/share/nginx/html && ls -la docker log streaming docker container logs -f <container name>","title":"Persistent Data: Bind Mounting"},{"location":"learning/docker/docker-notes/#database-passwords-in-containers","text":"When running postgres now, you'll need to either set a password, or tell it to allow any connection (which was the default before this change). -you need to either set a password with the environment variable: POSTGRES_PASSWORD=mypasswd Or tell it to ignore passwords with the environment variable: POSTGRES_HOST_AUTH_METHOD=trust","title":"Database Passwords in Containers"},{"location":"learning/docker/docker-notes/#making-it-easier-with-docker-compose-the-multi-container-tool","text":"Why: configure relationships between containers Why: save our docker container run settings in easy-to-read file Why: create one-liner developer environment startups YAML-formatted file that describes our solution options for: containers, networks, volumes A CLI tool docker-compose used for local dev/test automation with those YAML files docker-compose.yml is default filename, but any can be used with docker-compose -f Not a production-grade tool but ideal for local development and test Two most common commands are: docker-compose up # setup volumes/networks and start all containers docker-compose down # stop all containers and remove cont/vol/net","title":"Making It Easier with Docker Compose: The Multi-Container Tool"},{"location":"learning/docker/docker-notes/#trying-out-basic-compose-commands","text":"docker-compose up docker-compose up -d # Running compose in bacground docker-compose down docker-compose down -v --rmi local/all # Removes images and volumes # Compose operations docker-compose logs docker-compose ps docker-compose top docker-compose build # Build images or docker-compose up --build","title":"Trying Out Basic Compose Commands"},{"location":"learning/docker/docker-notes/#swarm-intro-and-creating-a-3-node-swarm-cluster","text":"Swarm Mode is a clustering solution built inside Docker Not enabled by default","title":"Swarm Intro and Creating a 3-Node Swarm Cluster"},{"location":"learning/docker/docker-notes/#docker-swarm-init-what-just-happened","text":"Lots of PKI and security automation Root Signing Certificate created for our Swarm Certificate is issued for first Manager node Join tokens are created Raft database created to store root CA, configs and secrets Encrypted by default on disk (1.13+) No need for another key/value system to hold orchestration/secrets Replicates logs amongst Managers via mutual TLS in \"control plane\"","title":"docker swarm init: What Just Happened?"},{"location":"learning/docker/docker-notes/#create-your-first-service-and-scale-it-locally","text":"docker info # swarm is down by default docker swarm init # start swarm docker node ls docker service create alpine ping 8 .8.8.8 # creates service frosty_newton docker service ls docker service ps frosty_newton docker container ls docker service update frosty_newton --replicas 3 # creates 3 replicas docker service ls docker service rm frosty_newton # deletes the service docker service ls docker container ls","title":"Create Your First Service and Scale it Locally"},{"location":"learning/docker/docker-notes/#creating-a-3-node-swarm-cluster","text":"docker-machine + VirtualBox - Free and runs locally, but requires a machine with 8GB memory docker-machine create node1 docker-machine ssh node1 docker-machine env node1 docker swarm init docker swarm init --advertise-addr node1 docker node ls docker node update --role manager node2 # Update role to existing node docker swarm join-token manager # Shows join token for manager role docker service create --replicas 3 alpine ping 8 .8.8.8 # Creates service with 3 replicas and starts ping process docker service ls docker service ps <service name> docker node ps docker node ps node2","title":"Creating a 3-Node Swarm Cluster"},{"location":"learning/docker/docker-notes/#scaling-out-with-overlay-networking","text":"# Create Backend network docker network create --driver overlay mydrupal docker network ls docker service create --name psql --netowrk mydrupal -e POSTGRES_PASSWORD = mypass postgres docker service ls docker service ps psql docker container logs psql <container name> # Create Frontend network docker service create --name drupal --network mydrupal -p 80 :80 drupal docker service inspect drupal","title":"Scaling Out with Overlay Networking"},{"location":"learning/docker/docker-notes/#scaling-out-with-routing-mesh","text":"docker service create --name search --replicas 3 -p 9200 :9200 elasticsearch:2 docker service ps search","title":"Scaling Out with Routing Mesh"},{"location":"learning/docker/docker-notes/#create-a-multi-service-multi-node-web-app","text":"docker network create -d overlay backend docker network create -d overlay frontend docker service create --name vote -p 80 :80 --network frontend \\ -- replica 2 dockersamples/examplevotingapp_vote:before docker service create --name redis --network frontend \\ --replica 1 redis:3.2 docker service create --name worker --network frontend --network backend dockersamples/examplevotingapp_worker docker service create --name db --network backend \\ --mount type = volume,source = db-data,target = /var/lib/postgresql/data postgres:9.4 docker service create --name result --network backend -p 5001 :80 COPY INFO docker service ls docker service logs worker","title":"Create a Multi-Service Multi-Node Web App"},{"location":"learning/docker/docker-notes/#swarm-stacks-and-production-grade-compose","text":"Docker adds a new layer of abstraction to Swarm called Stacks Stacks accept Compose files as their declarative definition for services, networks, and volumes We use docker stack deploy rather then docker service create Stacks manages all those objects for us, including overlay network per stack. Adds stack name to start of their name Compose now ignores deploy: , Swarm ignores build: docker stack deploy -c example-voting-app-stack.yml voteapp docker stack ls docker stack services voteapp docker stack ps voteapp","title":"Swarm Stacks and Production Grade Compose"},{"location":"learning/docker/docker-notes/#using-secrets-in-swarm-services","text":"What is a Secret? - Usernames and passwords - TLS certificates and keys - SSH keys - Any data you would prefer not be \"on front page of news\" docker secret create psql_usr psql_usr.txt echo \"myDBpassWORD\" | docker secret create psql_pass - TAB COMPLETION docker secret inspect psql_usr docker service create --name psql --secret psql_user \\ --secret psql_pass -e POSTGRES_PASSWORD_FILE = /run/secrets/psql_pass \\ -e POSTGRES_USER_FILE = /run/secrets/psql_user postgres docker exec -it <container name> bash cat /run/secrets/psql_user","title":"Using Secrets in Swarm Services"},{"location":"learning/docker/docker-notes/#swarm-app-lifecycle","text":"","title":"Swarm App Lifecycle"},{"location":"learning/docker/docker-notes/#full-app-lifecycle-dev-build-and-deploy-with-a-single-compose-design","text":"Single set of Compose files for: - Local docker-compose up development environment - Remote docker-compose up CI environment - Remote docker stack deploy production environment docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d docker-compose -f docker-compose.yml -f docker-compose.prod.yml config","title":"Full App Lifecycle: Dev, Build and Deploy With a Single Compose Design"},{"location":"learning/docker/docker-notes/#service-updates-changing-things-in-flight","text":"Provides rolling replacement of tasks/containers in a service Limits downtime (be careful with \"prevents\" downtime) Will replace containers for most changes Has many, many cli options to control the update Create options will usually change, adding -add or -rm to them Includes rollback and healthcheck options Also has scale & rollback subcommand for quicker access docker service scale web=4 and docker service rollback web Just update the image used to a newer version docker service update --image myapp:1.2.1 <servicename> Adding an environment variable and remove a port docker service update --env-add NODE_ENV=production --publish-rm 8080 Change number of replicas of two services docker service scale web=8 api=6 docker service create -p 8088 :80 --name web nginx:1.13.7 docker service scale web = 5 docker service update --image nginx:1.13.6 web docker service update --publish-rm 8088 --publish-add 9090 :80 docker service update --force web # forces rebalancing of the service without changing anything docker service rm web","title":"Service Updates: Changing Things In Flight"},{"location":"learning/docker/docker-notes/#healthchecks-in-dockerfiles","text":"HEALTHCHECK was added in 1.12 Supported in Dockerfile, Compose YAML, docker run, and Swarm Services Docker engine will exec's the command in the container (e.g. curl localhost) It expects exit 0 (OK) or exit 1 (Error) Three container states: starting, healthy, unhealthy Much better then \"is binary still running?\" Options for healthcheck command --interval = DURATION ( default: 30s ) --timeout = DURATION ( default: 30s ) --start-period = DURATION ( default: 0s ) ( 17 .09+ ) --retries = N ( default: 3 ) docker container run --name p2 -d --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres docker service create --name p2 --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres","title":"Healthchecks in Dockerfiles"},{"location":"learning/docker/docker-notes/#container-registries-image-storage-and-distribution","text":"","title":"Container Registries: Image Storage and Distribution"},{"location":"learning/docker/docker-notes/#run-a-private-docker-registry","text":"Secure your Registry with TLS Storage cleanup via Garbage Collection Enable Hub caching via \"--registry-mirror\" # Run the registry image docker container run -d -p 5000 :5000 --name registry registry # Re-tag an existing image and push it to your new registry docker pull hello-world docker run hello-world docker tag hello-world 127 .0.0.1:5000/hello-world docker push 127 .0.0.1:5000/hello-world # Remove that image from local cache and pull it from new registry docker image remove hello-world docker image remove 127 .0.0.1:5000/hello-world docker pull 127 .0.0.1:5000/hello-world:latest # Re-create registry using a bind mount and see how it stores data docker container kill registry docker container rm registry docker container run -d -p 5000 :5000 --name registry -v $( pwd ) /registry-data:/var/lib/registry registry","title":"Run a Private Docker Registry"},{"location":"learning/docker/docker-notes/#using-docker-registry-with-swarm","text":"docker node ls docker service create --name registry --publish 5000 :5000 registry docker service ps registry docker pull nginx docker tag nginx 127 .0.0.1:5000/nginx docker push 127 .0.0.1:5000/nginx docker service create --name nginx -p 80 :80 --replicas 5 --detach = false 127 .0.0.1:5000/nginx docker service ps nginx","title":"Using Docker Registry With Swarm"},{"location":"learning/docker/docker-notes/#using-docker-in-production","text":"Focus on Dockerfiles first. Study ENTRYPOINT of Hub official images. Use it for config of images before CMD is executed. use ENTRYPOINT to set default values for all environments and then overide using ENV values. EntryPoint vs CMD FROM official distros. Make it == start, log all things in stdout/stderr, documented in file, lean and scale. Using SaaS for - Image Registry, Logging, Monitoring, Look at CNCF Landscape Using Layer 7 Reverse Proxy if port 80 and 443 are used by multiple apps","title":"Using Docker in Production"},{"location":"learning/docker/docker-notes/#docker-security","text":"Docker Security Checklist Docker Engine Security Docker Security Tools Seccomp App Armor Docker Bench CIS Docker checklist Running Docker as non root user # Creating non root user in alpine RUN addgroup -g 1000 node \\ && adduser -u 1000 -G node -s /bin/sh -D node # Creating non root user in stretch RUN groupadd --gid 1000 node \\ && useradd --uid 1000 --gid node --shell /bin/bash --create-home node Sample Dockerfile with USER User Namespaces Shift Left Security Trivy - Image Scanning Sysdig Falco Appamror Profiles Seccomp Profile","title":"Docker Security"},{"location":"learning/docker/docker-notes/#docker-context","text":"Start a node on paly with Docker Copy the IP of the node Set the Docker Context with the Host Name of the node and port 2375 Contexts are created in the home folder of user called .docker/context docker context create --docker \"host=tcp://<Host Name>:2375\" <context-name> docker context ls docker context use <context-name> docker ps # Should show the new context of play with docker # Overriding Context to default in commandline docker -c default ps docker -c <context-name> ps # Looping through all the context and executing ps for c in ` docker context ls -q ` ; do ` docker -c $c ps ` ; done # Creates the image in all context for c in ` docker context ls -q ` ; do ` docker -c $c run hello-world ` ; done","title":"Docker Context"},{"location":"learning/docker/docker-notes/#recommendations","text":"To change permissions on file system (chown or chmod) use a Entrypoint script. Look up to official images for examples for Entrypoint One App or Website use one container, specially if using an orchestrator like K8s or Docker Swarm. Scaling is also a benefit due to one-one relationship. Changing Docker IP range Use Cloud DB as service instead of in containers Run one process per container Strict Separation of Config from Code. Use Env variables to achieve this. Using Development workflow in Compose Write all the ENV variables at the top of Dockerfile Using Env variables in Dockerfile Override Env variables in Docker Compose file say for Dev testing Using Env variables in Docker Entrypoint to write into Application config files during start up. Secrets and Application specific config goes into specific ENV var blocks. Tis can be changed. Defaults or data specific to SERVER or LANGUAGE goes to another ENV block and can be kept static. This avoids them being set for each ENV. Encrypting traffic for local development use Lets Encrypt ad store them in .cert folder in Home Directory. Encrypting traffic for production use Lets Encrypt and maybe Traefik as Front proxy. See example using Swarm COPY vs ADD. Use COPY to copy artefacts in the same repo to the image. Use ADD when you want to download something from the Internet or to untar or unzip. You can also replace using wget statements with ADD. Combine multiple RUN into a single statement. Delete packages which are downloaded and installed also in a single command to save image size. No secrets like configs, certificates should be saved in Image. Pass them during runtime. Always have a CMD in the image, even if its inheriting it from BASE image Version apt packages and BASE images Use multistage Dokcer builds to have Dev dependencies and Prod dependencies separate. Have healthchecks in K8s instead of Dockerfile Use DNS RoundRobin for Database inside Compose file so it switches of Virtual IP on the Overlay network and gives direct access from FrontEnd Service to Backend container. Setting resource limits inside Compose file DRY your compose files using templates","title":"Recommendations"},{"location":"learning/k8s/ks8-notes/","text":"Tailing logs from multiple containers on laptop K8s Tutorials K8s DNS Kubectl Usage Convention K8s API Reference Operator Hub Awesome Operator List Why Kubernetes Orchestration: Next logical step in journey to faster DevOps First, understand why you may need orchestration Not every solution needs orchestration Servers + Change Rate = Benefit of orchestration K8s Learning Resources Play with k8s Katacoda Install Kubernetes Linux - Microk8s Install SNAP first using apt-get sudo snap install microk8s --classic --channel = 1 .17/stable # Install specific k8s version microk8s.enable dns # Enbale DNS microk8s.status # Check status Windows - Minikube minikube start --kubernetes-version = '1.17.4' # Install specific k8s version minikube ip # IP of the machine minikube status # Check status minikube stop # Stop minkube service Kubernetes Container Abstractions Pod: one or more containers running together on one Node. Basic unit of deployment. Containers are always in pods Controller: For creating/updating pods and other objects. Many types of Controllers inc. Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob, etc. Service: network endpoint to connect to a pod Namespace: Filtered group of objects in cluster - Secrets, ConfigMaps, and more. Our First Pod With Kubectl run Two ways to deploy Pods (containers): Via commands, or via YAML Object hieracrhy - Pods -> ReplicaSet -> Deployment kubectl run my-nginx --image nginx # Creates a single pod kubectl run nginx-pod --generator = run-pod/v1 -- image nginx # Another way to create pod kubectl get pods # list the pod kubectl create deployment nginx --image nginx # Creates a deployment kubectl deployment deployment nginx # Deletes a deployment kubectl create deployment nginx --image nginx --dry-run --port 80 -- expose # Using Dry run option Scaling ReplicaSets kubectl create deployment my-apache --image httpd kubectl scale deploy/my-apache --replicas 2 # Scale up by 2 kubectl scale deployment my-apache --replicas 2 # Scale up by 2 kubectl get all Inspecting Kubernetes Objects kubectl get deploy,pods # Get multiple resources in one line kubectl get pods -o wide # Get all pods, in wide format ( gives more info ) kubectl get pods --show-labels # Get all pods and show labels kubectl logs deployment/my-apache kubectl logs deployment/my-apache --follow --tail 1 # Show the last line kubectl logs -l run = my-apache # Show logs using label kubectl describe pod/my-apache-<pod id> # Shows the pod configuration including events kubectl get pods -w # Watches the pods in real time kubectl delete pod/my-apache-<pod id> # Deletes a single instance Exposing Kubernetes Ports A service is a stable address for pod(s) If we want to connect to pod(s), we need a service CoreDNS allows us to resolve services by name There are different types of services ClusterIP NodePort LoadBalancer ExternalName ClusterIP and NodePort services are always available in Kubernetes kubectl expose creates a service for existing pods Basic Service Types ClusterIP (default) Single, internal virtual IP allocated Only reachable from within cluster (nodes and pods) Pods can reach service on apps port number NodePort High port allocated on each node Port is open on every node\u2019s IP Anyone can connect (if they can reach node) Other pods need to be updated to this port LoadBalancer Controls a LB endpoint external to the cluster Only available when infra provider gives you a LB (AWS ELB, etc) Creates NodePort+ClusterIP services, tells LB to send to NodePort ExternalName Adds CNAME DNS record to CoreDNS only Not used for Pods, but for giving pods a DNS name to use for something outside Kubernetes # To show how to reach a ClusterIP deployment which is only accessible from the cluster in a Laptop kubectl create deployment httpenv --image = bretfisher/httpenv # simple http server kubectl scale deployment/httpenv --replicas = 5 kubectl expose deployment/httpenv --port 8888 # Create a ClusterIP service ( default ) kubectl get service # Shows services # Uses Generator option and launches the pod and gives BASH terminal kubectl run --generator run-pod/v1 tmp-shell --rm -it --image bretfisher/netshoot -- bash # Launch another pod to run curl curl httpenv:8888 curl [ ip of service ] :8888 # Creating a NodePort and LoadBalancer Service Nodeport Port Range: 30000 to 32767 Did you know that a NodePort service also creates a ClusterIP? These three service types are additive, each one creates the ones above it: ClusterIP NodePort LoadBalancer If you're on Docker Desktop, it provides a built-in LoadBalancer that publishes the --port on localhost If you're on kubeadm, minikube, or microk8s No built-in LB You can still run the command, it'll just stay at LoadBalancer recieves the packet on 8888, then transfers it to the Nodeport of the Node and then to the ClusterIP of the service. \"pending\" (but its NodePort works) kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort kubectl get services curl localhost:<Node Port> # Get this from svc output kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer kubectl get services curl localhost:8888 # Pod Port kubectl delete service/httpenv service/httpenv-np kubectl delete service/httpenv-lb deployment/httpenv Kubernetes Services DNS Internal DNS is provided by CoreDNS Services also have a FQDN curl <hostname>.<namespace>.svc.cluster.local curl <hostname> kubectl get namespaces curl <hostname>.<namespace>.svc.cluster.local Kubernetes Management Techniques Run, Expose and Create Generators These commands use helper templates called \"generators\" Every resource in Kubernetes has a specification or \"spec\" You can output those templates with --dry-run -o yaml kubectl create deployment sample --image nginx --dry-run -o yaml You can use those YAML defaults as a starting point Generators are \"opinionated defaults\" Generator Examples \u2022 Using dry-run with yaml output we can see the generators kubectl create deployment test --image nginx --dry-run -o yaml kubectl create job test --image nginx --dry-run -o yaml kubectl expose deployment/test --port 80 --dry-run -o yaml - You need the deployment to exist before this works Imperative vs. Declarative Imperative : Focus on how a program operates Declarative : Focus on what a program should accomplish - Example: \"I'd like a cup of coffee\" Imperative : I boil water, scoop out 42 grams of medium-fine grounds, poor over 700 grams of water, etc. Declarative : \"Barista, I'd like a a cup of coffee\". (Barista is the engine that works through the steps, including retrying to make a cup, and is only finished when I have a cup) Kubernetes Imperative Examples: kubectl run, kubectl create deployment, kubectl update We start with a state we know (no deployment exists) We ask kubectl run to create a deployment Different commands are required to change that deployment Different commands are required per object Imperative is easier when you know the state Imperative is easier to get started Imperative is easier for humans at the CLI Imperative is NOT easy to automate Kubernetes Declarative Example: kubectl apply -f my-resources.yaml We don't know the current state We only know what we want the end result to be (yaml contents) Same command each time (tiny exception for delete) Resources can be all in a file, or many files (apply a whole dir) Requires understanding the YAML keys and values More work than kubectl run for just starting a pod The easiest way to automate The eventual path to GitOps happiness Three Management Approaches Imperative commands: run, expose, scale, edit, create deployment Best for dev/learning/personal projects Easy to learn, hardest to manage over time Imperative Commands Imperative objects: create -f file.yml, replace -f file.yml, delete... Good for prod of small environments, single file per command Store your changes in git-based yaml files Hard to automate Imperative Config File Declarative objects: apply -f file.yml or dir\\, diff Best for prod, easier to automate Harder to understand and predict changes Declarative Config File Recommendations Most Important Rule : Don't mix the three approaches Recommendations: Learn the Imperative CLI for easy control of local and test setups Move to apply -f file.yml and apply -f directory for prod Store yaml in git, git commit each change before you apply This trains you for later doing GitOps (where git commits are automatically applied to clusters) Moving to Declarative Kubernetes YAML Using kubectl apply create/update resources in a file kubectl apply -f myfile.yaml create/update a whole directory of yaml kubectl apply -f myyaml/ create/update from a URL kubectl apply -f https://bret.run/pod.yml Be careful, lets look at it first (browser or curl) # Using Shell curl -L https://bret.run/pod # Using Windows CMD Win PoSH? start https://bret.run/pod.yml Kubernetes Configuration YAML Kubernetes configuration file (YAML or JSON) Each file contains one or more manifests Each manifest describes an API object (deployment, job, secret) Each manifest needs four parts (root key:values in the file) apiVersion: kind: metadata: spec: Building Your YAML Files kind : We can get a list of resources the cluster supports kubectl api-resources Notice some resources have multiple API's (old vs. new) apiVersion : We can get the API versions the cluster supports kubectl api-versions metadata : only name is required spec : Where all the action is at! Building Your YAML spec - explain Command We can get all the keys each kind supports kubectl explain services --recursive Oh boy! Let's slow down kubectl explain services.spec We can walk through the spec this way kubectl explain services.spec.type spec: can have sub spec: of other resources kubectl explain deployment.spec.template.spec.volumes.nfs.server Use kubectl api-versions or kubectl api-resources along with kubectl explain as documentation on explain could be old We can also use docs kubernetes.io/docs/reference/#api-reference Dry Runs With Apply YAML dry-run a create (client side only) kubectl apply -f app.yml --dry-run dry-run a create/update on server kubectl apply -f app.yml --server-dry-run see a diff visually kubectl diff -f app.yml Difference between dry-run and diff Labels and Label Selectors Labels goes under metadata: in your YAML Simple list of key: value for identifying your resource later by selecting, grouping, or filtering for it Common examples include tier: frontend, app: api, env: prod, customer: acme.co Not meant to hold complex, large, or non- identifying info, which is what annotations are for filter a get command kubectl get pods -l app=nginx apply only matching labels kubectl apply -f myfile.yaml -l app=nginx Label Recommendation Label Selectors (Use case for Labels) The \"glue\" telling Services and Deployments which pods are theirs Many resources use Label Selectors to \"link\" resource dependencies You'll see these match up in the Service and Deployment YAML Using Label selectors Use Labels and Selectors to control which pods go to which nodes Assigning Pods to Nodes Taints and Tolerations also control node placement Taints and Tolerations Your Next Steps, and The Future of Kubernetes Storage in Kubernetes Storage and stateful workloads are harder in all systems Containers make it both harder and easier than before StatefulSets is a new resource type, making Pods more sticky Recommendation : avoid stateful workloads for first few deployments until you're good at the basics Use db-as-a-service whenever you can Volumes in Kubernetes Creating and connecting Volumes: 2 types Volumes Tied to lifecycle of a Pod All containers in a single Pod can share them PersistentVolumes Created at the cluster level, outlives a Pod Separates storage config from Pod using it Multiple Pods can share them CSI plugins are the new way to connect to storage Ingress None of our Service types work at OSI Layer 7 (HTTP) How do we route outside connections based on hostname or URL? Example Usecase: app1.com and app2.com are 2 different deployments in the cluster and both listen on port 443. You will need Ingress to understand the DNS and route traffic to those apps Ingress Controllers (optional) do this with 3 rd party proxies Nginx is popular, but Traefik, HAProxy, F5, Envoy, Istio, etc. Recommendation: Check out Traefik Implementation is specific to Controller chosen Why Controller - To configure LB which is outside the cluster CRD's and The Operator Pattern You can add 3 rd party Resources and Controllers This extends Kubernetes API and CLI A pattern is starting to emerge of using these together Operator : automate deployment and management of complex apps e.g. Databases, monitoring tools, backups, and custom ingresses Higher Deployment Abstractions All our kubectl commands just talk to the Kubernetes API Kubernetes has limited built-in templating, versioning, tracking, and management of your apps Helm is the most popular Compose on Kubernetes comes with Docker Desktop Remember these are optional, and your distro may have a preference Most distros support Helm Templating YAML Many of the deployment tools have templating options You'll need a solution as the number of environments/apps grow Helm was the first \"winner\" in this space, but can be complex Official Kustomize feature works out-of-the-box (as of 1.14) docker app and compose-on-kubernetes are Docker's way Kubernetes Dashboard Default GUI for \"upstream\" Kubernetes Clouds don't have it by default Let's you view resources and upload YAML Safety first! Namespaces and Context Namespaces limit scope, aka \"virtual clusters\" Not related to Docker/Linux namespaces Won't need them in small clusters There are some built-in, to hide system stuff from kubectl \"users\" kubectl get namespaces kubectl get all --all-namespaces - Context changes kubectl cluster and namespace - See ~/.kube/config file kubectl config get-contexts # Selectively show output of Kube config kubectl config get-contexts -o name kubectl config set*","title":"Ks8 notes"},{"location":"learning/k8s/ks8-notes/#why-kubernetes","text":"Orchestration: Next logical step in journey to faster DevOps First, understand why you may need orchestration Not every solution needs orchestration Servers + Change Rate = Benefit of orchestration","title":"Why Kubernetes"},{"location":"learning/k8s/ks8-notes/#k8s-learning-resources","text":"Play with k8s Katacoda","title":"K8s Learning Resources"},{"location":"learning/k8s/ks8-notes/#install-kubernetes","text":"Linux - Microk8s Install SNAP first using apt-get sudo snap install microk8s --classic --channel = 1 .17/stable # Install specific k8s version microk8s.enable dns # Enbale DNS microk8s.status # Check status Windows - Minikube minikube start --kubernetes-version = '1.17.4' # Install specific k8s version minikube ip # IP of the machine minikube status # Check status minikube stop # Stop minkube service","title":"Install Kubernetes"},{"location":"learning/k8s/ks8-notes/#kubernetes-container-abstractions","text":"Pod: one or more containers running together on one Node. Basic unit of deployment. Containers are always in pods Controller: For creating/updating pods and other objects. Many types of Controllers inc. Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob, etc. Service: network endpoint to connect to a pod Namespace: Filtered group of objects in cluster - Secrets, ConfigMaps, and more.","title":"Kubernetes Container Abstractions"},{"location":"learning/k8s/ks8-notes/#our-first-pod-with-kubectl-run","text":"Two ways to deploy Pods (containers): Via commands, or via YAML Object hieracrhy - Pods -> ReplicaSet -> Deployment kubectl run my-nginx --image nginx # Creates a single pod kubectl run nginx-pod --generator = run-pod/v1 -- image nginx # Another way to create pod kubectl get pods # list the pod kubectl create deployment nginx --image nginx # Creates a deployment kubectl deployment deployment nginx # Deletes a deployment kubectl create deployment nginx --image nginx --dry-run --port 80 -- expose # Using Dry run option","title":"Our First Pod With Kubectl run"},{"location":"learning/k8s/ks8-notes/#scaling-replicasets","text":"kubectl create deployment my-apache --image httpd kubectl scale deploy/my-apache --replicas 2 # Scale up by 2 kubectl scale deployment my-apache --replicas 2 # Scale up by 2 kubectl get all","title":"Scaling ReplicaSets"},{"location":"learning/k8s/ks8-notes/#inspecting-kubernetes-objects","text":"kubectl get deploy,pods # Get multiple resources in one line kubectl get pods -o wide # Get all pods, in wide format ( gives more info ) kubectl get pods --show-labels # Get all pods and show labels kubectl logs deployment/my-apache kubectl logs deployment/my-apache --follow --tail 1 # Show the last line kubectl logs -l run = my-apache # Show logs using label kubectl describe pod/my-apache-<pod id> # Shows the pod configuration including events kubectl get pods -w # Watches the pods in real time kubectl delete pod/my-apache-<pod id> # Deletes a single instance","title":"Inspecting Kubernetes Objects"},{"location":"learning/k8s/ks8-notes/#exposing-kubernetes-ports","text":"A service is a stable address for pod(s) If we want to connect to pod(s), we need a service CoreDNS allows us to resolve services by name There are different types of services ClusterIP NodePort LoadBalancer ExternalName ClusterIP and NodePort services are always available in Kubernetes kubectl expose creates a service for existing pods","title":"Exposing Kubernetes Ports"},{"location":"learning/k8s/ks8-notes/#basic-service-types","text":"ClusterIP (default) Single, internal virtual IP allocated Only reachable from within cluster (nodes and pods) Pods can reach service on apps port number NodePort High port allocated on each node Port is open on every node\u2019s IP Anyone can connect (if they can reach node) Other pods need to be updated to this port LoadBalancer Controls a LB endpoint external to the cluster Only available when infra provider gives you a LB (AWS ELB, etc) Creates NodePort+ClusterIP services, tells LB to send to NodePort ExternalName Adds CNAME DNS record to CoreDNS only Not used for Pods, but for giving pods a DNS name to use for something outside Kubernetes # To show how to reach a ClusterIP deployment which is only accessible from the cluster in a Laptop kubectl create deployment httpenv --image = bretfisher/httpenv # simple http server kubectl scale deployment/httpenv --replicas = 5 kubectl expose deployment/httpenv --port 8888 # Create a ClusterIP service ( default ) kubectl get service # Shows services # Uses Generator option and launches the pod and gives BASH terminal kubectl run --generator run-pod/v1 tmp-shell --rm -it --image bretfisher/netshoot -- bash # Launch another pod to run curl curl httpenv:8888 curl [ ip of service ] :8888 #","title":"Basic Service Types"},{"location":"learning/k8s/ks8-notes/#creating-a-nodeport-and-loadbalancer-service","text":"Nodeport Port Range: 30000 to 32767 Did you know that a NodePort service also creates a ClusterIP? These three service types are additive, each one creates the ones above it: ClusterIP NodePort LoadBalancer If you're on Docker Desktop, it provides a built-in LoadBalancer that publishes the --port on localhost If you're on kubeadm, minikube, or microk8s No built-in LB You can still run the command, it'll just stay at LoadBalancer recieves the packet on 8888, then transfers it to the Nodeport of the Node and then to the ClusterIP of the service. \"pending\" (but its NodePort works) kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort kubectl get services curl localhost:<Node Port> # Get this from svc output kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer kubectl get services curl localhost:8888 # Pod Port kubectl delete service/httpenv service/httpenv-np kubectl delete service/httpenv-lb deployment/httpenv","title":"Creating a NodePort and LoadBalancer Service"},{"location":"learning/k8s/ks8-notes/#kubernetes-services-dns","text":"Internal DNS is provided by CoreDNS Services also have a FQDN curl <hostname>.<namespace>.svc.cluster.local curl <hostname> kubectl get namespaces curl <hostname>.<namespace>.svc.cluster.local","title":"Kubernetes Services DNS"},{"location":"learning/k8s/ks8-notes/#kubernetes-management-techniques","text":"","title":"Kubernetes Management Techniques"},{"location":"learning/k8s/ks8-notes/#run-expose-and-create-generators","text":"These commands use helper templates called \"generators\" Every resource in Kubernetes has a specification or \"spec\" You can output those templates with --dry-run -o yaml kubectl create deployment sample --image nginx --dry-run -o yaml You can use those YAML defaults as a starting point Generators are \"opinionated defaults\"","title":"Run, Expose and Create Generators"},{"location":"learning/k8s/ks8-notes/#generator-examples","text":"\u2022 Using dry-run with yaml output we can see the generators kubectl create deployment test --image nginx --dry-run -o yaml kubectl create job test --image nginx --dry-run -o yaml kubectl expose deployment/test --port 80 --dry-run -o yaml - You need the deployment to exist before this works","title":"Generator Examples"},{"location":"learning/k8s/ks8-notes/#imperative-vs-declarative","text":"Imperative : Focus on how a program operates Declarative : Focus on what a program should accomplish - Example: \"I'd like a cup of coffee\" Imperative : I boil water, scoop out 42 grams of medium-fine grounds, poor over 700 grams of water, etc. Declarative : \"Barista, I'd like a a cup of coffee\". (Barista is the engine that works through the steps, including retrying to make a cup, and is only finished when I have a cup)","title":"Imperative vs. Declarative"},{"location":"learning/k8s/ks8-notes/#kubernetes-imperative","text":"Examples: kubectl run, kubectl create deployment, kubectl update We start with a state we know (no deployment exists) We ask kubectl run to create a deployment Different commands are required to change that deployment Different commands are required per object Imperative is easier when you know the state Imperative is easier to get started Imperative is easier for humans at the CLI Imperative is NOT easy to automate","title":"Kubernetes Imperative"},{"location":"learning/k8s/ks8-notes/#kubernetes-declarative","text":"Example: kubectl apply -f my-resources.yaml We don't know the current state We only know what we want the end result to be (yaml contents) Same command each time (tiny exception for delete) Resources can be all in a file, or many files (apply a whole dir) Requires understanding the YAML keys and values More work than kubectl run for just starting a pod The easiest way to automate The eventual path to GitOps happiness","title":"Kubernetes Declarative"},{"location":"learning/k8s/ks8-notes/#three-management-approaches","text":"Imperative commands: run, expose, scale, edit, create deployment Best for dev/learning/personal projects Easy to learn, hardest to manage over time Imperative Commands Imperative objects: create -f file.yml, replace -f file.yml, delete... Good for prod of small environments, single file per command Store your changes in git-based yaml files Hard to automate Imperative Config File Declarative objects: apply -f file.yml or dir\\, diff Best for prod, easier to automate Harder to understand and predict changes Declarative Config File","title":"Three Management Approaches"},{"location":"learning/k8s/ks8-notes/#recommendations","text":"Most Important Rule : Don't mix the three approaches Recommendations: Learn the Imperative CLI for easy control of local and test setups Move to apply -f file.yml and apply -f directory for prod Store yaml in git, git commit each change before you apply This trains you for later doing GitOps (where git commits are automatically applied to clusters)","title":"Recommendations"},{"location":"learning/k8s/ks8-notes/#moving-to-declarative-kubernetes-yaml","text":"","title":"Moving to Declarative Kubernetes YAML"},{"location":"learning/k8s/ks8-notes/#using-kubectl-apply","text":"create/update resources in a file kubectl apply -f myfile.yaml create/update a whole directory of yaml kubectl apply -f myyaml/ create/update from a URL kubectl apply -f https://bret.run/pod.yml Be careful, lets look at it first (browser or curl) # Using Shell curl -L https://bret.run/pod # Using Windows CMD Win PoSH? start https://bret.run/pod.yml","title":"Using kubectl apply"},{"location":"learning/k8s/ks8-notes/#kubernetes-configuration-yaml","text":"Kubernetes configuration file (YAML or JSON) Each file contains one or more manifests Each manifest describes an API object (deployment, job, secret) Each manifest needs four parts (root key:values in the file) apiVersion: kind: metadata: spec:","title":"Kubernetes Configuration YAML"},{"location":"learning/k8s/ks8-notes/#building-your-yaml-files","text":"kind : We can get a list of resources the cluster supports kubectl api-resources Notice some resources have multiple API's (old vs. new) apiVersion : We can get the API versions the cluster supports kubectl api-versions metadata : only name is required spec : Where all the action is at!","title":"Building Your YAML Files"},{"location":"learning/k8s/ks8-notes/#building-your-yaml-spec-explain-command","text":"We can get all the keys each kind supports kubectl explain services --recursive Oh boy! Let's slow down kubectl explain services.spec We can walk through the spec this way kubectl explain services.spec.type spec: can have sub spec: of other resources kubectl explain deployment.spec.template.spec.volumes.nfs.server Use kubectl api-versions or kubectl api-resources along with kubectl explain as documentation on explain could be old We can also use docs kubernetes.io/docs/reference/#api-reference","title":"Building Your YAML spec - explain Command"},{"location":"learning/k8s/ks8-notes/#dry-runs-with-apply-yaml","text":"dry-run a create (client side only) kubectl apply -f app.yml --dry-run dry-run a create/update on server kubectl apply -f app.yml --server-dry-run see a diff visually kubectl diff -f app.yml Difference between dry-run and diff","title":"Dry Runs With Apply YAML"},{"location":"learning/k8s/ks8-notes/#labels-and-label-selectors","text":"Labels goes under metadata: in your YAML Simple list of key: value for identifying your resource later by selecting, grouping, or filtering for it Common examples include tier: frontend, app: api, env: prod, customer: acme.co Not meant to hold complex, large, or non- identifying info, which is what annotations are for filter a get command kubectl get pods -l app=nginx apply only matching labels kubectl apply -f myfile.yaml -l app=nginx Label Recommendation","title":"Labels and Label Selectors"},{"location":"learning/k8s/ks8-notes/#label-selectors-use-case-for-labels","text":"The \"glue\" telling Services and Deployments which pods are theirs Many resources use Label Selectors to \"link\" resource dependencies You'll see these match up in the Service and Deployment YAML Using Label selectors Use Labels and Selectors to control which pods go to which nodes Assigning Pods to Nodes Taints and Tolerations also control node placement Taints and Tolerations","title":"Label Selectors (Use case for Labels)"},{"location":"learning/k8s/ks8-notes/#your-next-steps-and-the-future-of-kubernetes","text":"","title":"Your Next Steps, and The Future of Kubernetes"},{"location":"learning/k8s/ks8-notes/#storage-in-kubernetes","text":"Storage and stateful workloads are harder in all systems Containers make it both harder and easier than before StatefulSets is a new resource type, making Pods more sticky Recommendation : avoid stateful workloads for first few deployments until you're good at the basics Use db-as-a-service whenever you can","title":"Storage in Kubernetes"},{"location":"learning/k8s/ks8-notes/#volumes-in-kubernetes","text":"Creating and connecting Volumes: 2 types Volumes Tied to lifecycle of a Pod All containers in a single Pod can share them PersistentVolumes Created at the cluster level, outlives a Pod Separates storage config from Pod using it Multiple Pods can share them CSI plugins are the new way to connect to storage","title":"Volumes in Kubernetes"},{"location":"learning/k8s/ks8-notes/#ingress","text":"None of our Service types work at OSI Layer 7 (HTTP) How do we route outside connections based on hostname or URL? Example Usecase: app1.com and app2.com are 2 different deployments in the cluster and both listen on port 443. You will need Ingress to understand the DNS and route traffic to those apps Ingress Controllers (optional) do this with 3 rd party proxies Nginx is popular, but Traefik, HAProxy, F5, Envoy, Istio, etc. Recommendation: Check out Traefik Implementation is specific to Controller chosen Why Controller - To configure LB which is outside the cluster","title":"Ingress"},{"location":"learning/k8s/ks8-notes/#crds-and-the-operator-pattern","text":"You can add 3 rd party Resources and Controllers This extends Kubernetes API and CLI A pattern is starting to emerge of using these together Operator : automate deployment and management of complex apps e.g. Databases, monitoring tools, backups, and custom ingresses","title":"CRD's and The Operator Pattern"},{"location":"learning/k8s/ks8-notes/#higher-deployment-abstractions","text":"All our kubectl commands just talk to the Kubernetes API Kubernetes has limited built-in templating, versioning, tracking, and management of your apps Helm is the most popular Compose on Kubernetes comes with Docker Desktop Remember these are optional, and your distro may have a preference Most distros support Helm","title":"Higher Deployment Abstractions"},{"location":"learning/k8s/ks8-notes/#templating-yaml","text":"Many of the deployment tools have templating options You'll need a solution as the number of environments/apps grow Helm was the first \"winner\" in this space, but can be complex Official Kustomize feature works out-of-the-box (as of 1.14) docker app and compose-on-kubernetes are Docker's way","title":"Templating YAML"},{"location":"learning/k8s/ks8-notes/#kubernetes-dashboard","text":"Default GUI for \"upstream\" Kubernetes Clouds don't have it by default Let's you view resources and upload YAML Safety first!","title":"Kubernetes Dashboard"},{"location":"learning/k8s/ks8-notes/#namespaces-and-context","text":"Namespaces limit scope, aka \"virtual clusters\" Not related to Docker/Linux namespaces Won't need them in small clusters There are some built-in, to hide system stuff from kubectl \"users\" kubectl get namespaces kubectl get all --all-namespaces - Context changes kubectl cluster and namespace - See ~/.kube/config file kubectl config get-contexts # Selectively show output of Kube config kubectl config get-contexts -o name kubectl config set*","title":"Namespaces and Context"},{"location":"server/","text":"Server Details","title":"Introduction"},{"location":"server/#server-details","text":"","title":"Server Details"},{"location":"server/install/","text":"Installation Centos Identifying Version Download the latest version of Centos from centos.org Creating Live USB Format a USB as NTFS. Download Fedora LiveUSB Creator or read the supported software for Centos in the downloads page. See the instructions to write the ISO into the USB Setting Up PC Format the HDD (optional) Ensure - Advanced Option, Legacy USB Support is Enabled In Boot priority, USB is the first drive Ensure the Boot option has Boot from USB selected above HDD Configuring Centos Full Installation Steps Create Custom partitions for /boot, /, /var, /tmp, /home Initial Server Setup Update Centos with latest patches yum update -y && yum upgrade -y yum clean all # To clean downloaded packages in /var/cache/yum Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Adding Non Root User Add and Delete Users Creating Volume Groups Securing Server Security based on articles mentioned here or here or DigitalOcean -keygen -t rsa -b 4096 It will create two files: id_rsa and id_rsa.pub in the ~/.ssh directory. When you add a passphrase to your SSH key, it encrypts the private key using 128-bit AES so that the private key is useless without the passphrase to decrypt it. cat ~/.ssh/id_rsa.pub For example, if we had a Linux VM named myserver with a user azureuser, we could use the following command to install the public key file and authorize the user with the key: ssh-copy-id -i ~/.ssh/id_rsa.pub azureuser@myserver Connect with SSH ssh jim@137.117.101.249 Try executing a few Linux commands ls -la / to show the root of the disk ps -l to show all the running processes dmesg to list all the kernel messages lsblk to list all the block devices - here you will see your drives Initialize data disks Any additional drives you create from scratch need to be initialized and formatted. dmesg | grep SCSI - list all the messages from the kernel for SCSI devices. Use fdisk to initialize the drive We can use the following command to create a new primary partition: (echo n; echo p; echo 1; echo ; echo ; echo w) | sudo fdisk /dev/sdc We need to write a file system to the partition with the mkfs command. sudo mkfs -t ext4 /dev/sdc1 We need to mount the drive to the file system. sudo mkdir /data & sudo mount /dev/sdc1 /data Always make sure to lock down ports used for administrative access. An even better approach is to create a VPN to link the virtual network to your private network and only allow RDP or SSH requests from that address range. You can also change the port used by SSH to be something other than the default. Keep in mind that changing ports is not sufficient to stop attacks. It simply makes it a little harder to discover.","title":"Installation"},{"location":"server/install/#installation","text":"","title":"Installation"},{"location":"server/install/#centos","text":"Identifying Version Download the latest version of Centos from centos.org Creating Live USB Format a USB as NTFS. Download Fedora LiveUSB Creator or read the supported software for Centos in the downloads page. See the instructions to write the ISO into the USB Setting Up PC Format the HDD (optional) Ensure - Advanced Option, Legacy USB Support is Enabled In Boot priority, USB is the first drive Ensure the Boot option has Boot from USB selected above HDD Configuring Centos Full Installation Steps Create Custom partitions for /boot, /, /var, /tmp, /home Initial Server Setup Update Centos with latest patches yum update -y && yum upgrade -y yum clean all # To clean downloaded packages in /var/cache/yum Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Adding Non Root User Add and Delete Users Creating Volume Groups Securing Server Security based on articles mentioned here or here or DigitalOcean -keygen -t rsa -b 4096 It will create two files: id_rsa and id_rsa.pub in the ~/.ssh directory. When you add a passphrase to your SSH key, it encrypts the private key using 128-bit AES so that the private key is useless without the passphrase to decrypt it. cat ~/.ssh/id_rsa.pub For example, if we had a Linux VM named myserver with a user azureuser, we could use the following command to install the public key file and authorize the user with the key: ssh-copy-id -i ~/.ssh/id_rsa.pub azureuser@myserver Connect with SSH ssh jim@137.117.101.249 Try executing a few Linux commands ls -la / to show the root of the disk ps -l to show all the running processes dmesg to list all the kernel messages lsblk to list all the block devices - here you will see your drives Initialize data disks Any additional drives you create from scratch need to be initialized and formatted. dmesg | grep SCSI - list all the messages from the kernel for SCSI devices. Use fdisk to initialize the drive We can use the following command to create a new primary partition: (echo n; echo p; echo 1; echo ; echo ; echo w) | sudo fdisk /dev/sdc We need to write a file system to the partition with the mkfs command. sudo mkfs -t ext4 /dev/sdc1 We need to mount the drive to the file system. sudo mkdir /data & sudo mount /dev/sdc1 /data Always make sure to lock down ports used for administrative access. An even better approach is to create a VPN to link the virtual network to your private network and only allow RDP or SSH requests from that address range. You can also change the port used by SSH to be something other than the default. Keep in mind that changing ports is not sufficient to stop attacks. It simply makes it a little harder to discover.","title":"Centos"},{"location":"server/mobile/","text":"Converting Android Device Into Linux Server Centos Pre-requisites Identify an Android mobile phone Factory reset to remove any unwanted apps and data Remove any apps or services from the device to free up space and memory Install Termux and AnLinux from Playstore Installation Start AnLinux and search for the distro. Select Centos Copy the installation command Open Termux terminal and paste the command Begin installation Access Mobile Configure Static Ip to Mobile Add Ip address and remote-hostname to Laptop's hosts file. Install SSH on Termux apt install openssh Start the ssh server sshd . This will also generate the private and public keys. Test the service locally ssh localhost -p 8022 . Accept the public key. ssh-copy-id copies the local-host\u2019s public key to the remote-host\u2019s authorized_keys file. ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-hostname> -p 8022 . Enter remote user password to complete transaction. Login from Laptop ssh <hostname> -p 8022 . Enter the passphrase. Install Python Configuring Centos Update Centos with latest patches yum install update -y && yum install upgrade -y Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Init Centos yum -y install initscripts & yum clean all Change Root Password passwd root Install Sudo yum install sudo -y The difference between wheel and sudo . In CentOS, a user belonging to the wheel group can execute su and directly ascend to root. Meanwhile, a sudo user would have use the sudo su first. Essentially, there is no real difference except for the syntax used to become root, and users belonging to both groups can use the sudo command. * Adding Non Root User bmmnb Add and Delete Users Securing Server Security based on articles mentioned here or here or DigitalOcean Configures a Hostname Reconfigures the Timezone Updates the entire System Creates a New Admin user so you can manage your server safely without the need of doing remote connections with root. Helps user Generate Secure RSA Keys, so that remote access to your server is done exclusive from your local pc and no Conventional password Configures, Optimize and secures the SSH Server (Some Settings Following CIS Benchmark) Configures IPTABLES Rules to protect the server from common attacks Disables unused FileSystems and Network protocols Protects the server against Brute Force attacks by installing a configuring fail2ban Installs and Configure Artillery as a Honeypot, Monitoring, Blocking and Alerting tool Installs PortSentry Installs RootKit Hunter Secures Root Home and Grub Configuration Files Installs Unhide to help Detect Malicious Hidden Processes Installs Tiger, A Security Auditing and Intrusion Prevention system Restrict Access to Apache Config Files Disables Compilers Creates Daily Cron job for System Updates Kernel Hardening via sysctl configuration File (Tweaked) /tmp Directory Hardening PSAD IDS installation Enables Process Accounting Enables Unattended Upgrades MOTD and Banners for Unauthorized access Disables USB Support for Improved Security (Optional) Configures a Restrictive Default UMASK Configures and enables Auditd Configures Auditd rules following CIS Benchmark Sysstat install ArpWatch install Additional Hardening steps following CIS Benchmark Secures Cron Automates the process of setting a GRUB Bootloader Password Secures Boot Settings Sets Secure File Permissions for Critical System Files","title":"Mobile"},{"location":"server/mobile/#converting-android-device-into-linux-server","text":"","title":"Converting Android Device Into Linux Server"},{"location":"server/mobile/#centos","text":"Pre-requisites Identify an Android mobile phone Factory reset to remove any unwanted apps and data Remove any apps or services from the device to free up space and memory Install Termux and AnLinux from Playstore Installation Start AnLinux and search for the distro. Select Centos Copy the installation command Open Termux terminal and paste the command Begin installation Access Mobile Configure Static Ip to Mobile Add Ip address and remote-hostname to Laptop's hosts file. Install SSH on Termux apt install openssh Start the ssh server sshd . This will also generate the private and public keys. Test the service locally ssh localhost -p 8022 . Accept the public key. ssh-copy-id copies the local-host\u2019s public key to the remote-host\u2019s authorized_keys file. ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-hostname> -p 8022 . Enter remote user password to complete transaction. Login from Laptop ssh <hostname> -p 8022 . Enter the passphrase. Install Python Configuring Centos Update Centos with latest patches yum install update -y && yum install upgrade -y Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Init Centos yum -y install initscripts & yum clean all Change Root Password passwd root Install Sudo yum install sudo -y The difference between wheel and sudo . In CentOS, a user belonging to the wheel group can execute su and directly ascend to root. Meanwhile, a sudo user would have use the sudo su first. Essentially, there is no real difference except for the syntax used to become root, and users belonging to both groups can use the sudo command. * Adding Non Root User bmmnb Add and Delete Users Securing Server Security based on articles mentioned here or here or DigitalOcean Configures a Hostname Reconfigures the Timezone Updates the entire System Creates a New Admin user so you can manage your server safely without the need of doing remote connections with root. Helps user Generate Secure RSA Keys, so that remote access to your server is done exclusive from your local pc and no Conventional password Configures, Optimize and secures the SSH Server (Some Settings Following CIS Benchmark) Configures IPTABLES Rules to protect the server from common attacks Disables unused FileSystems and Network protocols Protects the server against Brute Force attacks by installing a configuring fail2ban Installs and Configure Artillery as a Honeypot, Monitoring, Blocking and Alerting tool Installs PortSentry Installs RootKit Hunter Secures Root Home and Grub Configuration Files Installs Unhide to help Detect Malicious Hidden Processes Installs Tiger, A Security Auditing and Intrusion Prevention system Restrict Access to Apache Config Files Disables Compilers Creates Daily Cron job for System Updates Kernel Hardening via sysctl configuration File (Tweaked) /tmp Directory Hardening PSAD IDS installation Enables Process Accounting Enables Unattended Upgrades MOTD and Banners for Unauthorized access Disables USB Support for Improved Security (Optional) Configures a Restrictive Default UMASK Configures and enables Auditd Configures Auditd rules following CIS Benchmark Sysstat install ArpWatch install Additional Hardening steps following CIS Benchmark Secures Cron Automates the process of setting a GRUB Bootloader Password Secures Boot Settings Sets Secure File Permissions for Critical System Files","title":"Centos"},{"location":"server/service/","text":"Create a systemd unit file for starting the application: Example service file can be found here: $ wget https://gist.githubusercontent.com/Artemmkin/ce82397cfc69d912df9cd648a8d69bec/raw/7193a36c9661c6b90e7e482d256865f085a853f2/raddit.service Move it to the systemd directory $ sudo mv raddit.service /etc/systemd/system/raddit.service Now start the application and enable autostart: $ sudo systemctl start raddit $ sudo systemctl enable raddit Verify that it's running: $ sudo systemctl status raddit","title":"Create a systemd unit file for starting the application:"},{"location":"server/service/#create-a-systemd-unit-file-for-starting-the-application","text":"Example service file can be found here: $ wget https://gist.githubusercontent.com/Artemmkin/ce82397cfc69d912df9cd648a8d69bec/raw/7193a36c9661c6b90e7e482d256865f085a853f2/raddit.service Move it to the systemd directory $ sudo mv raddit.service /etc/systemd/system/raddit.service Now start the application and enable autostart: $ sudo systemctl start raddit $ sudo systemctl enable raddit Verify that it's running: $ sudo systemctl status raddit","title":"Create a systemd unit file for starting the application:"},{"location":"server/volume-groups/","text":"Configuration Centos Prerequisites Install EPEL Packages yum install epel-release Install ntfs-3g and fuse packages to support NTFS drives yum install ntfs-3g fuse -y Format the new NTFS filestsystem to xfs mkfs.xfs -f /dev/sda Identifying Additional HDD Run blkid to verify if HDD is detected. On detection of HDD, follow the below steps to configure as XFS file system. * Run fdisk -l to list the devices, partitions and volume groups * Scan your hard drive for Bad Sectors or Bad Blocks 1. Partitioning HDD (Optional and for Primary HDD) * On identifying the device(HDD), say /dev/sda as the target device to partition, run fdisk /dev/sda * These steps below are not for secondary HDD as partitioning will not utilize the entire space * Option m to list the operations, Option p to print the current partitions * Note the start and end block ids of the current partitions * Option d to delete the current partitions, select the partition to delete 2 * Option n to create new partition, select only 1 partition for the HDD, select default start and end block id, it should match the one printed above * Option t to select the type of partition, followed by 8e to select as Linux LVM * Format the new partition to xfs mkfs.xfs -f /dev/sda1 as only one partition was created * Mount the new partition to test mount -t xfs /dev/sda1 /data * Make entry in /etc/fstab to make it permanent * Verify the mounted partition by running df -Th to see free and used space Securing Drive and adding Redundancy Software RAID and LVM For the partitions that are needed as Raid 1, use fdsik /dev/sdb3 ,Press \u2018t\u2019 to change the partition type. Press \u2018fd\u2019 to choose Linux Raid Auto. Install Raid 1 on /dev/sdb3 and /dev/sda1 using mdadm --create /dev/data --level=1 --raid-devices=2 /dev/sdb3 /dev/sda1 Encrypting Raid devices Volume Group Setup Logical Volume Manager (LVM) is a software-based RAID-like system that lets you create \"pools\" of storage and add hard drive space to those pools as needed. * LVM Basic * Create, expand, and encrypt storage pools Volume Group Lifecycle Migrate Data from one LV to another Identify the current Physical Volume and Volume Group mapping pvscan or pvs List the partitions and volumes in the Volume Groups vgdisplay centos-digital -v Logical volume and device mapping lvs -o+devices List all devices and device mapping lsscsi Before Reducing logical volume, move to another drive Format /dev/sdb3 to a physical volume: pvcreate /dev/sdb3 Give /dev/sdb3 to the volume group centos-digital: vgextend centos-digital /dev/sdb3 Migrate our logical volume Users from /dev/sdb2 to /dev/sdb3, pvmove -n Users /dev/sdb2 /dev/sdb3 Verify if the data is migrated lsblk Reduce your logical volume to make place for your new partition lvresize -L 25G /dev/centos-digital/home Use fdisk to reduce the size of /dev/sdb2 to 80G as total for volume group centos-digital was 75G (yes, a little bit bigger as we used in lvresize!) Use fdisk to create a new LVM partition, /dev/sdb3, with the new available space (it will be around 400G). Check if the kernel could automatically detect the partition change. See if /proc/partition matches the new state (thus, /dev/sdb3 is visible). If not, you need to reboot. (Probably it will.) Make /dev/sdb2 to be as big again pvresize /dev/sdb2","title":"Configuration"},{"location":"server/volume-groups/#configuration","text":"","title":"Configuration"},{"location":"server/volume-groups/#centos","text":"Prerequisites Install EPEL Packages yum install epel-release Install ntfs-3g and fuse packages to support NTFS drives yum install ntfs-3g fuse -y Format the new NTFS filestsystem to xfs mkfs.xfs -f /dev/sda Identifying Additional HDD Run blkid to verify if HDD is detected. On detection of HDD, follow the below steps to configure as XFS file system. * Run fdisk -l to list the devices, partitions and volume groups * Scan your hard drive for Bad Sectors or Bad Blocks 1. Partitioning HDD (Optional and for Primary HDD) * On identifying the device(HDD), say /dev/sda as the target device to partition, run fdisk /dev/sda * These steps below are not for secondary HDD as partitioning will not utilize the entire space * Option m to list the operations, Option p to print the current partitions * Note the start and end block ids of the current partitions * Option d to delete the current partitions, select the partition to delete 2 * Option n to create new partition, select only 1 partition for the HDD, select default start and end block id, it should match the one printed above * Option t to select the type of partition, followed by 8e to select as Linux LVM * Format the new partition to xfs mkfs.xfs -f /dev/sda1 as only one partition was created * Mount the new partition to test mount -t xfs /dev/sda1 /data * Make entry in /etc/fstab to make it permanent * Verify the mounted partition by running df -Th to see free and used space Securing Drive and adding Redundancy Software RAID and LVM For the partitions that are needed as Raid 1, use fdsik /dev/sdb3 ,Press \u2018t\u2019 to change the partition type. Press \u2018fd\u2019 to choose Linux Raid Auto. Install Raid 1 on /dev/sdb3 and /dev/sda1 using mdadm --create /dev/data --level=1 --raid-devices=2 /dev/sdb3 /dev/sda1 Encrypting Raid devices Volume Group Setup Logical Volume Manager (LVM) is a software-based RAID-like system that lets you create \"pools\" of storage and add hard drive space to those pools as needed. * LVM Basic * Create, expand, and encrypt storage pools Volume Group Lifecycle Migrate Data from one LV to another Identify the current Physical Volume and Volume Group mapping pvscan or pvs List the partitions and volumes in the Volume Groups vgdisplay centos-digital -v Logical volume and device mapping lvs -o+devices List all devices and device mapping lsscsi Before Reducing logical volume, move to another drive Format /dev/sdb3 to a physical volume: pvcreate /dev/sdb3 Give /dev/sdb3 to the volume group centos-digital: vgextend centos-digital /dev/sdb3 Migrate our logical volume Users from /dev/sdb2 to /dev/sdb3, pvmove -n Users /dev/sdb2 /dev/sdb3 Verify if the data is migrated lsblk Reduce your logical volume to make place for your new partition lvresize -L 25G /dev/centos-digital/home Use fdisk to reduce the size of /dev/sdb2 to 80G as total for volume group centos-digital was 75G (yes, a little bit bigger as we used in lvresize!) Use fdisk to create a new LVM partition, /dev/sdb3, with the new available space (it will be around 400G). Check if the kernel could automatically detect the partition change. See if /proc/partition matches the new state (thus, /dev/sdb3 is visible). If not, you need to reboot. (Probably it will.) Make /dev/sdb2 to be as big again pvresize /dev/sdb2","title":"Centos"}]}