{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Leslie's Notebook \u00b6 Install mkdocs using command pip install mkdocs Topics \u00b6 Server - Setup basic Linux server Kubernetes - Setup Kubernetes IDE - Tips and Tricks","title":"Installation"},{"location":"#welcome-to-leslies-notebook","text":"Install mkdocs using command pip install mkdocs","title":"Welcome to Leslie's Notebook"},{"location":"#topics","text":"Server - Setup basic Linux server Kubernetes - Setup Kubernetes IDE - Tips and Tricks","title":"Topics"},{"location":"developer/","text":"Windows \u00b6 Update WSL2 first (by default WLS1 is enabled) Install Ubuntu from Microsoft Stores Install Visual Studio Code Update Linux packages sudo apt update sudo apt -y upgrade To find the home directory in Ubuntu explorer.exe . Install Windows Terminal for Miscrosoft Store Install Menlo font (from Powerlevel10k site) To test the terminal color output, run this code in the terminal for code in { 30 ..37 } ; do \\ echo -en \"\\e[ ${ code } m\" '\\\\e[' \" $code \" 'm' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;1m\" '\\\\e[' \" $code \" ';1m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;3m\" '\\\\e[' \" $code \" ';3m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;4m\" '\\\\e[' \" $code \" ';4m' \"\\e[0m\" ; \\ echo -e \" \\e[ $(( code+60 )) m\" '\\\\e[' \" $(( code+60 )) \" 'm' \"\\e[0m\" ; \\ done Generate SSH keys ssh-keygen -t rsa -b 4096 -f ~/.ssh/raddit-user -C raddit-user -C is the comment, you can also write -C 'keys generated on 20th Oct 2022' ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user Using the .ssh config files (~/.ssh/config) # 1. Generate public & private ssh keys: ` ssh-keygen -t rsa ` # Type in a name which will be put in `~/.ssh` directory # 2. To bypass password prompt, you should add the `foo.pub` file to the `authorized_keys` file on the # server's `~/.ssh` directory. You can do a pipe via ssh: ` cat mykey.pub | ssh myuser@mysite.com -p 123 'cat >> .ssh/authorized_keys' ` # 3. Add the publickey name to the `~/.ssh/config` file like this: Host bitbucket.org IdentityFile ~/.ssh/myprivatekeyfile # the leading spaces are important! Port 123 # 4. Verify and then SSH into the remote server. To check if your config is right type: `ssh -T git@github.com` ssh root@mysite.com or ssh mysite.com # if you setup the User setting in config Copy ssh keys to an existing server ssh-copy-id root@192.168.0.10 - This will copy default public key id_rsa.pub to server 192.168.0.10 in .ssh/authorized_key file under root user. Ubuntu \u00b6 Use bootable USB created using ventoy Press F12 at startup and select the bootable USB, select Ubuntu is image to begin installation Configure linux partitions as encrypted Add swap partion instead of efi as we will dual boot into same system. Part 2 Change root password. sudo passwd root Move Windows above Ubuntu in boot menu. Use Grub Customizer Part 3 Backup and restore data using rsync. Install programs using dotfiles. Adding SSH Keys to servers SSH Client Config Test SSH connections Edit setings on the new terminal to make Ubuntu as the default terminal. Also set the fontFace and https://www.the-digital-life.com/en/awesome-wsl-wsl2-terminal/ Create Sudo User Securing Sudoers #sudo visudo /etc/sudoers.d/leslie leslie ALL =( ALL:ALL ) NOPASSWD: /usr/bin/docker, /usr/sbin/reboot, /usr/sbin/shutdown, /usr/bin/apt-get, /usr/local/bin/docker-compose Switching remote URLs from HTTPS to SSH \u00b6 List your existing remotes in order to get the name of the remote you want to change. $ git remote -v > origin https://github.com/USERNAME/REPOSITORY.git ( fetch ) > origin https://github.com/USERNAME/REPOSITORY.git ( push ) Change your remote's URL from HTTPS to SSH with the git remote set-url command. $ git remote set-url origin git@github.com:USERNAME/REPOSITORY.git git remote set-url origin git@github.com :leslieclif/notebook.git Inspirational dotfile repos \u00b6 https://www.freecodecamp.org/news/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/ Dotfiles Intial Automation Tmux and Otherconfig nickjj/dotfiles https://github.com/jieverson/dotfiles-win/blob/master/install.sh Bashrc Automation \u00b6 https://victoria.dev/blog/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/ https://victoria.dev/blog/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/ Vagrant setup \u00b6 https://www.techdrabble.com/ansible/36-install-ansible-molecule-vagrant-on-windows-wsl Tmux \u00b6 Every Hacker should have a great terminal | TMUX - Medium \u00b6 Tmux Basics Tmux Config VSCode \u00b6 Key Shortcuts Mastering Terminal Examples \u00b6 TLS Certificate - Manual PI \u00b6 TFTP Boot Boot Methods Network Boot from Ubuntu K3s Cluster with Netboot DHCP, TFTP and NFS","title":"Developer Setup"},{"location":"developer/#windows","text":"Update WSL2 first (by default WLS1 is enabled) Install Ubuntu from Microsoft Stores Install Visual Studio Code Update Linux packages sudo apt update sudo apt -y upgrade To find the home directory in Ubuntu explorer.exe . Install Windows Terminal for Miscrosoft Store Install Menlo font (from Powerlevel10k site) To test the terminal color output, run this code in the terminal for code in { 30 ..37 } ; do \\ echo -en \"\\e[ ${ code } m\" '\\\\e[' \" $code \" 'm' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;1m\" '\\\\e[' \" $code \" ';1m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;3m\" '\\\\e[' \" $code \" ';3m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;4m\" '\\\\e[' \" $code \" ';4m' \"\\e[0m\" ; \\ echo -e \" \\e[ $(( code+60 )) m\" '\\\\e[' \" $(( code+60 )) \" 'm' \"\\e[0m\" ; \\ done Generate SSH keys ssh-keygen -t rsa -b 4096 -f ~/.ssh/raddit-user -C raddit-user -C is the comment, you can also write -C 'keys generated on 20th Oct 2022' ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user Using the .ssh config files (~/.ssh/config) # 1. Generate public & private ssh keys: ` ssh-keygen -t rsa ` # Type in a name which will be put in `~/.ssh` directory # 2. To bypass password prompt, you should add the `foo.pub` file to the `authorized_keys` file on the # server's `~/.ssh` directory. You can do a pipe via ssh: ` cat mykey.pub | ssh myuser@mysite.com -p 123 'cat >> .ssh/authorized_keys' ` # 3. Add the publickey name to the `~/.ssh/config` file like this: Host bitbucket.org IdentityFile ~/.ssh/myprivatekeyfile # the leading spaces are important! Port 123 # 4. Verify and then SSH into the remote server. To check if your config is right type: `ssh -T git@github.com` ssh root@mysite.com or ssh mysite.com # if you setup the User setting in config Copy ssh keys to an existing server ssh-copy-id root@192.168.0.10 - This will copy default public key id_rsa.pub to server 192.168.0.10 in .ssh/authorized_key file under root user.","title":"Windows"},{"location":"developer/#ubuntu","text":"Use bootable USB created using ventoy Press F12 at startup and select the bootable USB, select Ubuntu is image to begin installation Configure linux partitions as encrypted Add swap partion instead of efi as we will dual boot into same system. Part 2 Change root password. sudo passwd root Move Windows above Ubuntu in boot menu. Use Grub Customizer Part 3 Backup and restore data using rsync. Install programs using dotfiles. Adding SSH Keys to servers SSH Client Config Test SSH connections Edit setings on the new terminal to make Ubuntu as the default terminal. Also set the fontFace and https://www.the-digital-life.com/en/awesome-wsl-wsl2-terminal/ Create Sudo User Securing Sudoers #sudo visudo /etc/sudoers.d/leslie leslie ALL =( ALL:ALL ) NOPASSWD: /usr/bin/docker, /usr/sbin/reboot, /usr/sbin/shutdown, /usr/bin/apt-get, /usr/local/bin/docker-compose","title":"Ubuntu"},{"location":"developer/#switching-remote-urls-from-https-to-ssh","text":"List your existing remotes in order to get the name of the remote you want to change. $ git remote -v > origin https://github.com/USERNAME/REPOSITORY.git ( fetch ) > origin https://github.com/USERNAME/REPOSITORY.git ( push ) Change your remote's URL from HTTPS to SSH with the git remote set-url command. $ git remote set-url origin git@github.com:USERNAME/REPOSITORY.git git remote set-url origin git@github.com :leslieclif/notebook.git","title":"Switching remote URLs from HTTPS to SSH"},{"location":"developer/#inspirational-dotfile-repos","text":"https://www.freecodecamp.org/news/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/ Dotfiles Intial Automation Tmux and Otherconfig nickjj/dotfiles https://github.com/jieverson/dotfiles-win/blob/master/install.sh","title":"Inspirational dotfile repos"},{"location":"developer/#bashrc-automation","text":"https://victoria.dev/blog/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/ https://victoria.dev/blog/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/","title":"Bashrc Automation"},{"location":"developer/#vagrant-setup","text":"https://www.techdrabble.com/ansible/36-install-ansible-molecule-vagrant-on-windows-wsl","title":"Vagrant setup"},{"location":"developer/#tmux","text":"","title":"Tmux"},{"location":"developer/#every-hacker-should-have-a-great-terminal--tmux---medium","text":"Tmux Basics Tmux Config","title":"Every Hacker should have a great terminal | TMUX - Medium"},{"location":"developer/#vscode","text":"Key Shortcuts Mastering Terminal","title":"VSCode"},{"location":"developer/#examples","text":"TLS Certificate - Manual","title":"Examples"},{"location":"developer/#pi","text":"TFTP Boot Boot Methods Network Boot from Ubuntu K3s Cluster with Netboot DHCP, TFTP and NFS","title":"PI"},{"location":"base/docker/docker-ubuntu2004-ansible/","text":"Ubuntu 20.04 LTS (Focal Fossa) Ansible Test Image \u00b6 Ubuntu 20.04 LTS (Focal Fossa) Docker container for Ansible playbook and role testing. Tags \u00b6 latest : Latest stable version of Ansible. The latest tag is a lightweight image for basic validation of Ansible playbooks. How to Build \u00b6 This image is built on Docker Hub automatically any time the upstream OS container is rebuilt, and any time a commit is made or merged to the master branch. But if you need to build the image on your own locally, do the following: Install Docker . cd into this directory. Run docker build -t ubuntu2004-ansible . How to Use \u00b6 Install Docker . Pull this image from Docker Hub: docker pull geerlingguy/docker-ubuntu2004-ansible:latest (or use the image you built earlier, e.g. ubuntu2004-ansible:latest ). Run a container from the image: docker run --detach --privileged --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro geerlingguy/docker-ubuntu2004-ansible:latest (to test my Ansible roles, I add in a volume mounted from the current working directory with --volume=`pwd`:/etc/ansible/roles/role_under_test:ro ). Use Ansible inside the container: a. docker exec --tty [container_id] env TERM=xterm ansible --version b. docker exec --tty [container_id] env TERM=xterm ansible-playbook /path/to/ansible/playbook.yml --syntax-check Notes \u00b6 I use Docker to test my Ansible roles and playbooks on multiple OSes using CI tools like Jenkins and Travis. This container allows me to test roles and playbooks using Ansible running locally inside the container. Important Note : I use this image for testing in an isolated environment\u2014not for production\u2014and the settings and configuration used may not be suitable for a secure and performant production environment. Use on production servers/in the wild at your own risk! Author \u00b6 Created in 2020 by Jeff Geerling , author of Ansible for DevOps .","title":"Ubuntu 20.04 LTS (Focal Fossa) Ansible Test Image"},{"location":"base/docker/docker-ubuntu2004-ansible/#ubuntu-2004-lts-focal-fossa-ansible-test-image","text":"Ubuntu 20.04 LTS (Focal Fossa) Docker container for Ansible playbook and role testing.","title":"Ubuntu 20.04 LTS (Focal Fossa) Ansible Test Image"},{"location":"base/docker/docker-ubuntu2004-ansible/#tags","text":"latest : Latest stable version of Ansible. The latest tag is a lightweight image for basic validation of Ansible playbooks.","title":"Tags"},{"location":"base/docker/docker-ubuntu2004-ansible/#how-to-build","text":"This image is built on Docker Hub automatically any time the upstream OS container is rebuilt, and any time a commit is made or merged to the master branch. But if you need to build the image on your own locally, do the following: Install Docker . cd into this directory. Run docker build -t ubuntu2004-ansible .","title":"How to Build"},{"location":"base/docker/docker-ubuntu2004-ansible/#how-to-use","text":"Install Docker . Pull this image from Docker Hub: docker pull geerlingguy/docker-ubuntu2004-ansible:latest (or use the image you built earlier, e.g. ubuntu2004-ansible:latest ). Run a container from the image: docker run --detach --privileged --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro geerlingguy/docker-ubuntu2004-ansible:latest (to test my Ansible roles, I add in a volume mounted from the current working directory with --volume=`pwd`:/etc/ansible/roles/role_under_test:ro ). Use Ansible inside the container: a. docker exec --tty [container_id] env TERM=xterm ansible --version b. docker exec --tty [container_id] env TERM=xterm ansible-playbook /path/to/ansible/playbook.yml --syntax-check","title":"How to Use"},{"location":"base/docker/docker-ubuntu2004-ansible/#notes","text":"I use Docker to test my Ansible roles and playbooks on multiple OSes using CI tools like Jenkins and Travis. This container allows me to test roles and playbooks using Ansible running locally inside the container. Important Note : I use this image for testing in an isolated environment\u2014not for production\u2014and the settings and configuration used may not be suitable for a secure and performant production environment. Use on production servers/in the wild at your own risk!","title":"Notes"},{"location":"base/docker/docker-ubuntu2004-ansible/#author","text":"Created in 2020 by Jeff Geerling , author of Ansible for DevOps .","title":"Author"},{"location":"base/scripts/linux-firewall/","text":"**Linux Security: The Complete Iptables Firewall Guide ** https://www.udemy.com/course/linux-security-the-complete-iptables-firewall-guide/?referralCode=36A7315343CFB110068B","title":"Index"},{"location":"base/terraform/tfe/","text":"TFE (Standalone) \u00b6 Terraform modules for deploying production ready Terraform Enterprise platform. Goals vs. !Goals \u00b6 What is this repository meant to do? The goal of this repository is to accelerate the installation of TFE for our clients across several common scenarios. In other words, this should get you 80% (or more) of what you need to deploy TFE. What is this repository NOT meant to do? This repository does not claim or meant to be the single golden standard for the installation of TFE. It is not expected that this repository can be used as-is, with zero modification, rather is customized per installation by swapping out and modifying the existing submodules. How to use this repository for a client \u00b6 Note: This is by no means the only way to interact with this repository, but simply the general idea of how it is intended to be used. Create a branch off of master. Pick the example that matches your use case the closest (see below). Create a folder specific to the client (i.e. ./clients/SOME-BIG_COMPANY) and copy over the example. Customize the code to your use case, testing in a sandbox environment. Test and Validate things work as expected. Zip up the code and ship to the client for code review, getting the code into their source control. Deploy and Validate in the client environment. Contribute back to master and example, submodule, or documentation changes that could be helpful. All Examples \u00b6 All examples are very similar but differ by the choice of load balancing. This section is applicable for each example. How to Run (quick) \u00b6 cd into an example folder. Create a terraform.tfvars file (check out the README.md in the examples folder for a complete list of variables) namespace = \"USERNAME-tfe\" location = \"centralus\" domain = \"company.com\" subdomain = \"tfe\" distribution = \"ubuntu\" public_ip_allowlist = [ \"x.x.x.x\", # Your current IP ] tags = { owner = \"USERNAME\" } Place your TFE License file in ./keys/tfe-license.rli If using the tls module provided, please do the following first: Create a blank pfx file touch ./keys/certificate.pfx . Target apply the TLS module terraform apply -target module.tls There is a concurrency issue, this will generate the PFX file format from the created cert. NOTE: Fix for this is in progress, feel free to create your own certs instead of using the provided tls module terraform plan and terraform apply Potential Issues \u00b6 One potential issue is the need for TLS certificates that are publicly trusted, these examples use Let's Encrypt (sub-module modules/tls-acme ) for such tasks however it does require you own a domain. You can instead use self-signed certificate (also available as a sub-module modules/tls-private ), however this will lead to VCS integration challenges for Public SaaS Git Repo's. Modules \u00b6 Examples are just wired up modules from the /modules/ directory. tls \u00b6 Helpful modules to generate TLS certificates. Optionally you can bring your own keys. There are two in this repository: tls-acme Uses Let's Encrypt. You must own a domain and have access to update DNS for the challenge. tls-private Creates a private CA and then generates a cert from that. networking \u00b6 Creates the needed Azure Networking layer that is needed for the example to run. Resources created: Azure Virtual Network (Vnet) Azure Subnets Azure Network Security Group (NSG) Associated NSG Rules Bastion (Azure VM) Needed to access underlying TFE Instance. Optional if you can network route by other means (Express Route, etc\u2026) external-services \u00b6 Creates the external services needed, specifically the database and object storage. Resources created: Azure Postgres Azure Storage Account configs \u00b6 Creates the configuration files and startup script needed to install TFE. To enable debugging, by default this module will create files in .terraform/* that have the exact same contents as what will be deployed to the instance. Specifically these files are created: .terraform/replicated-conf.json replicated config file .terraform/replicated-tfe-conf.json TFE config files .terraform/startup_script.sh startup script to run on VM first boot One handy trick to quickly develop/change these configs is to run a terraform apply -target module.configs and validate that these files look correct. To configure TLS use this block: tls_config = { self-signed = false # When true, the replicated config will be \"self-signed\", when false it will be \"server-path\" cert = \"FULL CHAIN PEM ENCODED CERT\" key = \"CERT PRIVATE KEY\" } To configure Airgap (if desired) use this block: tfe_airgap = { enabled = true # When true adds airgap bits to startup script url = \"URL TO DOWNLOAD AIRGAP BUNDLE\" replicated_url = \"URL TO DOWNLOAD REPLICATED TAR\" } key-vault \u00b6 Creates an Azure Key Vault and write secrets to it. This is helpful when you wish to keep secrets out of the startup script and rather have the instance pull these during first boot. Resource created: Azure Key Vault Firewall active to restrict to the subnet the instance is running on load-balancer \u00b6 Helpful modules to create the load-balancer level. This will vary based on which example you are using. More details are in the \"Specific Examples\" section There are three in this repository: public-load-balancer TLS Pass-through Requires public IP public-application-gateway TLS Termination and Pass-through (Cert must be on both AAG and the instance) App Gateway v2 private-application-gateway TLS Termination and Pass-through (Cert must be on both AAG and the instance) App Gateway v1 tfe \u00b6 Creates the infrastructure to run Terraform Enterprise. Resource created: Azure VM Scale Set (VMSS) Creates an Managed Service Identity (MSI) Authorizes the VMSS MSI to read secrets from Azure Key Vault Specific Examples \u00b6 There are several end to end examples to demonstrate a few common scenarios. Each example can be run to go from nothing, to deployed TFE application. Public Load Balancer \u00b6 The Azure Public Load Balancer is a layer-4 LB and offers the simplest solution Azure has to offer. In this mode you must do TLS pass-through and can not use a Web Application Firewall (WAF), although this is often mitigated with other firewall appliances that sit in front of the Load Balancer. Requirements TFE License (*.rli) Publicly Trusted TLS [VCS Integration and SSO] Public DNS Entry [VCS Integration and SSO] For more details view the example README Public Application Gateway \u00b6 The Azure Public Application Gateway is a layer-7 LB and offers more features at the cost of complexity (and reliability). In this mode you can do TLS termination, however, you must also serve the same certificate on the backend instances essentially creating a pass-through scenario. You can use a Web Application Firewall (WAF) in this configuration, however we do not at this time have specific configurations for TFE specifically, rather generic OWASP Top 10 type attack vector protections. Overall the Application Gateway typically causes more problems than it solves. In the Public configuration, Application Gateway can utilize version 2 of the PaaS in Azure. Requirements TFE License (*.rli) Publicly Trusted TLS [VCS Integration and SSO] Public DNS Entry [VCS Integration and SSO] For more details view the example README Private Application Gateway \u00b6 The Azure Private Application Gateway is a layer-7 LB and offers more features at the cost of complexity (and reliability). In this mode you can do TLS termination, however, you must also serve the same certificate on the backend instances essentially creating a pass-through scenario, you must also upload a private CA bundle to the Application Gateway. Overall the Application Gateway typically causes more problems than it solves. In the Private configuration, Application Gateway can utilize ONLY version 1 of the PaaS in Azure. Requirements TFE License (*.rli) For more details view the example README Azure Highlights & Gotchas \u00b6 Notes around Azure specifics. Virtual Machine Scale Sets \u00b6 The Azure VMSS is used in an attempt to self heal, but also recover from a zonal outage. These should always be kept to a scale of 1. Self Healing The goal is for the VMSS to monitor the TFE app health check and re-provision the instance automatically when there is an issue. Instance status can be obtained with the VMSS Health Extension and enabled on a VMSS. The extension details can be found on the instance at /var/lib/waagent/Microsoft.ManagedServices.ApplicationHealthLinux-1.0.0 . Specifically the status folder will contain logs of status calls to help identify if errors are occurring. Note: In the status folder there will be a file for each version of the extension you have deployed, numbered such as 4.status . This file is replaced each time the health check is made, so do not expect new files, instead watch the time stamp. Testing shows this file is updated every 5 seconds. Health check must be on port 80, when using port 443 the health check failed without any good logs (my guess would be a TLS error since it doesn't have SANS entries for the instance IPs). { \"protocol\" : \"http\", \"port\" : 80, \"requestPath\" : \"/_health_check\" } Installing the extension will get you health checks, and you are able to see the status in the VMSS in the Azure portal. However, to turn on Automatic Instance Repair, the extension must be installed first. This presents a chicken/egg situation with Terraform. VMSS is created first. VMSS extension is added to the VMSS config. Automatic instance repair can now be turned on. Potential issues: With the health extension turned on, the threshold for instance action is less than 2 minutes. If the app restarts it will be enough to cause it to re-provision the instance. This is not configurable\u2026 Any instance must be alive for at least 30 minutes before action can be taken on it, so if the app fails to start on first boot, it will take 30 minutes before another attempt will be made. 30 mins is the minimum value. How to turn on automatic instance repair with Terraform on a VMSS during creation of the VMSS? Public Azure Load Balancer \u00b6 The Public Azure Load Balancer is not capable of doing TLS Termination so you must place the full chain TLS Cert/Key in PEM format on the instance running TFE. Standard Load Balancers are zone redundant by default, no additional configuration needed. Private Azure Load Balancer \u00b6 For a completely private TFE installation that is not reachable by the internet, the Private Azure Load Balancer will not work. Instead you must opt for the Private Application Gateway. This is due to a limitation with the Private Azure Load Balancer's inability to do loopback (i.e. the instance running TFE must be able to resolve to itself, from itself, through the Load Balancer) Azure Application Gateway \u00b6 There are two versions indicated by SKU's such as \"Standard\" and \"Standard_v2\". Both versions are rather difficult to work with and lacking consistency found in other cloud providers. Private Application Gateway - must use v1 (\"Standard\") SKU's TODO \u00b6 Add RHEL support Add airgap support Add WAF examples to the AAG's Figure out proper VMSS self healing configuration (i.e. can kill replicated/TFE, and the app will rebuild) Determine cleaner way to generate a PFX from PEM encoded cert/issuer/keys Known Issues \u00b6 Some Known Issues Private Azure Application Gateway unable to set specific Zones \u00b6 Error: Error Creating/Updating Application Gateway \"tfe-aag-appgateway\" (Resource Group \"tfe-aag-rg\"): network.ApplicationGatewaysClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code=\"SubscriptionNotRegisteredForFeature\" Message=\"Subscription /subscriptions//resourceGroups//providers/Microsoft.Network/subscriptions/ is not registered for feature AllowApplicationGatewayZonePinning required to carry out the requested operation.\" Details=[] Tools \u00b6 Here are a few helpful tools for working with this repository. sshuttle \u00b6 Poor man's VPN, allows proxy access through a bastion to get to private network instances. config ssh_ident = \"./.terraform/id_rsa.pem\" ssh_username = \"tfeadmin\" ssh_bastion = \"bastion-credible-kitten.centralus.cloudapp.azure.com\" ssh_cidr = \"10.0.0.0/24\" ssh_instance = \"10.0.0.6\" bastion ssh-add $ssh_ident && sshuttle -r \" ${ ssh_username } @ ${ ssh_bastion } \" $ssh_cidr client ssh-add $ssh_ident && ssh \" ${ ssh_username } @ ${ ssh_instance } \" pre-commit-terraform \u00b6 Great utility for keep documentation up to date, also used for formatting and light validation. brew install pre-commit terraform-docs Ensure versions are at least: * pre-commit >= 1.20.0 * terraform-docs >= 0.8.2 brew upgrade pre-commit terraform-docs terraform-docs openssl \u00b6 TLS certificate manipulation. Inspect a PFX for full chain openssl pkcs12 -info -in certificate.pfx","title":"TFE (Standalone)"},{"location":"base/terraform/tfe/#tfe-standalone","text":"Terraform modules for deploying production ready Terraform Enterprise platform.","title":"TFE (Standalone)"},{"location":"base/terraform/tfe/#goals-vs-goals","text":"What is this repository meant to do? The goal of this repository is to accelerate the installation of TFE for our clients across several common scenarios. In other words, this should get you 80% (or more) of what you need to deploy TFE. What is this repository NOT meant to do? This repository does not claim or meant to be the single golden standard for the installation of TFE. It is not expected that this repository can be used as-is, with zero modification, rather is customized per installation by swapping out and modifying the existing submodules.","title":"Goals vs. !Goals"},{"location":"base/terraform/tfe/#how-to-use-this-repository-for-a-client","text":"Note: This is by no means the only way to interact with this repository, but simply the general idea of how it is intended to be used. Create a branch off of master. Pick the example that matches your use case the closest (see below). Create a folder specific to the client (i.e. ./clients/SOME-BIG_COMPANY) and copy over the example. Customize the code to your use case, testing in a sandbox environment. Test and Validate things work as expected. Zip up the code and ship to the client for code review, getting the code into their source control. Deploy and Validate in the client environment. Contribute back to master and example, submodule, or documentation changes that could be helpful.","title":"How to use this repository for a client"},{"location":"base/terraform/tfe/#all-examples","text":"All examples are very similar but differ by the choice of load balancing. This section is applicable for each example.","title":"All Examples"},{"location":"base/terraform/tfe/#how-to-run-quick","text":"cd into an example folder. Create a terraform.tfvars file (check out the README.md in the examples folder for a complete list of variables) namespace = \"USERNAME-tfe\" location = \"centralus\" domain = \"company.com\" subdomain = \"tfe\" distribution = \"ubuntu\" public_ip_allowlist = [ \"x.x.x.x\", # Your current IP ] tags = { owner = \"USERNAME\" } Place your TFE License file in ./keys/tfe-license.rli If using the tls module provided, please do the following first: Create a blank pfx file touch ./keys/certificate.pfx . Target apply the TLS module terraform apply -target module.tls There is a concurrency issue, this will generate the PFX file format from the created cert. NOTE: Fix for this is in progress, feel free to create your own certs instead of using the provided tls module terraform plan and terraform apply","title":"How to Run (quick)"},{"location":"base/terraform/tfe/#potential-issues","text":"One potential issue is the need for TLS certificates that are publicly trusted, these examples use Let's Encrypt (sub-module modules/tls-acme ) for such tasks however it does require you own a domain. You can instead use self-signed certificate (also available as a sub-module modules/tls-private ), however this will lead to VCS integration challenges for Public SaaS Git Repo's.","title":"Potential Issues"},{"location":"base/terraform/tfe/#modules","text":"Examples are just wired up modules from the /modules/ directory.","title":"Modules"},{"location":"base/terraform/tfe/#tls","text":"Helpful modules to generate TLS certificates. Optionally you can bring your own keys. There are two in this repository: tls-acme Uses Let's Encrypt. You must own a domain and have access to update DNS for the challenge. tls-private Creates a private CA and then generates a cert from that.","title":"tls"},{"location":"base/terraform/tfe/#networking","text":"Creates the needed Azure Networking layer that is needed for the example to run. Resources created: Azure Virtual Network (Vnet) Azure Subnets Azure Network Security Group (NSG) Associated NSG Rules Bastion (Azure VM) Needed to access underlying TFE Instance. Optional if you can network route by other means (Express Route, etc\u2026)","title":"networking"},{"location":"base/terraform/tfe/#external-services","text":"Creates the external services needed, specifically the database and object storage. Resources created: Azure Postgres Azure Storage Account","title":"external-services"},{"location":"base/terraform/tfe/#configs","text":"Creates the configuration files and startup script needed to install TFE. To enable debugging, by default this module will create files in .terraform/* that have the exact same contents as what will be deployed to the instance. Specifically these files are created: .terraform/replicated-conf.json replicated config file .terraform/replicated-tfe-conf.json TFE config files .terraform/startup_script.sh startup script to run on VM first boot One handy trick to quickly develop/change these configs is to run a terraform apply -target module.configs and validate that these files look correct. To configure TLS use this block: tls_config = { self-signed = false # When true, the replicated config will be \"self-signed\", when false it will be \"server-path\" cert = \"FULL CHAIN PEM ENCODED CERT\" key = \"CERT PRIVATE KEY\" } To configure Airgap (if desired) use this block: tfe_airgap = { enabled = true # When true adds airgap bits to startup script url = \"URL TO DOWNLOAD AIRGAP BUNDLE\" replicated_url = \"URL TO DOWNLOAD REPLICATED TAR\" }","title":"configs"},{"location":"base/terraform/tfe/#key-vault","text":"Creates an Azure Key Vault and write secrets to it. This is helpful when you wish to keep secrets out of the startup script and rather have the instance pull these during first boot. Resource created: Azure Key Vault Firewall active to restrict to the subnet the instance is running on","title":"key-vault"},{"location":"base/terraform/tfe/#load-balancer","text":"Helpful modules to create the load-balancer level. This will vary based on which example you are using. More details are in the \"Specific Examples\" section There are three in this repository: public-load-balancer TLS Pass-through Requires public IP public-application-gateway TLS Termination and Pass-through (Cert must be on both AAG and the instance) App Gateway v2 private-application-gateway TLS Termination and Pass-through (Cert must be on both AAG and the instance) App Gateway v1","title":"load-balancer"},{"location":"base/terraform/tfe/#tfe","text":"Creates the infrastructure to run Terraform Enterprise. Resource created: Azure VM Scale Set (VMSS) Creates an Managed Service Identity (MSI) Authorizes the VMSS MSI to read secrets from Azure Key Vault","title":"tfe"},{"location":"base/terraform/tfe/#specific-examples","text":"There are several end to end examples to demonstrate a few common scenarios. Each example can be run to go from nothing, to deployed TFE application.","title":"Specific Examples"},{"location":"base/terraform/tfe/#public-load-balancer","text":"The Azure Public Load Balancer is a layer-4 LB and offers the simplest solution Azure has to offer. In this mode you must do TLS pass-through and can not use a Web Application Firewall (WAF), although this is often mitigated with other firewall appliances that sit in front of the Load Balancer. Requirements TFE License (*.rli) Publicly Trusted TLS [VCS Integration and SSO] Public DNS Entry [VCS Integration and SSO] For more details view the example README","title":"Public Load Balancer"},{"location":"base/terraform/tfe/#public-application-gateway","text":"The Azure Public Application Gateway is a layer-7 LB and offers more features at the cost of complexity (and reliability). In this mode you can do TLS termination, however, you must also serve the same certificate on the backend instances essentially creating a pass-through scenario. You can use a Web Application Firewall (WAF) in this configuration, however we do not at this time have specific configurations for TFE specifically, rather generic OWASP Top 10 type attack vector protections. Overall the Application Gateway typically causes more problems than it solves. In the Public configuration, Application Gateway can utilize version 2 of the PaaS in Azure. Requirements TFE License (*.rli) Publicly Trusted TLS [VCS Integration and SSO] Public DNS Entry [VCS Integration and SSO] For more details view the example README","title":"Public Application Gateway"},{"location":"base/terraform/tfe/#private-application-gateway","text":"The Azure Private Application Gateway is a layer-7 LB and offers more features at the cost of complexity (and reliability). In this mode you can do TLS termination, however, you must also serve the same certificate on the backend instances essentially creating a pass-through scenario, you must also upload a private CA bundle to the Application Gateway. Overall the Application Gateway typically causes more problems than it solves. In the Private configuration, Application Gateway can utilize ONLY version 1 of the PaaS in Azure. Requirements TFE License (*.rli) For more details view the example README","title":"Private Application Gateway"},{"location":"base/terraform/tfe/#azure-highlights--gotchas","text":"Notes around Azure specifics.","title":"Azure Highlights &amp; Gotchas"},{"location":"base/terraform/tfe/#virtual-machine-scale-sets","text":"The Azure VMSS is used in an attempt to self heal, but also recover from a zonal outage. These should always be kept to a scale of 1. Self Healing The goal is for the VMSS to monitor the TFE app health check and re-provision the instance automatically when there is an issue. Instance status can be obtained with the VMSS Health Extension and enabled on a VMSS. The extension details can be found on the instance at /var/lib/waagent/Microsoft.ManagedServices.ApplicationHealthLinux-1.0.0 . Specifically the status folder will contain logs of status calls to help identify if errors are occurring. Note: In the status folder there will be a file for each version of the extension you have deployed, numbered such as 4.status . This file is replaced each time the health check is made, so do not expect new files, instead watch the time stamp. Testing shows this file is updated every 5 seconds. Health check must be on port 80, when using port 443 the health check failed without any good logs (my guess would be a TLS error since it doesn't have SANS entries for the instance IPs). { \"protocol\" : \"http\", \"port\" : 80, \"requestPath\" : \"/_health_check\" } Installing the extension will get you health checks, and you are able to see the status in the VMSS in the Azure portal. However, to turn on Automatic Instance Repair, the extension must be installed first. This presents a chicken/egg situation with Terraform. VMSS is created first. VMSS extension is added to the VMSS config. Automatic instance repair can now be turned on. Potential issues: With the health extension turned on, the threshold for instance action is less than 2 minutes. If the app restarts it will be enough to cause it to re-provision the instance. This is not configurable\u2026 Any instance must be alive for at least 30 minutes before action can be taken on it, so if the app fails to start on first boot, it will take 30 minutes before another attempt will be made. 30 mins is the minimum value. How to turn on automatic instance repair with Terraform on a VMSS during creation of the VMSS?","title":"Virtual Machine Scale Sets"},{"location":"base/terraform/tfe/#public-azure-load-balancer","text":"The Public Azure Load Balancer is not capable of doing TLS Termination so you must place the full chain TLS Cert/Key in PEM format on the instance running TFE. Standard Load Balancers are zone redundant by default, no additional configuration needed.","title":"Public Azure Load Balancer"},{"location":"base/terraform/tfe/#private-azure-load-balancer","text":"For a completely private TFE installation that is not reachable by the internet, the Private Azure Load Balancer will not work. Instead you must opt for the Private Application Gateway. This is due to a limitation with the Private Azure Load Balancer's inability to do loopback (i.e. the instance running TFE must be able to resolve to itself, from itself, through the Load Balancer)","title":"Private Azure Load Balancer"},{"location":"base/terraform/tfe/#azure-application-gateway","text":"There are two versions indicated by SKU's such as \"Standard\" and \"Standard_v2\". Both versions are rather difficult to work with and lacking consistency found in other cloud providers. Private Application Gateway - must use v1 (\"Standard\") SKU's","title":"Azure Application Gateway"},{"location":"base/terraform/tfe/#todo","text":"Add RHEL support Add airgap support Add WAF examples to the AAG's Figure out proper VMSS self healing configuration (i.e. can kill replicated/TFE, and the app will rebuild) Determine cleaner way to generate a PFX from PEM encoded cert/issuer/keys","title":"TODO"},{"location":"base/terraform/tfe/#known-issues","text":"Some Known Issues","title":"Known Issues"},{"location":"base/terraform/tfe/#private-azure-application-gateway-unable-to-set-specific-zones","text":"Error: Error Creating/Updating Application Gateway \"tfe-aag-appgateway\" (Resource Group \"tfe-aag-rg\"): network.ApplicationGatewaysClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code=\"SubscriptionNotRegisteredForFeature\" Message=\"Subscription /subscriptions//resourceGroups//providers/Microsoft.Network/subscriptions/ is not registered for feature AllowApplicationGatewayZonePinning required to carry out the requested operation.\" Details=[]","title":"Private Azure Application Gateway unable to set specific Zones"},{"location":"base/terraform/tfe/#tools","text":"Here are a few helpful tools for working with this repository.","title":"Tools"},{"location":"base/terraform/tfe/#sshuttle","text":"Poor man's VPN, allows proxy access through a bastion to get to private network instances. config ssh_ident = \"./.terraform/id_rsa.pem\" ssh_username = \"tfeadmin\" ssh_bastion = \"bastion-credible-kitten.centralus.cloudapp.azure.com\" ssh_cidr = \"10.0.0.0/24\" ssh_instance = \"10.0.0.6\" bastion ssh-add $ssh_ident && sshuttle -r \" ${ ssh_username } @ ${ ssh_bastion } \" $ssh_cidr client ssh-add $ssh_ident && ssh \" ${ ssh_username } @ ${ ssh_instance } \"","title":"sshuttle"},{"location":"base/terraform/tfe/#pre-commit-terraform","text":"Great utility for keep documentation up to date, also used for formatting and light validation. brew install pre-commit terraform-docs Ensure versions are at least: * pre-commit >= 1.20.0 * terraform-docs >= 0.8.2 brew upgrade pre-commit terraform-docs terraform-docs","title":"pre-commit-terraform"},{"location":"base/terraform/tfe/#openssl","text":"TLS certificate manipulation. Inspect a PFX for full chain openssl pkcs12 -info -in certificate.pfx","title":"openssl"},{"location":"base/terraform/tfe/examples/private-application-gateway/","text":"Private Application Gateway \u00b6 Providers \u00b6 Name Version azurerm >= 2.0 local n/a tls n/a Inputs \u00b6 Name Description Type Default Required certificate_password The PFX certificate password. string \"\" no certificate_path The path on disk that has the PFX certificate. string \"./keys/certificate.pfx\" no distribution The images tested for the TFE submodule. (ubuntu or rhel). any n/a yes domain The domain you wish to use, this will be subdomained. example.com any n/a yes location The location to place all the resources. any n/a yes namespace The name to prefix to resources to keep them unique. any n/a yes public_ip_allowlist List of public IP addresses to allow into the network. This is required for access to the PaaS services (AKV, SA, Postgres) and the bastion. list [] no subdomain The subdomain you wish to use mycompany-tfe any n/a yes tags Tags to apply to the resource group/resources. map {} no tfe_airgap_url The encoded Storage Account SAS URL to download an airgap bundle. string \"\" no tfe_license_file Full local path to a valid TFE license file (*.rli) string \"./keys/tfe-license.rli\" no tfe_replicated_url The encoded Storage Account SAS URL to download an replicated tar. string \"\" no vm_admin_username The username to login to the TFE Virtual Machines string \"tfeadmin\" no vnet_address_space The virtual network address CIDR. string \"10.0.0.0/16\" no Outputs \u00b6 Name Description bastion Bastion access values. tfe TFE access values.","title":"Private Application Gateway"},{"location":"base/terraform/tfe/examples/private-application-gateway/#private-application-gateway","text":"","title":"Private Application Gateway"},{"location":"base/terraform/tfe/examples/private-application-gateway/#providers","text":"Name Version azurerm >= 2.0 local n/a tls n/a","title":"Providers"},{"location":"base/terraform/tfe/examples/private-application-gateway/#inputs","text":"Name Description Type Default Required certificate_password The PFX certificate password. string \"\" no certificate_path The path on disk that has the PFX certificate. string \"./keys/certificate.pfx\" no distribution The images tested for the TFE submodule. (ubuntu or rhel). any n/a yes domain The domain you wish to use, this will be subdomained. example.com any n/a yes location The location to place all the resources. any n/a yes namespace The name to prefix to resources to keep them unique. any n/a yes public_ip_allowlist List of public IP addresses to allow into the network. This is required for access to the PaaS services (AKV, SA, Postgres) and the bastion. list [] no subdomain The subdomain you wish to use mycompany-tfe any n/a yes tags Tags to apply to the resource group/resources. map {} no tfe_airgap_url The encoded Storage Account SAS URL to download an airgap bundle. string \"\" no tfe_license_file Full local path to a valid TFE license file (*.rli) string \"./keys/tfe-license.rli\" no tfe_replicated_url The encoded Storage Account SAS URL to download an replicated tar. string \"\" no vm_admin_username The username to login to the TFE Virtual Machines string \"tfeadmin\" no vnet_address_space The virtual network address CIDR. string \"10.0.0.0/16\" no","title":"Inputs"},{"location":"base/terraform/tfe/examples/private-application-gateway/#outputs","text":"Name Description bastion Bastion access values. tfe TFE access values.","title":"Outputs"},{"location":"base/terraform/tfe/examples/public-application-gateway/","text":"Public Application Gateway \u00b6 Providers \u00b6 Name Version azurerm >= 2.0 local n/a tls n/a Inputs \u00b6 Name Description Type Default Required certificate_password The PFX certificate password. string \"\" no certificate_path The path on disk that has the PFX certificate. string \"./keys/certificate.pfx\" no distribution The images tested for the TFE submodule. (ubuntu or rhel). any n/a yes domain The domain you wish to use, this will be subdomained. example.com . any n/a yes location The location to place all the resources. any n/a yes namespace The name to prefix to resources to keep them unique. any n/a yes public_ip_allowlist List of public IP addresses to allow into the network. This is required for access to the PaaS services (AKV, SA, Postgres) and the bastion. list [] no subdomain The subdomain you wish to use mycompany-tfe any n/a yes tags Tags to apply to the resource group/resources. map {} no tfe_airgap_url The encoded Storage Account SAS URL to download an airgap bundle. string \"\" no tfe_license_file Full local path to a valid TFE license file (*.rli) string \"./keys/tfe-license.rli\" no tfe_replicated_url The encoded Storage Account SAS URL to download an replicated tar. string \"\" no vm_admin_username The username to login to the TFE Virtual Machines string \"tfeadmin\" no vnet_address_space The virtual network address CIDR. string \"10.0.0.0/16\" no Outputs \u00b6 Name Description bastion Bastion access values. tfe TFE access values.","title":"Public Application Gateway"},{"location":"base/terraform/tfe/examples/public-application-gateway/#public-application-gateway","text":"","title":"Public Application Gateway"},{"location":"base/terraform/tfe/examples/public-application-gateway/#providers","text":"Name Version azurerm >= 2.0 local n/a tls n/a","title":"Providers"},{"location":"base/terraform/tfe/examples/public-application-gateway/#inputs","text":"Name Description Type Default Required certificate_password The PFX certificate password. string \"\" no certificate_path The path on disk that has the PFX certificate. string \"./keys/certificate.pfx\" no distribution The images tested for the TFE submodule. (ubuntu or rhel). any n/a yes domain The domain you wish to use, this will be subdomained. example.com . any n/a yes location The location to place all the resources. any n/a yes namespace The name to prefix to resources to keep them unique. any n/a yes public_ip_allowlist List of public IP addresses to allow into the network. This is required for access to the PaaS services (AKV, SA, Postgres) and the bastion. list [] no subdomain The subdomain you wish to use mycompany-tfe any n/a yes tags Tags to apply to the resource group/resources. map {} no tfe_airgap_url The encoded Storage Account SAS URL to download an airgap bundle. string \"\" no tfe_license_file Full local path to a valid TFE license file (*.rli) string \"./keys/tfe-license.rli\" no tfe_replicated_url The encoded Storage Account SAS URL to download an replicated tar. string \"\" no vm_admin_username The username to login to the TFE Virtual Machines string \"tfeadmin\" no vnet_address_space The virtual network address CIDR. string \"10.0.0.0/16\" no","title":"Inputs"},{"location":"base/terraform/tfe/examples/public-application-gateway/#outputs","text":"Name Description bastion Bastion access values. tfe TFE access values.","title":"Outputs"},{"location":"base/terraform/tfe/examples/public-load-balancer/","text":"Public Load Balancer \u00b6 Providers \u00b6 Name Version azurerm >= 2.0 local n/a tls n/a Inputs \u00b6 Name Description Type Default Required certificate_password The PFX certificate password. string \"\" no certificate_path The path on disk that has the PFX certificate. string \"./keys/certificate.pfx\" no distribution The images tested for the TFE submodule. (ubuntu or rhel). any n/a yes domain The domain you wish to use, this will be subdomained. example.com . any n/a yes location The location to place all the resources. any n/a yes namespace The name to prefix to resources to keep them unique. any n/a yes public_ip_allowlist List of public IP addresses to allow into the network. This is required for access to the PaaS services (AKV, SA, Postgres) and the bastion. list [] no subdomain The subdomain you wish to use mycompany-tfe any n/a yes tags Tags to apply to the resource group/resources. map {} no tfe_airgap_url The encoded Storage Account SAS URL to download an airgap bundle. string \"\" no tfe_license_file Full local path to a valid TFE license file (*.rli) string \"./keys/tfe-license.rli\" no tfe_replicated_url The encoded Storage Account SAS URL to download an replicated tar. string \"\" no vm_admin_username The username to login to the TFE Virtual Machines. string \"tfeadmin\" no vnet_address_space The virtual network address CIDR. string \"10.0.0.0/16\" no Outputs \u00b6 Name Description bastion Bastion access values. tfe TFE access values.","title":"Public Load Balancer"},{"location":"base/terraform/tfe/examples/public-load-balancer/#public-load-balancer","text":"","title":"Public Load Balancer"},{"location":"base/terraform/tfe/examples/public-load-balancer/#providers","text":"Name Version azurerm >= 2.0 local n/a tls n/a","title":"Providers"},{"location":"base/terraform/tfe/examples/public-load-balancer/#inputs","text":"Name Description Type Default Required certificate_password The PFX certificate password. string \"\" no certificate_path The path on disk that has the PFX certificate. string \"./keys/certificate.pfx\" no distribution The images tested for the TFE submodule. (ubuntu or rhel). any n/a yes domain The domain you wish to use, this will be subdomained. example.com . any n/a yes location The location to place all the resources. any n/a yes namespace The name to prefix to resources to keep them unique. any n/a yes public_ip_allowlist List of public IP addresses to allow into the network. This is required for access to the PaaS services (AKV, SA, Postgres) and the bastion. list [] no subdomain The subdomain you wish to use mycompany-tfe any n/a yes tags Tags to apply to the resource group/resources. map {} no tfe_airgap_url The encoded Storage Account SAS URL to download an airgap bundle. string \"\" no tfe_license_file Full local path to a valid TFE license file (*.rli) string \"./keys/tfe-license.rli\" no tfe_replicated_url The encoded Storage Account SAS URL to download an replicated tar. string \"\" no vm_admin_username The username to login to the TFE Virtual Machines. string \"tfeadmin\" no vnet_address_space The virtual network address CIDR. string \"10.0.0.0/16\" no","title":"Inputs"},{"location":"base/terraform/tfe/examples/public-load-balancer/#outputs","text":"Name Description bastion Bastion access values. tfe TFE access values.","title":"Outputs"},{"location":"base/terraform/tfe/modules/configs/","text":"TFE Configs \u00b6 Creates all the configuration files needed to install and configure TFE. Output is simply a BASH script for maximum portability. Not all cloud images support Cloud-Init, so this was an active decision. RHEL \u00b6 Azure CLI Install Azure RHEL Images Providers \u00b6 Name Version local n/a random n/a template n/a Inputs \u00b6 Name Description Type Default Required add_bash_debug If set to true, write some helpful debugging Bash bits to /etc/profile.d/tfe.sh. bool false no distribution Type of linux distribution to use. (ubuntu or rhel). string \"ubuntu\" no hostname FQDN of the tfe application (i.e. tfe.company.com). any n/a yes license_file Path to license file for the application. string n/a yes object_store_config Object storage configuration. object({ account_name = string account_key = string container = string }) { \"account_key\": \"\", \"account_name\": \"\", \"container\": \"\" } no postgres_config Postgres configuration. object({ netloc = string dbname = string user = string password = string extra_params = string }) { \"dbname\": \"\", \"extra_params\": \"\", \"netloc\": \"\", \"password\": \"\", \"user\": \"\" } no release_sequence The sequence ID for the Terraform Enterprise version to pin the cluster to. string \"latest\" no settings-source Source of the settings, either local or azkeyvault. When local, you must pass in tls_config, postgres_config, and object_store_config. When azkeyvault, settings are read from azure key vault at run time. string \"local\" no tfe_airgap TFE airgap bundlle url. object({ enabled = bool url = string replicated_url = string }) { \"enabled\": false, \"replicated_url\": \"\", \"url\": \"\" } no tfe_install_url TFE installer script url. Defaults to HashiCorp hosted link. string \"https://install.terraform.io/ptfe/stable\" no tls_config TLS configuration to use with TFE. Default will use self-signed. object({ self-signed = bool cert = string key = string }) { \"cert\": \"\", \"key\": \"\", \"self-signed\": true } no Outputs \u00b6 Name Description console_password The generated password for the admin console. key_vault_secrets Secrets that can be written to Azure Key Vault and then read from the instance during first boot. startup_script Rendered BASH file to use instead of cloud-init (for cases when cloud-init is not present).","title":"TFE Configs"},{"location":"base/terraform/tfe/modules/configs/#tfe-configs","text":"Creates all the configuration files needed to install and configure TFE. Output is simply a BASH script for maximum portability. Not all cloud images support Cloud-Init, so this was an active decision.","title":"TFE Configs"},{"location":"base/terraform/tfe/modules/configs/#rhel","text":"Azure CLI Install Azure RHEL Images","title":"RHEL"},{"location":"base/terraform/tfe/modules/configs/#providers","text":"Name Version local n/a random n/a template n/a","title":"Providers"},{"location":"base/terraform/tfe/modules/configs/#inputs","text":"Name Description Type Default Required add_bash_debug If set to true, write some helpful debugging Bash bits to /etc/profile.d/tfe.sh. bool false no distribution Type of linux distribution to use. (ubuntu or rhel). string \"ubuntu\" no hostname FQDN of the tfe application (i.e. tfe.company.com). any n/a yes license_file Path to license file for the application. string n/a yes object_store_config Object storage configuration. object({ account_name = string account_key = string container = string }) { \"account_key\": \"\", \"account_name\": \"\", \"container\": \"\" } no postgres_config Postgres configuration. object({ netloc = string dbname = string user = string password = string extra_params = string }) { \"dbname\": \"\", \"extra_params\": \"\", \"netloc\": \"\", \"password\": \"\", \"user\": \"\" } no release_sequence The sequence ID for the Terraform Enterprise version to pin the cluster to. string \"latest\" no settings-source Source of the settings, either local or azkeyvault. When local, you must pass in tls_config, postgres_config, and object_store_config. When azkeyvault, settings are read from azure key vault at run time. string \"local\" no tfe_airgap TFE airgap bundlle url. object({ enabled = bool url = string replicated_url = string }) { \"enabled\": false, \"replicated_url\": \"\", \"url\": \"\" } no tfe_install_url TFE installer script url. Defaults to HashiCorp hosted link. string \"https://install.terraform.io/ptfe/stable\" no tls_config TLS configuration to use with TFE. Default will use self-signed. object({ self-signed = bool cert = string key = string }) { \"cert\": \"\", \"key\": \"\", \"self-signed\": true } no","title":"Inputs"},{"location":"base/terraform/tfe/modules/configs/#outputs","text":"Name Description console_password The generated password for the admin console. key_vault_secrets Secrets that can be written to Azure Key Vault and then read from the instance during first boot. startup_script Rendered BASH file to use instead of cloud-init (for cases when cloud-init is not present).","title":"Outputs"},{"location":"base/terraform/tfe/modules/external-services/","text":"Azure External Services \u00b6 Create the database and object storage external servics in Azure. Azure Postgres Azure Storage Account Providers \u00b6 Name Version azurerm n/a random n/a Inputs \u00b6 Name Description Type Default Required common_tags The tags to apply to all resources. map {} no location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes postgres_sku_name SKU Short name: tier + family + cores string \"GP_Gen5_2\" no postgres_user Postgres user name. string \"psqladmin\" no public_ip_allowlist List of public IPs that need direct access to the PaaS in the Vnet (Optional). list(string) [] no resource_group_name Name of the Resource Group to place resources in. any n/a yes storage_account Storage Account Azure settings. object({ tier = string type = string }) { \"tier\": \"Standard\", \"type\": \"LRS\" } no subnet_id The subnet id to place the External Services in. any n/a yes Outputs \u00b6 Name Description object_storage_config Object storage configuration. postgres_config Database storage configuration.","title":"Azure External Services"},{"location":"base/terraform/tfe/modules/external-services/#azure-external-services","text":"Create the database and object storage external servics in Azure. Azure Postgres Azure Storage Account","title":"Azure External Services"},{"location":"base/terraform/tfe/modules/external-services/#providers","text":"Name Version azurerm n/a random n/a","title":"Providers"},{"location":"base/terraform/tfe/modules/external-services/#inputs","text":"Name Description Type Default Required common_tags The tags to apply to all resources. map {} no location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes postgres_sku_name SKU Short name: tier + family + cores string \"GP_Gen5_2\" no postgres_user Postgres user name. string \"psqladmin\" no public_ip_allowlist List of public IPs that need direct access to the PaaS in the Vnet (Optional). list(string) [] no resource_group_name Name of the Resource Group to place resources in. any n/a yes storage_account Storage Account Azure settings. object({ tier = string type = string }) { \"tier\": \"Standard\", \"type\": \"LRS\" } no subnet_id The subnet id to place the External Services in. any n/a yes","title":"Inputs"},{"location":"base/terraform/tfe/modules/external-services/#outputs","text":"Name Description object_storage_config Object storage configuration. postgres_config Database storage configuration.","title":"Outputs"},{"location":"base/terraform/tfe/modules/key-vault/","text":"Azure Key Vault \u00b6 Create a Key Vault to store TFE specific settings that can be retrieved during the first boot. Will also add an access policy to the AKV for the identity creating the AKV, this will allow visibility of secrets in the Azure Portal. However, this can be removed and is only here for debugging purposes. Providers \u00b6 Name Version azurerm n/a random n/a Inputs \u00b6 Name Description Type Default Required common_tags The tags to apply to all resources. map {} no location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes public_ip_allowlist List of public IPs that need direct access to the PaaS in the Vnet (Optional). list(string) [] no resource_group_name Name of the Resource Group to place resources in. any n/a yes secrets Secrets to save to the Azure Key Vault. list(object({ name = string content_type = string value = string })) [] no subnet_id The subnet id to place the External Services in. any n/a yes Outputs \u00b6 Name Description keyvault Key vault secrets configuration.","title":"Azure Key Vault"},{"location":"base/terraform/tfe/modules/key-vault/#azure-key-vault","text":"Create a Key Vault to store TFE specific settings that can be retrieved during the first boot. Will also add an access policy to the AKV for the identity creating the AKV, this will allow visibility of secrets in the Azure Portal. However, this can be removed and is only here for debugging purposes.","title":"Azure Key Vault"},{"location":"base/terraform/tfe/modules/key-vault/#providers","text":"Name Version azurerm n/a random n/a","title":"Providers"},{"location":"base/terraform/tfe/modules/key-vault/#inputs","text":"Name Description Type Default Required common_tags The tags to apply to all resources. map {} no location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes public_ip_allowlist List of public IPs that need direct access to the PaaS in the Vnet (Optional). list(string) [] no resource_group_name Name of the Resource Group to place resources in. any n/a yes secrets Secrets to save to the Azure Key Vault. list(object({ name = string content_type = string value = string })) [] no subnet_id The subnet id to place the External Services in. any n/a yes","title":"Inputs"},{"location":"base/terraform/tfe/modules/key-vault/#outputs","text":"Name Description keyvault Key vault secrets configuration.","title":"Outputs"},{"location":"base/terraform/tfe/modules/networking/","text":"Azure Networking \u00b6 Creates a base set of networking infrastructure that can be used to deploy instances to. Also will create a Bastion to allow for access to the private instances. Virtual Network Subnets (names/CIDR's that are provided) Bastion Network Security Group rules to provide initial set of security Providers \u00b6 Name Version azurerm n/a random n/a Inputs \u00b6 Name Description Type Default Required bastion Bastion public ssh username and key. object({ username = string public_key = string }) { \"public_key\": \"\", \"username\": \"\" } no common_tags The tags to apply to all resources. map {} no location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes public_ip_allowlist List of public IPs that need direct access to the PaaS in the Vnet. list(string) [] no resource_group_name Name of the Resource Group to place resources in. any n/a yes subnet_address_spaces A list of subnet address spaces and names. list(object({ name = string address_space = string })) n/a yes vnet_address_space The network address CIDR for the Vnet address space. any n/a yes Outputs \u00b6 Name Description bastion Bastion connectiong configuration. location n/a networking Networking configuration. resource_group_name n/a","title":"Azure Networking"},{"location":"base/terraform/tfe/modules/networking/#azure-networking","text":"Creates a base set of networking infrastructure that can be used to deploy instances to. Also will create a Bastion to allow for access to the private instances. Virtual Network Subnets (names/CIDR's that are provided) Bastion Network Security Group rules to provide initial set of security","title":"Azure Networking"},{"location":"base/terraform/tfe/modules/networking/#providers","text":"Name Version azurerm n/a random n/a","title":"Providers"},{"location":"base/terraform/tfe/modules/networking/#inputs","text":"Name Description Type Default Required bastion Bastion public ssh username and key. object({ username = string public_key = string }) { \"public_key\": \"\", \"username\": \"\" } no common_tags The tags to apply to all resources. map {} no location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes public_ip_allowlist List of public IPs that need direct access to the PaaS in the Vnet. list(string) [] no resource_group_name Name of the Resource Group to place resources in. any n/a yes subnet_address_spaces A list of subnet address spaces and names. list(object({ name = string address_space = string })) n/a yes vnet_address_space The network address CIDR for the Vnet address space. any n/a yes","title":"Inputs"},{"location":"base/terraform/tfe/modules/networking/#outputs","text":"Name Description bastion Bastion connectiong configuration. location n/a networking Networking configuration. resource_group_name n/a","title":"Outputs"},{"location":"base/terraform/tfe/modules/private-application-gateway/","text":"Azure Application Gateway (Private) \u00b6 Creates a private Application Gateway. Providers \u00b6 Name Version azurerm n/a Inputs \u00b6 Name Description Type Default Required common_tags The tags to apply to all resources. map {} no hostname FQDN of the tfe application (i.e. tfe.company.com). any n/a yes ip_address The static IP address to assign the App Gateway on the private network. any n/a yes location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes resource_group_name Name of the Resource Group to place resources in. any n/a yes subnet_id The subnet id to place the External Services in. any n/a yes tls TLS Certificate configuration. object({ name = string pfx_b64 = string pfx_password = string cert_b64 = string }) n/a yes Outputs \u00b6 Name Description application_gateway_id Application Gateway id. backend_address_pool_id Backend addresss pool, for use with the VM Scale Set. health_probe_id Health Probe Id for the health checks on the load balancer. private_ip Private IP of the Application Gateway.","title":"Azure Application Gateway (Private)"},{"location":"base/terraform/tfe/modules/private-application-gateway/#azure-application-gateway-private","text":"Creates a private Application Gateway.","title":"Azure Application Gateway (Private)"},{"location":"base/terraform/tfe/modules/private-application-gateway/#providers","text":"Name Version azurerm n/a","title":"Providers"},{"location":"base/terraform/tfe/modules/private-application-gateway/#inputs","text":"Name Description Type Default Required common_tags The tags to apply to all resources. map {} no hostname FQDN of the tfe application (i.e. tfe.company.com). any n/a yes ip_address The static IP address to assign the App Gateway on the private network. any n/a yes location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes resource_group_name Name of the Resource Group to place resources in. any n/a yes subnet_id The subnet id to place the External Services in. any n/a yes tls TLS Certificate configuration. object({ name = string pfx_b64 = string pfx_password = string cert_b64 = string }) n/a yes","title":"Inputs"},{"location":"base/terraform/tfe/modules/private-application-gateway/#outputs","text":"Name Description application_gateway_id Application Gateway id. backend_address_pool_id Backend addresss pool, for use with the VM Scale Set. health_probe_id Health Probe Id for the health checks on the load balancer. private_ip Private IP of the Application Gateway.","title":"Outputs"},{"location":"base/terraform/tfe/modules/public-application-gateway/","text":"Azure Application Gateway (Public) \u00b6 Creates a public Application Gateway with a Public IP. Providers \u00b6 Name Version azurerm n/a random n/a Inputs \u00b6 Name Description Type Default Required common_tags The tags to apply to all resources. map {} no hostname FQDN of the tfe application (i.e. tfe.company.com) any n/a yes location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes resource_group_name Name of the Resource Group to place resources in. any n/a yes subnet_id The subnet id to place the External Services in. any n/a yes tls TLS Certificate configuration. object({ name = string pfx_b64 = string pfx_password = string }) n/a yes Outputs \u00b6 Name Description application_gateway_id Application Gateway id. backend_address_pool_id Backend addresss pool, for use with the VM Scale Set. health_probe_id Health Probe Id for the health checks on the Application Gateway. load_balancer_domain_label Public FQDN of the Application Gateway. DNS provided by Azure. public_ip Public IP of the Application Gateway.","title":"Azure Application Gateway (Public)"},{"location":"base/terraform/tfe/modules/public-application-gateway/#azure-application-gateway-public","text":"Creates a public Application Gateway with a Public IP.","title":"Azure Application Gateway (Public)"},{"location":"base/terraform/tfe/modules/public-application-gateway/#providers","text":"Name Version azurerm n/a random n/a","title":"Providers"},{"location":"base/terraform/tfe/modules/public-application-gateway/#inputs","text":"Name Description Type Default Required common_tags The tags to apply to all resources. map {} no hostname FQDN of the tfe application (i.e. tfe.company.com) any n/a yes location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes resource_group_name Name of the Resource Group to place resources in. any n/a yes subnet_id The subnet id to place the External Services in. any n/a yes tls TLS Certificate configuration. object({ name = string pfx_b64 = string pfx_password = string }) n/a yes","title":"Inputs"},{"location":"base/terraform/tfe/modules/public-application-gateway/#outputs","text":"Name Description application_gateway_id Application Gateway id. backend_address_pool_id Backend addresss pool, for use with the VM Scale Set. health_probe_id Health Probe Id for the health checks on the Application Gateway. load_balancer_domain_label Public FQDN of the Application Gateway. DNS provided by Azure. public_ip Public IP of the Application Gateway.","title":"Outputs"},{"location":"base/terraform/tfe/modules/public-load-balancer/","text":"README \u00b6 Providers \u00b6 Name Version azurerm n/a random n/a Inputs \u00b6 Name Description Type Default Required common_tags The tags to apply to all resources. map {} no lb_sku Load Balancer SKU (Standard or Basic). string \"Standard\" no location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes resource_group_name Name of the Resource Group to place resources in. any n/a yes Outputs \u00b6 Name Description backend_address_pool_id Backend addresss pool, for use with the VM Scale Set. health_probe_id Health Probe Id for the health checks on the load balancer. load_balancer_domain_label Public FQDN of the Load Balancer. DNS provided by Azure. load_balancer_id Load Balancer id. public_ip Public IP of the Load Balancer.","title":"README"},{"location":"base/terraform/tfe/modules/public-load-balancer/#readme","text":"","title":"README"},{"location":"base/terraform/tfe/modules/public-load-balancer/#providers","text":"Name Version azurerm n/a random n/a","title":"Providers"},{"location":"base/terraform/tfe/modules/public-load-balancer/#inputs","text":"Name Description Type Default Required common_tags The tags to apply to all resources. map {} no lb_sku Load Balancer SKU (Standard or Basic). string \"Standard\" no location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes resource_group_name Name of the Resource Group to place resources in. any n/a yes","title":"Inputs"},{"location":"base/terraform/tfe/modules/public-load-balancer/#outputs","text":"Name Description backend_address_pool_id Backend addresss pool, for use with the VM Scale Set. health_probe_id Health Probe Id for the health checks on the load balancer. load_balancer_domain_label Public FQDN of the Load Balancer. DNS provided by Azure. load_balancer_id Load Balancer id. public_ip Public IP of the Load Balancer.","title":"Outputs"},{"location":"base/terraform/tfe/modules/tfe/","text":"Terraform Enterprise \u00b6 Creates the infrastructure to host TFE. Providers \u00b6 Name Version azurerm n/a Inputs \u00b6 Name Description Type Default Required common_tags The tags to apply to all resources. map {} no keyvault_id The Azure KeyVault id to use for secrets. string \"\" no lb Load balancer to attach to the VMSS. Type must be one of the following ['ALB', 'AAG']. object({ type = string backend_address_pool_id = string health_probe_id = string }) n/a yes location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes resource_group_name Name of the Resource Group to place resources in. any n/a yes startup_script Startup script to install and configure TFE. string \"\" no subnet_id The subnet id to place the External Services in. any n/a yes tfe_image Marketplace image for VMSS. object({ publisher = string offer = string sku = string version = string }) { \"offer\": \"UbuntuServer\", \"publisher\": \"Canonical\", \"sku\": \"18.04-LTS\", \"version\": \"latest\" } no vm_sku The VM instance SKU to use. string \"Standard_D2s_v3\" no vm_user VM username and public ssh key. object({ username = string public_key = string }) n/a yes Outputs \u00b6 No output.","title":"Terraform Enterprise"},{"location":"base/terraform/tfe/modules/tfe/#terraform-enterprise","text":"Creates the infrastructure to host TFE.","title":"Terraform Enterprise"},{"location":"base/terraform/tfe/modules/tfe/#providers","text":"Name Version azurerm n/a","title":"Providers"},{"location":"base/terraform/tfe/modules/tfe/#inputs","text":"Name Description Type Default Required common_tags The tags to apply to all resources. map {} no keyvault_id The Azure KeyVault id to use for secrets. string \"\" no lb Load balancer to attach to the VMSS. Type must be one of the following ['ALB', 'AAG']. object({ type = string backend_address_pool_id = string health_probe_id = string }) n/a yes location The Azure region to deploy all infrastructure to. any n/a yes namespace Name to assign to resources for easy organization. any n/a yes resource_group_name Name of the Resource Group to place resources in. any n/a yes startup_script Startup script to install and configure TFE. string \"\" no subnet_id The subnet id to place the External Services in. any n/a yes tfe_image Marketplace image for VMSS. object({ publisher = string offer = string sku = string version = string }) { \"offer\": \"UbuntuServer\", \"publisher\": \"Canonical\", \"sku\": \"18.04-LTS\", \"version\": \"latest\" } no vm_sku The VM instance SKU to use. string \"Standard_D2s_v3\" no vm_user VM username and public ssh key. object({ username = string public_key = string }) n/a yes","title":"Inputs"},{"location":"base/terraform/tfe/modules/tfe/#outputs","text":"No output.","title":"Outputs"},{"location":"base/terraform/tfe/modules/tls-acme/","text":"Let's Encrypt TLS Certificates \u00b6 Creates publicly trusted TLS Certificates. Requires access to a domain you own and the ability to provide a DNS challenge request. This example uses Azure DNS as the challenge, but many others are supported. Providers \u00b6 Name Version acme n/a local n/a null n/a tls n/a Inputs \u00b6 Name Description Type Default Required certificate_password The PFX certificate password. any n/a yes certificate_path The path on disk that has the PFX certificate. any n/a yes domain The domain you wish to use, this will be subdomained. example.com any n/a yes hostname The full hostname that will be used. tfe.example.com . any n/a yes Outputs \u00b6 Name Description cert Full chain certificate in PEM format. key Certification key in PEM format. name Readable name of the certificate. password The password used to create the PFX. pfx_b64 The PFX certification in base64.","title":"Let's Encrypt TLS Certificates"},{"location":"base/terraform/tfe/modules/tls-acme/#lets-encrypt-tls-certificates","text":"Creates publicly trusted TLS Certificates. Requires access to a domain you own and the ability to provide a DNS challenge request. This example uses Azure DNS as the challenge, but many others are supported.","title":"Let's Encrypt TLS Certificates"},{"location":"base/terraform/tfe/modules/tls-acme/#providers","text":"Name Version acme n/a local n/a null n/a tls n/a","title":"Providers"},{"location":"base/terraform/tfe/modules/tls-acme/#inputs","text":"Name Description Type Default Required certificate_password The PFX certificate password. any n/a yes certificate_path The path on disk that has the PFX certificate. any n/a yes domain The domain you wish to use, this will be subdomained. example.com any n/a yes hostname The full hostname that will be used. tfe.example.com . any n/a yes","title":"Inputs"},{"location":"base/terraform/tfe/modules/tls-acme/#outputs","text":"Name Description cert Full chain certificate in PEM format. key Certification key in PEM format. name Readable name of the certificate. password The password used to create the PFX. pfx_b64 The PFX certification in base64.","title":"Outputs"},{"location":"base/terraform/tfe/modules/tls-private/","text":"Private TLS Certificates \u00b6 Creates private CA, then creates a private TLS cert from that CA. Not recommended for production, but provides a good base for testing common scenarios with private certificate authorities. Providers \u00b6 Name Version local n/a null n/a tls n/a Inputs \u00b6 Name Description Type Default Required certificate_duration Length in hours for the certificate and authority to be valid. Defaults to 6 months. number 4320 no certificate_password The PFX certificate password. any n/a yes certificate_path The path on disk that has the PFX certificate. any n/a yes domain The domain you wish to use, this will be subdomained. example.com any n/a yes hostname The full hostname that will be used. tfe.example.com any n/a yes Outputs \u00b6 Name Description cert Full chain certificate in PEM format. cert_b64 The PEM certification in base64. key Certification key in PEM format. name Readable name of the certificate. password The password used to create the PFX. pfx_b64 The PFX certification in base64.","title":"Private TLS Certificates"},{"location":"base/terraform/tfe/modules/tls-private/#private-tls-certificates","text":"Creates private CA, then creates a private TLS cert from that CA. Not recommended for production, but provides a good base for testing common scenarios with private certificate authorities.","title":"Private TLS Certificates"},{"location":"base/terraform/tfe/modules/tls-private/#providers","text":"Name Version local n/a null n/a tls n/a","title":"Providers"},{"location":"base/terraform/tfe/modules/tls-private/#inputs","text":"Name Description Type Default Required certificate_duration Length in hours for the certificate and authority to be valid. Defaults to 6 months. number 4320 no certificate_password The PFX certificate password. any n/a yes certificate_path The path on disk that has the PFX certificate. any n/a yes domain The domain you wish to use, this will be subdomained. example.com any n/a yes hostname The full hostname that will be used. tfe.example.com any n/a yes","title":"Inputs"},{"location":"base/terraform/tfe/modules/tls-private/#outputs","text":"Name Description cert Full chain certificate in PEM format. cert_b64 The PEM certification in base64. key Certification key in PEM format. name Readable name of the certificate. password The password used to create the PFX. pfx_b64 The PFX certification in base64.","title":"Outputs"},{"location":"base/vagrant/","text":"Updating hosts file \u00b6 Update IP addresses of VM which can then be accessed by Ansible /etc/ansible/hosts Creating Base Box \u00b6 Create base machine using centos. Keep the VM in running state, then run the below commands vagrant package --output centos8-server.box vagrant box add leslie/centos8 centos8-server.box Update Vagrantfile to use the new box Running the test.yml \u00b6 ansible-playbook test.yml --connection=local OPTION 3: Run Ansible from a remote machine \u00b6 NOTE: Before running Ansible, complete the following on the \u00b6 remote machine: \u00b6 # - Copy the playbook.yml, index.html, and vhost.tpl: \u00b6 scp playbook.yml index.html vhost.tpl @ : \u00b6 # - Copy the private key used for the vagrant user: \u00b6 KEY=$(vagrant ssh-config | grep IdentityFile | awk '{print $2}') \u00b6 scp \"${KEY}\" @ : \u00b6 # - Create a host inventory file: \u00b6 ssh @ \u00b6 sudo vim /etc/ansible/hosts \u00b6 # insert the following: \u00b6 ansible_host= ansible_port=2222 ansible_user=vagrant ansible_private_key_file= \u00b6 # - Verify you can ping: \u00b6 ansible all -m ping \u00b6 # | SUCCESS => { \u00b6 # \"changed\": false, \u00b6 # \"ping\": \"pong\" \u00b6 # } \u00b6 # - Run the Ansible Playbook: \u00b6 cd \u00b6 ansible-playbook playbook.yml \u00b6 # NOTE: If you destroy and rebuild the vagrant box you will want to \u00b6 remove the old entry in the remote machine's ~/.ssh/known_hosts \u00b6 file, otherwise you will get an error when trying to ping or \u00b6 run the playbook. \u00b6 # -*- mode: ruby -*- # vi: set ft=ruby : # All Vagrant configuration is done below. The \"2\" in Vagrant.configure # configures the configuration version (we support older styles for # backwards compatibility). Please don't change it unless you know what # you're doing. Vagrant . configure ( \"2\" ) do | config | # The most common configuration options are documented and commented below. # For a complete reference, please see the online documentation at # https://docs.vagrantup.com. # Every Vagrant development environment requires a box. You can search for # boxes at https://vagrantcloud.com/search. config . vm . box = \"centos/8\" config . vbguest . installer_options = { allow_kernel_upgrade : true } config . ssh . insert_key = false # Disable automatic box update checking. If you disable this, then # boxes will only be checked for updates when the user runs # `vagrant box outdated`. This is not recommended. # config.vm.box_check_update = false # Create a forwarded port mapping which allows access to a specific port # within the machine from a port on the host machine. In the example below, # accessing \"localhost:8080\" will access port 80 on the guest machine. # NOTE: This will enable public access to the opened port # config.vm.network \"forwarded_port\", guest: 80, host: 8080 # Create a forwarded port mapping which allows access to a specific port # within the machine from a port on the host machine and only allow access # via 127.0.0.1 to disable public access # config.vm.network \"forwarded_port\", guest: 80, host: 8080, host_ip: \"127.0.0.1\" # Create a private network, which allows host-only access to the machine # using a specific IP. config . vm . define \"server\" do | machine | machine . vm . network \"private_network\" , ip : \"192.168.99.100\" end # Create a public network, which generally matched to bridged network. # Bridged networks make the machine appear as another physical device on # your network. # config.vm.network \"public_network\" # Share an additional folder to the guest VM. The first argument is # the path on the host to the actual folder. The second argument is # the path on the guest to mount the folder. And the optional third # argument is a set of non-required options. # Commenting the below line for testing in WSL # config.vm.synced_folder \".\", \"/vagrant\" config . vm . synced_folder \"/mnt/c/Users/Leslie/testprojects/vagrant\" , \"/vagrant\" , disabled : true # Provider-specific configuration so you can fine-tune various # backing providers for Vagrant. These expose provider-specific options. # Example for VirtualBox: # config . vm . provider \"virtualbox\" do | vb | # # Display the VirtualBox GUI when booting the machine vb . gui = false # # # Customize the amount of memory on the VM: vb . memory = \"512\" vb . customize [ \"modifyvm\" , :id , \"--uartmode1\" , \"disconnected\" ] end # # View the documentation for the provider you are using for more # information on available options. # Enable provisioning with a shell script. Additional provisioners such as # Puppet, Chef, Ansible, Salt, and Docker are also available. Please see the # documentation for more information about their specific syntax and use. # config.vm.provision \"shell\", inline: <<-SHELL # sudo yum update -y # yum install python3 python3-pip -y # pip3 install --upgrade pip # pip3 install ansible --user # SHELL config . vm . provision \"ansible\" do | ansible | ansible . playbook = \"test.yml\" ansible . inventory_path = \"inventory\" ansible . verbose = true #ansible.install = true ansible . limit = \"all\" end end ####################### #servers=[ # { # :hostname => \"server\", # :box => \"centos/8\", # :box => \"leslie/centos8\", # :ip => \"192.168.99.100\" #}, #{ # :hostname => \"node1\", # :box => \"leslie/centos8\", # :ip => \"192.168.99.101\" # } #] servers . each do | machine | config . vm . define machine [ :hostname ] do | node | node . vm . box = machine [ :box ] # Below 2 statements are complementary. To update guest additions enable first statement node . vbguest . installer_options = { allow_kernel_upgrade : true } #node.vm.box_check_update = false node . ssh . insert_key = false # node.vm.network \"forwarded_port\", guest: 80, host: 8080 node . vm . hostname = machine [ :hostname ] node . vm . network :private_network , ip : machine [ :ip ] # node.vm.synced_folder \".\", \"/vagrant\" node . vm . synced_folder \"/mnt/c/Users/Leslie/testprojects/vagrant\" , \"/vagrant\" , disabled : true # node.vm.provision \"file\", source: \"./copiedfile.txt\", destination: \"/home/vagrant/copiedfile.txt\" node . vm . provider :virtualbox do | vb | vb . gui = false # vb.name = \"centos8_vagrant_base\" vb . memory = \"512\" vb . cpus = 1 vb . customize [ \"modifyvm\" , :id , \"--uartmode1\" , \"disconnected\" ] end #serverCount += 1 # Only execute once the Ansible provisioner, # when all the machines are up and ready. #if serverCount == 2 # config.vm.provision \"ansible\" do |ansible| # ansible.playbook = \"test.yml\" # ansible.inventory_path = \"inventory\" # ansible.verbose = true #ansible.install = true # Disable default limit to connect to all the machines # ansible.limit = \"all\" # end #end end end #######################","title":"Updating hosts file"},{"location":"base/vagrant/#updating-hosts-file","text":"Update IP addresses of VM which can then be accessed by Ansible /etc/ansible/hosts","title":"Updating hosts file"},{"location":"base/vagrant/#creating-base-box","text":"Create base machine using centos. Keep the VM in running state, then run the below commands vagrant package --output centos8-server.box vagrant box add leslie/centos8 centos8-server.box Update Vagrantfile to use the new box","title":"Creating Base Box"},{"location":"base/vagrant/#running-the-testyml","text":"ansible-playbook test.yml --connection=local","title":"Running the test.yml"},{"location":"base/vagrant/#option-3-run-ansible-from-a-remote-machine","text":"","title":"OPTION 3: Run Ansible from a remote machine"},{"location":"base/vagrant/#note-before-running-ansible-complete-the-following-on-the","text":"","title":"NOTE: Before running Ansible, complete the following on the"},{"location":"base/vagrant/#remote-machine","text":"#","title":"remote machine:"},{"location":"base/vagrant/#--copy-the-playbookyml-indexhtml-and-vhosttpl","text":"","title":"- Copy the playbook.yml, index.html, and vhost.tpl:"},{"location":"base/vagrant/#scp-playbookyml-indexhtml-vhosttpl-","text":"#","title":"scp playbook.yml index.html vhost.tpl @:"},{"location":"base/vagrant/#--copy-the-private-key-used-for-the-vagrant-user","text":"","title":"- Copy the private key used for the vagrant user:"},{"location":"base/vagrant/#keyvagrant-ssh-config--grep-identityfile--awk-print-2","text":"","title":"KEY=$(vagrant ssh-config | grep IdentityFile | awk '{print $2}')"},{"location":"base/vagrant/#scp-key-","text":"#","title":"scp \"${KEY}\" @:"},{"location":"base/vagrant/#--create-a-host-inventory-file","text":"","title":"- Create a host inventory file:"},{"location":"base/vagrant/#ssh-","text":"","title":"ssh @"},{"location":"base/vagrant/#sudo-vim-etcansiblehosts","text":"","title":"sudo vim /etc/ansible/hosts"},{"location":"base/vagrant/#-insert-the-following","text":"","title":"# insert the following:"},{"location":"base/vagrant/#ansible_host-ansible_port2222-ansible_uservagrant-ansible_private_key_file","text":"#","title":" ansible_host= ansible_port=2222 ansible_user=vagrant ansible_private_key_file="},{"location":"base/vagrant/#--verify-you-can-ping","text":"","title":"- Verify you can ping:"},{"location":"base/vagrant/#ansible-all--m-ping","text":"","title":"ansible all -m ping"},{"location":"base/vagrant/#---success--","text":"","title":"#  | SUCCESS =&gt; {"},{"location":"base/vagrant/#-----changed-false","text":"","title":"#     \"changed\": false,"},{"location":"base/vagrant/#-----ping-pong","text":"","title":"#     \"ping\": \"pong\""},{"location":"base/vagrant/#-","text":"#","title":"# }"},{"location":"base/vagrant/#--run-the-ansible-playbook","text":"","title":"- Run the Ansible Playbook:"},{"location":"base/vagrant/#cd","text":"","title":"cd "},{"location":"base/vagrant/#ansible-playbook-playbookyml","text":"#","title":"ansible-playbook playbook.yml"},{"location":"base/vagrant/#note-if-you-destroy-and-rebuild-the-vagrant-box-you-will-want-to","text":"","title":"NOTE: If you destroy and rebuild the vagrant box you will want to"},{"location":"base/vagrant/#remove-the-old-entry-in-the-remote-machines-sshknown_hosts","text":"","title":"remove the old entry in the remote machine's ~/.ssh/known_hosts"},{"location":"base/vagrant/#file-otherwise-you-will-get-an-error-when-trying-to-ping-or","text":"","title":"file, otherwise you will get an error when trying to ping or"},{"location":"base/vagrant/#run-the-playbook","text":"# -*- mode: ruby -*- # vi: set ft=ruby : # All Vagrant configuration is done below. The \"2\" in Vagrant.configure # configures the configuration version (we support older styles for # backwards compatibility). Please don't change it unless you know what # you're doing. Vagrant . configure ( \"2\" ) do | config | # The most common configuration options are documented and commented below. # For a complete reference, please see the online documentation at # https://docs.vagrantup.com. # Every Vagrant development environment requires a box. You can search for # boxes at https://vagrantcloud.com/search. config . vm . box = \"centos/8\" config . vbguest . installer_options = { allow_kernel_upgrade : true } config . ssh . insert_key = false # Disable automatic box update checking. If you disable this, then # boxes will only be checked for updates when the user runs # `vagrant box outdated`. This is not recommended. # config.vm.box_check_update = false # Create a forwarded port mapping which allows access to a specific port # within the machine from a port on the host machine. In the example below, # accessing \"localhost:8080\" will access port 80 on the guest machine. # NOTE: This will enable public access to the opened port # config.vm.network \"forwarded_port\", guest: 80, host: 8080 # Create a forwarded port mapping which allows access to a specific port # within the machine from a port on the host machine and only allow access # via 127.0.0.1 to disable public access # config.vm.network \"forwarded_port\", guest: 80, host: 8080, host_ip: \"127.0.0.1\" # Create a private network, which allows host-only access to the machine # using a specific IP. config . vm . define \"server\" do | machine | machine . vm . network \"private_network\" , ip : \"192.168.99.100\" end # Create a public network, which generally matched to bridged network. # Bridged networks make the machine appear as another physical device on # your network. # config.vm.network \"public_network\" # Share an additional folder to the guest VM. The first argument is # the path on the host to the actual folder. The second argument is # the path on the guest to mount the folder. And the optional third # argument is a set of non-required options. # Commenting the below line for testing in WSL # config.vm.synced_folder \".\", \"/vagrant\" config . vm . synced_folder \"/mnt/c/Users/Leslie/testprojects/vagrant\" , \"/vagrant\" , disabled : true # Provider-specific configuration so you can fine-tune various # backing providers for Vagrant. These expose provider-specific options. # Example for VirtualBox: # config . vm . provider \"virtualbox\" do | vb | # # Display the VirtualBox GUI when booting the machine vb . gui = false # # # Customize the amount of memory on the VM: vb . memory = \"512\" vb . customize [ \"modifyvm\" , :id , \"--uartmode1\" , \"disconnected\" ] end # # View the documentation for the provider you are using for more # information on available options. # Enable provisioning with a shell script. Additional provisioners such as # Puppet, Chef, Ansible, Salt, and Docker are also available. Please see the # documentation for more information about their specific syntax and use. # config.vm.provision \"shell\", inline: <<-SHELL # sudo yum update -y # yum install python3 python3-pip -y # pip3 install --upgrade pip # pip3 install ansible --user # SHELL config . vm . provision \"ansible\" do | ansible | ansible . playbook = \"test.yml\" ansible . inventory_path = \"inventory\" ansible . verbose = true #ansible.install = true ansible . limit = \"all\" end end ####################### #servers=[ # { # :hostname => \"server\", # :box => \"centos/8\", # :box => \"leslie/centos8\", # :ip => \"192.168.99.100\" #}, #{ # :hostname => \"node1\", # :box => \"leslie/centos8\", # :ip => \"192.168.99.101\" # } #] servers . each do | machine | config . vm . define machine [ :hostname ] do | node | node . vm . box = machine [ :box ] # Below 2 statements are complementary. To update guest additions enable first statement node . vbguest . installer_options = { allow_kernel_upgrade : true } #node.vm.box_check_update = false node . ssh . insert_key = false # node.vm.network \"forwarded_port\", guest: 80, host: 8080 node . vm . hostname = machine [ :hostname ] node . vm . network :private_network , ip : machine [ :ip ] # node.vm.synced_folder \".\", \"/vagrant\" node . vm . synced_folder \"/mnt/c/Users/Leslie/testprojects/vagrant\" , \"/vagrant\" , disabled : true # node.vm.provision \"file\", source: \"./copiedfile.txt\", destination: \"/home/vagrant/copiedfile.txt\" node . vm . provider :virtualbox do | vb | vb . gui = false # vb.name = \"centos8_vagrant_base\" vb . memory = \"512\" vb . cpus = 1 vb . customize [ \"modifyvm\" , :id , \"--uartmode1\" , \"disconnected\" ] end #serverCount += 1 # Only execute once the Ansible provisioner, # when all the machines are up and ready. #if serverCount == 2 # config.vm.provision \"ansible\" do |ansible| # ansible.playbook = \"test.yml\" # ansible.inventory_path = \"inventory\" # ansible.verbose = true #ansible.install = true # Disable default limit to connect to all the machines # ansible.limit = \"all\" # end #end end end #######################","title":"run the playbook."},{"location":"base/vagrant/provision/ansible/roles/internal/common/","text":"Common Role \u00b6 Only holds the list of dependencies that is required","title":"Index"},{"location":"base/vagrant/provision/ansible/roles/internal/common/#common-role","text":"Only holds the list of dependencies that is required","title":"Common Role"},{"location":"devops/","text":"Devops Origin Devops Metrics Trunk based Development behind Feature Flags Feature Flag Platform Cloud Native Paradigm \u00b6 1. Devops: (How to Build) - It assumes you have embraced Agile ways of working and have broken down silos in your organization that allow teams to work in harmony to deliver software. - From a practise perspective, you have abandoned waterfall , work in short cycles and are hyper focused on delivering software quickly. 2. Microservices: (What to Build) - Cloud Native approaches advocate for buidling applications with a microservices architecture . You aim to build systems that are modular, service based and lossely coupled. - This can be achieved by directing integrations through API interfaces and by applying design principles similar to those defined by the 12-Factor App model. - In general, cloud native applications are made of small services that can be developed and released independently. 3. Automation: (How to Deploy) - Cloud Native apps rely heavily on automation . - The switch to building modular systems with many services makes automated CI/CD pipelines more critical for building and packaging software into an artifact. - After automated tests and quality assurance gates pass, the artifacts are stored in a repository where they wait for an automated agent to deploy and release them into an environment . - What is released into the system and the system\u2019s configuration is determined by code that describes the immutable versions of the infrastructure. 4. Containers: (Where to run) - Cloud native centers around the idea that you run applications using containers on scalable infrastructure. - Container runtimes like Docker allows us run software based on images. An image packages an applications and its environment together which makes the software portable and easier to run in any environment. - To manage how we scale infrastructure and deploy containerized applications, the cloud native aproach uses a container orchestrator where K8s is the most popular. - Taking it a step further, service mesh can be used if advanced management of our distributed application's security, policies and traffic is required. 5. GitOps: (How to Operate) - GitOps tells us how to operate cloud native applications. - GitOps influences how we use automation to deliver , deploy and orchestrate running containers the most. Why Devops \u00b6 \"Firms today experience a much higher velocity of business change. Market opportunities appear or dissolve in months or weeks instead of years.\" Annual updates are no longer feasible against modern competition. Updates and bug fixes need to be available right away. It isn't just our management that wants to speed up our releases. Management is simply reacting to the demands of our customers. If customers can't get what they want from us, they'll go somewhere else. The rules have changed, and organizations around the world are now adapting their approach to software development accordingly. Agile methods and practices don't promise to solve every problem. But they do promise to establish a culture and environment where solutions emerge through collaboration, continual planning and learning, and a desire to ship high quality software more often. Devops \u00b6 DevOps is a union of people, processes and products to enable continuous delivery of value to our customers DevOps is not : A methodology A specific piece of software A quick fix for an organization's challenges Just a team or a job title (although these titles are reasonably common in the industry) Our goal is to give our customers value continously . We do that by working together with a shared set of practices and tools. Devops Practices Agile planning . Together, we'll create a backlog of work that everyone on the team and in management can see. We'll prioritize the items so we know what we need to work on first. The backlog can include user stories, bugs, and any other information that helps us. Continuous integration (CI) . We'll automate how we build and test our code. We'll run that every time a team member commits changes to version control. Continuous delivery (CD) . CD is how we test, configure, and deploy from a build to a QA or production environment. Monitoring . We'll use telemetry to get information about an application's performance and usage patterns. We can use that information to improve as we iterate. Devops Benefits - Deployment Frequency, Lead Time for Changes, Change Failure Rate, and Time to Restore DevOps helps companies experiment with ways to increase customer adoption and satisfaction. It can lead to better organizational performance, and often to higher profitability and market share. 1. Deploy more frequently . Practices such as monitoring, continuous testing, database change management, and integrating security earlier in the software development process help elite performers deploy more frequently, and with greater predictability and security. 1. Reduce lead time from commit to deploy . Lead time is the time it takes for a feature to make it to the customer. By working in smaller batches, automating manual processes, and deploying more frequently, elite performers can achieve in hours or days what once took weeks or even months. 1. Reduce change failure rate . A new feature that fails in production or that causes other features to break can create a lost opportunity between you and your users. As high-performing teams mature, they reduce their change failure rate over time. 1. Recover from incidents more quickly . When incidents do occur, elite performers are able to recover more quickly. Acting on metrics helps elite performers recover more quickly while also deploying more frequently. - How you implement cloud infrastructure also matters. The cloud improves software delivery performance , and teams that adopt essential cloud characteristics are more likely to become elite performers. - DevOps is a key reason many elite performers are able to deliver value to customers, in the form of new features and improvements, more quickly than their competitors. - In a comparison between elite performers and low performers, elite performers deploy more frequently, more quickly, and with fewer failures. This mindset helps them better adapt to changing market conditions, experiment with new features, and recover from incidents with greater resiliency. DevOps gives you a path to becoming an elite performer. - Even for elite performers, change happens gradually, often starting with the most immediate challenges or pain points. Adopting DevOps practices takes time. Process \u00b6 Value stream maps (VSMs) \u00b6 The purpose of a VSM is to visually show where in the process a team creates value and where there's waste. The goal, of course, is to arrive at a process that delivers maximum value to the customer with minimum waste. A VSM can help you pinpoint those areas that either don't contribute any value or that actually reduce the value of the product. The first step to setting up a DevOps practice is to assess your current process. This means analyzing: Your existing artifacts, such as deployment packages and NuGet, as well as your container repositories. Your existing test management tools. Your existing work management tools. Recommending migration and integration strategies. With a VSM, you'll get a sense of where the team fits into the DevOps maturity model. As it turns out, more mature teams typically release faster, with greater confidence, and with fewer bugs than less mature teams. Existing Process They use a waterfall approach. Management sets the priorities. Developers write code and hand the build off to QA. QA tests and then hands off to ops for deployment. (Development processes) Waterfall could be acceptable for a small team, but here the goals aren't always clear and they seem to change frequently. Testing is delayed until late in the process. That means it's harder and more expensive to fix bugs and make changes. (Test processes) There's no clear definition of what \"done\" means. Each team member has their own idea. There's no overall business goal that everyone agrees on. Some code is in a centralized version-control system. Many tools and scripts exist only on network file shares. There are many manual processes. a. Deploy builds to the pre-production servers for more testing. b. Often, the pre-production servers are out of sync with the latest patches and updates that are needed to run the website. c. Deploying to pre-production doesn't add value, it's necessary Communication is haphazard and depends on email, Word docs, and spreadsheets. Feedback is also infrequent and inconsistent. Total lead time is the time it takes for a feature to make it to the customer. Process time is the time spent on a feature that has value to the customer. Here, the process time includes four days for coding plus one day to deploy the feature, which gives a total of five days. Activity Ratio (Efficiency) = Process Time / Total lead time We want to minimize the time we spend that has no value to the customer. We can really improve our efficiency by adopting a DevOps approach . Choose an Agile approach to software development \u00b6 Being Agile means learning from experience and continually improving. Agile is an approach to software development. Agile is a term that's used to describe approaches to software development, emphasizing incremental delivery, team collaboration, continual planning, and continual learning. Agile isn't a process as much as it is a philosophy or mindset for planning the work that a team will do. It's based on iterative development and helps a team better plan for and react to the inevitable changes that occur in software development. Iterative software development shortens the DevOps lifecycle by executing against work in smaller increments, usually called sprints. Sprints are typically 1-4 weeks long. Agile development is often contrasted with traditional or waterfall development, where larger projects are planned up front and executed against that plan. Agile requires both a Definition of Done and explicit value delivered to customers in every sprint. Agile Manifesto We value: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan Example of Agile Mindset A team delivers value to the customer, gets feedback, and then modifies their backlog based on that feedback. They learn that their automated builds are missing key tests and include work in their next sprint to address it. They find that certain features perform poorly in production and make plans to improve performance. Someone on the team hears of a new practice and the team decides to try it out for a few sprints. Recommendations for adopting Agile Create an organizational structure that supports Agile practices. Vertical teams span the architecture and are aligned with product outcomes. Mentor team members on Agile techniques and practices. Train team members in Agile techniques such as how to run stand-up and review meetings. Enable in-team and cross-team collaboration. Cultural change . It's important that team members have a quiet, comfortable place to work. They need spaces where they can focus, without a lot of distractions and noise. To give team members more control, meetings need an agenda and strict time frames. Cross-functional teams . Cross-functional teams add new skills and perspectives that can broaden everyone's ability to solve challenges creatively. Cross-functional teams also make the entire organization more cohesive. They reduce turf wars and increase the sense that everyone is working toward a common goal. Tools for collaboration . Good tools can help your Agile team members collaborate more effectively, both within the team and with other teams. For example: Teams, Slack, Skype, Google Hangouts, Asana, Trello, GoToMeeting and monday.com. Scrum is a framework used by teams to manage their work. Scrum implements the principles of Agile as a concrete set of artifacts, practices, and roles. A sprint is the amount of time we have to complete our tasks. Sprints help keep us focused. At the end, we can have a short retrospective meeting to share what we've accomplished. After that, we can plan the next one. The product of a sprint is called the increment or potentially shippable increment . - All coding, testing, and quality verification must be done each and every sprint. Unless a team is properly set up, the results can fall short of expectations. Key success factors for Agile development teams: Diligent backlog refinement . An Agile development team works off of a backlog of requirements, often called user stories. The backlog is prioritized so the most important user stories are at the top. The product owner owns the backlog and adds, changes, and reprioritizes user stories based on the customer's needs. Integrate early and often . Continuous integration and continuous delivery (CI/CD) sets your team up for the fast pace of Agile development. As soon as possible, automate the build, test, and deployment pipelines. This should be one of the first things a team sets up when starting a new project. CI/CD forces a team to fix deployment issues as they occur, ensuring the product is always ready to ship. Minimize technical debt . Technical debt includes anything the team must do to deploy production quality code and keep it running in production. Examples are bugs, performance issues, operational issues, accessibility, and others. When refining the backlog, there are some key considerations to remember. Refining user stories is often a long-lead activity. A user story is not refined unless the team says it is. User stories further down the backlog can remain ambiguous. There are some key CI/CD activities that are critially important to effective Agile development. Unit testing . Unit tests are the first defense against human error. Unit tests should be considered part of coding and checked in with the code. Executing unit tests should be part of every build. Failed unit tests mean a failed build. Build automation . The build system should automatically pull code and tests directly from source control when builds execute. Branch and build policies . Configure branch and build policies to build automatically as the team checks code in to a specific branch. Deploy to an environment . Set up a release pipeline that automatically deploys built projects to an environment that mimics production. To establish an Agile culture, start by trying to ship the product at the end of every sprint. It won't be easy at first, but when a team attempts it, they quickly discover all the things that should be happening, but aren't. Continuous Integration \u00b6 Continuous Integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. - CI encourages developers to share their code and unit tests by merging their changes into a shared version control repository after every small task completion. - Committing code triggers an automated build system to grab the latest code from the shared repository and to build, test, and validate the full main, or trunk, branch. - Teams can leverage modern version control systems such as Git to create short-lived feature branches to isolate their work. A developer submits a pull request when the feature is complete and, on approval of the pull request, the changes get merged into the main branch. Then the developer can delete the previous feature branch. Development teams repeat the process for additional work. The team can establish branch policies to ensure the main branch meets desired quality criteria. Teams use build definitions to ensure that every commit to the main branch triggers the automated build and testing processes. Implementing CI this way ensures bugs are caught earlier in the development cycle, which makes them less expensive to fix. Automated tests run for every build to ensure builds maintain a consistent quality. Test Principles Tests should be written at the lowest level possible . The majority of tests should run as part of the build, so focus on making that as easy as possible. It's not feasible to test every aspect of a service at this level, but the principle to keep in mind is that heavier functional tests should not be used where lighter unit tests could produce the same results. Consider a parallel build system that can run unit tests for an assembly as soon as that assembly and associated test assembly drop. Write once, run anywhere, including the production system . It's a best practice for functional tests to only use the public API of the product. Design the product for testability . Shifting the balance strongly in favor of unit testing over functional testing requires teams to make design and implementation choices that support testability. The principle to keep clearly in mind is that designing for testability must become a primary part of the discussion about design and code quality. Test code is product code, and only reliable tests survive . Apply the same level of care in the design and implementation of tests and test frameworks. Maintain a very high bar for reliability and discourage the use of UI tests as they tend to be unreliable. Testing infrastructure is a shared service . Testing should be viewed as a shared service for the entire team. If the tests can be run in every environment from local development through production, then they will have the same reliability as the product code. Test ownership follows product ownership . Tests should sit right next to the product code in a repo. If there are components to be tested at that component boundary, don't rely on others to test the component. Push the accountability to the person who is writing the code. Shift left The goal for shifting left is to move quality upstream by performing testing tasks earlier in the pipeline. Through a combination of test and process improvements, this both reduces the time it takes for tests to be run, as well as the impact of failures later on. Most importantly, it ensures that most of the testing is completed even before a change is merged into main . Continuous Delivery \u00b6 Continuous Delivery (CD) is the process to build, test, configure, and deploy from a build to a production environment. - Multiple testing or staging environments create a Release Pipeline to automate the creation of infrastructure and deployment of a new build. Successive environments support progressively longer-running activities of integration, load, and user acceptance testing. Modern release pipelines allow development teams to deploy new features fast and safely. Issues found in production can be remediated quickly by rolling forward with a new deployment. In this way, CD creates a continuous stream of customer value. - Without CD, software release cycles were previously a bottleneck for application and operation teams. Manual processes led to unreliable releases that produced delays and errors. These teams often relied on handoffs that resulted in issues during release cycles. - The automated release pipeline allows a \"fail fast\" approach to validation, where the tests most likely to fail quickly are run first and longer-running tests happen only after the faster ones complete successfully. - CD is a lean practice with the goal to keep production fresh by achieving the shortest path from the availability of new code in version control or new components in package management to deployment. - By automation, CD minimizes the time to deploy and time to mitigate or time to remediate production incidents (TTM and TTR). In lean terms, this optimizes process time and eliminates idle time. - CD is helped considerably by the complementary practices of Infrastructure as Code and monitoring. Deployment Patterns 1. Sequential rings : CD may sequence multiple deployment rings for progressive exposure (also known as \"controlling the blast radius\"). Progressive exposure groups users who get to try new releases to monitor their experience in rings. The first deployment ring is often a canary used to test new versions in production before a broader rollout. CD automates deployment from one ring to the next and may optionally depend on an approval step, in which a decision maker signs off on the changes electronically. - CD may create an auditable record of the approval in order to satisfy regulatory procedures or other control objectives. - CD also supports two other patterns for progressive exposure beside sequential rings. 2. Blue/Green deployment relies on keeping an existing (blue) version live while a new (green) one is deployed. Typically, this uses load balancing to direct increasing amounts of traffic to the green deployment. If monitoring discovers an incident, traffic can be rerouted to the blue deployment still running. 3. Feature flags (or feature toggles) comprise another technique used for experimentation and dark launches. Feature flags turn features on or off for different end users based on their identity and group membership. Key takeaways With the right practices, it's possible to make delivery a productive and painless part of the DevOps cycle. 1. Deploy often 1. Stay green throughout the sprint 1. Use consistent deployment tooling in development, test, and production 1. Use a continuous delivery platform that allows automation and authorization 1. Follow safe deployment practices Shift right to test in production - One of the most effective ways DevOps teams can improve velocity is by shifting their quality goals left. In this sense, they are pushing aspects of testing earlier in the pipeline in order to ultimately reduce the amount of time it takes for new code investments to reach production and operate reliably. - The full breadth and diversity of the production environment is hard to replicate in a lab. The real workload of customer traffic is also hard to simulate. And even if tests are built and optimized, it becomes a significant responsibility to maintain those profiles and behaviors as the production demand evolves over time. - Moreover, the production environment keeps changing. It's never constant and, even if your app doesn't change, everything underneath it is constantly changing. The infrastructure it relies on keeps changing. So over a period of time, teams find that certain types of testing just needs to happen in production. Testing in production is the practice of using real deployments to validate and measure an application's behavior and performance in the production environment. It serves two important purposes: It validates the quality of a given production deployment. It validates the health and quality of the constantly changing production environment. To safeguard the production environment, it's necessary to roll out changes in a progressive and controlled manner. This is typically done via the ring model of deployments and with feature flags. The first ring should be the smallest size necessary to run the standard integration suite. This is where obvious errors, such as misconfigurations, will be discovered before any customers are impacted. Once the initial ring is validated, the next ring can broaden to include a subset of real users. The usage of the new production services by real customers becomes the test run. For example, a bug that prevents a shopper from completing their purchase is very bad, so it would always be better to catch that issue when less than 1% of customers are on that ring, as opposed to a different model where all customers were switched at once. If everything looks good so far, the deployment can progress through further rings and tests until it's used by everyone. However, full deployment doesn't mean that testing is over; tracking telemetry is crticially important for testing in production. It's arguably the highest quality test data because it's literally the test results of the real customer workload. It tracks failures, exceptions, performance metrics, security events, etc. The telemetry also helps detect anomalies. Fault injection and chaos engineering . Teams often employ fault injection and chaos engineering to see how a system behaves under failure conditions. This helps to validate that the resiliency mechanisms implemented actually work. It also helps to validate that a failure starting in one subsystem is contained within that subsystem and doesn't cascade to produce a major outage for the entire product. Fault injection also helps create more realistic training drills for live site engineers so that they can be better prepared to deal with real incidents. Fault testing with a circuit breaker A circuit breaker is a mechanism that cuts off a given component from a larger system. Circuit breakers can be intentionally triggered to evaluate two important scenarios: When the circuit breaker opens, does the fallback work? It may work with unit tests, but there's no way to know for sure that it will behave as expected in production without injecting a fault to trigger it. Does the circuit breaker open when it needs to? Does it have the right sensitivity threshold configured? Fault injection may force latency and/or disconnect dependencies in order to observe breaker responsiveness. In addition to evaluating that the right behavior is occurring, it's important to determine whether it happens quickly enough. Chaos engineering can be an effective tool, but it should be limited to canary environments. For example, it should only be used against environments that have little or no customer impact. It's a good practice to automate fault injection experiments because they are expensive tests and the system is always changing. Infrastructure as Code \u00b6 Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. - IaC is a key DevOps practice and is used in conjunction with continuous delivery. - Infrastructure as Code evolved to solve the problem of environment drift in the release pipeline. - Without IaC, teams must maintain the settings of individual deployment environments. Over time, each environment becomes a snowflake , that is, a unique configuration that cannot be reproduced automatically. Inconsistency among environments leads to issues during deployments. With snowflakes, administration and maintenance of infrastructure involves manual processes which were hard to track and contributed to errors. - Idempotence is a principle of Infrastructure as Code. Idempotence is the property that a deployment command always sets the target environment into the same configuration, regardless of the environment's starting state. - Idempotency is achieved by either automatically configuring an existing target or by discarding the existing target and recreating a fresh environment. Benefits of IaC 1. Teams who implement IaC can deliver stable environments rapidly and at scale. 2. Teams avoid manual configuration of environments and enforce consistency by representing the desired state of their environments via code. 3. Infrastructure deployments with IaC are repeatable and prevent runtime issues caused by configuration drift or missing dependencies. 4. DevOps teams can work together with a unified set of practices and tools to deliver applications and their supporting infrastructure rapidly, reliably, and at scale. Monitoring \u00b6 Monitoring provides feedback from production. Monitoring delivers information about an application's performance and usage patterns. Effective monitoring is essential to allow DevOps teams to deliver at speed, get feedback from production, and increase customers satisfaction, acquisition and retention. One goal of monitoring is to achieve high availability by minimizing key metrics that are measured in terms of time When performance or other issues arise, rich diagnostic data about the issues are fed back to development teams via automated monitoring. That's time to detect (TTD) . DevOps teams act on the information to mitigate the issues as quickly as possible so that users are no longer affected. That's time to mitigate (TTM) . Resolution times are measured, and teams work to improve over time. After mitigation, teams work on how to remediate problems at root cause so that they do not recur. That's time to remediate (TTR) . A second goal of monitoring is to enable validated learning by tracking usage. The core concept of validated learning is that every deployment is an opportunity to track experimental results that support or diminish the hypotheses that led to the deployment. Tracking usage and differences between versions allows teams to measure the impact of change and drive business decisions. If a hypothesis is diminished, the team can fail fast or pivot . If the hypothesis is supported, then the team can double down or persevere . These data-informed decisions lead to new hypotheses and prioritization of the backlog. Telemetry is the mechanism for collecting data from monitoring. - Telemetry can use agents that are installed in the deployment environments, an SDK that relies on markers inserted into source code, server logging, or a combination of these. Typically, telemetry will distinguish between the data pipeline optimized for real-time alerting and dashboards and higher-volume data needed for troubleshooting or usage analytics. Synthetic monitoring uses a consistent set of transactions to assess performance and availability. - Synthetic transactions are predictable tests that have the advantage of allowing comparison from release to release in a highly predictable manner. Real user monitoring (RUM) , on the other hand, means measurement of experience from the user's browser, mobile device or desktop, and accounts for last mile conditions such as cellular networks, internet routing, and caching. - A well-monitored deployment streams the data about its health and performance so that the team can spot production incidents immediately. - Combined with a continuous deployment release pipeline, monitoring will detect new anomalies and allow for prompt mitigation. This allows discovery of the unknown unknowns in application behavior that cannot be foreseen in pre-production environments. DevSecOps \u00b6 \"Fundamentally, if somebody wants to get in, they're getting in\u2026accept that. What we tell clients is: number one, you're in the fight, whether you thought you were or not. Number two, you almost certainly are penetrated.\" \u2013 Michael Hayden, Former Director of NSA and CIA The mindset shift to a DevSecOps culture includes an important thinking about not only preventing breaches, but assuming them as well. ============================================================ **Preventing breaches** **Assuming breaches** ------------------------------------------------------------ Threat models War game exercises Code reviews Central security monitors Security testing Live site penetration tests Security development lifecycle (SDL) The most important thing to focus on is that practicing techniques that assume breaches helps the team answer questions about their security on their own time, so they don't have to figure it all out during a real security emergency. Common questions the team needs to think through: How will we detect an attack? How will respond if there is an attack or penetration? How will we recover from an attack, such as when data has been leaked or tampered with? Key DevSecOps practices First, teams should focus on improving their mean time to detection and mean time to recovery . These are metrics that indicate how long it takes to detect a breach and how long it takes to recover, respectively. They can be tracked through ongoing live site testing of security response plans. When evaluating potential policies, improving these metrics should be an important consideration. Teams should also practice defense in depth . When a breach happens, it often results in the attacker getting access to internal networks and everything they have to offer. While it would be ideal to stop them before it gets that far, a policy of assuming breaches would drive teams to minimize their exposure from an attacker who has already gotten in. Finally, teams should perform periodic post-breach assessments of the practices and environments. After a breach has been resolved, the team should evaluate the performance of the policies, as well as their own adherence to them. This serves to not only ensure the policies are effective, but also that the team is actually following them. Every breach, whether real or practiced, should be seen as an opportunity to improve. Strategies for mitigating threats - Some security holes are due to issues in dependencies like operating systems and libraries, so keeping them up-to-date is critical. - Others are due to bugs in system code that require careful analysis to find and fix. - Poor secret management is the cause of many breaches, as is social engineering. Attack vectors ============================================================ **Privilege** **Attack** ------------------------------------------------------------ Can they send emails? Phish colleagues Can they access other machines? Log on, mimikatz, repeat Can they modify source Inject code Can they modify the build/release process? Inject code, run scripts Can they access a test environment? If a production environment takes a dependency on the test environment, exploit it Can they access the production environment? So many options... How can the blue team defend against this? Store secrets in protected vaults Remove local admin accounts Restrict SAMR Credential Guard Remove dual-homed servers Separate subscriptions Multi-factor authentication Privileged access workstations Detect with ATP & Azure Security Center Secret management - Use a hierarchy of vaults to eliminate the duplication of secrets. - Also consider how and when secrets are accessed. Some are used at deploy-time when building environment configurations, whereas others are accessed at run-time. Deploy-time secrets typically require a new deployment in order to pick up new settings, whereas run-time secrets are accessed when needed and can be updated at any time. The red team should include some security-minded engineers and developers deeply familiar with the code. It's also helpful to augment the team with a penetration testing specialist, if possible. If there are no specialists in-house, many companies provide this service along with mentoring. The blue team should be made up of ops-minded engineers who have a deep understanding of the systems and logging available. They have the best chance of detecting and addressing suspicious behavior. Expect the red team to be effective in the early war games. They should be able to succeed through fairly simple attacks, such as by finding poorly protected secrets, SQL injection, and successful phishing campaigns. Take plenty of time between rounds to apply fixes and feedback on policies. This will vary by organization, but you don't want to start the next round until everyone is confident that the previous round has been mined for all it's worth. After a few rounds, the red team will need to rely on more sophisticated techniques, such as cross-site scripting (XSS), deserialization exploits, and engineering system vulnerabilities. it will also help to bring in additional outside security experts in areas like Active Directory in order to attack more obscure exploits. By this time, the blue team should not only have a hardened platform to defend, but will also make use of comprehensive, centralized logging for post-breach forensics. \"Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.\" \u2013 John Lambert (MSTIC) Over time, the red team will take much longer to reach objectives. When they do, it will often requiring discovery and chaining of multiple vulnerabilities to have a limited impact. Through the use of real-time monitoring tools, the blue team should start to catch them in real-time. Any security risks or lessons learned should be documented in a backlog of repair items. Teams should define a service level agreement (SLA) for how quickly security risks will be addressed. Severe risks should be addressed as soon as possible, whereas minor issues may have a two-sprint deadline. Lessons learned War games are a really effective way to change DevSecOps culture and keep security top-of-mind. Phishing attacks are very effective for attackers and should not be underestimated. The impact can be contained by limiting production access and requiring two-factor authentication. Control of the engineering system leads to control of everything. Be sure to strictly control access to the build/release agent, queue, pool, and definition. Practice defense in depth to make it harder for attackers. Every boundary they have to breach slows them down and offers another opportunity to catch them. Don't ever cross trust realms. Production should never trust anything in test.","title":"Devops"},{"location":"devops/#cloud-native-paradigm","text":"1. Devops: (How to Build) - It assumes you have embraced Agile ways of working and have broken down silos in your organization that allow teams to work in harmony to deliver software. - From a practise perspective, you have abandoned waterfall , work in short cycles and are hyper focused on delivering software quickly. 2. Microservices: (What to Build) - Cloud Native approaches advocate for buidling applications with a microservices architecture . You aim to build systems that are modular, service based and lossely coupled. - This can be achieved by directing integrations through API interfaces and by applying design principles similar to those defined by the 12-Factor App model. - In general, cloud native applications are made of small services that can be developed and released independently. 3. Automation: (How to Deploy) - Cloud Native apps rely heavily on automation . - The switch to building modular systems with many services makes automated CI/CD pipelines more critical for building and packaging software into an artifact. - After automated tests and quality assurance gates pass, the artifacts are stored in a repository where they wait for an automated agent to deploy and release them into an environment . - What is released into the system and the system\u2019s configuration is determined by code that describes the immutable versions of the infrastructure. 4. Containers: (Where to run) - Cloud native centers around the idea that you run applications using containers on scalable infrastructure. - Container runtimes like Docker allows us run software based on images. An image packages an applications and its environment together which makes the software portable and easier to run in any environment. - To manage how we scale infrastructure and deploy containerized applications, the cloud native aproach uses a container orchestrator where K8s is the most popular. - Taking it a step further, service mesh can be used if advanced management of our distributed application's security, policies and traffic is required. 5. GitOps: (How to Operate) - GitOps tells us how to operate cloud native applications. - GitOps influences how we use automation to deliver , deploy and orchestrate running containers the most.","title":"Cloud Native Paradigm"},{"location":"devops/#why-devops","text":"\"Firms today experience a much higher velocity of business change. Market opportunities appear or dissolve in months or weeks instead of years.\" Annual updates are no longer feasible against modern competition. Updates and bug fixes need to be available right away. It isn't just our management that wants to speed up our releases. Management is simply reacting to the demands of our customers. If customers can't get what they want from us, they'll go somewhere else. The rules have changed, and organizations around the world are now adapting their approach to software development accordingly. Agile methods and practices don't promise to solve every problem. But they do promise to establish a culture and environment where solutions emerge through collaboration, continual planning and learning, and a desire to ship high quality software more often.","title":"Why Devops"},{"location":"devops/#devops","text":"DevOps is a union of people, processes and products to enable continuous delivery of value to our customers DevOps is not : A methodology A specific piece of software A quick fix for an organization's challenges Just a team or a job title (although these titles are reasonably common in the industry) Our goal is to give our customers value continously . We do that by working together with a shared set of practices and tools. Devops Practices Agile planning . Together, we'll create a backlog of work that everyone on the team and in management can see. We'll prioritize the items so we know what we need to work on first. The backlog can include user stories, bugs, and any other information that helps us. Continuous integration (CI) . We'll automate how we build and test our code. We'll run that every time a team member commits changes to version control. Continuous delivery (CD) . CD is how we test, configure, and deploy from a build to a QA or production environment. Monitoring . We'll use telemetry to get information about an application's performance and usage patterns. We can use that information to improve as we iterate. Devops Benefits - Deployment Frequency, Lead Time for Changes, Change Failure Rate, and Time to Restore DevOps helps companies experiment with ways to increase customer adoption and satisfaction. It can lead to better organizational performance, and often to higher profitability and market share. 1. Deploy more frequently . Practices such as monitoring, continuous testing, database change management, and integrating security earlier in the software development process help elite performers deploy more frequently, and with greater predictability and security. 1. Reduce lead time from commit to deploy . Lead time is the time it takes for a feature to make it to the customer. By working in smaller batches, automating manual processes, and deploying more frequently, elite performers can achieve in hours or days what once took weeks or even months. 1. Reduce change failure rate . A new feature that fails in production or that causes other features to break can create a lost opportunity between you and your users. As high-performing teams mature, they reduce their change failure rate over time. 1. Recover from incidents more quickly . When incidents do occur, elite performers are able to recover more quickly. Acting on metrics helps elite performers recover more quickly while also deploying more frequently. - How you implement cloud infrastructure also matters. The cloud improves software delivery performance , and teams that adopt essential cloud characteristics are more likely to become elite performers. - DevOps is a key reason many elite performers are able to deliver value to customers, in the form of new features and improvements, more quickly than their competitors. - In a comparison between elite performers and low performers, elite performers deploy more frequently, more quickly, and with fewer failures. This mindset helps them better adapt to changing market conditions, experiment with new features, and recover from incidents with greater resiliency. DevOps gives you a path to becoming an elite performer. - Even for elite performers, change happens gradually, often starting with the most immediate challenges or pain points. Adopting DevOps practices takes time.","title":"Devops"},{"location":"devops/#process","text":"","title":"Process"},{"location":"devops/#value-stream-maps-vsms","text":"The purpose of a VSM is to visually show where in the process a team creates value and where there's waste. The goal, of course, is to arrive at a process that delivers maximum value to the customer with minimum waste. A VSM can help you pinpoint those areas that either don't contribute any value or that actually reduce the value of the product. The first step to setting up a DevOps practice is to assess your current process. This means analyzing: Your existing artifacts, such as deployment packages and NuGet, as well as your container repositories. Your existing test management tools. Your existing work management tools. Recommending migration and integration strategies. With a VSM, you'll get a sense of where the team fits into the DevOps maturity model. As it turns out, more mature teams typically release faster, with greater confidence, and with fewer bugs than less mature teams. Existing Process They use a waterfall approach. Management sets the priorities. Developers write code and hand the build off to QA. QA tests and then hands off to ops for deployment. (Development processes) Waterfall could be acceptable for a small team, but here the goals aren't always clear and they seem to change frequently. Testing is delayed until late in the process. That means it's harder and more expensive to fix bugs and make changes. (Test processes) There's no clear definition of what \"done\" means. Each team member has their own idea. There's no overall business goal that everyone agrees on. Some code is in a centralized version-control system. Many tools and scripts exist only on network file shares. There are many manual processes. a. Deploy builds to the pre-production servers for more testing. b. Often, the pre-production servers are out of sync with the latest patches and updates that are needed to run the website. c. Deploying to pre-production doesn't add value, it's necessary Communication is haphazard and depends on email, Word docs, and spreadsheets. Feedback is also infrequent and inconsistent. Total lead time is the time it takes for a feature to make it to the customer. Process time is the time spent on a feature that has value to the customer. Here, the process time includes four days for coding plus one day to deploy the feature, which gives a total of five days. Activity Ratio (Efficiency) = Process Time / Total lead time We want to minimize the time we spend that has no value to the customer. We can really improve our efficiency by adopting a DevOps approach .","title":"Value stream maps (VSMs)"},{"location":"devops/#choose-an-agile-approach-to-software-development","text":"Being Agile means learning from experience and continually improving. Agile is an approach to software development. Agile is a term that's used to describe approaches to software development, emphasizing incremental delivery, team collaboration, continual planning, and continual learning. Agile isn't a process as much as it is a philosophy or mindset for planning the work that a team will do. It's based on iterative development and helps a team better plan for and react to the inevitable changes that occur in software development. Iterative software development shortens the DevOps lifecycle by executing against work in smaller increments, usually called sprints. Sprints are typically 1-4 weeks long. Agile development is often contrasted with traditional or waterfall development, where larger projects are planned up front and executed against that plan. Agile requires both a Definition of Done and explicit value delivered to customers in every sprint. Agile Manifesto We value: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan Example of Agile Mindset A team delivers value to the customer, gets feedback, and then modifies their backlog based on that feedback. They learn that their automated builds are missing key tests and include work in their next sprint to address it. They find that certain features perform poorly in production and make plans to improve performance. Someone on the team hears of a new practice and the team decides to try it out for a few sprints. Recommendations for adopting Agile Create an organizational structure that supports Agile practices. Vertical teams span the architecture and are aligned with product outcomes. Mentor team members on Agile techniques and practices. Train team members in Agile techniques such as how to run stand-up and review meetings. Enable in-team and cross-team collaboration. Cultural change . It's important that team members have a quiet, comfortable place to work. They need spaces where they can focus, without a lot of distractions and noise. To give team members more control, meetings need an agenda and strict time frames. Cross-functional teams . Cross-functional teams add new skills and perspectives that can broaden everyone's ability to solve challenges creatively. Cross-functional teams also make the entire organization more cohesive. They reduce turf wars and increase the sense that everyone is working toward a common goal. Tools for collaboration . Good tools can help your Agile team members collaborate more effectively, both within the team and with other teams. For example: Teams, Slack, Skype, Google Hangouts, Asana, Trello, GoToMeeting and monday.com. Scrum is a framework used by teams to manage their work. Scrum implements the principles of Agile as a concrete set of artifacts, practices, and roles. A sprint is the amount of time we have to complete our tasks. Sprints help keep us focused. At the end, we can have a short retrospective meeting to share what we've accomplished. After that, we can plan the next one. The product of a sprint is called the increment or potentially shippable increment . - All coding, testing, and quality verification must be done each and every sprint. Unless a team is properly set up, the results can fall short of expectations. Key success factors for Agile development teams: Diligent backlog refinement . An Agile development team works off of a backlog of requirements, often called user stories. The backlog is prioritized so the most important user stories are at the top. The product owner owns the backlog and adds, changes, and reprioritizes user stories based on the customer's needs. Integrate early and often . Continuous integration and continuous delivery (CI/CD) sets your team up for the fast pace of Agile development. As soon as possible, automate the build, test, and deployment pipelines. This should be one of the first things a team sets up when starting a new project. CI/CD forces a team to fix deployment issues as they occur, ensuring the product is always ready to ship. Minimize technical debt . Technical debt includes anything the team must do to deploy production quality code and keep it running in production. Examples are bugs, performance issues, operational issues, accessibility, and others. When refining the backlog, there are some key considerations to remember. Refining user stories is often a long-lead activity. A user story is not refined unless the team says it is. User stories further down the backlog can remain ambiguous. There are some key CI/CD activities that are critially important to effective Agile development. Unit testing . Unit tests are the first defense against human error. Unit tests should be considered part of coding and checked in with the code. Executing unit tests should be part of every build. Failed unit tests mean a failed build. Build automation . The build system should automatically pull code and tests directly from source control when builds execute. Branch and build policies . Configure branch and build policies to build automatically as the team checks code in to a specific branch. Deploy to an environment . Set up a release pipeline that automatically deploys built projects to an environment that mimics production. To establish an Agile culture, start by trying to ship the product at the end of every sprint. It won't be easy at first, but when a team attempts it, they quickly discover all the things that should be happening, but aren't.","title":"Choose an Agile approach to software development"},{"location":"devops/#continuous-integration","text":"Continuous Integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. - CI encourages developers to share their code and unit tests by merging their changes into a shared version control repository after every small task completion. - Committing code triggers an automated build system to grab the latest code from the shared repository and to build, test, and validate the full main, or trunk, branch. - Teams can leverage modern version control systems such as Git to create short-lived feature branches to isolate their work. A developer submits a pull request when the feature is complete and, on approval of the pull request, the changes get merged into the main branch. Then the developer can delete the previous feature branch. Development teams repeat the process for additional work. The team can establish branch policies to ensure the main branch meets desired quality criteria. Teams use build definitions to ensure that every commit to the main branch triggers the automated build and testing processes. Implementing CI this way ensures bugs are caught earlier in the development cycle, which makes them less expensive to fix. Automated tests run for every build to ensure builds maintain a consistent quality. Test Principles Tests should be written at the lowest level possible . The majority of tests should run as part of the build, so focus on making that as easy as possible. It's not feasible to test every aspect of a service at this level, but the principle to keep in mind is that heavier functional tests should not be used where lighter unit tests could produce the same results. Consider a parallel build system that can run unit tests for an assembly as soon as that assembly and associated test assembly drop. Write once, run anywhere, including the production system . It's a best practice for functional tests to only use the public API of the product. Design the product for testability . Shifting the balance strongly in favor of unit testing over functional testing requires teams to make design and implementation choices that support testability. The principle to keep clearly in mind is that designing for testability must become a primary part of the discussion about design and code quality. Test code is product code, and only reliable tests survive . Apply the same level of care in the design and implementation of tests and test frameworks. Maintain a very high bar for reliability and discourage the use of UI tests as they tend to be unreliable. Testing infrastructure is a shared service . Testing should be viewed as a shared service for the entire team. If the tests can be run in every environment from local development through production, then they will have the same reliability as the product code. Test ownership follows product ownership . Tests should sit right next to the product code in a repo. If there are components to be tested at that component boundary, don't rely on others to test the component. Push the accountability to the person who is writing the code. Shift left The goal for shifting left is to move quality upstream by performing testing tasks earlier in the pipeline. Through a combination of test and process improvements, this both reduces the time it takes for tests to be run, as well as the impact of failures later on. Most importantly, it ensures that most of the testing is completed even before a change is merged into main .","title":"Continuous Integration"},{"location":"devops/#continuous-delivery","text":"Continuous Delivery (CD) is the process to build, test, configure, and deploy from a build to a production environment. - Multiple testing or staging environments create a Release Pipeline to automate the creation of infrastructure and deployment of a new build. Successive environments support progressively longer-running activities of integration, load, and user acceptance testing. Modern release pipelines allow development teams to deploy new features fast and safely. Issues found in production can be remediated quickly by rolling forward with a new deployment. In this way, CD creates a continuous stream of customer value. - Without CD, software release cycles were previously a bottleneck for application and operation teams. Manual processes led to unreliable releases that produced delays and errors. These teams often relied on handoffs that resulted in issues during release cycles. - The automated release pipeline allows a \"fail fast\" approach to validation, where the tests most likely to fail quickly are run first and longer-running tests happen only after the faster ones complete successfully. - CD is a lean practice with the goal to keep production fresh by achieving the shortest path from the availability of new code in version control or new components in package management to deployment. - By automation, CD minimizes the time to deploy and time to mitigate or time to remediate production incidents (TTM and TTR). In lean terms, this optimizes process time and eliminates idle time. - CD is helped considerably by the complementary practices of Infrastructure as Code and monitoring. Deployment Patterns 1. Sequential rings : CD may sequence multiple deployment rings for progressive exposure (also known as \"controlling the blast radius\"). Progressive exposure groups users who get to try new releases to monitor their experience in rings. The first deployment ring is often a canary used to test new versions in production before a broader rollout. CD automates deployment from one ring to the next and may optionally depend on an approval step, in which a decision maker signs off on the changes electronically. - CD may create an auditable record of the approval in order to satisfy regulatory procedures or other control objectives. - CD also supports two other patterns for progressive exposure beside sequential rings. 2. Blue/Green deployment relies on keeping an existing (blue) version live while a new (green) one is deployed. Typically, this uses load balancing to direct increasing amounts of traffic to the green deployment. If monitoring discovers an incident, traffic can be rerouted to the blue deployment still running. 3. Feature flags (or feature toggles) comprise another technique used for experimentation and dark launches. Feature flags turn features on or off for different end users based on their identity and group membership. Key takeaways With the right practices, it's possible to make delivery a productive and painless part of the DevOps cycle. 1. Deploy often 1. Stay green throughout the sprint 1. Use consistent deployment tooling in development, test, and production 1. Use a continuous delivery platform that allows automation and authorization 1. Follow safe deployment practices Shift right to test in production - One of the most effective ways DevOps teams can improve velocity is by shifting their quality goals left. In this sense, they are pushing aspects of testing earlier in the pipeline in order to ultimately reduce the amount of time it takes for new code investments to reach production and operate reliably. - The full breadth and diversity of the production environment is hard to replicate in a lab. The real workload of customer traffic is also hard to simulate. And even if tests are built and optimized, it becomes a significant responsibility to maintain those profiles and behaviors as the production demand evolves over time. - Moreover, the production environment keeps changing. It's never constant and, even if your app doesn't change, everything underneath it is constantly changing. The infrastructure it relies on keeps changing. So over a period of time, teams find that certain types of testing just needs to happen in production. Testing in production is the practice of using real deployments to validate and measure an application's behavior and performance in the production environment. It serves two important purposes: It validates the quality of a given production deployment. It validates the health and quality of the constantly changing production environment. To safeguard the production environment, it's necessary to roll out changes in a progressive and controlled manner. This is typically done via the ring model of deployments and with feature flags. The first ring should be the smallest size necessary to run the standard integration suite. This is where obvious errors, such as misconfigurations, will be discovered before any customers are impacted. Once the initial ring is validated, the next ring can broaden to include a subset of real users. The usage of the new production services by real customers becomes the test run. For example, a bug that prevents a shopper from completing their purchase is very bad, so it would always be better to catch that issue when less than 1% of customers are on that ring, as opposed to a different model where all customers were switched at once. If everything looks good so far, the deployment can progress through further rings and tests until it's used by everyone. However, full deployment doesn't mean that testing is over; tracking telemetry is crticially important for testing in production. It's arguably the highest quality test data because it's literally the test results of the real customer workload. It tracks failures, exceptions, performance metrics, security events, etc. The telemetry also helps detect anomalies. Fault injection and chaos engineering . Teams often employ fault injection and chaos engineering to see how a system behaves under failure conditions. This helps to validate that the resiliency mechanisms implemented actually work. It also helps to validate that a failure starting in one subsystem is contained within that subsystem and doesn't cascade to produce a major outage for the entire product. Fault injection also helps create more realistic training drills for live site engineers so that they can be better prepared to deal with real incidents. Fault testing with a circuit breaker A circuit breaker is a mechanism that cuts off a given component from a larger system. Circuit breakers can be intentionally triggered to evaluate two important scenarios: When the circuit breaker opens, does the fallback work? It may work with unit tests, but there's no way to know for sure that it will behave as expected in production without injecting a fault to trigger it. Does the circuit breaker open when it needs to? Does it have the right sensitivity threshold configured? Fault injection may force latency and/or disconnect dependencies in order to observe breaker responsiveness. In addition to evaluating that the right behavior is occurring, it's important to determine whether it happens quickly enough. Chaos engineering can be an effective tool, but it should be limited to canary environments. For example, it should only be used against environments that have little or no customer impact. It's a good practice to automate fault injection experiments because they are expensive tests and the system is always changing.","title":"Continuous Delivery"},{"location":"devops/#infrastructure-as-code","text":"Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. - IaC is a key DevOps practice and is used in conjunction with continuous delivery. - Infrastructure as Code evolved to solve the problem of environment drift in the release pipeline. - Without IaC, teams must maintain the settings of individual deployment environments. Over time, each environment becomes a snowflake , that is, a unique configuration that cannot be reproduced automatically. Inconsistency among environments leads to issues during deployments. With snowflakes, administration and maintenance of infrastructure involves manual processes which were hard to track and contributed to errors. - Idempotence is a principle of Infrastructure as Code. Idempotence is the property that a deployment command always sets the target environment into the same configuration, regardless of the environment's starting state. - Idempotency is achieved by either automatically configuring an existing target or by discarding the existing target and recreating a fresh environment. Benefits of IaC 1. Teams who implement IaC can deliver stable environments rapidly and at scale. 2. Teams avoid manual configuration of environments and enforce consistency by representing the desired state of their environments via code. 3. Infrastructure deployments with IaC are repeatable and prevent runtime issues caused by configuration drift or missing dependencies. 4. DevOps teams can work together with a unified set of practices and tools to deliver applications and their supporting infrastructure rapidly, reliably, and at scale.","title":"Infrastructure as Code"},{"location":"devops/#monitoring","text":"Monitoring provides feedback from production. Monitoring delivers information about an application's performance and usage patterns. Effective monitoring is essential to allow DevOps teams to deliver at speed, get feedback from production, and increase customers satisfaction, acquisition and retention. One goal of monitoring is to achieve high availability by minimizing key metrics that are measured in terms of time When performance or other issues arise, rich diagnostic data about the issues are fed back to development teams via automated monitoring. That's time to detect (TTD) . DevOps teams act on the information to mitigate the issues as quickly as possible so that users are no longer affected. That's time to mitigate (TTM) . Resolution times are measured, and teams work to improve over time. After mitigation, teams work on how to remediate problems at root cause so that they do not recur. That's time to remediate (TTR) . A second goal of monitoring is to enable validated learning by tracking usage. The core concept of validated learning is that every deployment is an opportunity to track experimental results that support or diminish the hypotheses that led to the deployment. Tracking usage and differences between versions allows teams to measure the impact of change and drive business decisions. If a hypothesis is diminished, the team can fail fast or pivot . If the hypothesis is supported, then the team can double down or persevere . These data-informed decisions lead to new hypotheses and prioritization of the backlog. Telemetry is the mechanism for collecting data from monitoring. - Telemetry can use agents that are installed in the deployment environments, an SDK that relies on markers inserted into source code, server logging, or a combination of these. Typically, telemetry will distinguish between the data pipeline optimized for real-time alerting and dashboards and higher-volume data needed for troubleshooting or usage analytics. Synthetic monitoring uses a consistent set of transactions to assess performance and availability. - Synthetic transactions are predictable tests that have the advantage of allowing comparison from release to release in a highly predictable manner. Real user monitoring (RUM) , on the other hand, means measurement of experience from the user's browser, mobile device or desktop, and accounts for last mile conditions such as cellular networks, internet routing, and caching. - A well-monitored deployment streams the data about its health and performance so that the team can spot production incidents immediately. - Combined with a continuous deployment release pipeline, monitoring will detect new anomalies and allow for prompt mitigation. This allows discovery of the unknown unknowns in application behavior that cannot be foreseen in pre-production environments.","title":"Monitoring"},{"location":"devops/#devsecops","text":"\"Fundamentally, if somebody wants to get in, they're getting in\u2026accept that. What we tell clients is: number one, you're in the fight, whether you thought you were or not. Number two, you almost certainly are penetrated.\" \u2013 Michael Hayden, Former Director of NSA and CIA The mindset shift to a DevSecOps culture includes an important thinking about not only preventing breaches, but assuming them as well. ============================================================ **Preventing breaches** **Assuming breaches** ------------------------------------------------------------ Threat models War game exercises Code reviews Central security monitors Security testing Live site penetration tests Security development lifecycle (SDL) The most important thing to focus on is that practicing techniques that assume breaches helps the team answer questions about their security on their own time, so they don't have to figure it all out during a real security emergency. Common questions the team needs to think through: How will we detect an attack? How will respond if there is an attack or penetration? How will we recover from an attack, such as when data has been leaked or tampered with? Key DevSecOps practices First, teams should focus on improving their mean time to detection and mean time to recovery . These are metrics that indicate how long it takes to detect a breach and how long it takes to recover, respectively. They can be tracked through ongoing live site testing of security response plans. When evaluating potential policies, improving these metrics should be an important consideration. Teams should also practice defense in depth . When a breach happens, it often results in the attacker getting access to internal networks and everything they have to offer. While it would be ideal to stop them before it gets that far, a policy of assuming breaches would drive teams to minimize their exposure from an attacker who has already gotten in. Finally, teams should perform periodic post-breach assessments of the practices and environments. After a breach has been resolved, the team should evaluate the performance of the policies, as well as their own adherence to them. This serves to not only ensure the policies are effective, but also that the team is actually following them. Every breach, whether real or practiced, should be seen as an opportunity to improve. Strategies for mitigating threats - Some security holes are due to issues in dependencies like operating systems and libraries, so keeping them up-to-date is critical. - Others are due to bugs in system code that require careful analysis to find and fix. - Poor secret management is the cause of many breaches, as is social engineering. Attack vectors ============================================================ **Privilege** **Attack** ------------------------------------------------------------ Can they send emails? Phish colleagues Can they access other machines? Log on, mimikatz, repeat Can they modify source Inject code Can they modify the build/release process? Inject code, run scripts Can they access a test environment? If a production environment takes a dependency on the test environment, exploit it Can they access the production environment? So many options... How can the blue team defend against this? Store secrets in protected vaults Remove local admin accounts Restrict SAMR Credential Guard Remove dual-homed servers Separate subscriptions Multi-factor authentication Privileged access workstations Detect with ATP & Azure Security Center Secret management - Use a hierarchy of vaults to eliminate the duplication of secrets. - Also consider how and when secrets are accessed. Some are used at deploy-time when building environment configurations, whereas others are accessed at run-time. Deploy-time secrets typically require a new deployment in order to pick up new settings, whereas run-time secrets are accessed when needed and can be updated at any time. The red team should include some security-minded engineers and developers deeply familiar with the code. It's also helpful to augment the team with a penetration testing specialist, if possible. If there are no specialists in-house, many companies provide this service along with mentoring. The blue team should be made up of ops-minded engineers who have a deep understanding of the systems and logging available. They have the best chance of detecting and addressing suspicious behavior. Expect the red team to be effective in the early war games. They should be able to succeed through fairly simple attacks, such as by finding poorly protected secrets, SQL injection, and successful phishing campaigns. Take plenty of time between rounds to apply fixes and feedback on policies. This will vary by organization, but you don't want to start the next round until everyone is confident that the previous round has been mined for all it's worth. After a few rounds, the red team will need to rely on more sophisticated techniques, such as cross-site scripting (XSS), deserialization exploits, and engineering system vulnerabilities. it will also help to bring in additional outside security experts in areas like Active Directory in order to attack more obscure exploits. By this time, the blue team should not only have a hardened platform to defend, but will also make use of comprehensive, centralized logging for post-breach forensics. \"Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.\" \u2013 John Lambert (MSTIC) Over time, the red team will take much longer to reach objectives. When they do, it will often requiring discovery and chaining of multiple vulnerabilities to have a limited impact. Through the use of real-time monitoring tools, the blue team should start to catch them in real-time. Any security risks or lessons learned should be documented in a backlog of repair items. Teams should define a service level agreement (SLA) for how quickly security risks will be addressed. Severe risks should be addressed as soon as possible, whereas minor issues may have a two-sprint deadline. Lessons learned War games are a really effective way to change DevSecOps culture and keep security top-of-mind. Phishing attacks are very effective for attackers and should not be underestimated. The impact can be contained by limiting production access and requiring two-factor authentication. Control of the engineering system leads to control of everything. Be sure to strictly control access to the build/release agent, queue, pool, and definition. Practice defense in depth to make it harder for attackers. Every boundary they have to breach slows them down and offers another opportunity to catch them. Don't ever cross trust realms. Production should never trust anything in test.","title":"DevSecOps"},{"location":"devops/cd/","text":"What is continuous delivery? \u00b6 Continuous Delivery (CD) is the process to build, test, configure, and deploy from a build to a production environment. CD by itself is a set of processes, tools, and techniques that enable rapid, reliable, and continuous delivery of software. So CD isn't only about setting up a pipeline, although that part is important. - CD is about setting up a working environment where: 1. We have a reliable and repeatable process for releasing and deploying software. 1. We automate as much as possible. 1. We don't put off doing something that's difficult or painful. Instead, we do it more often so that we figure out how to make it routine. 1. We keep everything in source control. 1. We all agree that done means released . 1. We build quality into the process. Quality is never an afterthought. 1. We're all responsible for the release process. We no longer work in silos. 1. We always try to improve. - CD helps software teams deliver reliable software updates to their customers at a rapid cadence. - CD also helps ensure that both customers and stakeholders have the latest features and fixes quickly. - This enables the full the benefits of CD - repeatable builds which are verified in a consistent way across environments. Plan a release pipeline \u00b6 Goal : We already have the build artifact. It's the .zip file that our existing build pipeline creates. But how do we deploy it to a live environment A basic CD pipeline contains a trigger to get the process going and at least one stage, or deployment phase. What is a pipeline stage? \u00b6 A stage is a part of the pipeline that can run independently and be triggered by different mechanisms. A mechanism might be the success of the previous stage, a schedule, or even a manual trigger. A stage is made up of jobs. A job is a series of steps that defines how to build, test, or deploy your software. Every stage is independent of every other stage. We could have a stage that builds the app and another stage that runs tests. What is an environment? \u00b6 You've likely used the term environment to refer to where your application or service is running. For example, your production environment might be where your end users access your application. - Following this example, your production environment might be: 1. A physical machine or virtual machine (VM). 1. A containerized environment, such as Kubernetes. 1. A managed service, such as Azure App Service. 1. A serverless environment, such as Azure Functions. - An artifact is deployed to an environment. How does pipelines perform deployment steps? \u00b6 To deploy your software, pipelines first needs to authenticate with the target environment. Pipelines provides different authentication mechanisms. To deploy your app to an Azure resource, such as a virtual machine or App Service, you need a service connection. A service connection provides secure access to your Azure subscription by using one of two methods: Service principal authentication Managed identities for Azure resources A service principal is an identity with a limited role that can access Azure resources. Think of a service principal as a service account that can do automated tasks on your behalf. Managed identities for Azure resources are a feature of Azure Active Directory (Azure AD). Managed identities simplify the process of working with service principals. Because managed identities exist on the Azure AD tenant, Azure infrastructure can automatically authenticate the service and manage the account for you. What information does pipeline analytics provide? \u00b6 Every pipeline provides reports that include metrics, trends, and insights. These reports can help you improve the efficiency of your pipeline. Reports include: The overall pass rate of your pipeline. The pass rate of any tests that are run in the pipeline. The average duration of your pipeline runs, including the build tasks, which take the most time to complete. Design the pipeline \u00b6 When you plan a release pipeline, you usually begin by identifying the stages, or major divisions, of that pipeline. Each stage typically maps to an environment. After you define which stages you need, consider how changes are promoted from one stage to the next. Each stage can define the success criteria that must be met before the build can move to the next stage. As a whole, these approaches are used for release management . What pipeline stages do you need? \u00b6 When you want to implement a release pipeline, it's important to first identify which stages you need. The stages you choose depend on your requirements. When a change is pushed to GitHub, a trigger causes the Build stage to run. The Build stage produces a build artifact as its output. Dev stage should be the first stop for the artifact after it's built. Developers can't always run the entire service from their local development environment. For example, an e-commerce system might require the website, the products database, a payment system, and so on. We need a stage that includes everything the app needs. Setting up a Dev stage would give us an environment where we can integrate the web app with a real database. That database might still hold fictitious data, but it brings us one step closer to our final app. We build the app each time we push a change to GitHub. Does that mean each build is promoted to the Dev stage after it finishes? Building continuously gives us important feedback about our build and test health. But we want to promote to the Dev stage only when we merge code into some central branch: either main or some other release branch . We can define a condition that promotes to the Dev stage only when changes happen on a release branch. The Dev stage runs only when the change happens in the release branch. You use a condition to specify this requirement. Test stage could be the next stop and runs only after the Dev stage succeeds. It can be deployed once a day or on demand to a test environment. Staging stage could be used to run additional stress tests in a preproduction environment. It can also be used to demo a working application before production deployment. This staging environment is often the last stop before a feature or bug fix reaches our users. Best way to handle promotion would be a release approval. A release approval lets you manually promote a change from one stage to the next. After management approves the build, we can deploy the build artifact to a production environment. We can define the criteria that promote changes from one stage to the next. But we've defined some manual criteria in our pipeline. I thought DevOps was about automating everything. DevOps is really about automating repetitive and error-prone tasks. Sometimes human intervention is necessary. For example, we get approval from management before we release new features. As we get more experience with our automated deployments, we can automate more of our manual steps to speed up the process. We can automate additional quality checks in the Test stage so we don't have to approve each build, for example. We test only one release at a time. We never change releases in the middle of the pipeline. We use the same release in the Dev stage as in the Staging stage, and every release has its own version number. If the release breaks in one of the stages, we fix it and build it again with a new version number. That new release then goes through the pipeline from the very beginning. How do you measure the quality of your release process? \u00b6 Team's design a pipeline that takes their app all the way from build to staging. The whole point of this pipeline isn't just to make their lives easier. It's to ensure the quality of the software they're delivering to their customers. The quality of your release process can't be measured directly. What you can measure is how well your process works. If you're constantly changing the process, this might be an indication that there's something wrong. Releases that fail consistently at a particular point in the pipeline might also indicate that there's a problem with the release process. Do they always fail after you deploy to a particular environment? Look for these and other patterns to see if some aspects of the release process are dependent or related. A good way to keep track of your release process quality is to create visualizations of the quality of the releases. For example, add a dashboard widget that shows you the status of every release. When you want to measure the quality of a release itself, you can perform all kinds of checks within the pipeline. For example, you can execute different types of tests, such as load tests and UI tests while running your pipeline. Using a quality gate is also a great way to check the quality of your release. There are many different quality gates. For example, work item gates can verify the quality of your requirements process. You can also add additional security and compliance checks. For example, do you comply with the 4-eyes principle, or do you have the proper traceability? Lastly, when you design a quality release process, think about what kind of documentation or release notes that you'll need to provide to the user. Keeping your documentation current can be difficult. You might want to consider using a tool, such as the Azure DevOps Release Notes Generator. The generator is a function app that contains a HTTP-triggered function. It creates a Markdown file whenever a new release is created in Azure DevOps, using Azure Blob Storage. Run functional tests in Pipelines \u00b6 We incorporated unit and code coverage tests into the build process . These tests help avoid regression bugs and ensure that the code meets the company's standards for quality and style. But what kinds of tests can you run after a service is operational and deployed to an environment? What is functional testing? \u00b6 Functional tests verify that each function of the software does what it should. How the software implements each function isn't important in these tests. What's important is that the software behaves correctly. You provide an input and check that the output is what you expect. - The team first defines what a functional test covers. They explore some types of functional tests. Then they decide on the first test to add to their pipeline. - UI tests are considered to be functional tests. I have to click through every step to make sure I get the correct result. And I have to do that for every browser we support. It's very time consuming. And as the website grows in complexity, UI testing won't be practical in the long run. Nonfunctional tests check characteristics like performance and reliability. - An example of a nonfunctional test is checking to see how many people can sign in to the app simultaneously. Load testing is another example of a nonfunctional test. What kinds of functional tests can we run? \u00b6 There are many kinds of functional tests. They vary by the functionality that you need to test and the time or effort that they typically require to run. Smoke testing verifies the most basic functionality of your application or service. These tests are often run before more complete and exhaustive tests. Smoke tests should run quickly. For example, say you're developing a website. Your smoke test might use curl to verify that the site is reachable and that fetching the home page produces a 200 (OK) HTTP status. If fetching the home page produces another status code, such as 404 (Not Found) or 500 (Internal Server Error), you know that the website isn't working. You also know that there's no reason to run other tests. Instead, you diagnose the error, fix it, and restart your tests. Unit testing verifies the most fundamental components of your program or library, such as an individual function or method. You specify one or more inputs along with the expected results. The test runner performs each test and checks to see whether the actual results match the expected results. As an example, let's say you have a function that performs an arithmetic operation that includes division. You might specify a few values that you expect your users to enter. You also specify edge-case values such as 0 and -1. If you expect a certain input to produce an error or exception, you can verify that the function produces that error. The UI tests that you'll run are also unit tests. Integration testing verifies that multiple software components work together to form a complete system. For example, an e-commerce system might include a website, a products database, and a payment system. You might write an integration test that adds items to the shopping cart and then purchases the items. The test verifies that the web application can connect to the products database and then fulfill the order. You can combine unit tests and integration tests to create a layered testing strategy . For example, you might run unit tests on each of your components before you run the integration tests. If all unit tests pass, you can move on to the integration tests with greater confidence. Regression testing helps determine whether code, configuration, or other changes affect the software's overall behavior. A regression occurs when existing behavior either changes or breaks after you add or change a feature. Regression testing is important because a change in one component can affect the behavior of another component. For example, say you optimize a database for write performance. The read performance of that database, which is handled by another component, might unexpectedly drop. The drop in read performance is a regression. You can use various strategies to test for regression. These strategies typically vary by the number of tests you run to verify that a new feature or bug fix doesn't break existing functionality. However, when you automate the tests, regression testing might involve just running all unit tests and integration tests each time the software changes. Sanity testing involves testing each major component of a piece of software to verify that the software appears to be working and can undergo more thorough testing. You can think of sanity tests as being less thorough than regression tests or unit tests. But sanity tests are broader than smoke tests. Although sanity testing can be automated, it's often done manually in response to a feature change or a bug fix . For example, a software tester who is validating a bug fix might also verify that other features are working by entering some typical values. If the software appears to be working as expected, it can then go through a more thorough test pass. User interface (UI) testing verifies the behavior of an application's user interface. UI tests help verify that the sequence, or order, of user interactions leads to the expected result. A unit test or integration test might verify that the UI receives data correctly. But UI testing helps verify that the user interface displays correctly and that the result functions as expected for the user. For example, a UI test might verify that the correct animation appears in response to a button click. A second test might verify that the same animation appears correctly when the window is resized. You can also use a capture-and-replay system to automatically build your UI tests. Usability testing is a form of manual testing that verifies an application's behavior from the user's perspective. Usability testing is typically done by the team that builds the software. Whereas UI testing focuses on whether a feature behaves as expected, usability testing helps verify that the software is intuitive and meets the user's needs. In other words, usability testing helps verify whether the software is \"usable.\" For example, say you have a website that includes a link to the user's profile. A UI test can verify that the link is present and that it brings up the user's profile when the link is clicked. However, if humans can't easily locate this link, they might become frustrated when they try to access their profile. User acceptance testing (UAT) , like usability testing, focuses on an application's behavior from the user's perspective. Unlike acceptance testing, UAT is typically done by real end users. Depending on the software, end users might be asked to complete specific tasks. Or they might be allowed to explore the software without following any specific guidelines. For custom software, UAT typically happens directly with the client. For more general-purpose software, teams might run beta tests. In beta tests, users from different geographic regions or users who have certain interests receive early access to the software. Feedback from testers can be direct or indirect. Direct feedback might come in the form of verbal comments. Indirect feedback can come in the form of measuring testers' body language, eye movements, or the time they take to complete certain tasks. How do I run functional tests in the pipeline? \u00b6 Ask yourself: In which stage will the tests run? On what system will the tests run? Will they run on the agent or on the infrastructure that hosts the application? - We can run them in the Test stage of our pipeline. We test the website from her Windows laptop because that's how most of our users visit the site. But we build on Linux and then deploy Azure App Service on Linux. How do we handle that? We can run them: On the agent : either a Microsoft agent or an agent that we host. On test infrastructure : either on-premises or in the cloud. Create a functional test plan \u00b6 If your team is just starting to incorporate functional tests into their pipeline (or even if you're already doing that), remember that you always need a plan. Many times, when someone asks team members about their performance testing plan, it's common for them to respond with a list of tools they are going to use. However, a list of tools isn't a plan . You also must work out how the testing environments will be configured, you need to determine the processes to be used, and you need to determine what success or failure looks like. Make sure your plan: Takes the expectations of the business into account. Takes the expectations of the target users into account. Defines the metrics you will use. Defines the KPIs you will use. It is also important to work out how you will monitor performance once the application has been deployed, and not just measure performance before it's released. Write the UI tests \u00b6 Once the test plan is ready, we're ready to write our tests. Create an NUnit project that includes Selenium. The project will be stored in the directory along with the app's source code. Write a test case that uses automation to click the specified link. The test case verifies that the expected modal window appears. Use the id attribute we saved to specify the parameters to the test case method. This task creates a sequence, or series, of tests. Configure the tests to run on Chrome, Firefox, and Microsoft Edge. This task creates a matrix of tests. Run the tests and watch each web browser come up automatically. Watch Selenium automatically run through the series of tests for each browser. In the console window, verify that all the tests pass. Run the UI tests in the pipeline \u00b6 The Build stage publishes only the app package as the build artifact. Publish task in the Build stage generates two build artifacts : the app package and the compiled UI tests. We build the UI tests during the Build stage to ensure that they'll compile during the Test stage. But we don't need to publish the compiled test code. We build it again during the Test stage when the tests run. The Test stage includes a second job that builds and runs the tests. Although we use a Linux agent to build the application, here we use a Windows agent to run the UI tests. We use a Windows agent because we runs manual tests on Windows, and that's what most customers use. Remember, tests that you repeatedly run manually are good candidates for automation. Run nonfunctional tests in Pipelines \u00b6 After your service is operational and deployed to an environment, how can you determine the application's performance under both realistic and heavy loads? Does your application expose any loopholes or weaknesses that might cause an information breach? What is nonfunctional testing? \u00b6 With automated functional tests, is there anything we should do in Staging to help increase the quality of our releases? Normally, after our sites are in production, I run performance, load, and stress tests. But I'd like to start running other kinds of tests as well, such as compliance tests and security tests. All of those tests are difficult to run manually. By using automation, we can run them both earlier and more frequently. - Nonfunctional testing always tests something that's measurable. The goal is to improve the product. - You might do that, for example, by improving how efficiently the application uses resources or by improving response times when many customers use it simultaneously. Here are some of the questions that nonfunctional tests can answer: 1. How does the application perform under normal circumstances? 1. How does the application perform when many users sign in concurrently? 1. How secure is the application? What kinds of nonfunctional tests can I run? \u00b6 There are many kinds of nonfunctional tests. Many of them fit in the broad categories of performance testing and security testing. Performance testing : The goal of performance testing is to improve the speed, scalability, and stability of an application . Testing for speed determines how quickly an application responds. Testing for scalability determines the maximum user load an application can handle. Testing for stability determines whether the application remains stable under different loads. Two common types of performance tests are load tests and stress tests. a. Load tests determine the performance of an application under realistic loads. For example, load tests can determine how well an application performs at the upper limit of its service-level agreement (SLA). Basically, load testing determines the behavior of the application when multiple users need it at the same time. Users aren't necessarily people. A load test for printer software, for example, might send the application large amounts of data. A load test for a mail server might simulate thousands of concurrent users. Load testing is also a good way to uncover problems that exist only when the application is operating at its limits. That's when issues such as buffer overflow and memory leaks can surface. b. Stress tests determine the stability and robustness of an application under heavy loads. The loads go beyond what's specified for the application. The stress tests determine whether the application will crash under these loads. If the application fails, the stress test checks to ensure that it fails gracefully. A graceful failure might, for example, issue an appropriate, informative error message. Scenarios in which applications must operate under abnormally heavy loads are common. For example, in case your video goes viral, you'll want to know how well the servers can handle the extra load. Another typical scenario is high traffic on shopping websites during holiday seasons. Security testing ensures that applications are free from vulnerabilities, threats, and risks. Thorough security testing finds all the possible loopholes and weaknesses of the system that might cause an information breach or a loss of revenue. There are many types of security testing. Two of them are penetration testing and compliance testing. a. Penetration testing, or pen testing , is a type of security testing that tests the insecure areas of the application. In particular, it tests for vulnerabilities that an attacker could exploit. An authorized, simulated cyber attack is usually a part of penetration testing. b. Compliance testing determines whether an application is compliant with some set of requirements, inside or outside the company. For example, healthcare organizations usually need to comply with HIPAA (Health Insurance Portability and Accountability Act of 1996), which provides data privacy and security provisions for safeguarding medical information. For example, on Linux systems, the default user mask must be 027 or more restrictive. A security test needs to prove that this requirement is met. Just as you did when you incorporated functional tests into your pipeline, focus on the types of nonfunctional tests that matter most. For example, if your team must adhere to certain compliance requirements, consider adding automated tests that provide a detailed status report. Manage release cadence in Pipelines by using deployment patterns \u00b6 Choose and implement a deployment pattern that helps you smoothly roll out new application features to your users. - You help the team solve another problem. How do they implement a deployment pattern that lets them release to production in a way that's best both for the company and for their users? You'll help them evaluate the possibilities and then implement the one that they choose. - Ops thinks it takes too long to release new features. They can't do anything until management approves the release and, right now, there's no smooth way to roll out the features after they give the OK. The process is not only long but messy. It's manual, and there's downtime. What are deployment patterns? \u00b6 A deployment pattern is an automated way to smoothly roll out new application features to your users. An appropriate deployment pattern helps you minimize downtime . - Some patterns also enable you to roll out new features progressively. That way, you can validate new features with select users before you make those features available to everyone. - A deployment pattern is an automated way to do the cutover. It's how we move the software from the final preproduction stage to live production. - Another advantage of a deployment pattern is that it gives us a chance to run tests that should really happen in production. Types of deployment patterns \u00b6 1. Blue-green deployment : A blue-green deployment reduces risk and downtime by running two identical environments. These environments are called blue and green. At any time, only one of the environments is live. A blue-green deployment typically involves a router or load balancer that helps control the flow of traffic. Let's say blue is live. As we prepare a new release, we do our final tests in the green environment. After the software is working in the green environment, we just switch the router so that all incoming requests go to the green environment. Blue-green deployment also gives us a fast way to do a rollback. If anything goes wrong in the green environment, then we just switch the router back to the blue environment. A blue-green deployment is something Ops can control. Switching a router is straightforward. It's easy and sounds safe. And in a blue-green deployment, management has an environment to evaluate. When they give the OK, we can easily switch. Canary releases : A canary release is a way to identify potential problems early without exposing all users to the issue. The idea is that we expose a new feature to only a small subset of users before we make it available to everyone. In a canary release, we monitor what happens when we release the feature. If the release has problems, then we apply a fix. After the canary release is known to be stable, we move it to the actual production environment. For example: You have a new feature for your website, and you're ready to deploy it. However, this feature is risky because it changes the way your users interact with the site. You can use canary release to a small group of early adopters who have signed up to see new features. Feature toggles : Feature toggles let us \"flip a switch\" at runtime. We can deploy new software without exposing any other new or changed functionality to our users. In this deployment pattern, we build new features behind a toggle. When a release occurs, the feature is \"off\" so that it doesn't affect the production software. Depending on how we configure the toggle, we can flip the switch to \"on\" and expose it how we want. The big advantage to the feature toggles pattern is that it helps us avoid too much branching. Merging branches can be painful. Dark launches : A dark launch is similar to a canary release or switching a feature toggle. Rather than expose a new feature to everyone, in a dark launch we release the feature to a small set of users. Those users don't know they're testing the feature for us. We don't even highlight the new feature to them. That's why it's called a dark launch. The software is gradually or unobtrusively released to users so we can get feedback and can test performance. For example: You're not sure how your users will react to your new feature. You want to release your feature to a small, random sample of users to see how they react. A/B testing : A/B testing compares two versions of a webpage or app to determine which one performs better. A/B testing is like an experiment. In A/B testing, we randomly show users two or more variations of a page. Then we use statistical analysis to decide which variation performs better for our goals. For example: The marketing team has asked you to add a banner to your company's website. They have two versions of this banner. They want to know which version produces more clickthroughs. You can use A/B testing deployment pattern to help the marketing team identify the better version. Progressive-exposure deployment : Progressive-exposure deployment is sometimes called ring-based deployment. It's another way to limit how changes affect users while making sure that those changes are valid in a production environment. Rings are basically an extension of the canary stage. The canary release releases to a stage to measure effect. Adding another ring is essentially the same idea. In a ring-based deployment, we deploy changes to risk-tolerant customers first. Then we progressively roll out to a larger set of customers. Choosing the right deployment pattern \u00b6 A good deployment pattern can help you minimize downtime. It can also enable you to roll out new features progressively to your users. The deployment pattern that you choose depends on your reasons for the deployment as well as your resources . Do you have canary testers in place? Will you employ a dark launch and choose testers who don't know that they are testers? If you have a trusted set of testers that progressively increases from a small set to a larger set, then could choose a progressive-exposure deployment. Or if you want to know if one version performs better than another version, you could choose A/B testing. Why are containers important? \u00b6 Dependency versioning challenges for QA QA test for multiple teams, and it can be challenging since each team uses their own technology stack. And even when they use the same underlying platforms, like .NET or Java, they often target different versions. QA sometimes spend half of their day simply getting test environments in a state where they can run the code they need to test. When something doesn't work, it's hard to tell whether there's a bug in the code or if they accidentally configured platform version 4.2.3 instead of 4.3.2. Overhead due to solving app isolation with VMs We have a few teams that have unique version requirements, so we have to publish their apps on their own virtual machines just to make sure their version and component requirements don't conflict with our other apps. - Besides the overhead involved in maintaining the extra set of VMs, it also costs us more than it would if those apps could run side by side. Configuration inconsistencies between deployment stages I was working on the peer-to-peer update system and had it all working on my machine. But when I handed it off for deployment, it didn't work in production. I had forgotten that I needed to open port 315 as part of the service. It took us over a day of troubleshooting to realize what was going on. Once we opened that up in production, things worked as expected. What is a container? It's more like a lightweight virtual machine designed to run directly on the host operating system. When you build your project, the output is a container that includes your software along with its dependencies. However, it's not a complete virtualized system, so it can spin up in a little as less than one second. Security and isolation are handled by the host operating system . When your container runs in a host process, the container is isolated from the other processes on that same host machine. This isolation allows your container to load whatever versions of components it needs, regardless of what other containers are doing. It also means you can easily run multiple containers on the same host simultaneously. Instead of shipping source code or a set of binaries, the entire container becomes the artifact. Containers already includes all the required versions of its dependencies. Containers are more of a packaging and deployment technology . They don't impact the fundamental software we're writing. We can just instruct our tools to produce a Docker container at the end of the build. What is Docker? \u00b6 Docker is a technology for automating the packaging and deployment of portable, self-sufficient containers. - Docker containers can be run anywhere a Docker host is found, whether on a development machine, a departmental server, an enterprise datacenter, or in the cloud. Azure provides multiple ways to run container-based applications, including App Service or as part of clusters managed with orchestration technologies like Kubernetes. Dependency versioning challenges for QA Applications are packaged as containers that bring the correct versions of their dependencies with them. Overhead due to solving app isolation with VMs Many isolated containers can be run on the same host with benefits over virtual machines including faster startup time to greater resource efficiency. Configuration inconsistencies between DevOps stages Containers ship with manifests that automate configuration requirements, such as which ports need to be exposed. Adopting Docker containers can be a key step on the path towards a microservices architecture. Why is container orchestration important? \u00b6 As development organizations scale, so do the complexities of the solutions they deliver. Over time, different products and services take increasing dependence on each other. This can result in different development and operations requirements for different components within a given application. Refactoring those components as their own distinct microservices can improve architectural efficiency. A microservice is a small, autonomous service designed to be self-contained and to fulfill a specific business capability. Microservices are typical apps, like our web app. The main difference is that instead of building and deploying a single monolithic app, we refactor any components that would be better maintained and managed as autonomous services. We then build those services to be good at what they do and deploy them to operate independently. Microservices are an architectural concept and not a deployment technology . Containers provide a great technical foundation for building and deploying these services, but this leads to a new question: how do we manage all of these containers floating around? This is where orchestration technologies like Kubernetes come in. Kubernetes is a platform for managing containerized workloads and services. It's a great option for organizations that have a growing number of containers that need to be deployed, integrated, and monitored in any environment. Kubernetes offers a way for us to deploy to different namespaces. This enables us to partition our deployments so that we can have entire environments dedicated to testing versus production. And since they all run in the same cluster and use the same containers, the testing experience should offer what we expect to see in production. Assuming we have our projects set up to build Docker containers, all we need to deploy to Kubernetes are some manifest files that describe the services and their deployments. What is Kubernetes? \u00b6 Kubernetes is a technology for orchestrating multi-container deployments. It provides a framework for running distributed systems in a declarative, responsive fashion . It automatically applies and enforces your deployment patterns to ensure that containers are deployed and run as intended. It also offers support for specialized release cadences, such as those using canary deployments . Complexity of multi-container deployments Kubernetes is designed, first and foremost, to automate the processes around deploying and maintaining container deployments. Consistency across environments and stages Just as containers ensure a consistent deployment for the apps they contain, Kubernetes ensures a consistent deployment for the containers a cluster manages. Azure DevOps support Azure DevOps offers first-class support for working with Kubernetes. Ease of development The impact of Kubernetes on a source project is comparable to that of adding Docker support, which is minimal and limited to declarative configuration. Benefits of Continous Delivery in the Release process \u00b6 The time it takes to get the build into the Test stage. The team achieved this improvement by using a scheduled trigger to deploy to Test every day at 3:00 AM. The time it takes to get the tested build into Staging . The team achieved this improvement by adding Selenium UI tests, a form of functional testing, to the Test stage. These automated tests are much faster than the manual versions. The time it takes to get the approved build from Staging to live . The team achieved this improvement by adding manual approval checks to the pipeline. When management signs off, Ops can release the changes from Staging to live.","title":"CD"},{"location":"devops/cd/#what-is-continuous-delivery","text":"Continuous Delivery (CD) is the process to build, test, configure, and deploy from a build to a production environment. CD by itself is a set of processes, tools, and techniques that enable rapid, reliable, and continuous delivery of software. So CD isn't only about setting up a pipeline, although that part is important. - CD is about setting up a working environment where: 1. We have a reliable and repeatable process for releasing and deploying software. 1. We automate as much as possible. 1. We don't put off doing something that's difficult or painful. Instead, we do it more often so that we figure out how to make it routine. 1. We keep everything in source control. 1. We all agree that done means released . 1. We build quality into the process. Quality is never an afterthought. 1. We're all responsible for the release process. We no longer work in silos. 1. We always try to improve. - CD helps software teams deliver reliable software updates to their customers at a rapid cadence. - CD also helps ensure that both customers and stakeholders have the latest features and fixes quickly. - This enables the full the benefits of CD - repeatable builds which are verified in a consistent way across environments.","title":"What is continuous delivery?"},{"location":"devops/cd/#plan-a-release-pipeline","text":"Goal : We already have the build artifact. It's the .zip file that our existing build pipeline creates. But how do we deploy it to a live environment A basic CD pipeline contains a trigger to get the process going and at least one stage, or deployment phase.","title":"Plan a release pipeline"},{"location":"devops/cd/#what-is-a-pipeline-stage","text":"A stage is a part of the pipeline that can run independently and be triggered by different mechanisms. A mechanism might be the success of the previous stage, a schedule, or even a manual trigger. A stage is made up of jobs. A job is a series of steps that defines how to build, test, or deploy your software. Every stage is independent of every other stage. We could have a stage that builds the app and another stage that runs tests.","title":"What is a pipeline stage?"},{"location":"devops/cd/#what-is-an-environment","text":"You've likely used the term environment to refer to where your application or service is running. For example, your production environment might be where your end users access your application. - Following this example, your production environment might be: 1. A physical machine or virtual machine (VM). 1. A containerized environment, such as Kubernetes. 1. A managed service, such as Azure App Service. 1. A serverless environment, such as Azure Functions. - An artifact is deployed to an environment.","title":"What is an environment?"},{"location":"devops/cd/#how-does-pipelines-perform-deployment-steps","text":"To deploy your software, pipelines first needs to authenticate with the target environment. Pipelines provides different authentication mechanisms. To deploy your app to an Azure resource, such as a virtual machine or App Service, you need a service connection. A service connection provides secure access to your Azure subscription by using one of two methods: Service principal authentication Managed identities for Azure resources A service principal is an identity with a limited role that can access Azure resources. Think of a service principal as a service account that can do automated tasks on your behalf. Managed identities for Azure resources are a feature of Azure Active Directory (Azure AD). Managed identities simplify the process of working with service principals. Because managed identities exist on the Azure AD tenant, Azure infrastructure can automatically authenticate the service and manage the account for you.","title":"How does pipelines perform deployment steps?"},{"location":"devops/cd/#what-information-does-pipeline-analytics-provide","text":"Every pipeline provides reports that include metrics, trends, and insights. These reports can help you improve the efficiency of your pipeline. Reports include: The overall pass rate of your pipeline. The pass rate of any tests that are run in the pipeline. The average duration of your pipeline runs, including the build tasks, which take the most time to complete.","title":"What information does pipeline analytics provide?"},{"location":"devops/cd/#design-the-pipeline","text":"When you plan a release pipeline, you usually begin by identifying the stages, or major divisions, of that pipeline. Each stage typically maps to an environment. After you define which stages you need, consider how changes are promoted from one stage to the next. Each stage can define the success criteria that must be met before the build can move to the next stage. As a whole, these approaches are used for release management .","title":"Design the pipeline"},{"location":"devops/cd/#what-pipeline-stages-do-you-need","text":"When you want to implement a release pipeline, it's important to first identify which stages you need. The stages you choose depend on your requirements. When a change is pushed to GitHub, a trigger causes the Build stage to run. The Build stage produces a build artifact as its output. Dev stage should be the first stop for the artifact after it's built. Developers can't always run the entire service from their local development environment. For example, an e-commerce system might require the website, the products database, a payment system, and so on. We need a stage that includes everything the app needs. Setting up a Dev stage would give us an environment where we can integrate the web app with a real database. That database might still hold fictitious data, but it brings us one step closer to our final app. We build the app each time we push a change to GitHub. Does that mean each build is promoted to the Dev stage after it finishes? Building continuously gives us important feedback about our build and test health. But we want to promote to the Dev stage only when we merge code into some central branch: either main or some other release branch . We can define a condition that promotes to the Dev stage only when changes happen on a release branch. The Dev stage runs only when the change happens in the release branch. You use a condition to specify this requirement. Test stage could be the next stop and runs only after the Dev stage succeeds. It can be deployed once a day or on demand to a test environment. Staging stage could be used to run additional stress tests in a preproduction environment. It can also be used to demo a working application before production deployment. This staging environment is often the last stop before a feature or bug fix reaches our users. Best way to handle promotion would be a release approval. A release approval lets you manually promote a change from one stage to the next. After management approves the build, we can deploy the build artifact to a production environment. We can define the criteria that promote changes from one stage to the next. But we've defined some manual criteria in our pipeline. I thought DevOps was about automating everything. DevOps is really about automating repetitive and error-prone tasks. Sometimes human intervention is necessary. For example, we get approval from management before we release new features. As we get more experience with our automated deployments, we can automate more of our manual steps to speed up the process. We can automate additional quality checks in the Test stage so we don't have to approve each build, for example. We test only one release at a time. We never change releases in the middle of the pipeline. We use the same release in the Dev stage as in the Staging stage, and every release has its own version number. If the release breaks in one of the stages, we fix it and build it again with a new version number. That new release then goes through the pipeline from the very beginning.","title":"What pipeline stages do you need?"},{"location":"devops/cd/#how-do-you-measure-the-quality-of-your-release-process","text":"Team's design a pipeline that takes their app all the way from build to staging. The whole point of this pipeline isn't just to make their lives easier. It's to ensure the quality of the software they're delivering to their customers. The quality of your release process can't be measured directly. What you can measure is how well your process works. If you're constantly changing the process, this might be an indication that there's something wrong. Releases that fail consistently at a particular point in the pipeline might also indicate that there's a problem with the release process. Do they always fail after you deploy to a particular environment? Look for these and other patterns to see if some aspects of the release process are dependent or related. A good way to keep track of your release process quality is to create visualizations of the quality of the releases. For example, add a dashboard widget that shows you the status of every release. When you want to measure the quality of a release itself, you can perform all kinds of checks within the pipeline. For example, you can execute different types of tests, such as load tests and UI tests while running your pipeline. Using a quality gate is also a great way to check the quality of your release. There are many different quality gates. For example, work item gates can verify the quality of your requirements process. You can also add additional security and compliance checks. For example, do you comply with the 4-eyes principle, or do you have the proper traceability? Lastly, when you design a quality release process, think about what kind of documentation or release notes that you'll need to provide to the user. Keeping your documentation current can be difficult. You might want to consider using a tool, such as the Azure DevOps Release Notes Generator. The generator is a function app that contains a HTTP-triggered function. It creates a Markdown file whenever a new release is created in Azure DevOps, using Azure Blob Storage.","title":"How do you measure the quality of your release process?"},{"location":"devops/cd/#run-functional-tests-in-pipelines","text":"We incorporated unit and code coverage tests into the build process . These tests help avoid regression bugs and ensure that the code meets the company's standards for quality and style. But what kinds of tests can you run after a service is operational and deployed to an environment?","title":"Run functional tests in Pipelines"},{"location":"devops/cd/#what-is-functional-testing","text":"Functional tests verify that each function of the software does what it should. How the software implements each function isn't important in these tests. What's important is that the software behaves correctly. You provide an input and check that the output is what you expect. - The team first defines what a functional test covers. They explore some types of functional tests. Then they decide on the first test to add to their pipeline. - UI tests are considered to be functional tests. I have to click through every step to make sure I get the correct result. And I have to do that for every browser we support. It's very time consuming. And as the website grows in complexity, UI testing won't be practical in the long run. Nonfunctional tests check characteristics like performance and reliability. - An example of a nonfunctional test is checking to see how many people can sign in to the app simultaneously. Load testing is another example of a nonfunctional test.","title":"What is functional testing?"},{"location":"devops/cd/#what-kinds-of-functional-tests-can-we-run","text":"There are many kinds of functional tests. They vary by the functionality that you need to test and the time or effort that they typically require to run. Smoke testing verifies the most basic functionality of your application or service. These tests are often run before more complete and exhaustive tests. Smoke tests should run quickly. For example, say you're developing a website. Your smoke test might use curl to verify that the site is reachable and that fetching the home page produces a 200 (OK) HTTP status. If fetching the home page produces another status code, such as 404 (Not Found) or 500 (Internal Server Error), you know that the website isn't working. You also know that there's no reason to run other tests. Instead, you diagnose the error, fix it, and restart your tests. Unit testing verifies the most fundamental components of your program or library, such as an individual function or method. You specify one or more inputs along with the expected results. The test runner performs each test and checks to see whether the actual results match the expected results. As an example, let's say you have a function that performs an arithmetic operation that includes division. You might specify a few values that you expect your users to enter. You also specify edge-case values such as 0 and -1. If you expect a certain input to produce an error or exception, you can verify that the function produces that error. The UI tests that you'll run are also unit tests. Integration testing verifies that multiple software components work together to form a complete system. For example, an e-commerce system might include a website, a products database, and a payment system. You might write an integration test that adds items to the shopping cart and then purchases the items. The test verifies that the web application can connect to the products database and then fulfill the order. You can combine unit tests and integration tests to create a layered testing strategy . For example, you might run unit tests on each of your components before you run the integration tests. If all unit tests pass, you can move on to the integration tests with greater confidence. Regression testing helps determine whether code, configuration, or other changes affect the software's overall behavior. A regression occurs when existing behavior either changes or breaks after you add or change a feature. Regression testing is important because a change in one component can affect the behavior of another component. For example, say you optimize a database for write performance. The read performance of that database, which is handled by another component, might unexpectedly drop. The drop in read performance is a regression. You can use various strategies to test for regression. These strategies typically vary by the number of tests you run to verify that a new feature or bug fix doesn't break existing functionality. However, when you automate the tests, regression testing might involve just running all unit tests and integration tests each time the software changes. Sanity testing involves testing each major component of a piece of software to verify that the software appears to be working and can undergo more thorough testing. You can think of sanity tests as being less thorough than regression tests or unit tests. But sanity tests are broader than smoke tests. Although sanity testing can be automated, it's often done manually in response to a feature change or a bug fix . For example, a software tester who is validating a bug fix might also verify that other features are working by entering some typical values. If the software appears to be working as expected, it can then go through a more thorough test pass. User interface (UI) testing verifies the behavior of an application's user interface. UI tests help verify that the sequence, or order, of user interactions leads to the expected result. A unit test or integration test might verify that the UI receives data correctly. But UI testing helps verify that the user interface displays correctly and that the result functions as expected for the user. For example, a UI test might verify that the correct animation appears in response to a button click. A second test might verify that the same animation appears correctly when the window is resized. You can also use a capture-and-replay system to automatically build your UI tests. Usability testing is a form of manual testing that verifies an application's behavior from the user's perspective. Usability testing is typically done by the team that builds the software. Whereas UI testing focuses on whether a feature behaves as expected, usability testing helps verify that the software is intuitive and meets the user's needs. In other words, usability testing helps verify whether the software is \"usable.\" For example, say you have a website that includes a link to the user's profile. A UI test can verify that the link is present and that it brings up the user's profile when the link is clicked. However, if humans can't easily locate this link, they might become frustrated when they try to access their profile. User acceptance testing (UAT) , like usability testing, focuses on an application's behavior from the user's perspective. Unlike acceptance testing, UAT is typically done by real end users. Depending on the software, end users might be asked to complete specific tasks. Or they might be allowed to explore the software without following any specific guidelines. For custom software, UAT typically happens directly with the client. For more general-purpose software, teams might run beta tests. In beta tests, users from different geographic regions or users who have certain interests receive early access to the software. Feedback from testers can be direct or indirect. Direct feedback might come in the form of verbal comments. Indirect feedback can come in the form of measuring testers' body language, eye movements, or the time they take to complete certain tasks.","title":"What kinds of functional tests can we run?"},{"location":"devops/cd/#how-do-i-run-functional-tests-in-the-pipeline","text":"Ask yourself: In which stage will the tests run? On what system will the tests run? Will they run on the agent or on the infrastructure that hosts the application? - We can run them in the Test stage of our pipeline. We test the website from her Windows laptop because that's how most of our users visit the site. But we build on Linux and then deploy Azure App Service on Linux. How do we handle that? We can run them: On the agent : either a Microsoft agent or an agent that we host. On test infrastructure : either on-premises or in the cloud.","title":"How do I run functional tests in the pipeline?"},{"location":"devops/cd/#create-a-functional-test-plan","text":"If your team is just starting to incorporate functional tests into their pipeline (or even if you're already doing that), remember that you always need a plan. Many times, when someone asks team members about their performance testing plan, it's common for them to respond with a list of tools they are going to use. However, a list of tools isn't a plan . You also must work out how the testing environments will be configured, you need to determine the processes to be used, and you need to determine what success or failure looks like. Make sure your plan: Takes the expectations of the business into account. Takes the expectations of the target users into account. Defines the metrics you will use. Defines the KPIs you will use. It is also important to work out how you will monitor performance once the application has been deployed, and not just measure performance before it's released.","title":"Create a functional test plan"},{"location":"devops/cd/#write-the-ui-tests","text":"Once the test plan is ready, we're ready to write our tests. Create an NUnit project that includes Selenium. The project will be stored in the directory along with the app's source code. Write a test case that uses automation to click the specified link. The test case verifies that the expected modal window appears. Use the id attribute we saved to specify the parameters to the test case method. This task creates a sequence, or series, of tests. Configure the tests to run on Chrome, Firefox, and Microsoft Edge. This task creates a matrix of tests. Run the tests and watch each web browser come up automatically. Watch Selenium automatically run through the series of tests for each browser. In the console window, verify that all the tests pass.","title":"Write the UI tests"},{"location":"devops/cd/#run-the-ui-tests-in-the-pipeline","text":"The Build stage publishes only the app package as the build artifact. Publish task in the Build stage generates two build artifacts : the app package and the compiled UI tests. We build the UI tests during the Build stage to ensure that they'll compile during the Test stage. But we don't need to publish the compiled test code. We build it again during the Test stage when the tests run. The Test stage includes a second job that builds and runs the tests. Although we use a Linux agent to build the application, here we use a Windows agent to run the UI tests. We use a Windows agent because we runs manual tests on Windows, and that's what most customers use. Remember, tests that you repeatedly run manually are good candidates for automation.","title":"Run the UI tests in the pipeline"},{"location":"devops/cd/#run-nonfunctional-tests-in-pipelines","text":"After your service is operational and deployed to an environment, how can you determine the application's performance under both realistic and heavy loads? Does your application expose any loopholes or weaknesses that might cause an information breach?","title":"Run nonfunctional tests in Pipelines"},{"location":"devops/cd/#what-is-nonfunctional-testing","text":"With automated functional tests, is there anything we should do in Staging to help increase the quality of our releases? Normally, after our sites are in production, I run performance, load, and stress tests. But I'd like to start running other kinds of tests as well, such as compliance tests and security tests. All of those tests are difficult to run manually. By using automation, we can run them both earlier and more frequently. - Nonfunctional testing always tests something that's measurable. The goal is to improve the product. - You might do that, for example, by improving how efficiently the application uses resources or by improving response times when many customers use it simultaneously. Here are some of the questions that nonfunctional tests can answer: 1. How does the application perform under normal circumstances? 1. How does the application perform when many users sign in concurrently? 1. How secure is the application?","title":"What is nonfunctional testing?"},{"location":"devops/cd/#what-kinds-of-nonfunctional-tests-can-i-run","text":"There are many kinds of nonfunctional tests. Many of them fit in the broad categories of performance testing and security testing. Performance testing : The goal of performance testing is to improve the speed, scalability, and stability of an application . Testing for speed determines how quickly an application responds. Testing for scalability determines the maximum user load an application can handle. Testing for stability determines whether the application remains stable under different loads. Two common types of performance tests are load tests and stress tests. a. Load tests determine the performance of an application under realistic loads. For example, load tests can determine how well an application performs at the upper limit of its service-level agreement (SLA). Basically, load testing determines the behavior of the application when multiple users need it at the same time. Users aren't necessarily people. A load test for printer software, for example, might send the application large amounts of data. A load test for a mail server might simulate thousands of concurrent users. Load testing is also a good way to uncover problems that exist only when the application is operating at its limits. That's when issues such as buffer overflow and memory leaks can surface. b. Stress tests determine the stability and robustness of an application under heavy loads. The loads go beyond what's specified for the application. The stress tests determine whether the application will crash under these loads. If the application fails, the stress test checks to ensure that it fails gracefully. A graceful failure might, for example, issue an appropriate, informative error message. Scenarios in which applications must operate under abnormally heavy loads are common. For example, in case your video goes viral, you'll want to know how well the servers can handle the extra load. Another typical scenario is high traffic on shopping websites during holiday seasons. Security testing ensures that applications are free from vulnerabilities, threats, and risks. Thorough security testing finds all the possible loopholes and weaknesses of the system that might cause an information breach or a loss of revenue. There are many types of security testing. Two of them are penetration testing and compliance testing. a. Penetration testing, or pen testing , is a type of security testing that tests the insecure areas of the application. In particular, it tests for vulnerabilities that an attacker could exploit. An authorized, simulated cyber attack is usually a part of penetration testing. b. Compliance testing determines whether an application is compliant with some set of requirements, inside or outside the company. For example, healthcare organizations usually need to comply with HIPAA (Health Insurance Portability and Accountability Act of 1996), which provides data privacy and security provisions for safeguarding medical information. For example, on Linux systems, the default user mask must be 027 or more restrictive. A security test needs to prove that this requirement is met. Just as you did when you incorporated functional tests into your pipeline, focus on the types of nonfunctional tests that matter most. For example, if your team must adhere to certain compliance requirements, consider adding automated tests that provide a detailed status report.","title":"What kinds of nonfunctional tests can I run?"},{"location":"devops/cd/#manage-release-cadence-in-pipelines-by-using-deployment-patterns","text":"Choose and implement a deployment pattern that helps you smoothly roll out new application features to your users. - You help the team solve another problem. How do they implement a deployment pattern that lets them release to production in a way that's best both for the company and for their users? You'll help them evaluate the possibilities and then implement the one that they choose. - Ops thinks it takes too long to release new features. They can't do anything until management approves the release and, right now, there's no smooth way to roll out the features after they give the OK. The process is not only long but messy. It's manual, and there's downtime.","title":"Manage release cadence in Pipelines by using deployment patterns"},{"location":"devops/cd/#what-are-deployment-patterns","text":"A deployment pattern is an automated way to smoothly roll out new application features to your users. An appropriate deployment pattern helps you minimize downtime . - Some patterns also enable you to roll out new features progressively. That way, you can validate new features with select users before you make those features available to everyone. - A deployment pattern is an automated way to do the cutover. It's how we move the software from the final preproduction stage to live production. - Another advantage of a deployment pattern is that it gives us a chance to run tests that should really happen in production.","title":"What are deployment patterns?"},{"location":"devops/cd/#types-of-deployment-patterns","text":"1. Blue-green deployment : A blue-green deployment reduces risk and downtime by running two identical environments. These environments are called blue and green. At any time, only one of the environments is live. A blue-green deployment typically involves a router or load balancer that helps control the flow of traffic. Let's say blue is live. As we prepare a new release, we do our final tests in the green environment. After the software is working in the green environment, we just switch the router so that all incoming requests go to the green environment. Blue-green deployment also gives us a fast way to do a rollback. If anything goes wrong in the green environment, then we just switch the router back to the blue environment. A blue-green deployment is something Ops can control. Switching a router is straightforward. It's easy and sounds safe. And in a blue-green deployment, management has an environment to evaluate. When they give the OK, we can easily switch. Canary releases : A canary release is a way to identify potential problems early without exposing all users to the issue. The idea is that we expose a new feature to only a small subset of users before we make it available to everyone. In a canary release, we monitor what happens when we release the feature. If the release has problems, then we apply a fix. After the canary release is known to be stable, we move it to the actual production environment. For example: You have a new feature for your website, and you're ready to deploy it. However, this feature is risky because it changes the way your users interact with the site. You can use canary release to a small group of early adopters who have signed up to see new features. Feature toggles : Feature toggles let us \"flip a switch\" at runtime. We can deploy new software without exposing any other new or changed functionality to our users. In this deployment pattern, we build new features behind a toggle. When a release occurs, the feature is \"off\" so that it doesn't affect the production software. Depending on how we configure the toggle, we can flip the switch to \"on\" and expose it how we want. The big advantage to the feature toggles pattern is that it helps us avoid too much branching. Merging branches can be painful. Dark launches : A dark launch is similar to a canary release or switching a feature toggle. Rather than expose a new feature to everyone, in a dark launch we release the feature to a small set of users. Those users don't know they're testing the feature for us. We don't even highlight the new feature to them. That's why it's called a dark launch. The software is gradually or unobtrusively released to users so we can get feedback and can test performance. For example: You're not sure how your users will react to your new feature. You want to release your feature to a small, random sample of users to see how they react. A/B testing : A/B testing compares two versions of a webpage or app to determine which one performs better. A/B testing is like an experiment. In A/B testing, we randomly show users two or more variations of a page. Then we use statistical analysis to decide which variation performs better for our goals. For example: The marketing team has asked you to add a banner to your company's website. They have two versions of this banner. They want to know which version produces more clickthroughs. You can use A/B testing deployment pattern to help the marketing team identify the better version. Progressive-exposure deployment : Progressive-exposure deployment is sometimes called ring-based deployment. It's another way to limit how changes affect users while making sure that those changes are valid in a production environment. Rings are basically an extension of the canary stage. The canary release releases to a stage to measure effect. Adding another ring is essentially the same idea. In a ring-based deployment, we deploy changes to risk-tolerant customers first. Then we progressively roll out to a larger set of customers.","title":"Types of deployment patterns"},{"location":"devops/cd/#choosing-the-right-deployment-pattern","text":"A good deployment pattern can help you minimize downtime. It can also enable you to roll out new features progressively to your users. The deployment pattern that you choose depends on your reasons for the deployment as well as your resources . Do you have canary testers in place? Will you employ a dark launch and choose testers who don't know that they are testers? If you have a trusted set of testers that progressively increases from a small set to a larger set, then could choose a progressive-exposure deployment. Or if you want to know if one version performs better than another version, you could choose A/B testing.","title":"Choosing the right deployment pattern"},{"location":"devops/cd/#why-are-containers-important","text":"Dependency versioning challenges for QA QA test for multiple teams, and it can be challenging since each team uses their own technology stack. And even when they use the same underlying platforms, like .NET or Java, they often target different versions. QA sometimes spend half of their day simply getting test environments in a state where they can run the code they need to test. When something doesn't work, it's hard to tell whether there's a bug in the code or if they accidentally configured platform version 4.2.3 instead of 4.3.2. Overhead due to solving app isolation with VMs We have a few teams that have unique version requirements, so we have to publish their apps on their own virtual machines just to make sure their version and component requirements don't conflict with our other apps. - Besides the overhead involved in maintaining the extra set of VMs, it also costs us more than it would if those apps could run side by side. Configuration inconsistencies between deployment stages I was working on the peer-to-peer update system and had it all working on my machine. But when I handed it off for deployment, it didn't work in production. I had forgotten that I needed to open port 315 as part of the service. It took us over a day of troubleshooting to realize what was going on. Once we opened that up in production, things worked as expected. What is a container? It's more like a lightweight virtual machine designed to run directly on the host operating system. When you build your project, the output is a container that includes your software along with its dependencies. However, it's not a complete virtualized system, so it can spin up in a little as less than one second. Security and isolation are handled by the host operating system . When your container runs in a host process, the container is isolated from the other processes on that same host machine. This isolation allows your container to load whatever versions of components it needs, regardless of what other containers are doing. It also means you can easily run multiple containers on the same host simultaneously. Instead of shipping source code or a set of binaries, the entire container becomes the artifact. Containers already includes all the required versions of its dependencies. Containers are more of a packaging and deployment technology . They don't impact the fundamental software we're writing. We can just instruct our tools to produce a Docker container at the end of the build.","title":"Why are containers important?"},{"location":"devops/cd/#what-is-docker","text":"Docker is a technology for automating the packaging and deployment of portable, self-sufficient containers. - Docker containers can be run anywhere a Docker host is found, whether on a development machine, a departmental server, an enterprise datacenter, or in the cloud. Azure provides multiple ways to run container-based applications, including App Service or as part of clusters managed with orchestration technologies like Kubernetes. Dependency versioning challenges for QA Applications are packaged as containers that bring the correct versions of their dependencies with them. Overhead due to solving app isolation with VMs Many isolated containers can be run on the same host with benefits over virtual machines including faster startup time to greater resource efficiency. Configuration inconsistencies between DevOps stages Containers ship with manifests that automate configuration requirements, such as which ports need to be exposed. Adopting Docker containers can be a key step on the path towards a microservices architecture.","title":"What is Docker?"},{"location":"devops/cd/#why-is-container-orchestration-important","text":"As development organizations scale, so do the complexities of the solutions they deliver. Over time, different products and services take increasing dependence on each other. This can result in different development and operations requirements for different components within a given application. Refactoring those components as their own distinct microservices can improve architectural efficiency. A microservice is a small, autonomous service designed to be self-contained and to fulfill a specific business capability. Microservices are typical apps, like our web app. The main difference is that instead of building and deploying a single monolithic app, we refactor any components that would be better maintained and managed as autonomous services. We then build those services to be good at what they do and deploy them to operate independently. Microservices are an architectural concept and not a deployment technology . Containers provide a great technical foundation for building and deploying these services, but this leads to a new question: how do we manage all of these containers floating around? This is where orchestration technologies like Kubernetes come in. Kubernetes is a platform for managing containerized workloads and services. It's a great option for organizations that have a growing number of containers that need to be deployed, integrated, and monitored in any environment. Kubernetes offers a way for us to deploy to different namespaces. This enables us to partition our deployments so that we can have entire environments dedicated to testing versus production. And since they all run in the same cluster and use the same containers, the testing experience should offer what we expect to see in production. Assuming we have our projects set up to build Docker containers, all we need to deploy to Kubernetes are some manifest files that describe the services and their deployments.","title":"Why is container orchestration important?"},{"location":"devops/cd/#what-is-kubernetes","text":"Kubernetes is a technology for orchestrating multi-container deployments. It provides a framework for running distributed systems in a declarative, responsive fashion . It automatically applies and enforces your deployment patterns to ensure that containers are deployed and run as intended. It also offers support for specialized release cadences, such as those using canary deployments . Complexity of multi-container deployments Kubernetes is designed, first and foremost, to automate the processes around deploying and maintaining container deployments. Consistency across environments and stages Just as containers ensure a consistent deployment for the apps they contain, Kubernetes ensures a consistent deployment for the containers a cluster manages. Azure DevOps support Azure DevOps offers first-class support for working with Kubernetes. Ease of development The impact of Kubernetes on a source project is comparable to that of adding Docker support, which is minimal and limited to declarative configuration.","title":"What is Kubernetes?"},{"location":"devops/cd/#benefits-of-continous-delivery-in-the-release-process","text":"The time it takes to get the build into the Test stage. The team achieved this improvement by using a scheduled trigger to deploy to Test every day at 3:00 AM. The time it takes to get the tested build into Staging . The team achieved this improvement by adding Selenium UI tests, a form of functional testing, to the Test stage. These automated tests are much faster than the manual versions. The time it takes to get the approved build from Staging to live . The team achieved this improvement by adding manual approval checks to the pipeline. When management signs off, Ops can release the changes from Staging to live.","title":"Benefits of Continous Delivery in the Release process"},{"location":"devops/ci/","text":"Introduction \u00b6 Continuous integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. A pipeline defines the continuous integration process for the app. It's made up of steps called tasks . It can be thought of as a script that defines how your build, test, and deployment steps are run. The pipeline runs when you submit code changes. You can configure the pipeline to run automatically, or you can run it manually. You connect your pipeline to a source repository like GitHub, Bitbucket, or Subversion. A build agent builds or deploys the code. An agent is installable software that runs one build or deployment job at a time. The final product of the pipeline is a build artifact . Think of an artifact as the smallest compiled unit that we need to test or deploy the app. For example, an artifact can be: A Java or .NET app packaged into a .jar or .zip file. A C++ or JavaScript library. A virtual machine, cloud, or Docker image. Pipeline as code refers to the concept of expressing your build definitions as code. - Build definition in YAML file to configure the build and release pipeline and stored directly with your app's source code. - When source code is checked in, pipeline is triggered and the build agent downloads the build defintion stored in the source code and then triggers the build and release tasks mentioned in the build defintion. - A continuous integration (CI) build is a build that runs when you push a change to a branch. - A pull request (PR) build is a build that runs when you open a pull request or when you push additional changes to an existing pull request. - A final CI build happens after the pull request is merged to main. The final CI build verifies that the changes are still good after the PR was merged. What is a build badge? - A badge is part of Microsoft Azure Pipelines. It has methods you can use to add an SVG image that shows the status of the build on your GitHub repository. - Most GitHub repositories include a file named README.md, which is a Markdown file that includes essential details and documentation about your project. GitHub renders this file on your project's home page. Choose a code flow strategy \u00b6 If your team is doing continuous delivery of software, I would suggest to adopt a much simpler workflow like Github flow. If, however, you are building software that is explicitly versioned , or if you need to support multiple versions of your software in the wild, then git-flow may still be as good of a fit to your team. Github-flow \u00b6 What does a branching workflow look like (a.k.a Github Flow )? Step 1 : When you begin to work on a new feature or bug fix, the first thing you want to do is make sure you're starting with the latest stable codebase. To do this, you can synchronize your local copy of the main branch with the server's copy. This pulls down all other developers' changes that have been pushed up to the main branch on the server since your last synchronization. Step 2 : To make sure you're working safely on your copy of the code, you create a new branch just for that feature or bug fix. Before you make changes to a file, you check out a new branch so that you know you're working on the files from that branch and not a different branch. You can switch branches anytime by checking out that branch. Step 3 : You're now safe to make whatever changes you want, because these changes are only in your branch. As you work, you can commit your changes to your branch to make sure you don't lose any work and to provide a way to roll back any changes you've made to previous versions. Before you can commit changes, you need to stage your files so that Git knows which ones you're ready to commit. Step 4 : The next step is to push , or upload, your local branch up to the remote repository (such as GitHub) so that others can see what you're working on. Don't worry, this won't merge your changes yet. You can push up your work as often as you'd like. In fact, that's a good way to back up your work or enable yourself to work from multiple computers. Step 5 : This step is a common one, but not required. When you're satisfied that your code is working as you want it to, you can pull , or merge, the remote main branch back into your local main branch. Changes have been taking place there that your local main branch doesn't have yet. After you've synchronized the remote main branch with yours, merge your local main branch into your working branch and test your build again. This process helps ensure that your feature works with the latest code. It also helps ensure that your work will integrate smoothly when you submit your pull request. Step 6 : You're finally ready to propose your changes to the remote main branch. To do this, you begin a pull request . When configured in Azure Pipelines or another CI/CD system, this step triggers the build process and you can watch your changes move through the pipeline. After the build succeeds and others approve your pull request, your code can be merged into the remote main branch. (It's still up to a human to merge the changes.) A remote is a Git repository where team members collaborate (like a repository on GitHub). git remote -v - You see that you have both fetch (download) and push (upload) access to your repository. Origin specifies your repository on GitHub. HEAD is the pointer to the current branch reference, which is in turn a pointer to the last commit made on that branch. That means HEAD will be the parent of the next commit that is created. It\u2019s generally simplest to think of HEAD as the snapshot of your last commit on that branch . The index is your proposed next commit . We\u2019ve also been referring to this concept as Git\u2019s \u201cStaging Area\u201d as this is what Git looks at when you run git commit . When you fork code from another repository, it's common to name the original remote (the one you forked from) as upstream. git remote add upstream <forked repo> . You also now have fetch access from the forked repository, which is beneficial when the forked repo changes . git fetch - The command goes out to that remote project and pulls down all the data from that remote project that you don\u2019t have yet. After you do this, you should have references to all the branches from that remote, which you can merge in or inspect at any time. So, git fetch origin fetches any new work that has been pushed to that server since you cloned (or last fetched from) it. It\u2019s important to note that the git fetch command only downloads the data to your local repository - it doesn\u2019t automatically merge it with any of your work or modify what you\u2019re currently working on. You have to merge it manually into your work when you\u2019re ready. git pull generally fetches data from the server you originally cloned from and automatically tries to merge it into the code you\u2019re currently working on. git pull is essentially a git fetch immediately followed by a git merge in most cases. It\u2019s better to simply use the fetch and merge commands explicitly as the magic of git pull can often be confusing. git push pushes code to a server to which you have write access and if nobody has pushed in the meantime. If you and someone else clone at the same time and they push upstream and then you push upstream, your push will rightly be rejected. You\u2019ll have to fetch their work first and incorporate it into yours before you\u2019ll be allowed to push. With the rebase command, you can take all the changes that were committed on one branch and replay them on a different branch. # This operation works by going to the common ancestor of the two branches (the one you\u2019re on and the one you\u2019re rebasing onto), getting the diff introduced by each commit of the branch you\u2019re on, saving those diffs to temporary files, resetting the current branch to the same commit as the branch you are rebasing onto, and finally applying each change in turn. git checkout server git rebase main # At this point, you can go back to the main branch and do a fast-forward merge. git checkout main git merge server There is no difference in the end product of the integration, but rebasing makes for a cleaner history. If you examine the log of a rebased branch, it looks like a linear history: it appears that all the work happened in series, even when it originally happened in parallel. You can rebase the server branch onto the main branch without having to check it out first by running git rebase <basebranch> <topicbranch> which checks out the topic branch (in this case, server ) for you and replays it onto the base branch ( main ): git rebase main server Then, you can fast-forward the base branch (main): git checkout main git merge server # You can remove the server branches because all the work is integrated and you don\u2019t need them anymore. git branch -d server Do not rebase commits that exist outside your repository and that people may have based work on. You can also simplify this by running a git pull --rebase instead of a normal git pull . If you are using git pull and want to make --rebase the default, you can set the pull.rebase config value with something like git config --global pull.rebase true . You can get the best of both worlds (Rebase vs. Merge ): rebase local changes before pushing to clean up your work, but never rebase anything that you\u2019ve pushed somewhere. Git Reset \u2192 3 Options Move HEAD \u2192 git reset --soft HEAD~ . When you reset back to HEAD~ (the parent of HEAD), you are moving the branch back to where it was, without changing the index or working directory. You could now update the index and run git commit again to accomplish what git commit --amend would have done Updating the Index \u2192 git reset --mixed HEAD~ . This is also the default, so if you specify no option at all (just git reset HEAD~ in this case). It undid your last commit, but also unstaged everything. You rolled back to before you ran all your git add and git commit commands. Updating the Working Directory (\u2013hard) \u2192 git reset --hard HEAD~ . It undid your last commit, the git add and git commit commands, and all the work you did in your working directory. It\u2019s important to note that this flag (\u2013hard) is the only way to make the reset command dangerous, and one of the very few cases where Git will actually destroy data. Any other invocation of reset can be pretty easily undone, but the --hard option cannot, since it forcibly overwrites files in the working directory. Reset With a Path \u2192 git reset file.txt . This has the practical effect of unstaging the file. Squashing. Say you have a series of commits with messages like \u201coops.\u201d, \u201cWIP\u201d and \u201cforgot this file\u201d. You can use reset to quickly and easily squash them into a single commit that makes you look really smart. Let\u2019s say you have a project where the first commit has one file, the second commit added a new file and changed the first, and the third commit changed the first file again. The second commit was a work in progress and you want to squash it down. You can run git reset --soft HEAD~2 to move the HEAD branch back to an older commit (the most recent commit you want to keep). And then simply run git commit again. Git-flow \u00b6 At the core, the development model is greatly inspired by existing models out there. The central repo holds two main branches with an infinite lifetime: main develop The main branch at origin should be familiar to every Git user. Parallel to the main branch, another branch exists called develop . We consider origin/main to be the main branch where the source code of HEAD always reflects a production-ready state . We consider origin/develop to be the main branch where the source code of HEAD always reflects a state with the latest delivered development changes for the next release. Some would call this the \u201cintegration branch\u201d. This is where any automatic nightly builds are built from. When the source code in the develop branch reaches a stable point and is ready to be released, all of the changes should be merged back into main somehow and then tagged with a release number. Therefore, each time when changes are merged back into main, this is a new production release by definition. Next to the main branches main and develop, our development model uses a variety of supporting branches to aid parallel development between team members, ease tracking of features, prepare for production releases and to assist in quickly fixing live production problems. Unlike the main branches, these branches always have a limited life time, since they will be removed eventually. The different types of branches we may use are: Feature branches Release branches Hotfix branches Feature branches May branch off from: develop . Must merge back into: develop . Branch naming convention: anything except main, develop, release- , or hotfix- . - When starting development of a feature, the target release in which this feature will be incorporated may well be unknown at that point. The essence of a feature branch is that it exists as long as the feature is in development, but will eventually be merged back into develop (to definitely add the new feature to the upcoming release) or discarded (in case of a disappointing experiment). - When starting work on a new feature, branch off from the develop branch. git checkout -b myfeature develop - Finished features may be merged into the develop branch to definitely add them to the upcoming release. git checkout develop # Switched to branch 'develop' git merge --no-ff myfeature # The --no-ff flag causes the merge to always create a new commit object, even if the merge could be performed with a fast-forward. This avoids losing information about the historical existence of a feature branch and groups together all commits that together added the feature. git branch -d myfeature #Deleted branch myfeature (was 05e9557). git push origin develop Release branches May branch off from: develop . Must merge back into: develop and main . Branch naming convention: release- . - Release branches support preparation of a new production release. Furthermore, they allow for minor bug fixes and preparing meta-data for a release (version number, build dates, etc.). By doing all of this work on a release branch, the develop branch is cleared to receive features for the next big release. - The * key moment** to branch off a new release branch from develop is when develop (almost) reflects the desired state of the new release. At least all features that are targeted for the release-to-be-built must be merged in to develop at this point in time. All features targeted at future releases may not\u2014they must wait until after the release branch is branched off. git checkout -b release-1.2 develop - This new branch may exist there for a while, until the release may be rolled out definitely. During that time, bug fixes may be applied in this branch (rather than on the develop branch). Adding large new features here is strictly prohibited. They must be merged into develop, and therefore, wait for the next big release. - When the state of the release branch is ready to become a real release, some actions need to be carried out. First, the release branch is merged into main .Next, that commit on main must be tagged for easy future reference to this historical version. Finally, the changes made on the release branch need to be merged back into develop , so that future releases also contain these bug fixes. git checkout main # Switched to branch 'main' git merge --no-ff release-1.2 # Merge made by recursive. git tag -a 1 .2 # The release is now done, and tagged for future reference. # To keep the changes made in the release branch, we need to merge those back into develop. git checkout develop # Switched to branch 'develop' git merge --no-ff release-1.2 # Delete release branch git branch -d release-1.2 Hotfix Branches May branch off from: main . Must merge back into: develop and main . Branch naming convention: hotfix-* - Hotfix branches are very much like release branches in that they are also meant to prepare for a new production release, albeit unplanned. - They arise from the necessity to act immediately upon an undesired state of a live production version. When a critical bug in a production version must be resolved immediately, a hotfix branch may be branched off from the corresponding tag on the master branch that marks the production version. git checkout -b hotfix-1.2.1 master - When finished, the bugfix needs to be merged back into master , but also needs to be merged back into develop , in order to safeguard that the bugfix is included in the next release as well. git checkout master # Switched to branch 'master' git merge --no-ff hotfix-1.2.1 # Merge made by recursive. git tag -a 1 .2.1 # Next, include the bugfix in develop git checkout develop # Switched to branch 'develop' git merge --no-ff hotfix-1.2.1 # Delete hot-fix branch git branch -d hotfix-1.2.1 - The one exception to the rule here is that, when a release branch currently exists, the hotfix changes need to be merged into that release branch, instead of develop . Back-merging the bugfix into the release branch will eventually result in the bugfix being merged into develop too, when the release branch is finished. - If work in develop immediately requires this bugfix and cannot wait for the release branch to be finished, you may safely merge the bugfix into develop now already as well. Pull Request Strategy \u00b6 Smaller PRs are easier to review, therefore more bugs and code flaws could be caught, which leads to a better quality of the code. Spilting PR Writing good and clear commit messages Automated Testing \u00b6 Automated testing uses software to execute your code and compare the actual results with the results you expect. Compare this with exploratory or manual testing , where a human typically follows instructions in a test plan to verify that software functions as expected. - Documentation and the ability to more easily refactor your code are two added benefits of automated testing. - Manual testing has its benefits. But as your code base grows in size, testing all features manually (including edge cases) can become repetitive, tedious, and error prone. Automated testing can help eliminate some of this burden and enable manual testers to focus on what they do best : ensuring that your users will have a positive experience with your software . - Focus most of your effort on writing tests that verify the foundational levels of your software, such as functions, classes, and methods. - You focus progressively less effort as features are combined, such as at the user interface (UI) layer. - The idea is that if you can verify that each lower-level component works as expected in isolation, tests at the higher levels need only verify that multiple components work together to get the expected result. - Unit tests are a great way to automatically test for regression bugs. Continuous testing means tests are run early in the development process and as every change moves through the pipeline. Shifting left means considering software quality and testing earlier in the development process. Shifting left often requires testers to get involved in the design process, even before any code for the feature is written. Automated tests can serve as a type of documentation as to how software should behave and why certain features exist. Automated test code often uses a human-readable format. The set of inputs you provide represent values your users might enter. Each associated output specifies the result your users should expect. Many developers follow the test-driven development , or TDD, method by writing their test code before implementing a new feature. The idea is to write a set of tests, often called specs, that initially fail. Then, the developer incrementally writes code to implement the feature until all tests pass. Not only do the specs document the requirements, but the TDD process helps ensure that only the necessary amount of code is written to implement the feature. When you have a set of passing tests, you're better able to experiment and refactor your code. When you make a change, all you need to do is run your tests and verify that they continue to pass. After you've met your refactoring goals, you can submit your change to the build pipeline so that everyone can benefit, but with a lower risk of something breaking. Types of Testing \u00b6 Development testing refers to tests you can run before you deploy the application to a test or production environment. lint testing , a form of static code analysis, checks your source code to determine whether it conforms to your team's style guide. Unit testing verifies the most fundamental components of your program or library, such as an individual function or method. You specify one or more inputs along with the expected results. The test runner performs each test and checks to see whether the actual and expected results match. Code coverage testing computes the percentage of your code that's covered by your unit tests. Code coverage testing can include conditional branches in your code to ensure that a function is completely covered. What makes a good test? \u00b6 Don't test for the sake of testing : Your tests should serve a purpose beyond being a checklist item to cross off. Write tests that verify that your critical code works as intended and doesn't break existing functionality. Keep your tests short : Tests should finish as quickly as possible, especially those that happen during the development and build phases. When tests are run as each change moves through the pipeline, you don't want them to be the bottleneck. Ensure that your tests are repeatable : Test runs should produce the same results each time, whether you run them on your computer, a coworker's computer, or in the build pipeline. Keep your tests focused : A common misconception is that tests are meant to cover code written by others. Ordinarily, your tests should cover only your code. For example, if you're using an open-source graphics library in your project, you don't need to test that library. Choose the right granularity : For example, if you're performing unit testing, an individual test shouldn't combine or test multiple functions or methods. Test each function separately and later write integration tests that verify that multiple components interact properly. Plan build dependencies for your pipeline \u00b6 What is a package? \u00b6 A package contains reusable code that other developers can use in their own projects, even though they didn't write it. - For compiled languages, a package typically contains the compiled binary code, such as .dll files in .NET, or .class files in Java. - For languages that are interpreted instead of compiled, such as JavaScript or Python, a package might include source code. - Either way, packages are typically compressed to ZIP or a similar format. Package systems will often define a unique file extension, such as .nupkg or .jar, to make the package's use clear. Compression can help reduce download time, and also produces a single file to make management simpler. - Packages also often contain one or more files that provide metadata, or information, about the package. This metadata might describe what the package does, specify its license terms, the author's contact information, and the package's version. Why should I build a package? \u00b6 One reason to create a package instead of duplicating code is to prevent drift . When code is duplicated, each copy can quickly diverge to satisfy the requirements of a particular app. It becomes difficult to migrate changes from one copy to the others. In other words, you lose the ability to improve the code in ways that benefit everyone. Packages also group related functionality into one reusable component. Depending on the programming language, a package can provide apps with access to certain types and functions, while restricting access to their implementation details. Another reason to build a package is to provide a consistent way to build and test that package's functionality. When code is duplicated, each app might build and test that code in different ways. One set of tests might include checks that another set could benefit from. One tradeoff is that with a package, you have another codebase to test and maintain. You must also be careful when adding features. How can I identify dependencies? \u00b6 If the goal is to reorganize your code into separate components, you need to identify those pieces of your app that can be removed, packaged to be reusable, stored in a central location, and versioned. You may even want to replace your own code with third-party components that are either open source or that you license. Here are some ways to identify dependencies: Duplicate code . If certain pieces of code appear in several places, that's a good indication that this code can be reused. Centralize these duplicate pieces of code and repackage them appropriately. High cohesion and low coupling . A second approach is to look for code elements that have a high cohesion to each other and low coupling with other parts of the code. In essence, high cohesion means keeping parts of a codebase that are related to each other in a single place. Low coupling, at the same time, is about separating unrelated parts of the code base as much as possible. Individual lifecycle . Look for parts of the code that have a similar lifecycle and can be deployed and released individually. If this code can be maintained by a separate team, it's a good indication that it can be packaged as a component outside of the solution. Stable parts . Some parts of your codebase might be stable and change infrequently. Check your code repository to find code with a low change frequency. Independent code and components . Whenever code and components are independent and unrelated to other parts of the system, they can potentially be isolated into separate dependencies. What kinds of packages are there? \u00b6 Each programming language or framework provides its own way to build packages. Popular package systems provide documentation about how the process works. NuGet : packages .NET libraries NPM : packages JavaScript libraries Maven : packages Java libraries Docker : packages software in isolated units called containers Where are packages hosted? \u00b6 You can host packages on your own network, or you can use a hosting service. A hosting service is often called a package repository or package registry. Many of these services provide free hosting for open source projects. A package feed refers to your package repository server. This server can be on the internet or behind your firewall on your network. When you host packages behind the firewall, you can include feeds to your own packages. You can also cache packages that you trust on your network when your systems can't connect to the internet. What elements make up a good dependency management strategy? \u00b6 A good dependency management strategy depends on these three elements: Standardization . Standardizing how you declare and resolve dependencies will help your automated release process remain repeatable and predictable. Packaging formats and sources . Each dependency should be packaged using the applicable format and stored in a central location. Versioning . You need to keep track of the changes that occur over time in dependencies just as you do with your own code. This means that dependencies should be versioned. How are packages versioned? \u00b6 Semantic Versioning is a popular versioning scheme. Here's the format: Major.Minor.Patch[-Suffix] A new Major version introduces breaking changes. Apps typically need to update how they use the package to work with a new major version. A new Minor version introduces new features, but is backward compatible with earlier versions. A new Patch introduces backward compatible bug fixes, but not new features. The -Suffix part is optional and identifies the package as a pre-release version. For example, 1.0.0-beta1 might identify the package as the first beta pre-release build for the 1.0.0 release. Include a versioning strategy in your build pipeline \u00b6 When you use a build pipeline, packages need versions before they can be consumed and tested. However, only after you've tested the package can you know its quality. Because package versions should never be changed, it becomes challenging to choose a certain version beforehand. A common use is to share package versions that have been tested, validated, or deployed but hold back packages still under development and not ready for public consumption. This approach works well with semantic versioning, which is useful for predicting the intent of a particular version. Essentially, allow a consumer to make a conscious decision to choose from released packages, or opt-in to prereleases of a certain quality level. Package security \u00b6 Ensuring the security of your packages is as important as ensuring the security of the rest of your code. One aspect of package security is securing access to the package feeds where a feed is where you store packages. - Setting permissions on the feed allows you to share your packages with as many or as few people as your scenario requires. Configure the pipeline to access security and license ratings There are several tools available from third parties to help you assess the security and license rating of the software packages you use. Some of these tools scan the packages as they are included in the build or CD pipeline. During the build process, the tool scans the packages and gives instantaneous feedback. During the CD process, the tool uses the build artifacts and performs scans. Two examples of such tools are WhiteSource Bolt and Black Duck . Benefits of Continous Integration in the Build process \u00b6 The time it takes to set up source control for new features. The team achieved this improvement by moving from centralized source control to Git, a form of distributed source control. By using distributed source control, they don't need to wait for files to be unlocked. The time it takes to deliver code to the tester. The team achieved this improvement by moving their build process to CI Pipelines. CI Pipelines automatically notifies the tester when a build is available. Developers no longer need to update spreadsheet to notify testers. The time it takes to test new features. The team achieved this improvement by unit-testing their code. They run unit tests each time a change moves through the build pipeline, so fewer bugs and regressions reach testers. The reduced workload means that testers can complete each manual test faster.","title":"CI"},{"location":"devops/ci/#introduction","text":"Continuous integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. A pipeline defines the continuous integration process for the app. It's made up of steps called tasks . It can be thought of as a script that defines how your build, test, and deployment steps are run. The pipeline runs when you submit code changes. You can configure the pipeline to run automatically, or you can run it manually. You connect your pipeline to a source repository like GitHub, Bitbucket, or Subversion. A build agent builds or deploys the code. An agent is installable software that runs one build or deployment job at a time. The final product of the pipeline is a build artifact . Think of an artifact as the smallest compiled unit that we need to test or deploy the app. For example, an artifact can be: A Java or .NET app packaged into a .jar or .zip file. A C++ or JavaScript library. A virtual machine, cloud, or Docker image. Pipeline as code refers to the concept of expressing your build definitions as code. - Build definition in YAML file to configure the build and release pipeline and stored directly with your app's source code. - When source code is checked in, pipeline is triggered and the build agent downloads the build defintion stored in the source code and then triggers the build and release tasks mentioned in the build defintion. - A continuous integration (CI) build is a build that runs when you push a change to a branch. - A pull request (PR) build is a build that runs when you open a pull request or when you push additional changes to an existing pull request. - A final CI build happens after the pull request is merged to main. The final CI build verifies that the changes are still good after the PR was merged. What is a build badge? - A badge is part of Microsoft Azure Pipelines. It has methods you can use to add an SVG image that shows the status of the build on your GitHub repository. - Most GitHub repositories include a file named README.md, which is a Markdown file that includes essential details and documentation about your project. GitHub renders this file on your project's home page.","title":"Introduction"},{"location":"devops/ci/#choose-a-code-flow-strategy","text":"If your team is doing continuous delivery of software, I would suggest to adopt a much simpler workflow like Github flow. If, however, you are building software that is explicitly versioned , or if you need to support multiple versions of your software in the wild, then git-flow may still be as good of a fit to your team.","title":"Choose a code flow strategy"},{"location":"devops/ci/#github-flow","text":"What does a branching workflow look like (a.k.a Github Flow )? Step 1 : When you begin to work on a new feature or bug fix, the first thing you want to do is make sure you're starting with the latest stable codebase. To do this, you can synchronize your local copy of the main branch with the server's copy. This pulls down all other developers' changes that have been pushed up to the main branch on the server since your last synchronization. Step 2 : To make sure you're working safely on your copy of the code, you create a new branch just for that feature or bug fix. Before you make changes to a file, you check out a new branch so that you know you're working on the files from that branch and not a different branch. You can switch branches anytime by checking out that branch. Step 3 : You're now safe to make whatever changes you want, because these changes are only in your branch. As you work, you can commit your changes to your branch to make sure you don't lose any work and to provide a way to roll back any changes you've made to previous versions. Before you can commit changes, you need to stage your files so that Git knows which ones you're ready to commit. Step 4 : The next step is to push , or upload, your local branch up to the remote repository (such as GitHub) so that others can see what you're working on. Don't worry, this won't merge your changes yet. You can push up your work as often as you'd like. In fact, that's a good way to back up your work or enable yourself to work from multiple computers. Step 5 : This step is a common one, but not required. When you're satisfied that your code is working as you want it to, you can pull , or merge, the remote main branch back into your local main branch. Changes have been taking place there that your local main branch doesn't have yet. After you've synchronized the remote main branch with yours, merge your local main branch into your working branch and test your build again. This process helps ensure that your feature works with the latest code. It also helps ensure that your work will integrate smoothly when you submit your pull request. Step 6 : You're finally ready to propose your changes to the remote main branch. To do this, you begin a pull request . When configured in Azure Pipelines or another CI/CD system, this step triggers the build process and you can watch your changes move through the pipeline. After the build succeeds and others approve your pull request, your code can be merged into the remote main branch. (It's still up to a human to merge the changes.) A remote is a Git repository where team members collaborate (like a repository on GitHub). git remote -v - You see that you have both fetch (download) and push (upload) access to your repository. Origin specifies your repository on GitHub. HEAD is the pointer to the current branch reference, which is in turn a pointer to the last commit made on that branch. That means HEAD will be the parent of the next commit that is created. It\u2019s generally simplest to think of HEAD as the snapshot of your last commit on that branch . The index is your proposed next commit . We\u2019ve also been referring to this concept as Git\u2019s \u201cStaging Area\u201d as this is what Git looks at when you run git commit . When you fork code from another repository, it's common to name the original remote (the one you forked from) as upstream. git remote add upstream <forked repo> . You also now have fetch access from the forked repository, which is beneficial when the forked repo changes . git fetch - The command goes out to that remote project and pulls down all the data from that remote project that you don\u2019t have yet. After you do this, you should have references to all the branches from that remote, which you can merge in or inspect at any time. So, git fetch origin fetches any new work that has been pushed to that server since you cloned (or last fetched from) it. It\u2019s important to note that the git fetch command only downloads the data to your local repository - it doesn\u2019t automatically merge it with any of your work or modify what you\u2019re currently working on. You have to merge it manually into your work when you\u2019re ready. git pull generally fetches data from the server you originally cloned from and automatically tries to merge it into the code you\u2019re currently working on. git pull is essentially a git fetch immediately followed by a git merge in most cases. It\u2019s better to simply use the fetch and merge commands explicitly as the magic of git pull can often be confusing. git push pushes code to a server to which you have write access and if nobody has pushed in the meantime. If you and someone else clone at the same time and they push upstream and then you push upstream, your push will rightly be rejected. You\u2019ll have to fetch their work first and incorporate it into yours before you\u2019ll be allowed to push. With the rebase command, you can take all the changes that were committed on one branch and replay them on a different branch. # This operation works by going to the common ancestor of the two branches (the one you\u2019re on and the one you\u2019re rebasing onto), getting the diff introduced by each commit of the branch you\u2019re on, saving those diffs to temporary files, resetting the current branch to the same commit as the branch you are rebasing onto, and finally applying each change in turn. git checkout server git rebase main # At this point, you can go back to the main branch and do a fast-forward merge. git checkout main git merge server There is no difference in the end product of the integration, but rebasing makes for a cleaner history. If you examine the log of a rebased branch, it looks like a linear history: it appears that all the work happened in series, even when it originally happened in parallel. You can rebase the server branch onto the main branch without having to check it out first by running git rebase <basebranch> <topicbranch> which checks out the topic branch (in this case, server ) for you and replays it onto the base branch ( main ): git rebase main server Then, you can fast-forward the base branch (main): git checkout main git merge server # You can remove the server branches because all the work is integrated and you don\u2019t need them anymore. git branch -d server Do not rebase commits that exist outside your repository and that people may have based work on. You can also simplify this by running a git pull --rebase instead of a normal git pull . If you are using git pull and want to make --rebase the default, you can set the pull.rebase config value with something like git config --global pull.rebase true . You can get the best of both worlds (Rebase vs. Merge ): rebase local changes before pushing to clean up your work, but never rebase anything that you\u2019ve pushed somewhere. Git Reset \u2192 3 Options Move HEAD \u2192 git reset --soft HEAD~ . When you reset back to HEAD~ (the parent of HEAD), you are moving the branch back to where it was, without changing the index or working directory. You could now update the index and run git commit again to accomplish what git commit --amend would have done Updating the Index \u2192 git reset --mixed HEAD~ . This is also the default, so if you specify no option at all (just git reset HEAD~ in this case). It undid your last commit, but also unstaged everything. You rolled back to before you ran all your git add and git commit commands. Updating the Working Directory (\u2013hard) \u2192 git reset --hard HEAD~ . It undid your last commit, the git add and git commit commands, and all the work you did in your working directory. It\u2019s important to note that this flag (\u2013hard) is the only way to make the reset command dangerous, and one of the very few cases where Git will actually destroy data. Any other invocation of reset can be pretty easily undone, but the --hard option cannot, since it forcibly overwrites files in the working directory. Reset With a Path \u2192 git reset file.txt . This has the practical effect of unstaging the file. Squashing. Say you have a series of commits with messages like \u201coops.\u201d, \u201cWIP\u201d and \u201cforgot this file\u201d. You can use reset to quickly and easily squash them into a single commit that makes you look really smart. Let\u2019s say you have a project where the first commit has one file, the second commit added a new file and changed the first, and the third commit changed the first file again. The second commit was a work in progress and you want to squash it down. You can run git reset --soft HEAD~2 to move the HEAD branch back to an older commit (the most recent commit you want to keep). And then simply run git commit again.","title":"Github-flow"},{"location":"devops/ci/#git-flow","text":"At the core, the development model is greatly inspired by existing models out there. The central repo holds two main branches with an infinite lifetime: main develop The main branch at origin should be familiar to every Git user. Parallel to the main branch, another branch exists called develop . We consider origin/main to be the main branch where the source code of HEAD always reflects a production-ready state . We consider origin/develop to be the main branch where the source code of HEAD always reflects a state with the latest delivered development changes for the next release. Some would call this the \u201cintegration branch\u201d. This is where any automatic nightly builds are built from. When the source code in the develop branch reaches a stable point and is ready to be released, all of the changes should be merged back into main somehow and then tagged with a release number. Therefore, each time when changes are merged back into main, this is a new production release by definition. Next to the main branches main and develop, our development model uses a variety of supporting branches to aid parallel development between team members, ease tracking of features, prepare for production releases and to assist in quickly fixing live production problems. Unlike the main branches, these branches always have a limited life time, since they will be removed eventually. The different types of branches we may use are: Feature branches Release branches Hotfix branches Feature branches May branch off from: develop . Must merge back into: develop . Branch naming convention: anything except main, develop, release- , or hotfix- . - When starting development of a feature, the target release in which this feature will be incorporated may well be unknown at that point. The essence of a feature branch is that it exists as long as the feature is in development, but will eventually be merged back into develop (to definitely add the new feature to the upcoming release) or discarded (in case of a disappointing experiment). - When starting work on a new feature, branch off from the develop branch. git checkout -b myfeature develop - Finished features may be merged into the develop branch to definitely add them to the upcoming release. git checkout develop # Switched to branch 'develop' git merge --no-ff myfeature # The --no-ff flag causes the merge to always create a new commit object, even if the merge could be performed with a fast-forward. This avoids losing information about the historical existence of a feature branch and groups together all commits that together added the feature. git branch -d myfeature #Deleted branch myfeature (was 05e9557). git push origin develop Release branches May branch off from: develop . Must merge back into: develop and main . Branch naming convention: release- . - Release branches support preparation of a new production release. Furthermore, they allow for minor bug fixes and preparing meta-data for a release (version number, build dates, etc.). By doing all of this work on a release branch, the develop branch is cleared to receive features for the next big release. - The * key moment** to branch off a new release branch from develop is when develop (almost) reflects the desired state of the new release. At least all features that are targeted for the release-to-be-built must be merged in to develop at this point in time. All features targeted at future releases may not\u2014they must wait until after the release branch is branched off. git checkout -b release-1.2 develop - This new branch may exist there for a while, until the release may be rolled out definitely. During that time, bug fixes may be applied in this branch (rather than on the develop branch). Adding large new features here is strictly prohibited. They must be merged into develop, and therefore, wait for the next big release. - When the state of the release branch is ready to become a real release, some actions need to be carried out. First, the release branch is merged into main .Next, that commit on main must be tagged for easy future reference to this historical version. Finally, the changes made on the release branch need to be merged back into develop , so that future releases also contain these bug fixes. git checkout main # Switched to branch 'main' git merge --no-ff release-1.2 # Merge made by recursive. git tag -a 1 .2 # The release is now done, and tagged for future reference. # To keep the changes made in the release branch, we need to merge those back into develop. git checkout develop # Switched to branch 'develop' git merge --no-ff release-1.2 # Delete release branch git branch -d release-1.2 Hotfix Branches May branch off from: main . Must merge back into: develop and main . Branch naming convention: hotfix-* - Hotfix branches are very much like release branches in that they are also meant to prepare for a new production release, albeit unplanned. - They arise from the necessity to act immediately upon an undesired state of a live production version. When a critical bug in a production version must be resolved immediately, a hotfix branch may be branched off from the corresponding tag on the master branch that marks the production version. git checkout -b hotfix-1.2.1 master - When finished, the bugfix needs to be merged back into master , but also needs to be merged back into develop , in order to safeguard that the bugfix is included in the next release as well. git checkout master # Switched to branch 'master' git merge --no-ff hotfix-1.2.1 # Merge made by recursive. git tag -a 1 .2.1 # Next, include the bugfix in develop git checkout develop # Switched to branch 'develop' git merge --no-ff hotfix-1.2.1 # Delete hot-fix branch git branch -d hotfix-1.2.1 - The one exception to the rule here is that, when a release branch currently exists, the hotfix changes need to be merged into that release branch, instead of develop . Back-merging the bugfix into the release branch will eventually result in the bugfix being merged into develop too, when the release branch is finished. - If work in develop immediately requires this bugfix and cannot wait for the release branch to be finished, you may safely merge the bugfix into develop now already as well.","title":"Git-flow"},{"location":"devops/ci/#pull-request-strategy","text":"Smaller PRs are easier to review, therefore more bugs and code flaws could be caught, which leads to a better quality of the code. Spilting PR Writing good and clear commit messages","title":"Pull Request Strategy"},{"location":"devops/ci/#automated-testing","text":"Automated testing uses software to execute your code and compare the actual results with the results you expect. Compare this with exploratory or manual testing , where a human typically follows instructions in a test plan to verify that software functions as expected. - Documentation and the ability to more easily refactor your code are two added benefits of automated testing. - Manual testing has its benefits. But as your code base grows in size, testing all features manually (including edge cases) can become repetitive, tedious, and error prone. Automated testing can help eliminate some of this burden and enable manual testers to focus on what they do best : ensuring that your users will have a positive experience with your software . - Focus most of your effort on writing tests that verify the foundational levels of your software, such as functions, classes, and methods. - You focus progressively less effort as features are combined, such as at the user interface (UI) layer. - The idea is that if you can verify that each lower-level component works as expected in isolation, tests at the higher levels need only verify that multiple components work together to get the expected result. - Unit tests are a great way to automatically test for regression bugs. Continuous testing means tests are run early in the development process and as every change moves through the pipeline. Shifting left means considering software quality and testing earlier in the development process. Shifting left often requires testers to get involved in the design process, even before any code for the feature is written. Automated tests can serve as a type of documentation as to how software should behave and why certain features exist. Automated test code often uses a human-readable format. The set of inputs you provide represent values your users might enter. Each associated output specifies the result your users should expect. Many developers follow the test-driven development , or TDD, method by writing their test code before implementing a new feature. The idea is to write a set of tests, often called specs, that initially fail. Then, the developer incrementally writes code to implement the feature until all tests pass. Not only do the specs document the requirements, but the TDD process helps ensure that only the necessary amount of code is written to implement the feature. When you have a set of passing tests, you're better able to experiment and refactor your code. When you make a change, all you need to do is run your tests and verify that they continue to pass. After you've met your refactoring goals, you can submit your change to the build pipeline so that everyone can benefit, but with a lower risk of something breaking.","title":"Automated Testing"},{"location":"devops/ci/#types-of-testing","text":"Development testing refers to tests you can run before you deploy the application to a test or production environment. lint testing , a form of static code analysis, checks your source code to determine whether it conforms to your team's style guide. Unit testing verifies the most fundamental components of your program or library, such as an individual function or method. You specify one or more inputs along with the expected results. The test runner performs each test and checks to see whether the actual and expected results match. Code coverage testing computes the percentage of your code that's covered by your unit tests. Code coverage testing can include conditional branches in your code to ensure that a function is completely covered.","title":"Types of Testing"},{"location":"devops/ci/#what-makes-a-good-test","text":"Don't test for the sake of testing : Your tests should serve a purpose beyond being a checklist item to cross off. Write tests that verify that your critical code works as intended and doesn't break existing functionality. Keep your tests short : Tests should finish as quickly as possible, especially those that happen during the development and build phases. When tests are run as each change moves through the pipeline, you don't want them to be the bottleneck. Ensure that your tests are repeatable : Test runs should produce the same results each time, whether you run them on your computer, a coworker's computer, or in the build pipeline. Keep your tests focused : A common misconception is that tests are meant to cover code written by others. Ordinarily, your tests should cover only your code. For example, if you're using an open-source graphics library in your project, you don't need to test that library. Choose the right granularity : For example, if you're performing unit testing, an individual test shouldn't combine or test multiple functions or methods. Test each function separately and later write integration tests that verify that multiple components interact properly.","title":"What makes a good test?"},{"location":"devops/ci/#plan-build-dependencies-for-your-pipeline","text":"","title":"Plan build dependencies for your pipeline"},{"location":"devops/ci/#what-is-a-package","text":"A package contains reusable code that other developers can use in their own projects, even though they didn't write it. - For compiled languages, a package typically contains the compiled binary code, such as .dll files in .NET, or .class files in Java. - For languages that are interpreted instead of compiled, such as JavaScript or Python, a package might include source code. - Either way, packages are typically compressed to ZIP or a similar format. Package systems will often define a unique file extension, such as .nupkg or .jar, to make the package's use clear. Compression can help reduce download time, and also produces a single file to make management simpler. - Packages also often contain one or more files that provide metadata, or information, about the package. This metadata might describe what the package does, specify its license terms, the author's contact information, and the package's version.","title":"What is a package?"},{"location":"devops/ci/#why-should-i-build-a-package","text":"One reason to create a package instead of duplicating code is to prevent drift . When code is duplicated, each copy can quickly diverge to satisfy the requirements of a particular app. It becomes difficult to migrate changes from one copy to the others. In other words, you lose the ability to improve the code in ways that benefit everyone. Packages also group related functionality into one reusable component. Depending on the programming language, a package can provide apps with access to certain types and functions, while restricting access to their implementation details. Another reason to build a package is to provide a consistent way to build and test that package's functionality. When code is duplicated, each app might build and test that code in different ways. One set of tests might include checks that another set could benefit from. One tradeoff is that with a package, you have another codebase to test and maintain. You must also be careful when adding features.","title":"Why should I build a package?"},{"location":"devops/ci/#how-can-i-identify-dependencies","text":"If the goal is to reorganize your code into separate components, you need to identify those pieces of your app that can be removed, packaged to be reusable, stored in a central location, and versioned. You may even want to replace your own code with third-party components that are either open source or that you license. Here are some ways to identify dependencies: Duplicate code . If certain pieces of code appear in several places, that's a good indication that this code can be reused. Centralize these duplicate pieces of code and repackage them appropriately. High cohesion and low coupling . A second approach is to look for code elements that have a high cohesion to each other and low coupling with other parts of the code. In essence, high cohesion means keeping parts of a codebase that are related to each other in a single place. Low coupling, at the same time, is about separating unrelated parts of the code base as much as possible. Individual lifecycle . Look for parts of the code that have a similar lifecycle and can be deployed and released individually. If this code can be maintained by a separate team, it's a good indication that it can be packaged as a component outside of the solution. Stable parts . Some parts of your codebase might be stable and change infrequently. Check your code repository to find code with a low change frequency. Independent code and components . Whenever code and components are independent and unrelated to other parts of the system, they can potentially be isolated into separate dependencies.","title":"How can I identify dependencies?"},{"location":"devops/ci/#what-kinds-of-packages-are-there","text":"Each programming language or framework provides its own way to build packages. Popular package systems provide documentation about how the process works. NuGet : packages .NET libraries NPM : packages JavaScript libraries Maven : packages Java libraries Docker : packages software in isolated units called containers","title":"What kinds of packages are there?"},{"location":"devops/ci/#where-are-packages-hosted","text":"You can host packages on your own network, or you can use a hosting service. A hosting service is often called a package repository or package registry. Many of these services provide free hosting for open source projects. A package feed refers to your package repository server. This server can be on the internet or behind your firewall on your network. When you host packages behind the firewall, you can include feeds to your own packages. You can also cache packages that you trust on your network when your systems can't connect to the internet.","title":"Where are packages hosted?"},{"location":"devops/ci/#what-elements-make-up-a-good-dependency-management-strategy","text":"A good dependency management strategy depends on these three elements: Standardization . Standardizing how you declare and resolve dependencies will help your automated release process remain repeatable and predictable. Packaging formats and sources . Each dependency should be packaged using the applicable format and stored in a central location. Versioning . You need to keep track of the changes that occur over time in dependencies just as you do with your own code. This means that dependencies should be versioned.","title":"What elements make up a good dependency management strategy?"},{"location":"devops/ci/#how-are-packages-versioned","text":"Semantic Versioning is a popular versioning scheme. Here's the format: Major.Minor.Patch[-Suffix] A new Major version introduces breaking changes. Apps typically need to update how they use the package to work with a new major version. A new Minor version introduces new features, but is backward compatible with earlier versions. A new Patch introduces backward compatible bug fixes, but not new features. The -Suffix part is optional and identifies the package as a pre-release version. For example, 1.0.0-beta1 might identify the package as the first beta pre-release build for the 1.0.0 release.","title":"How are packages versioned?"},{"location":"devops/ci/#include-a-versioning-strategy-in-your-build-pipeline","text":"When you use a build pipeline, packages need versions before they can be consumed and tested. However, only after you've tested the package can you know its quality. Because package versions should never be changed, it becomes challenging to choose a certain version beforehand. A common use is to share package versions that have been tested, validated, or deployed but hold back packages still under development and not ready for public consumption. This approach works well with semantic versioning, which is useful for predicting the intent of a particular version. Essentially, allow a consumer to make a conscious decision to choose from released packages, or opt-in to prereleases of a certain quality level.","title":"Include a versioning strategy in your build pipeline"},{"location":"devops/ci/#package-security","text":"Ensuring the security of your packages is as important as ensuring the security of the rest of your code. One aspect of package security is securing access to the package feeds where a feed is where you store packages. - Setting permissions on the feed allows you to share your packages with as many or as few people as your scenario requires. Configure the pipeline to access security and license ratings There are several tools available from third parties to help you assess the security and license rating of the software packages you use. Some of these tools scan the packages as they are included in the build or CD pipeline. During the build process, the tool scans the packages and gives instantaneous feedback. During the CD process, the tool uses the build artifacts and performs scans. Two examples of such tools are WhiteSource Bolt and Black Duck .","title":"Package security"},{"location":"devops/ci/#benefits-of-continous-integration-in-the-build-process","text":"The time it takes to set up source control for new features. The team achieved this improvement by moving from centralized source control to Git, a form of distributed source control. By using distributed source control, they don't need to wait for files to be unlocked. The time it takes to deliver code to the tester. The team achieved this improvement by moving their build process to CI Pipelines. CI Pipelines automatically notifies the tester when a build is available. Developers no longer need to update spreadsheet to notify testers. The time it takes to test new features. The team achieved this improvement by unit-testing their code. They run unit tests each time a change moves through the build pipeline, so fewer bugs and regressions reach testers. The reduced workload means that testers can complete each manual test faster.","title":"Benefits of Continous Integration in the Build process"},{"location":"devops/devops-engineer/","text":"Developer Skills Roadmap Devops Skills Roadmap Devops Skillset Search for Autopilot and Technology to find for automation scripts or patterns A DevOps Engineer \u00b6 A DevOps Engineer works on automating the various processes and operations. Some of the responsibilities include 1) automating software code build, testing, and deployment 2) handle IT infrastructure, automate provisioning, upgrades, manage environments 3) monitor application and system performances. - It's not really a job role, but more of a mindset of how dev's and sysadmin's come together to solve problems. DevOps to me is like saying \"We Do Agile\". It speaks more to how you approach work and the team then your actual job tasking. Most DevOps job descriptions really just mean \"We need you to take the dev team's code, and create/manage an automated path of tools that test the code, build the runtime environments (servers and containers) and get the code onto them in a reliable and consistent way.\" So here's a longer job description : \"We need someone who understands code and dev tools, but can also get that code setup in CI/CD, knows git, and also knows how to manage and troubleshoot servers. They know AWS/Azure and how to start from zero and build out a whole stack of services, and then back it up, monitor it for health and performance, set up logging and alerting, and maybe knows some security like TLS, SSH, certificates and key generation, IP/Sec, VPN's, Firewall basics.\" - You must be a self-starter that is good at break/fix, problem analysis, and \"systems thinking\". - The three core qualities I think that set the best of any IT role so far in front of others are: 1. Empathy, the ability to hear what others are saying and put yourself in their shoes to better understand the problem or desired outcome. 1. The drive to help others, to even put them first. 1. A deep curiosity for understanding how things work. Code, servers, or networks. Always be learning. Journey \u00b6 Take Linux courses. Take AWS courses. Learn a ton about networking, the OSI Layers, how TCP packets are made up, how firewalls and NAT really work. Learn common sysadmin CLI tools like SSH, Bash, package managers, and how drivers and system services are configured. Force yourself to use network storage (NFS, iSCSI), load balancers, and do backups and restores of databases. Automate everything you can. Pick a system automation tool like Ansible or SaltStack or Puppet and start using it to control servers rather than manual SSH commands. Pick a monitoring and logging tool and get confident in them. Use them for even your smallest personal projects. Learn the basics of these things, then over time, go deeper in the areas you gravitate to. GitHub and Git Flow. Learn AWS basics. Not just the how, but also the why and when to use each tool. Skip 75% of their products and focus on the core tools everyone uses: 1. EC2, VPC's, Security Groups, Elastic IP's, ELB's, Route 53 2. Storage . EBS, EFS, S3 3. Lambda, CloudWatch, CodeDeploy 4. CloudFormation \u2190 key to AWS infrastructure automation and \"infrastructure as code\" but you need to know the services above first and why they exist before automating their creation. Learn TCP/IP networking, NAT firewalls, and the 7 OSI layer basics. Learn Jenkins and the CI/CD workflow. Learn Docker, and then how to use it in Jenkins to build, test, and deploy containers to servers. Learn K8s for creating a container cluster to deploy containers on many servers as easy as one. Learn Linux. Take an admin course online. Pick one distribution to know better than the others. Learn Ansible for automating sysadmin tasks across many servers as easy as one. Learn Terraform for creating servers in any cloud via \"infrastructure as code\". Learn Nginx or HAProxy for HTTP Reverse Proxy. cAdvisor, Prometheus, and Grafana for Monitoring. Use something like \"swarmprom\" to learn how the above tools work together to give you graphs and alerting of your apps and cluster. REX-Ray, used shared network storage to store your persistent data (databases). Common points to consider for application deployment Does the ecosystem for that language prefer file-based configs or can it also do envvar based (the best way for containers)? How should it be built for containers? Inside a language, there can be config standards that are specific to a toolkit, so there might be multiple ways depending on what the dev team is doing. What are the sysadmin-concerns about that language? Does it have memory management settings that usually need to be changed (Java)? Is it single threaded so that you'll need to manage solutions for multi-core servers (Node.js)? Does it have common caching or temp dir settings and permission issues (PHP)? Does it require special ways of installing on a server that are different then developers are used to locally (lots of them, but Ruby can be the worst)? Often you'll get a code repo from a developer that you need to change the way it deploys for testing and production, which means you need to understand how to install the language build/runtime environment, envvars, and dependencies. Usually, doing this on a server is different than how the developer did it on their computer. Devops Daily Routine \u00b6 Create a very basic hello world app in a language of their choice. Use git to commit that code to a remote git repository. Understand modern CVS workflows like pull-requests, bug tracking systems, code commits, code merges, etc. Create documentation a simple way for other devs to download that repo and get started coding on it as well. (working in a team and having empathy for others is important, remember) Use their choice of CI to test that app in basic ways. CI should run on every code commit. Provision cloud servers, storage, networking, firewalls, and load balancers for the app. Use CD to automate the deployment of code after successful CI to the servers. Do this for a test and prod set of servers. Store the configs of those two environments in a way that's easily manageable and has change tracking. Deploy basic backups, monitoring, alerting, and log collection. Leave the fancy stuff for Ops-focused people. Do all of this \"infrastructure as code\" style where they use configuration files to store the settings of all these systems. If they can do that all with the tools of their choice, then it shows they understand the full lifecycle of applications and how all the parts fit together. Understanding both the Dev and Ops roles in these basic ways helps you fully support both teams and bridge the gap.","title":"Skills"},{"location":"devops/devops-engineer/#a-devops-engineer","text":"A DevOps Engineer works on automating the various processes and operations. Some of the responsibilities include 1) automating software code build, testing, and deployment 2) handle IT infrastructure, automate provisioning, upgrades, manage environments 3) monitor application and system performances. - It's not really a job role, but more of a mindset of how dev's and sysadmin's come together to solve problems. DevOps to me is like saying \"We Do Agile\". It speaks more to how you approach work and the team then your actual job tasking. Most DevOps job descriptions really just mean \"We need you to take the dev team's code, and create/manage an automated path of tools that test the code, build the runtime environments (servers and containers) and get the code onto them in a reliable and consistent way.\" So here's a longer job description : \"We need someone who understands code and dev tools, but can also get that code setup in CI/CD, knows git, and also knows how to manage and troubleshoot servers. They know AWS/Azure and how to start from zero and build out a whole stack of services, and then back it up, monitor it for health and performance, set up logging and alerting, and maybe knows some security like TLS, SSH, certificates and key generation, IP/Sec, VPN's, Firewall basics.\" - You must be a self-starter that is good at break/fix, problem analysis, and \"systems thinking\". - The three core qualities I think that set the best of any IT role so far in front of others are: 1. Empathy, the ability to hear what others are saying and put yourself in their shoes to better understand the problem or desired outcome. 1. The drive to help others, to even put them first. 1. A deep curiosity for understanding how things work. Code, servers, or networks. Always be learning.","title":"A DevOps Engineer"},{"location":"devops/devops-engineer/#journey","text":"Take Linux courses. Take AWS courses. Learn a ton about networking, the OSI Layers, how TCP packets are made up, how firewalls and NAT really work. Learn common sysadmin CLI tools like SSH, Bash, package managers, and how drivers and system services are configured. Force yourself to use network storage (NFS, iSCSI), load balancers, and do backups and restores of databases. Automate everything you can. Pick a system automation tool like Ansible or SaltStack or Puppet and start using it to control servers rather than manual SSH commands. Pick a monitoring and logging tool and get confident in them. Use them for even your smallest personal projects. Learn the basics of these things, then over time, go deeper in the areas you gravitate to. GitHub and Git Flow. Learn AWS basics. Not just the how, but also the why and when to use each tool. Skip 75% of their products and focus on the core tools everyone uses: 1. EC2, VPC's, Security Groups, Elastic IP's, ELB's, Route 53 2. Storage . EBS, EFS, S3 3. Lambda, CloudWatch, CodeDeploy 4. CloudFormation \u2190 key to AWS infrastructure automation and \"infrastructure as code\" but you need to know the services above first and why they exist before automating their creation. Learn TCP/IP networking, NAT firewalls, and the 7 OSI layer basics. Learn Jenkins and the CI/CD workflow. Learn Docker, and then how to use it in Jenkins to build, test, and deploy containers to servers. Learn K8s for creating a container cluster to deploy containers on many servers as easy as one. Learn Linux. Take an admin course online. Pick one distribution to know better than the others. Learn Ansible for automating sysadmin tasks across many servers as easy as one. Learn Terraform for creating servers in any cloud via \"infrastructure as code\". Learn Nginx or HAProxy for HTTP Reverse Proxy. cAdvisor, Prometheus, and Grafana for Monitoring. Use something like \"swarmprom\" to learn how the above tools work together to give you graphs and alerting of your apps and cluster. REX-Ray, used shared network storage to store your persistent data (databases). Common points to consider for application deployment Does the ecosystem for that language prefer file-based configs or can it also do envvar based (the best way for containers)? How should it be built for containers? Inside a language, there can be config standards that are specific to a toolkit, so there might be multiple ways depending on what the dev team is doing. What are the sysadmin-concerns about that language? Does it have memory management settings that usually need to be changed (Java)? Is it single threaded so that you'll need to manage solutions for multi-core servers (Node.js)? Does it have common caching or temp dir settings and permission issues (PHP)? Does it require special ways of installing on a server that are different then developers are used to locally (lots of them, but Ruby can be the worst)? Often you'll get a code repo from a developer that you need to change the way it deploys for testing and production, which means you need to understand how to install the language build/runtime environment, envvars, and dependencies. Usually, doing this on a server is different than how the developer did it on their computer.","title":"Journey"},{"location":"devops/devops-engineer/#devops-daily-routine","text":"Create a very basic hello world app in a language of their choice. Use git to commit that code to a remote git repository. Understand modern CVS workflows like pull-requests, bug tracking systems, code commits, code merges, etc. Create documentation a simple way for other devs to download that repo and get started coding on it as well. (working in a team and having empathy for others is important, remember) Use their choice of CI to test that app in basic ways. CI should run on every code commit. Provision cloud servers, storage, networking, firewalls, and load balancers for the app. Use CD to automate the deployment of code after successful CI to the servers. Do this for a test and prod set of servers. Store the configs of those two environments in a way that's easily manageable and has change tracking. Deploy basic backups, monitoring, alerting, and log collection. Leave the fancy stuff for Ops-focused people. Do all of this \"infrastructure as code\" style where they use configuration files to store the settings of all these systems. If they can do that all with the tools of their choice, then it shows they understand the full lifecycle of applications and how all the parts fit together. Understanding both the Dev and Ops roles in these basic ways helps you fully support both teams and bridge the gap.","title":"Devops Daily Routine"},{"location":"devops/gitops/","text":"Introduction \u00b6 GitOps is the next phase of infrastructure management. In conjunction with DevOps and Kubernetes, your business can achieve a higher lever of stability, efficiency, and reliability in the software development lifecycle. GitOps ensures that the software and deployment lifecycle is more predictable and repeatable , which makes your business more profitable. What is GitOps? \u00b6 Operations that are driven thorugh Git is called GitOps . - GitOps is a paradigm that empowers developers to undertake tasks that might otherwise be handled by operations. - Operations are the processes and services that are overseen by a company\u2019s IT department. - This may include technology and infrastructure management (including software), quality assurance, network administration, and device management. - Traditionally, developers don\u2019t function under the operations umbrella. - This can place development and operations in their own silos. - GitOps aims to remove those silos, and enable operations to employ the same tools and methodologies that developers use for efficient collaboration. - GitOps, as its name implies, relies on Git as the only source of truth; even for code related to IT operations. - GitOps is possible due to Infrastructure as Code (IaC) tools that allow you to create and manage your infrastructure using declarative configuration files. The three basic components of GitOps are the following: Infrastructure as Code (IaC) , a methodology that stores all infrastructure configuration as code. Merge Requests (MRs) to serve as a change mechanism for infrastructure updates. Continuous Integration/Continuous Delivery (CI/CD) that automates building, testing, and deploying applications, and services. Gitops Principles \u00b6 1. Describe the state of the system declaratively. 2. Store the system's desired state in git. 3. Change the system desired state using git commit and automation. No manual changes are present in Gitops. 4. An Operator within the system ensures an drift is automatically corrected. GitOps vs. DevOps \u00b6 GitOps borrows best practices from DevOps and applies them to infrastructure automation. This includes version control, collaboration, compliance, and CI/CD. Tools like Kubernetes have helped automate the software development lifecycle. Because so many businesses use container deployment to scale applications and services, they often depend upon third-party, cloud-based services to host their infrastructure. This has led to the rise of infrastructure automation to achieve a level of elasticity not possible with traditional infrastructure. DevOps assists in the automation of the software development lifecycle, while GitOps contributes to the automation of infrastructure. There are a few key differences between GitOps and DevOps. First, GitOps uses Git to manage infrastructure provisioning and software deployment. DevOps, on the other hand, focuses primarily on CI/CD and does not focus on any one tool. The primary focus of GitOps is to ensure that DevOps is done correctly, whereas DevOps focuses less on correctness. GitOps is also less flexible than DevOps. It is also much easier to adopt GitOps in a business that already employs DevOps. GitOps and Kubernetes \u00b6 GitOps focuses on automating infrastructure, so it\u2019s a perfect workflow for businesses that employ Kubernetes. When you employ GitOps and Kubernetes: GitOps ensures everything operates as it was intended. Kubernetes ensures stability and availability. Kubernetes always makes sure a deployed application or service remains in a stable state and scales as needed . When GitOps is along for the ride, it ensures everything runs as it should, including the infrastructure necessary for the deployments. GitOps serves as the glue between application build/delivery (Kubernetes) and where the application is to run. Traditional vs GitOps \u00b6 Common pattern used by organizations is the CIOps . In this pattern, a change is made to the application in the source repository. This triggers the build system. The CI pipeline builds the software and executes tests against it and packages it into a container image and is stored in a container registry. CI pipeline then triggeres the CD pipeline which has permission to make calls against the K8s API. This the point where the security concern is raised. Once the cluster state is updated in ETCD, the cluster begins to materialize the desired state by pulling the built image from the container registry and deploying it against the nodes. If the application change is not correct, it is difficult to recover from a bad deployment as the state is updated and the cluster has become unstable. Disadvantages of CIOps There is little separation between CI and CD system as it could be the same tool. Its like forcing the CI tool to do continuous delivery which it was not supposed to do. CI tool needs priviledges to make K8s API calls to perform deployments. It violates the trust boundary and can allow an attacker to use a compromised pipeline to execute a malicious code. Its not stable as success or failure of deployments is unknown. This is true in case a deployment fails and leaves the cluster in an unstable state. Not a feasible option for multiple clusters or a single large cluster. Gitops pattern First store the desired state declaratively in git. Pull the desired state into the system using an operator. Use a control loop to keep all of this in sync. It keeps the CI pipeline intact from CIOps except for removing the trigger for the CD pipeline. Under Gitops, the sole responsiblity of CI is to produce an artifact, in this case its a container image that is placed within an artifact repository. The Operator takes over from CI. It monitors the desired state in git for any changes which are made to a separate Environment repo which are made using pull requests . Once operator detects a change to the K8s manifest in the repo and provides this to the K8s control plane using the API to update the new state of the cluster. This causes the cluster to pull the new image and deploys it across the nodes. Gitops Usecases \u00b6 1. Infrastructure Operations : Gitops expects resources can be rapidly deployed using Cloud services. Teams use tools like Trraform to declare the infrastructure in manifest to create K8s cluster in a particular region. 2. Application Operations : Flux is used to ensure applications are deployed in an automated fashion to a K8s cluster 3. Release Operations : GitOps practises can be applied to declaratively define complex release strategies like Canary or Blue green deployments. Tools like Flagger fall into this space. The GitOps Workflow \u00b6 The traditional application lifecycle resembles the following: Design --> Build --> Image --> Test --> Deploy When you add GitOps into the mix, that lifecycle looks as follows: Design --> Build --> Image --> Test --> Monitor --> Log changes/events --> Alert when a change has occurred --> Update With a Kubernetes workflow as your source of truth all necessary code is stored in a Git repository with the help of automation. Anyone with Kubernetes management rights can create pull requests, edit code, and issue merge requests to the repository. Once a merge request is complete, the automated GitOps operator detects the changes, another automator declares if the change is operational, and the change is automatically deployed to the cluster. Within the GitOps workflow you not only have a high level of automation, but there\u2019s also a much higher probability that every deployment works exactly as expected. Using Flux as the gitops operator within Kubernetes. Using Flux and Flagger together to bring in Progressive or Blue-Green Deployment strategies. An alternate workflow created by Gitlab Advantages of GitOps CI and CD are decoupled into separate concerns. CI pipeline is left intact. However the CD pipeline is shifted inside the cluster where an Operator is responsible for automating deployment of changes to ensure it matches the desired state found in git. This eliminates the potential security concerns associated with the traditional push model from CIOps. It also provides recoverabilty as the cluster can be quickly rebuilt from source code. GitOps Tools \u00b6 There are several tools useful to GitOps, some of those tools include: Git - a version control system. GitHub - a code repository for housing your code. Cloud Build - a service that executes the build step of your deployment lifecycle using pre-packaged docker containers that include all of the appropriate tooling. CircleCI - a SaaS-style build engine that simplifies the build steps and can serve as a CI/CD engine. Kubernetes - a container orchestration platform that can be seamlessly integrated with GitOps. Helm - a robust tool for configuring Kubernetes resources. Flux - the GitOps operator for Kubernetes which automatically adjusts the Kubernetes cluster configuration based on the configurations found in your Git repo. Flagger - automates the detection of errors in code and prevents those errors from being deployed. Prometheus - a powerful GitOps monitoring tool that can generate alerts that are detected by Flagger. Quay - an application registry and container image manager.","title":"GitOps"},{"location":"devops/gitops/#introduction","text":"GitOps is the next phase of infrastructure management. In conjunction with DevOps and Kubernetes, your business can achieve a higher lever of stability, efficiency, and reliability in the software development lifecycle. GitOps ensures that the software and deployment lifecycle is more predictable and repeatable , which makes your business more profitable.","title":"Introduction"},{"location":"devops/gitops/#what-is-gitops","text":"Operations that are driven thorugh Git is called GitOps . - GitOps is a paradigm that empowers developers to undertake tasks that might otherwise be handled by operations. - Operations are the processes and services that are overseen by a company\u2019s IT department. - This may include technology and infrastructure management (including software), quality assurance, network administration, and device management. - Traditionally, developers don\u2019t function under the operations umbrella. - This can place development and operations in their own silos. - GitOps aims to remove those silos, and enable operations to employ the same tools and methodologies that developers use for efficient collaboration. - GitOps, as its name implies, relies on Git as the only source of truth; even for code related to IT operations. - GitOps is possible due to Infrastructure as Code (IaC) tools that allow you to create and manage your infrastructure using declarative configuration files. The three basic components of GitOps are the following: Infrastructure as Code (IaC) , a methodology that stores all infrastructure configuration as code. Merge Requests (MRs) to serve as a change mechanism for infrastructure updates. Continuous Integration/Continuous Delivery (CI/CD) that automates building, testing, and deploying applications, and services.","title":"What is GitOps?"},{"location":"devops/gitops/#gitops-principles","text":"1. Describe the state of the system declaratively. 2. Store the system's desired state in git. 3. Change the system desired state using git commit and automation. No manual changes are present in Gitops. 4. An Operator within the system ensures an drift is automatically corrected.","title":"Gitops Principles"},{"location":"devops/gitops/#gitops-vs-devops","text":"GitOps borrows best practices from DevOps and applies them to infrastructure automation. This includes version control, collaboration, compliance, and CI/CD. Tools like Kubernetes have helped automate the software development lifecycle. Because so many businesses use container deployment to scale applications and services, they often depend upon third-party, cloud-based services to host their infrastructure. This has led to the rise of infrastructure automation to achieve a level of elasticity not possible with traditional infrastructure. DevOps assists in the automation of the software development lifecycle, while GitOps contributes to the automation of infrastructure. There are a few key differences between GitOps and DevOps. First, GitOps uses Git to manage infrastructure provisioning and software deployment. DevOps, on the other hand, focuses primarily on CI/CD and does not focus on any one tool. The primary focus of GitOps is to ensure that DevOps is done correctly, whereas DevOps focuses less on correctness. GitOps is also less flexible than DevOps. It is also much easier to adopt GitOps in a business that already employs DevOps.","title":"GitOps vs. DevOps"},{"location":"devops/gitops/#gitops-and-kubernetes","text":"GitOps focuses on automating infrastructure, so it\u2019s a perfect workflow for businesses that employ Kubernetes. When you employ GitOps and Kubernetes: GitOps ensures everything operates as it was intended. Kubernetes ensures stability and availability. Kubernetes always makes sure a deployed application or service remains in a stable state and scales as needed . When GitOps is along for the ride, it ensures everything runs as it should, including the infrastructure necessary for the deployments. GitOps serves as the glue between application build/delivery (Kubernetes) and where the application is to run.","title":"GitOps and Kubernetes"},{"location":"devops/gitops/#traditional-vs-gitops","text":"Common pattern used by organizations is the CIOps . In this pattern, a change is made to the application in the source repository. This triggers the build system. The CI pipeline builds the software and executes tests against it and packages it into a container image and is stored in a container registry. CI pipeline then triggeres the CD pipeline which has permission to make calls against the K8s API. This the point where the security concern is raised. Once the cluster state is updated in ETCD, the cluster begins to materialize the desired state by pulling the built image from the container registry and deploying it against the nodes. If the application change is not correct, it is difficult to recover from a bad deployment as the state is updated and the cluster has become unstable. Disadvantages of CIOps There is little separation between CI and CD system as it could be the same tool. Its like forcing the CI tool to do continuous delivery which it was not supposed to do. CI tool needs priviledges to make K8s API calls to perform deployments. It violates the trust boundary and can allow an attacker to use a compromised pipeline to execute a malicious code. Its not stable as success or failure of deployments is unknown. This is true in case a deployment fails and leaves the cluster in an unstable state. Not a feasible option for multiple clusters or a single large cluster. Gitops pattern First store the desired state declaratively in git. Pull the desired state into the system using an operator. Use a control loop to keep all of this in sync. It keeps the CI pipeline intact from CIOps except for removing the trigger for the CD pipeline. Under Gitops, the sole responsiblity of CI is to produce an artifact, in this case its a container image that is placed within an artifact repository. The Operator takes over from CI. It monitors the desired state in git for any changes which are made to a separate Environment repo which are made using pull requests . Once operator detects a change to the K8s manifest in the repo and provides this to the K8s control plane using the API to update the new state of the cluster. This causes the cluster to pull the new image and deploys it across the nodes.","title":"Traditional vs GitOps"},{"location":"devops/gitops/#gitops-usecases","text":"1. Infrastructure Operations : Gitops expects resources can be rapidly deployed using Cloud services. Teams use tools like Trraform to declare the infrastructure in manifest to create K8s cluster in a particular region. 2. Application Operations : Flux is used to ensure applications are deployed in an automated fashion to a K8s cluster 3. Release Operations : GitOps practises can be applied to declaratively define complex release strategies like Canary or Blue green deployments. Tools like Flagger fall into this space.","title":"Gitops Usecases"},{"location":"devops/gitops/#the-gitops-workflow","text":"The traditional application lifecycle resembles the following: Design --> Build --> Image --> Test --> Deploy When you add GitOps into the mix, that lifecycle looks as follows: Design --> Build --> Image --> Test --> Monitor --> Log changes/events --> Alert when a change has occurred --> Update With a Kubernetes workflow as your source of truth all necessary code is stored in a Git repository with the help of automation. Anyone with Kubernetes management rights can create pull requests, edit code, and issue merge requests to the repository. Once a merge request is complete, the automated GitOps operator detects the changes, another automator declares if the change is operational, and the change is automatically deployed to the cluster. Within the GitOps workflow you not only have a high level of automation, but there\u2019s also a much higher probability that every deployment works exactly as expected. Using Flux as the gitops operator within Kubernetes. Using Flux and Flagger together to bring in Progressive or Blue-Green Deployment strategies. An alternate workflow created by Gitlab Advantages of GitOps CI and CD are decoupled into separate concerns. CI pipeline is left intact. However the CD pipeline is shifted inside the cluster where an Operator is responsible for automating deployment of changes to ensure it matches the desired state found in git. This eliminates the potential security concerns associated with the traditional push model from CIOps. It also provides recoverabilty as the cluster can be quickly rebuilt from source code.","title":"The GitOps Workflow"},{"location":"devops/gitops/#gitops-tools","text":"There are several tools useful to GitOps, some of those tools include: Git - a version control system. GitHub - a code repository for housing your code. Cloud Build - a service that executes the build step of your deployment lifecycle using pre-packaged docker containers that include all of the appropriate tooling. CircleCI - a SaaS-style build engine that simplifies the build steps and can serve as a CI/CD engine. Kubernetes - a container orchestration platform that can be seamlessly integrated with GitOps. Helm - a robust tool for configuring Kubernetes resources. Flux - the GitOps operator for Kubernetes which automatically adjusts the Kubernetes cluster configuration based on the configurations found in your Git repo. Flagger - automates the detection of errors in code and prevents those errors from being deployed. Prometheus - a powerful GitOps monitoring tool that can generate alerts that are detected by Flagger. Quay - an application registry and container image manager.","title":"GitOps Tools"},{"location":"devops/iac/","text":"Cloud Computing Models \u00b6 IaaS \u00b6 IaaS is a mature computing model that first became popular about a decade ago. It is currently the most common cloud computing paradigm. IaaS cloud providers, offer IaaS services from their extensive pool of physical servers in their data centers. These vendors use a hypervisor , also known as a Virtual Machine Monitor (VMM) , to create the virtual service. A hypervisor is a type of emulator that runs on an actual hardware host, which is referred to as the host machine. It runs a Virtual Machine (VM) that mimics an actual server or network. Some common types of hypervisors include Xen, Oracle VirtualBox, Oracle VM, KVM, and VMware ESX. The most common way of creating an IaaS VM is by using cloud orchestration technologies. These programs choose a hypervisor to run the VM on and then create the virtual machine. They also frequently allocate storage and add firewalls, logging services, and networking essentials including IP addresses. Advanced services might include clustering, encryption, billing, load balancing, and more complicated Virtual Local Area Networks (VLAN). A Virtual Private Cloud (VPC) can assist in further isolating the cloud resources. Both Central Processing Unit (CPU) and Graphics Processing Unit (GPU) systems are typically available. Customers of IaaS access their virtualized infrastructure over the internet. They use a visual dashboard or Graphical User Interface (GUI) to quickly create or modify devices, often with the push of a button. The dashboard can also be used to monitor performance, collect data, troubleshoot, and track costs. All services are provided on a pay-as-you-go model. Services can also be provisioned programmatically using APIs. This technique is often used together with Infrastructure as Code (IaC) technologies, which deploys the infrastructure using scripts. IaC allows users to standardize common infrastructure tasks and test their deployments using automation. One important point about IaaS is the customer does not control the underlying physical hardware components and interconnections. These remain under the control of the cloud provider. Users of IaaS are typically responsible for the selection and installation of the operating system and all software applications, including databases and middleware . PaaS \u00b6 PaaS extends the IaaS model to include operating systems, web servers, tools, databases, and other managed services that is handled by the cloud providers. The PaaS user is still responsible for adding and managing applications on top of the vendor-provided platform . This makes deployment even easier at the cost of some flexibility. PaaS services are often used for software and application development . SaaS \u00b6 SaaS provides software services to customers on demand . Users access the software on the provider\u2019s server, typically through a web browser. SaaS clients are only responsible for configuring the application, maintaining an access control list, and the actual content . SaaS is usually geared towards a completely different audience. Most SaaS clients do not require a platform and do not need IaaS capabilities. Serverless \u00b6 Serverless computing eliminates the need to manage your infrastructure . It lies somewhere between PaaS and SaaS in term of the control/ease-of-use it provides, but is not exactly like either model. The name \u201cserverless\u201d is somewhat misleading. Servers are being used, but the end-user has no knowledge or visibility of them. Serverless computing provisions all resources on-demand, and automatically and dynamically scales them up and down in close to real-time. This makes more efficient use of computer resources because the provider allocates the memory, CPU, and networking resources based on the calculated demand. Serverless computing is well suited to a microservices model and is frequently used in software development. However, it can add more latency, and cold starts can affect the system\u2019s performance. Conatiners \u00b6 Containers, such as Docker, are another option for implementing virtualization, but they follow a completely different model. Containers do not use hypervisors. They run on a Linux partition directly on the hardware. Containers could be said to offer operating-system-level virtualization. Containers offer better performance than hypervisor-based servers at the cost of some additional complexity and have become increasingly popular. Reasons to Use IaaS \u00b6 The IaaS model is particularly well suited to the following scenarios. Backup and recovery : IaaS services are handy for backing up applications and data, and for rapid recovery systems for on-site networks. In this case, the IaaS network typically mirrors the configuration of the on-site servers. Testing and rapid development : IaaS allows for quick prototyping and efficient automated testing. Servers can be created and deleted as required. IaaS facilitates the testing of an application on different platforms and networks. It is also useful for temporary or experimental workflows. Legal and compliance requirements : IaaS systems are a good choice for data retention and other record-keeping requirements. High-performance and specialized computing : The cost of buying high-performance equipment capable of specialized tasks might otherwise be prohibitive for smaller businesses. IaaS enables smaller businesses to access advanced systems capable of handling data analysis, computer modeling, and 3-D graphics. Managing unpredictable demand : The ability to scale up and down means IaaS is a good choice for unpredictable scenarios when the demand is unknown or might vary dramatically. It allows companies to handle unexpected surges. Rapid migration to the cloud : IaaS APIs allow for the easy translation of the original network and configuration into IaaS specifications. Application and web development : IaaS is also frequently used for web hosting. Advantages and Disadvantages \u00b6 Advantages: It reduces maintenance, operating costs, and lab space. IaaS allows businesses to focus on their core activities instead of running a on-premises servers. It eliminates the need for capital expenditures on equipment. The pay-as-you-go operating expense of IaaS is easier to budget for than large capital expenditures. IaaS networks can react rapidly to changing business demands, by quickly expanding or contracting. New services are easily created, and customers only use and pay for what they need. IaaS elegantly handles system backups and redundancy and increases reliability. For example, cloud providers have multiple labs and are hardened against failures. Providers of IaaS offer different packages with different levels of memory and performance. It is easy for a customer to find the right package for their network. Customers can also upgrade or downgrade according to their current situation. IaaS providers have more expertise with hardware and networking technologies and can provide advice and support. Many IaaS vendors have geographically diverse locations. This makes it easier for organizations to position their resources closer to their end-users. It also provides an even greater level of redundancy and protection in the case of local outages or failures. IaaS can provide better security because vendors are more familiar with updated security protocols. Disadvantages: The biggest drawback is the relative lack of flexibility and low-level visibility compared to on-premises servers. Customers cannot deploy a system that their IaaS vendor does not offer, and they cannot control attributes such as IP address blocks and subnets. However, this is usually not a big concern for most deployments. Customers should also be aware that most hypervisors support multiple users, and their performance and throughput could be affected by other customers at times. Migration Strategies \u00b6 Several options are available to organizations that want to relocate their existing network to use an IaaS model. Staged : In a staged migration, certain components of the old network are moved over to IaaS before others. For instance, the database or the data could be moved over first. Meanwhile, the original on-site servers are still used to access the database. This strategy reduces the overall risk of the move. Re-hosting : This method is also known as the \u201clift and shift\u201d strategy. The existing configuration, data, and applications are migrated to an IaaS model without any significant modifications. The new IaaS servers are configured the same way the original physical servers were. Refactoring : Refactoring re-engineers the environment from scratch to take advantage of IaaS capabilities. This might involve a more detailed API roll-out using IaC products, closer attention to scalability, and more efficient use of cloud resources. During the move, candidate tasks for automation and streamlining can be identified. Hybrid : With this strategy, infrastructure items are moved to IaaS selectively. Some resources might remain on the old network for security, logistical, business, or legal reasons. IaaS Deployment Strategies \u00b6 Each IaaS deployment is unique, but the following high-level principles generally apply. Understand and be clear about the business requirements and the budget before proceeding with any deployments. Carefully review and understand the policies of the cloud provider and their plans, packages, and products. Be clear about the capabilities of the virtualized infrastructure, including the throughput, storage, and memory/performance of each item. Consider how any existing databases and servers should be migrated using one of the techniques in the Migration Strategies section. Attempt to reduce downtime. Schedule a maintenance window for the migration. Test the new network before live deployment. Consider how much storage and what storage types should be used. The main types of storage are object storage, file storage, and block storage. Object storage has become more popular recently because its distributed architecture fits well with the IaaS model. Consider the resiliency and reliability requirements for the network. If necessary, determine the level of support and the service package that is required. Decide what network metrics are important, and monitor these items during and after the initial deployment. Scrutinize the entire network as a single system and continue to regularly maintain, adjust, and optimize it. Terraform vs Ansible \u00b6 Both Ansible and Terraform are tools for implementing Infrastructure as Code, although they focus on different components. Ansible is geared towards configuration management whereas Terraform\u2019s strength lies in service and cloud orchestration. There could also be situations where the two tools are best used together. Terraform is a service orchestration tool which is optimized for cloud and multi-cloud networks. It ensures an environment is in its desired state, stores this state, and restores the system after it is reloaded. It does not focus on configuration management. Ansible is a ** configuration management tool**. It excels in provisioning software and devices, and deploying the applications that run on top of the infrastructure. It operates on a particular device in isolation from the network and ensures it is functioning normally. There is some overlap between the tools because Ansible can perform some service orchestration. Its playbooks can be extended to deploy applications in a cloud, and it features modules for most major cloud providers. But it is not as good at orchestrating services and interconnected, dependent applications . The Main Uses for Terraform \u00b6 Terraform is an open source IaC tool that is very straightforward to use. Its main purpose is to build and scale Cloud services and to manage the state of the network . Terraform does not specialize in software configuration, and does not install and manage software on existing devices. Instead, it is geared towards creating, modifying, and destroying servers and other Cloud resources. This means it is most commonly found in data centers and in software-defined networking (SDN) environments . It works effectively with both lower-level elements, including storage and networking devices, and higher-level Software as a Service (SaSS) entries. In terms of state management, it maps the actual resources back to the configuration, stores metadata, and improves network performance. Terraform can manage external service providers, including cloud networks, and in-house solutions. It is especially useful for multi-tier or N-tier applications , such as web servers that use a database layer. Because Terraform models the dependencies between applications and add-ons, it ensures the database layer is ready before any web servers are launched. Terraform is cloud agnostic, and can manage multiple clouds to increase fault tolerance. A single configuration file can oversee multiple providers and handle cross-cloud dependencies. Terraform is very efficient for demos or other disposable environments due to the ease of creating a network on a cloud provider. It helps manage parallel environments, so it is a good choice for testing, validating bug fixes, and formal acceptance. The Main Uses for Ansible \u00b6 The main purpose of Red Hat\u2019s Ansible is IT automation . Ansible automates software provisioning, configuration management, application deployment, and continuous integration (CI) pipelines. Ansible runs on most Linux distributions, and can provision both Linux and Windows-based devices. The design goals of Ansible are to be minimal, consistent, secure, reliable, and easy to learn. It is straightforward to install, and no special programming skills are necessarily required to use it. Ansible handles all types of infrastructure platforms, including bare metal, virtualized devices such as hypervisors, and cloud networks. It integrates well with legacy applications and existing automated scripts, and is designed to manage the complex, multi-faceted facilities found in large businesses. Ansible supports idempotent behavior, which means it can place the node into the same state every time. This is necessary for consistency and standardized behavior. IaC \u00b6 Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. IaC can be applied throughout the lifecycle, both on the initial build, as well as throughout the life of the infrastructure. Commonly, these are referred to as Day 0 and Day 1 activities. \u201cDay 0\u201d code provisions and configures your initial infrastructure. \u201cDay 0\u201d code provisions and configures your initial infrastructure. If your infrastructure never changes after the initial build (no OS updates, no patches, no app configurations, etc.) then you may not need tools that support subsequent updates, changes, and expansions. \u201cDay 1\u201d refers to OS and application configurations you apply after you\u2019ve initially built your infrastructure. IaC makes changes idempotent, consistent, repeatable, and predictable. With IaC, we can test the code and review the results before the code is applied to our target environments. Should a result not align to our expectations, we iterate on the code until the results pass our tests and align to our expectations. Following this pattern allows for the outcome to be predicted before the code is applied to a production environment. Once ready for use, we can then apply that code via automation, at scale, ensuring consistency and repeatability in how it is applied. Since code is checked into version control systems such as GitHub, GitLab, BitBucket, etc., it is possible to review how the infrastructure evolves over time. The idempotent characteristic provided by IaC tools ensures that, even if the same code is applied multiple times, the result remains the same. Steps to Define your Infrastructure Scope : Identify the Infrastructure for your project. Author : Write configuration to define your infrastructure. Initialize : Install the required Terraform providers. Plan : Preview the changes Terraform will make. Apply : Make the changes to your infrastructure. Using Terraform has several advantages over manually managing your infrastructure: Terraform can manage infrastructure on multiple cloud platforms. The human-readable configuration language helps you write infrastructure code quickly. Terraform's state allows you to track resource changes throughout your deployments. You can commit your configurations to version control to safely collaborate on infrastructure. The terraform {} block contains Terraform settings, including the required providers Terraform will use to provision your infrastructure. A provider is a plugin that Terraform uses to create and manage your resources. Use resource blocks to define components of your infrastructure. A resource might be a physical or virtual component such as an EC2 instance, or it can be a logical resource such as a Heroku application. Resource blocks have two strings before the block: the resource type and the resource name.","title":"IAC"},{"location":"devops/iac/#cloud-computing-models","text":"","title":"Cloud Computing Models"},{"location":"devops/iac/#iaas","text":"IaaS is a mature computing model that first became popular about a decade ago. It is currently the most common cloud computing paradigm. IaaS cloud providers, offer IaaS services from their extensive pool of physical servers in their data centers. These vendors use a hypervisor , also known as a Virtual Machine Monitor (VMM) , to create the virtual service. A hypervisor is a type of emulator that runs on an actual hardware host, which is referred to as the host machine. It runs a Virtual Machine (VM) that mimics an actual server or network. Some common types of hypervisors include Xen, Oracle VirtualBox, Oracle VM, KVM, and VMware ESX. The most common way of creating an IaaS VM is by using cloud orchestration technologies. These programs choose a hypervisor to run the VM on and then create the virtual machine. They also frequently allocate storage and add firewalls, logging services, and networking essentials including IP addresses. Advanced services might include clustering, encryption, billing, load balancing, and more complicated Virtual Local Area Networks (VLAN). A Virtual Private Cloud (VPC) can assist in further isolating the cloud resources. Both Central Processing Unit (CPU) and Graphics Processing Unit (GPU) systems are typically available. Customers of IaaS access their virtualized infrastructure over the internet. They use a visual dashboard or Graphical User Interface (GUI) to quickly create or modify devices, often with the push of a button. The dashboard can also be used to monitor performance, collect data, troubleshoot, and track costs. All services are provided on a pay-as-you-go model. Services can also be provisioned programmatically using APIs. This technique is often used together with Infrastructure as Code (IaC) technologies, which deploys the infrastructure using scripts. IaC allows users to standardize common infrastructure tasks and test their deployments using automation. One important point about IaaS is the customer does not control the underlying physical hardware components and interconnections. These remain under the control of the cloud provider. Users of IaaS are typically responsible for the selection and installation of the operating system and all software applications, including databases and middleware .","title":"IaaS"},{"location":"devops/iac/#paas","text":"PaaS extends the IaaS model to include operating systems, web servers, tools, databases, and other managed services that is handled by the cloud providers. The PaaS user is still responsible for adding and managing applications on top of the vendor-provided platform . This makes deployment even easier at the cost of some flexibility. PaaS services are often used for software and application development .","title":"PaaS"},{"location":"devops/iac/#saas","text":"SaaS provides software services to customers on demand . Users access the software on the provider\u2019s server, typically through a web browser. SaaS clients are only responsible for configuring the application, maintaining an access control list, and the actual content . SaaS is usually geared towards a completely different audience. Most SaaS clients do not require a platform and do not need IaaS capabilities.","title":"SaaS"},{"location":"devops/iac/#serverless","text":"Serverless computing eliminates the need to manage your infrastructure . It lies somewhere between PaaS and SaaS in term of the control/ease-of-use it provides, but is not exactly like either model. The name \u201cserverless\u201d is somewhat misleading. Servers are being used, but the end-user has no knowledge or visibility of them. Serverless computing provisions all resources on-demand, and automatically and dynamically scales them up and down in close to real-time. This makes more efficient use of computer resources because the provider allocates the memory, CPU, and networking resources based on the calculated demand. Serverless computing is well suited to a microservices model and is frequently used in software development. However, it can add more latency, and cold starts can affect the system\u2019s performance.","title":"Serverless"},{"location":"devops/iac/#conatiners","text":"Containers, such as Docker, are another option for implementing virtualization, but they follow a completely different model. Containers do not use hypervisors. They run on a Linux partition directly on the hardware. Containers could be said to offer operating-system-level virtualization. Containers offer better performance than hypervisor-based servers at the cost of some additional complexity and have become increasingly popular.","title":"Conatiners"},{"location":"devops/iac/#reasons-to-use-iaas","text":"The IaaS model is particularly well suited to the following scenarios. Backup and recovery : IaaS services are handy for backing up applications and data, and for rapid recovery systems for on-site networks. In this case, the IaaS network typically mirrors the configuration of the on-site servers. Testing and rapid development : IaaS allows for quick prototyping and efficient automated testing. Servers can be created and deleted as required. IaaS facilitates the testing of an application on different platforms and networks. It is also useful for temporary or experimental workflows. Legal and compliance requirements : IaaS systems are a good choice for data retention and other record-keeping requirements. High-performance and specialized computing : The cost of buying high-performance equipment capable of specialized tasks might otherwise be prohibitive for smaller businesses. IaaS enables smaller businesses to access advanced systems capable of handling data analysis, computer modeling, and 3-D graphics. Managing unpredictable demand : The ability to scale up and down means IaaS is a good choice for unpredictable scenarios when the demand is unknown or might vary dramatically. It allows companies to handle unexpected surges. Rapid migration to the cloud : IaaS APIs allow for the easy translation of the original network and configuration into IaaS specifications. Application and web development : IaaS is also frequently used for web hosting.","title":"Reasons to Use IaaS"},{"location":"devops/iac/#advantages-and-disadvantages","text":"Advantages: It reduces maintenance, operating costs, and lab space. IaaS allows businesses to focus on their core activities instead of running a on-premises servers. It eliminates the need for capital expenditures on equipment. The pay-as-you-go operating expense of IaaS is easier to budget for than large capital expenditures. IaaS networks can react rapidly to changing business demands, by quickly expanding or contracting. New services are easily created, and customers only use and pay for what they need. IaaS elegantly handles system backups and redundancy and increases reliability. For example, cloud providers have multiple labs and are hardened against failures. Providers of IaaS offer different packages with different levels of memory and performance. It is easy for a customer to find the right package for their network. Customers can also upgrade or downgrade according to their current situation. IaaS providers have more expertise with hardware and networking technologies and can provide advice and support. Many IaaS vendors have geographically diverse locations. This makes it easier for organizations to position their resources closer to their end-users. It also provides an even greater level of redundancy and protection in the case of local outages or failures. IaaS can provide better security because vendors are more familiar with updated security protocols. Disadvantages: The biggest drawback is the relative lack of flexibility and low-level visibility compared to on-premises servers. Customers cannot deploy a system that their IaaS vendor does not offer, and they cannot control attributes such as IP address blocks and subnets. However, this is usually not a big concern for most deployments. Customers should also be aware that most hypervisors support multiple users, and their performance and throughput could be affected by other customers at times.","title":"Advantages and Disadvantages"},{"location":"devops/iac/#migration-strategies","text":"Several options are available to organizations that want to relocate their existing network to use an IaaS model. Staged : In a staged migration, certain components of the old network are moved over to IaaS before others. For instance, the database or the data could be moved over first. Meanwhile, the original on-site servers are still used to access the database. This strategy reduces the overall risk of the move. Re-hosting : This method is also known as the \u201clift and shift\u201d strategy. The existing configuration, data, and applications are migrated to an IaaS model without any significant modifications. The new IaaS servers are configured the same way the original physical servers were. Refactoring : Refactoring re-engineers the environment from scratch to take advantage of IaaS capabilities. This might involve a more detailed API roll-out using IaC products, closer attention to scalability, and more efficient use of cloud resources. During the move, candidate tasks for automation and streamlining can be identified. Hybrid : With this strategy, infrastructure items are moved to IaaS selectively. Some resources might remain on the old network for security, logistical, business, or legal reasons.","title":"Migration Strategies"},{"location":"devops/iac/#iaas-deployment-strategies","text":"Each IaaS deployment is unique, but the following high-level principles generally apply. Understand and be clear about the business requirements and the budget before proceeding with any deployments. Carefully review and understand the policies of the cloud provider and their plans, packages, and products. Be clear about the capabilities of the virtualized infrastructure, including the throughput, storage, and memory/performance of each item. Consider how any existing databases and servers should be migrated using one of the techniques in the Migration Strategies section. Attempt to reduce downtime. Schedule a maintenance window for the migration. Test the new network before live deployment. Consider how much storage and what storage types should be used. The main types of storage are object storage, file storage, and block storage. Object storage has become more popular recently because its distributed architecture fits well with the IaaS model. Consider the resiliency and reliability requirements for the network. If necessary, determine the level of support and the service package that is required. Decide what network metrics are important, and monitor these items during and after the initial deployment. Scrutinize the entire network as a single system and continue to regularly maintain, adjust, and optimize it.","title":"IaaS Deployment Strategies"},{"location":"devops/iac/#terraform-vs-ansible","text":"Both Ansible and Terraform are tools for implementing Infrastructure as Code, although they focus on different components. Ansible is geared towards configuration management whereas Terraform\u2019s strength lies in service and cloud orchestration. There could also be situations where the two tools are best used together. Terraform is a service orchestration tool which is optimized for cloud and multi-cloud networks. It ensures an environment is in its desired state, stores this state, and restores the system after it is reloaded. It does not focus on configuration management. Ansible is a ** configuration management tool**. It excels in provisioning software and devices, and deploying the applications that run on top of the infrastructure. It operates on a particular device in isolation from the network and ensures it is functioning normally. There is some overlap between the tools because Ansible can perform some service orchestration. Its playbooks can be extended to deploy applications in a cloud, and it features modules for most major cloud providers. But it is not as good at orchestrating services and interconnected, dependent applications .","title":"Terraform vs Ansible"},{"location":"devops/iac/#the-main-uses-for-terraform","text":"Terraform is an open source IaC tool that is very straightforward to use. Its main purpose is to build and scale Cloud services and to manage the state of the network . Terraform does not specialize in software configuration, and does not install and manage software on existing devices. Instead, it is geared towards creating, modifying, and destroying servers and other Cloud resources. This means it is most commonly found in data centers and in software-defined networking (SDN) environments . It works effectively with both lower-level elements, including storage and networking devices, and higher-level Software as a Service (SaSS) entries. In terms of state management, it maps the actual resources back to the configuration, stores metadata, and improves network performance. Terraform can manage external service providers, including cloud networks, and in-house solutions. It is especially useful for multi-tier or N-tier applications , such as web servers that use a database layer. Because Terraform models the dependencies between applications and add-ons, it ensures the database layer is ready before any web servers are launched. Terraform is cloud agnostic, and can manage multiple clouds to increase fault tolerance. A single configuration file can oversee multiple providers and handle cross-cloud dependencies. Terraform is very efficient for demos or other disposable environments due to the ease of creating a network on a cloud provider. It helps manage parallel environments, so it is a good choice for testing, validating bug fixes, and formal acceptance.","title":"The Main Uses for Terraform"},{"location":"devops/iac/#the-main-uses-for-ansible","text":"The main purpose of Red Hat\u2019s Ansible is IT automation . Ansible automates software provisioning, configuration management, application deployment, and continuous integration (CI) pipelines. Ansible runs on most Linux distributions, and can provision both Linux and Windows-based devices. The design goals of Ansible are to be minimal, consistent, secure, reliable, and easy to learn. It is straightforward to install, and no special programming skills are necessarily required to use it. Ansible handles all types of infrastructure platforms, including bare metal, virtualized devices such as hypervisors, and cloud networks. It integrates well with legacy applications and existing automated scripts, and is designed to manage the complex, multi-faceted facilities found in large businesses. Ansible supports idempotent behavior, which means it can place the node into the same state every time. This is necessary for consistency and standardized behavior.","title":"The Main Uses for Ansible"},{"location":"devops/iac/#iac","text":"Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. IaC can be applied throughout the lifecycle, both on the initial build, as well as throughout the life of the infrastructure. Commonly, these are referred to as Day 0 and Day 1 activities. \u201cDay 0\u201d code provisions and configures your initial infrastructure. \u201cDay 0\u201d code provisions and configures your initial infrastructure. If your infrastructure never changes after the initial build (no OS updates, no patches, no app configurations, etc.) then you may not need tools that support subsequent updates, changes, and expansions. \u201cDay 1\u201d refers to OS and application configurations you apply after you\u2019ve initially built your infrastructure. IaC makes changes idempotent, consistent, repeatable, and predictable. With IaC, we can test the code and review the results before the code is applied to our target environments. Should a result not align to our expectations, we iterate on the code until the results pass our tests and align to our expectations. Following this pattern allows for the outcome to be predicted before the code is applied to a production environment. Once ready for use, we can then apply that code via automation, at scale, ensuring consistency and repeatability in how it is applied. Since code is checked into version control systems such as GitHub, GitLab, BitBucket, etc., it is possible to review how the infrastructure evolves over time. The idempotent characteristic provided by IaC tools ensures that, even if the same code is applied multiple times, the result remains the same. Steps to Define your Infrastructure Scope : Identify the Infrastructure for your project. Author : Write configuration to define your infrastructure. Initialize : Install the required Terraform providers. Plan : Preview the changes Terraform will make. Apply : Make the changes to your infrastructure. Using Terraform has several advantages over manually managing your infrastructure: Terraform can manage infrastructure on multiple cloud platforms. The human-readable configuration language helps you write infrastructure code quickly. Terraform's state allows you to track resource changes throughout your deployments. You can commit your configurations to version control to safely collaborate on infrastructure. The terraform {} block contains Terraform settings, including the required providers Terraform will use to provision your infrastructure. A provider is a plugin that Terraform uses to create and manage your resources. Use resource blocks to define components of your infrastructure. A resource might be a physical or virtual component such as an EC2 instance, or it can be a logical resource such as a Heroku application. Resource blocks have two strings before the block: the resource type and the resource name.","title":"IaC"},{"location":"devops/questions/","text":"Personal \u00b6 Tell us about your roles and responsibilties Any key learnings which you are proud off Devops \u00b6 How will you plan a new Devops process for a Microservices Architecture? What are the different phases in a Devops process and design it? How will you convince a customer and teams to onboard to Devops practise? What are Devops metrics? Plan and Design a CI process and how will you include Secops in it to make it Devsecops? Difference between CI and CD and explain that with a help of Gitflow strategy? Explain which branch is used in which process? Suppose a bug comes in PR, how is the branching strategy affected? How will you sync branches which are in Testing phases with the Bugfix? What are deployment strategies? (Answer Blue-Green, Canary, Rolling) What are the drawbacks and how to select an appropriate strategy in a microservices architecture? Difference between git fetch and git pull Linux \u00b6 How does TLS authentication happen between a client and a server? How does DNS resolution happen? Troubleshoot a Linux Server by checking various metrics? Trobleshoot an application that is running as a service? How will you find a file in the filesystem with a text \"Hello\" when location or filename is not known? Significance of EXIT command? How will catch EXIT in a shell script (hint Trap) Pipelines \u00b6 How is a pipeline triggered and describe the automation process? Jenkinsfile and how is a shared library used? Maven and build process plugins tht you have worked with? What are steps present in a CI pipeline? What base images did you use? How are various images for tooling maintained using pipelines? Central pipeline libraries and how will you trigger them from code repos which have the calling function? How will you build and deploy images that have been modified in a pipeline? Image maintenance and the way to update them in an automated manner if the base image is modified? Docker \u00b6 Dockerfile structure and its main components? Explian entire process of building and deploying a image to Dockerhub or a private registry? Difference between CMD and RUN? Difference between CMD and ENTRYPOINT? Remove unused docker containers from a machine without removing the volumes? How will you exec into a running container? Kubernetes \u00b6 Describe Kubernetes components? Describe K8s Networking viz Container to Container communication, Pod to Pod communication and Node to Node communication? Describe a Deployment resource structure and how is a service defined / mapped within it? How will you scale an appliciation managed by a Deployment object without modifying the YAML? Describe an Ingress resource structure? How to link a Ingress resource to a correct Ingress controller in a multi-controller environment? Troubleshoot an application in a given environment, describe each step from identifying the cluster to navigating to the namespace? Troubleshoot master plane when kubectl access is not present? Helm \u00b6 Basic Structure of a Helmchart and explain? What will happen when you deploy a helmchart and the resources already exists How will you sync resources in an environment which have been modified outside the helm upgrade process? How will you pass multiple values using files in command line? Terraform \u00b6 What are terraform modules? What are terraform providers? Terraform variables and how are they used? How will you override a value during execution (Hint: using tfvars)? Variables defined in vars.tf and inside TFE? Which one will be picked up? How will you override them during execution? How are different environments mapped in a single terraform modules","title":"Personal"},{"location":"devops/questions/#personal","text":"Tell us about your roles and responsibilties Any key learnings which you are proud off","title":"Personal"},{"location":"devops/questions/#devops","text":"How will you plan a new Devops process for a Microservices Architecture? What are the different phases in a Devops process and design it? How will you convince a customer and teams to onboard to Devops practise? What are Devops metrics? Plan and Design a CI process and how will you include Secops in it to make it Devsecops? Difference between CI and CD and explain that with a help of Gitflow strategy? Explain which branch is used in which process? Suppose a bug comes in PR, how is the branching strategy affected? How will you sync branches which are in Testing phases with the Bugfix? What are deployment strategies? (Answer Blue-Green, Canary, Rolling) What are the drawbacks and how to select an appropriate strategy in a microservices architecture? Difference between git fetch and git pull","title":"Devops"},{"location":"devops/questions/#linux","text":"How does TLS authentication happen between a client and a server? How does DNS resolution happen? Troubleshoot a Linux Server by checking various metrics? Trobleshoot an application that is running as a service? How will you find a file in the filesystem with a text \"Hello\" when location or filename is not known? Significance of EXIT command? How will catch EXIT in a shell script (hint Trap)","title":"Linux"},{"location":"devops/questions/#pipelines","text":"How is a pipeline triggered and describe the automation process? Jenkinsfile and how is a shared library used? Maven and build process plugins tht you have worked with? What are steps present in a CI pipeline? What base images did you use? How are various images for tooling maintained using pipelines? Central pipeline libraries and how will you trigger them from code repos which have the calling function? How will you build and deploy images that have been modified in a pipeline? Image maintenance and the way to update them in an automated manner if the base image is modified?","title":"Pipelines"},{"location":"devops/questions/#docker","text":"Dockerfile structure and its main components? Explian entire process of building and deploying a image to Dockerhub or a private registry? Difference between CMD and RUN? Difference between CMD and ENTRYPOINT? Remove unused docker containers from a machine without removing the volumes? How will you exec into a running container?","title":"Docker"},{"location":"devops/questions/#kubernetes","text":"Describe Kubernetes components? Describe K8s Networking viz Container to Container communication, Pod to Pod communication and Node to Node communication? Describe a Deployment resource structure and how is a service defined / mapped within it? How will you scale an appliciation managed by a Deployment object without modifying the YAML? Describe an Ingress resource structure? How to link a Ingress resource to a correct Ingress controller in a multi-controller environment? Troubleshoot an application in a given environment, describe each step from identifying the cluster to navigating to the namespace? Troubleshoot master plane when kubectl access is not present?","title":"Kubernetes"},{"location":"devops/questions/#helm","text":"Basic Structure of a Helmchart and explain? What will happen when you deploy a helmchart and the resources already exists How will you sync resources in an environment which have been modified outside the helm upgrade process? How will you pass multiple values using files in command line?","title":"Helm"},{"location":"devops/questions/#terraform","text":"What are terraform modules? What are terraform providers? Terraform variables and how are they used? How will you override a value during execution (Hint: using tfvars)? Variables defined in vars.tf and inside TFE? Which one will be picked up? How will you override them during execution? How are different environments mapped in a single terraform modules","title":"Terraform"},{"location":"ide/","text":"IDE Tips and Tricks \u00b6 VS Code \u00b6 Intellij \u00b6 How to open multiple modules in the same workspace? Open the first module / project. Select File -> Project Structure -> Modules. Select Copy (icon) and open Browse your local directory and select the project you would like to add. Module name will resolve automatically. When you want to reopen to project with multiple sub-projects, in order to avoid re-doing steps as described above, just go to File -> Open Recent -> 'Your Big Project'. IDE Shortcuts Option Description Ctrl + Space Code Completion Ctrl + Shift + / Comments selected lines of code Ctrl + N Navigate to any class by typing the name in the editor Ctrl + B Jump to declaration","title":"IDE Tips and Tricks"},{"location":"ide/#ide-tips-and-tricks","text":"","title":"IDE Tips and Tricks"},{"location":"ide/#vs-code","text":"","title":"VS Code"},{"location":"ide/#intellij","text":"How to open multiple modules in the same workspace? Open the first module / project. Select File -> Project Structure -> Modules. Select Copy (icon) and open Browse your local directory and select the project you would like to add. Module name will resolve automatically. When you want to reopen to project with multiple sub-projects, in order to avoid re-doing steps as described above, just go to File -> Open Recent -> 'Your Big Project'. IDE Shortcuts Option Description Ctrl + Space Code Completion Ctrl + Shift + / Comments selected lines of code Ctrl + N Navigate to any class by typing the name in the editor Ctrl + B Jump to declaration","title":"Intellij"},{"location":"ide/markdown/","text":"General Syntax \u00b6 MarkDown Cheat Sheet Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Horizontal Rule ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks Emphasis Bold **rendered as bold text** Italics _rendered as italicized text_ BlockQuote > Blockquotes Nested BlockQuote > First Level >> Second Level >>> Third Level List Unordered * or + or - Ordered 1. Code Block ` or [```html] (3 backticks and follwed by the language) Links Inline Links [Text](http://text.io) Link titles [Text](https://github.com/site/ \"Visit Site!\") Images ![Minion](http://octodex.github.com/images/minion.png) Using Mkdocs formatting \u00b6 Tabbed Data Inline Examples Output p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} , p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} . Markdown $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ , \\( p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } \\) . Adding Tips Inline Configuration This is an example of a tip. Make the paragragh tabbed inline with the heading Adding Danger Reminder This is a call to action Adding Note Note Adding notes Adding Summary Summary This is to sumarize the information New Information New 7.1 New Info Adding Note Collapsible tab Click Me! Thanks! Success Collapsible tab Success Content. Warning Collapsible tab Warning Content. Adding Settings Gear Basic Software Setup sudo apt install Adding multi-level collapisble tabs Details must contain a blank line before they start. Use ??? to start a details block or ???+ if you want to start a details block whose default state is 'open'. Follow the start of the block with an optional class or classes (separated with spaces) and the summary contained in quotes. Content is placed below the header and must be indented. Open styled details Nested details! And more content again. Adding checklist inside summary Tasklist eggs bread milk Adding strikethrough and subscript Tilde Tilde is syntactically built around the ~ character. It adds support for inserting sub scripts and adds an easy way to place text in a < del > tag. Showing Critic changes Critic Added CSS changes in extra.css to activate Critic change higlights. This is deleted This is added Showing Emojis Inline Code Highlighting InlineHilite utilizes the following syntax to insert inline highlighted code: `:::language mycode` or `#!language mycode` . Inline Highlighted Code Example Here is some code: import pymdownx ; pymdownx . __version__ . The mock shebang will be treated like text here: #!js var test = 0; . Marking words Mark Example mark me Preserve Tab spaces ============================================================ T Tp Sp D Dp S D7 T ------------------------------------------------------------ A F#m Bm E C#m D E7 A A# Gm Cm F Dm D# F7 A# B\u266d Gm Cm F Dm E\u266dm F7 B\u266d Showing Line Number in code import foo.bar import car - Highlighting specific line numbers \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz Highlighting line range and specific lines import foo import boo.baz import foo.bar.baz class Foo : def __init__ ( self ): self . foo = None self . bar = None self . baz = None","title":"Markdown"},{"location":"ide/markdown/#general-syntax","text":"MarkDown Cheat Sheet Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Horizontal Rule ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks Emphasis Bold **rendered as bold text** Italics _rendered as italicized text_ BlockQuote > Blockquotes Nested BlockQuote > First Level >> Second Level >>> Third Level List Unordered * or + or - Ordered 1. Code Block ` or [```html] (3 backticks and follwed by the language) Links Inline Links [Text](http://text.io) Link titles [Text](https://github.com/site/ \"Visit Site!\") Images ![Minion](http://octodex.github.com/images/minion.png)","title":"General Syntax"},{"location":"ide/markdown/#using-mkdocs-formatting","text":"Tabbed Data Inline Examples Output p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} , p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} . Markdown $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ , \\( p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } \\) . Adding Tips Inline Configuration This is an example of a tip. Make the paragragh tabbed inline with the heading Adding Danger Reminder This is a call to action Adding Note Note Adding notes Adding Summary Summary This is to sumarize the information New Information New 7.1 New Info Adding Note Collapsible tab Click Me! Thanks! Success Collapsible tab Success Content. Warning Collapsible tab Warning Content. Adding Settings Gear Basic Software Setup sudo apt install Adding multi-level collapisble tabs Details must contain a blank line before they start. Use ??? to start a details block or ???+ if you want to start a details block whose default state is 'open'. Follow the start of the block with an optional class or classes (separated with spaces) and the summary contained in quotes. Content is placed below the header and must be indented. Open styled details Nested details! And more content again. Adding checklist inside summary Tasklist eggs bread milk Adding strikethrough and subscript Tilde Tilde is syntactically built around the ~ character. It adds support for inserting sub scripts and adds an easy way to place text in a < del > tag. Showing Critic changes Critic Added CSS changes in extra.css to activate Critic change higlights. This is deleted This is added Showing Emojis Inline Code Highlighting InlineHilite utilizes the following syntax to insert inline highlighted code: `:::language mycode` or `#!language mycode` . Inline Highlighted Code Example Here is some code: import pymdownx ; pymdownx . __version__ . The mock shebang will be treated like text here: #!js var test = 0; . Marking words Mark Example mark me Preserve Tab spaces ============================================================ T Tp Sp D Dp S D7 T ------------------------------------------------------------ A F#m Bm E C#m D E7 A A# Gm Cm F Dm D# F7 A# B\u266d Gm Cm F Dm E\u266dm F7 B\u266d Showing Line Number in code import foo.bar import car - Highlighting specific line numbers \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz Highlighting line range and specific lines import foo import boo.baz import foo.bar.baz class Foo : def __init__ ( self ): self . foo = None self . bar = None self . baz = None","title":"Using Mkdocs formatting"},{"location":"ide/vim/","text":"Vim \u00b6 Achieving comfort with the editor helps you focus on solving the K8s exam challenges VIM Tutorial # Vim config to be set in the exam terminal echo \"set ts=2 sts=2 sw=2 et number ai\" >> ~/.vimrc source ~/.vimrc ################ # These stand for: # # ts - tabstop to indent using 2 spaces on pressing tab key # sts - softtabstop to move 2 cursor spaces on pressing tab key # sw - shiftwidth to shift by 2 spaces on pressing tab key # et - expandtab to insert space character instead of tab on pressing tab key # number for line numbers while editing # There is one additional useful config - ai to allow autoindent on pressing return key (but this messes when # copy pasting text) Shortcuts \u00b6 <linenumber>G # Go to line number :set paste # Tells vim to shutdown autoindent and makes it ready for paste # Say you copy paste in wrong position and want to tab all the lines shift + v + arrow up or down # selects all the lines for movement <number of places to indent>+> # 2> will indent by 2 places all the selected lines # Similarly to unindent 2< # OR Shift + > # OR Shift + . # Say when copying from html you also copy hidden Tab characters :set list # Shows all hidden Tab characters. ^I is for Tab :retab # Fix this issue by replacing ^I with spaces # Handle multiline values like certificate data which spans multiple lines in YAML without leaving the editor # Open the file and go to the line where you want to add this data. # Add a | character to tell YAML this is a multi line value like request: | and Press enter :read !base64 certificate.pem # This will invoke base64 command on data from .pem file and read it into the next line. # After the data is copied, indent 2> after selecting the entire contents.","title":"Vim"},{"location":"ide/vim/#vim","text":"Achieving comfort with the editor helps you focus on solving the K8s exam challenges VIM Tutorial # Vim config to be set in the exam terminal echo \"set ts=2 sts=2 sw=2 et number ai\" >> ~/.vimrc source ~/.vimrc ################ # These stand for: # # ts - tabstop to indent using 2 spaces on pressing tab key # sts - softtabstop to move 2 cursor spaces on pressing tab key # sw - shiftwidth to shift by 2 spaces on pressing tab key # et - expandtab to insert space character instead of tab on pressing tab key # number for line numbers while editing # There is one additional useful config - ai to allow autoindent on pressing return key (but this messes when # copy pasting text)","title":"Vim"},{"location":"ide/vim/#shortcuts","text":"<linenumber>G # Go to line number :set paste # Tells vim to shutdown autoindent and makes it ready for paste # Say you copy paste in wrong position and want to tab all the lines shift + v + arrow up or down # selects all the lines for movement <number of places to indent>+> # 2> will indent by 2 places all the selected lines # Similarly to unindent 2< # OR Shift + > # OR Shift + . # Say when copying from html you also copy hidden Tab characters :set list # Shows all hidden Tab characters. ^I is for Tab :retab # Fix this issue by replacing ^I with spaces # Handle multiline values like certificate data which spans multiple lines in YAML without leaving the editor # Open the file and go to the line where you want to add this data. # Add a | character to tell YAML this is a multi line value like request: | and Press enter :read !base64 certificate.pem # This will invoke base64 command on data from .pem file and read it into the next line. # After the data is copied, indent 2> after selecting the entire contents.","title":"Shortcuts"},{"location":"k8s/","text":"Kubernetes \u00b6","title":"Kubernetes"},{"location":"k8s/#kubernetes","text":"","title":"Kubernetes"},{"location":"k8s/1-tips/","text":"alias k=kubectl # will already be pre-configured export do=\"\u2013dry-run=client -o yaml\" # k create deploy nginx \u2013image=nginx $do export now=\"\u2013force \u2013grace-period 0\" # k delete pod x $now alias ke='kubectl explain' alias pe='k explain po \u2013recursive' alias kgp='k get po' alias kns='k config set-context \u2013current \u2013namespace' https://hackernoon.com/ckad-and-cka-certifications-which-to-take-first-and-how-to-prepare-bh4437mc https://www.nisheetsinvhal.com/how-i-scored-a-perfect-100-on-cka/ CNCF Tips # Vim config to be set in the exam terminal echo \"set ts=2 sts=2 sw=2 et number ai\" >> ~/.vimrc source ~/.vimrc # This command will ensure that you set the namespace correctly for your current context. kubectl config view --minify | grep namespace # To test the service, launch a temporary POD with curl kubectl curl --image = alpine/curl -it --rm -- sh # Aliases ######## Choose any style # Step 1: enabled auto-complete feature in the bash shell after setting the aliases source < ( kubectl completion bash ) echo \"source <(kubectl completion bash)\" >> ~/.bashrc complete -F __start_kubectl k alias k = kubectl alias kr = 'k run' alias krun = \"k run -h | grep '# ' -A2\" alias kg = 'k get' alias kd = 'k describe' alias ke = 'kubectl explain' alias kaf = 'k apply -f' alias kdf = 'k delete -f' alias kdp = 'k delete po' alias kgp = 'k get po' # if you\u2019re sure that the Pod that you\u2019re deleting has no Persistent Volumes attached to it (or other resources external to the Pod), then use below command alias kpd = 'k delete pod --force --grace-period=0' # Step 2: create short cut using env variable export dr = \"--dry-run=client -o yaml\" # Using k run mypod --image=nginx $dr > mypod.yaml # Step 3: To avoid typing namespaces in the imperative command, set it once and then type all commands k config get-contexts # Copy paste the set-context and use-context alias kns = \"kubectl config set-context --current --namespace\" # Set the alias as above # Set the namespace where you want to execute the commands, like in myns kns myns # Tip: always remember to switch back to default namespace for the next question kns default # Changing namespaces # Suppose your have namespaces test1, test2, test3 kubectl -n test1 get pods # Now run the last command but in different namespace like test2 ^test1^test2 # Show all the Cert files in ETCD configuration cat /etc/kubernetes/manifest/etcd.yaml | grep file # *** Important - Store the YAML files in home folder wth question number, so in case of review, you can verify and apply it say in correct namespace # For Windows: Ctrl+Insert to copy and Shift+Insert to paste. # In addition, you might find it helpful to use the Notepad (see top menu under 'Exam Controls') to manipulate text before pasting to the command line. # Only a single terminal console is available during the exam. Terminal multiplexers such as GNU Screen and tmux can be used to create virtual consoles. # You can switch the cluster/configuration context using a command such as the following: kubectl config use-context <cluster/context name> # Nodes making up each cluster can be reached via ssh, using a command such as the following: ssh <nodename> # You can assume elevated privileges on any node by issuing the following command: sudo -i # You can also use `sudo` to execute commands with elevated privileges at any time # You must return to the base node (`hostname node-1`) after completing each task. # When you want to get some data quickly from a node without getting a shell ssh <nodename> < command to execute> # This will give you the output instead of changing nodes # You can use kubectl and the appropriate context to work on any cluster from the base node. # When connected to a cluster member via ssh, you will only be able to work on that particular cluster via kubectl. # For your convenience, all environments, in other words, the base system and the cluster nodes, have the following additional command-line tools pre-installed and pre-configured: # - kubectl with k alias and Bash autocompletion # - jq for YAML/JSON processing # - tmux for terminal multiplexing # - curl and wget for testing web services # - man and man pages for further documentation # Where no explicit namespace is specified, the default namespace should be acted upon. # If you need to destroy/recreate a resource to perform a certain task, it is your responsibility to back up the resource definition appropriately prior to destroying the resource. # Dont waste time waiting for prompt to come back when deleting pods, use the --force flag <Careful> Shortcuts po Pod rs ReplciaSet deploy Deployment svc Service ds DaemonSet ns Namespace netpol Network Policy pv Persistent Volume pvc Persistent Volume Claims sa Service Account sts StatefulSets Implicit Commands # Do not copy paste from documentation into OS Editor. Copy into Notepad and then make changes. [Not recommended] to avoid wasting time in formatting. Type in Editor directly or use implicit commands # Use Nano as preferred editor KUBE_EDITOR = nano kubectl edit deploy nginx # Nano Editor Shortcuts # Set Context and Namespace kubectl config set-context <cluster> --namespace = myns # Copy this command in notepad and save time # Explain Commands # When is it useful: sometimes when editing/creating yaml files, it is not clear where exaclty rsource should be placed (indented) in the file. Using this command gives a quick overview of resources structure as well as helpful explanation. Sometimes this is faster then looking up in k8s docs. kubectl explain cronjob.spec.jobTemplate --recursive | less kubectl explain pods.spec.containers --recursive | less # Generators --restart --dry-run # Tip kubectl describe po # Without po name, it will describe all pods which saves time in typing # If the edit to po is not saved, do a force replace # First exit the editor using q!, then execute the replace command kubectl replace --force -f /tmp/<temp-file> # This is the temp location k8s stores the edit yaml kubectl run nginx --image = nginx # (deployment) kubectl run nginx --image = nginx --restart = Never # (pod) kubectl create job nginx --image = nginx --dry-run = client -o yaml > job.yml #(job) kubectl create cronjob nginx --image = nginx --dry-run = client \\ --schedule = \"* * * * *\" -o yaml > cronjob.yml # (cronJob) kubectl run nginx -image = nginx \\ --restart = Never \\ --port = 80 \\ --namespace = myname \\ --serviceaccount = mysa1 \\ --env = HOSTNAME = local \\ --labels = bu = finance,env = dev \\ --requests = 'cpu=100m,memory=256Mi' \\ --limits = 'cpu=200m,memory=512Mi' \\ --dry-run -o yaml - /bin/sh -c 'echo hello world' > pod.yaml kubectl create deployment frontend --replicas = 2 \\ --labels = run = load-balancer-example --image = busybox --port = 8080 kubectl expose deployment frontend --type = NodePort --name = frontend-service --port = 6262 --target-port = 8080 # OR kubectl create service clusterip my-cs --tcp = 5678 :8080 --dry-run -o yaml kubectl set serviceaccount deployment frontend myuser # If we specify two selectors separated by a comma, only the objects that satisfy both will be returned. This is a logical AND operation: kubectl get pods --selector = \"bu=finance,env=dev\" # We can also ask if a label is one of a set of values. Here we ask for all pods where the app label is set to alpaca bandicoot (which will be all six pods): kubectl get pods --selector = \"env in (dev,test)\" # Use of grep when selector filter doesnt work kubectl describe pods | grep --context = 10 annotations: kubectl describe pods | grep --context = 10 Events: # Check last 10 events on pod k describe pod <pod-name> | grep -i events -A 10 # Determine proper api_group/version for a resource # When is it useful: after creating/modyfing pod or during troubleshooting exercise check quickly if there are no errors in pod k api-resources | grep -i \"resource name\" k api-versions | grep -i \"api_group name\" # Example: k api-resources | grep -i deploy # -> produces apps in APIGROUPS column k api-versions | grep -i apps # -> produces apps/v1 # Quickly find kube api server setting # When is it useful: since on all the exams, kubernetes services are running as pods, it is faster to check settings with grep rather than move to folder and look at the file. ps -ef --forest | grep kube-apiserver | grep \"search string\" # Example: ps -ef --forest | grep kube-apiserver | grep admission-plugins # -> find admission plugins config # Use busybox for running utilities # When is it useful: this command will create temporary busybox pod. Full features of Busybox - https://busybox.net/downloads/BusyBox.html kubectl run -it --rm debug --image = busybox --restart = Never -- sh # Verify pod connectivity # When it is useful: when making changes to a pod, it is very important to veryify if it works. One of the best ways to verify is to check pod connectivity. If successful this command will return a response. kubectl run -it --rm debug --image = radial/busyboxplus:curl --restart = Never -- curl http://servicename # There is no way to add environment variable from a Secret or ConfigMap imperatively from CLI. So use the `--env SOMETHING=this --dry-run -o yaml` to generate a quick template then vim edit it to match the desired configuration. This is very useful considering you cannot copy-paste a whole yaml from documentation to the exam terminal. # Create k8s resource on the fly from copied YAML # When is it useful: sometimes it's quicker to just grab YAML from k8s documentation page and create a resource much quicker than writting YAML yourself cat <<EOF | kubectl create -f - <YAML content goes here> EOF # Command alternative: alternatively use cat > filename.yaml [ enter ] [ Ctrl + Shift - to paste file content ] [ enter - adds one line to the file ] [ Ctrl + C - exit ] # after that use vim/nano to edit the file and create resource based on it # Save time on editing and re-creating running pods. # During the exams you are often asked to change existing pod spec. This usually requires: # 1. saving pod config as yaml file kubectl get po <pod name> <optional -n namespace> -o yaml > <filename>.yaml # check if file was correctly saves cat <filename>.yaml # 2. deleting existing pod kubectl delete po <pod name> <optional -n namespace> --wait = false # 3. editing the file and making required changes vim <or nano> <filename>.yaml # 4. creating new pod from the file kubectl create -f <filename>.yaml # The other template to remember is that of a Pod. It is especially useful for creating Static Pods on other Nodes. # Do not skip the part about jsonpath thinking that it is too easy to come in the exam. Remember that jsonpath is required for sorting output and custom columns. # Unix Bash one-liners #if-else a = 10 ; b = 5 ; if [ $a -le $b ] ; then echo \"a is small\" ; else echo \"b is small\" ; fi # while x = 1 ; while [ $x -le 10 ] ; do echo \"welcome $x times\" ; x = $(( x+1 )) ; done # for PODS = $( kubectl get pods -o jsonpath -template = '{.items[*].metadata.name}' ) for x in $PODS ; do kubectl delete pods ${ x } sleep 60 done # Examples args: [ \"-c\" , \"while true;do date >> /var/log/app.txt;sleep 5;done\" ] args: [ /bin/sh, -c, 'i=0; while true; do echo \"$i:$(date)\";i=$((i+1))\";sleep 1;done' ] args: [ \"-c\" , \"mkdir -p collect;while true;do cat /var/data/* > /collect/data.txt;sleep 5;done\" ] # Create file with implicit commands kubectl run busybox --image = busybox --dry-run = client -o yaml --restart = Never > yamlfile.yaml kubectl create job my-job --dry-run = client -o yaml --image = busybox -- date > yamlfile.yaml kubectl get deploy/nginx -o yaml > 1 .yaml kubectl run wordpress --image = wordpress --expose --port = 8989 --restart = Never -o yaml # Command shouls always at the end and all kubectl options before this kubectl run test --image = busybox --restart = Never --dry-run = client -o yaml -- /bin/sh -c 'echo test;sleep 100' > yamlfile.yaml # OR kubectl run test --image = busybox --restart = Never --dry-run = client -o yaml -- command sleep 1000 > yamlfile.yaml # (Notice that -- /bin/sh comes at the end. This will create yaml file.) kubectl run busybox --image = busybox --dry-run = client -o yaml --restart = Never -- /bin/sh -c \"while true; do echo hello; echo hello again;done\" > pod.yaml # Test script to check if the application is live behind a service and is loadbalancing. # A curl pod is running in kube-public namespace for testing for i in { 1 ..20 } ; do kubectl exec --namespace = kube-public curl -- sh -c 'test=`wget -qO- -T 2 http://webapp-service.default.svc.cluster.local:8080/ready 2>&1` && echo \"$test OK\" || echo \"Failed\"' ; echo \"\" done # Type the above command and hit enter, Linux will substitute the namespace and rerun the get pods command ################# alias k = 'kubectl' alias kc = 'k config view --minify | grep name' # Many pods are created to save time when you go wrong during the exam alias kpd = 'k delete pod --force --grace-period=0' alias kdp = 'kubectl describe pod' alias krh = 'kubectl run --help | more' alias kgh = 'kubectl get --help | more' alias c = 'clear' alias kd = 'kubectl describe' alias ke = 'kubectl explain' alias kf = 'kubectl create -f' alias kg = 'kubectl get pods --show-labels' alias kr = 'kubectl replace -f' alias kh = 'kubectl --help | more' alias krh = 'kubectl run --help | more' alias kgn = 'kubectl get namespaces' alias l = 'ls -lrt' alias ll = 'vi ls -rt | tail -1' alias kga = 'k get pod --all-namespaces' alias kgaa = 'kubectl get all --show-labels' # Viewing resource utilization kubectl top node kubectl top pod watch kubectl top node -n 5 # Runs the command every 5 sec - Linux Hard Way Mummshad - Kubernetes the Hard Way Tools used \u00b6 vim kubectl kubeadm systemctl & journalctl nslookup Vim \u00b6 General Tips \u00b6 Flag Questions You Can\u2019t Immediately Answer The exam environment comes with built-in flagging functionality. If you read a question that you don\u2019t immediately know how to answer, flag it. Flagged questions will be highlighted in your question list, allowing you to return to them quickly once you\u2019ve finished answering the ones you\u2019re comfortable with. Keep Score Along with flagging capabilities, the exam provides you with a built-in notepad. Make use of the notepad to keep a running tally of questions you\u2019ve answered and their percentage value (this number is given to you with each question). Format of scoring Question Number %weight Total I like to keep a list of the questions I haven\u2019t answered and their associated percentages too. It\u2019s a good way to inform how you should prioritise your time, particularly towards the end of the exam. If you're cutting it fine with the 74% pass mark, your time is probably better spent on a 7% question, than a 2% one. Save YAML Files by Question Number If you have to create a YAML file when answering a question, for example a Pod config file, make sure you name the file according to the question number. I rely heavily on a solid muscle memory for all commands. The most atomic part of a cluster is a Pod so creating one through kubectl run --generator=run-pod/v1 nginx --image nginx should be out under 10 seconds or so. Even though the exam allows you to copy-paste content from the official documentation, it has a limit of 2-3 lines so I had to be prepared to \u2013dry-run -o yaml > q2.yaml each time and edit the file as per the question. Useful commands # list running processes $ ps -aux # search for string in the output $ ps -aux | grep -i 'string' # search for multiple expressions in the output (exp can be plain a string too) $ ps -aux | grep -e 'exp-one' -e 'exp-two' # get details about network interfaces $ ifconfig # list network interfaces and their IP address $ ip a # get the route details $ ip r # check service status and also show logs $ systemctl status kubelet # restart a service $ systemctl restart kubelet # reload the service daemon, if you changed the service file $ systemctl daemon reload # detailed logs of the service $ journalctl -u kubelet # list out ports, protocol and what processes are listening on those ports $ netstat -tunlp #-tupan Troubleshooting Tips \u00b6 What is the scope of the issue? > Entire Custer > User > Pod > Service How Long the issue has been going on? What can be done to reproduce the issue? Establish a Probable cause. Develop a Hypothesis \u2192 > Clue 1 > + Clue 2 > + Kubernetes Knowledge ======================== Probable Cause Experiment - Test your Hypothesis","title":"1 tips"},{"location":"k8s/1-tips/#tools-used","text":"vim kubectl kubeadm systemctl & journalctl nslookup","title":"Tools used"},{"location":"k8s/1-tips/#vim","text":"","title":"Vim"},{"location":"k8s/1-tips/#general-tips","text":"Flag Questions You Can\u2019t Immediately Answer The exam environment comes with built-in flagging functionality. If you read a question that you don\u2019t immediately know how to answer, flag it. Flagged questions will be highlighted in your question list, allowing you to return to them quickly once you\u2019ve finished answering the ones you\u2019re comfortable with. Keep Score Along with flagging capabilities, the exam provides you with a built-in notepad. Make use of the notepad to keep a running tally of questions you\u2019ve answered and their percentage value (this number is given to you with each question). Format of scoring Question Number %weight Total I like to keep a list of the questions I haven\u2019t answered and their associated percentages too. It\u2019s a good way to inform how you should prioritise your time, particularly towards the end of the exam. If you're cutting it fine with the 74% pass mark, your time is probably better spent on a 7% question, than a 2% one. Save YAML Files by Question Number If you have to create a YAML file when answering a question, for example a Pod config file, make sure you name the file according to the question number. I rely heavily on a solid muscle memory for all commands. The most atomic part of a cluster is a Pod so creating one through kubectl run --generator=run-pod/v1 nginx --image nginx should be out under 10 seconds or so. Even though the exam allows you to copy-paste content from the official documentation, it has a limit of 2-3 lines so I had to be prepared to \u2013dry-run -o yaml > q2.yaml each time and edit the file as per the question. Useful commands # list running processes $ ps -aux # search for string in the output $ ps -aux | grep -i 'string' # search for multiple expressions in the output (exp can be plain a string too) $ ps -aux | grep -e 'exp-one' -e 'exp-two' # get details about network interfaces $ ifconfig # list network interfaces and their IP address $ ip a # get the route details $ ip r # check service status and also show logs $ systemctl status kubelet # restart a service $ systemctl restart kubelet # reload the service daemon, if you changed the service file $ systemctl daemon reload # detailed logs of the service $ journalctl -u kubelet # list out ports, protocol and what processes are listening on those ports $ netstat -tunlp #-tupan","title":"General Tips"},{"location":"k8s/1-tips/#troubleshooting-tips","text":"What is the scope of the issue? > Entire Custer > User > Pod > Service How Long the issue has been going on? What can be done to reproduce the issue? Establish a Probable cause. Develop a Hypothesis \u2192 > Clue 1 > + Clue 2 > + Kubernetes Knowledge ======================== Probable Cause Experiment - Test your Hypothesis","title":"Troubleshooting Tips"},{"location":"k8s/2-exam/","text":"CKA exam testing \u00b6 Preparation \u00b6 Q: Create a Job that run 60 time with 2 jobs running in parallel https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ Q: Find which Pod is taking max CPU Use kubectl top to find CPU usage per pod Q: List all PersistentVolumes sorted by their name Use kubectl get pv --sort-by= <- this problem is buggy & also by default kubectl give the output sorted by name. Q: Create a NetworkPolicy to allow connect to port 8080 by busybox pod only https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ Make sure to use apiVersion: extensions/v1beta1 which works on both 1.6 and 1.7 Q: fixing broken nodes, see https://kubernetes.io/docs/concepts/architecture/nodes/ Q: etcd backup, see https://kubernetes.io/docs/getting-started-guides/ubuntu/backups/ https://www.mirantis.com/blog/everything-you-ever-wanted-to-know-about-using-etcd-with-kubernetes-v1-6-but-were-afraid-to-ask/ Q: TLS bootstrapping, see https://coreos.com/kubernetes/docs/latest/openssl.html https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/ cloudflare/cfssl Q: You have a Container with a volume mount. Add a init container that creates an empty file in the volume. (only trick is to mount the volume to init-container as well) https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo The app is running! && sleep 3600'] volumeMounts: - mountPath: /cache name: cache-volume initContainers: - name: init-touch-file image: busybox volumeMounts: - mountPath: /data name: cache-volume command: ['sh', '-c', 'echo \"\" > /data/harshal.txt'] volumes: - name: cache-volume emptyDir: {} ```` Q: When running a redis key-value store in your pre-production environments many deployments are incoming from CI and leaving behind a lot of stale cache data in redis which is causing test failures. The CI admin has requested that each time a redis key-value-store is deployed in staging that it not persist its data. Create a pod named non-persistent-redis that specifies a named-volume with name app-cache, and mount path /data/redis. It should launch in the staging namespace and the volume MUST NOT be persistent. Create a Pod with EmptyDir and in the YAML file add namespace: CI Q: Setting up K8s master components with a binaries/from tar balls: Also, convert CRT to PEM: openssl x509 -in abc.crt -out abc.pem - https://coreos.com/kubernetes/docs/latest/openssl.html - https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md - https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/08-bootstrapping-kubernetes-controllers.md - https://gist.github.com/mhausenblas/0e09c448517669ef5ece157fd4a5dc4b - https://kubernetes.io/docs/getting-started-guides/scratch/ - http://alexander.holbreich.org/kubernetes-on-ubuntu/ maybe dashboard? - https://kubernetes.io/docs/getting-started-guides/binary_release/ - http://kamalmarhubi.com/blog/2015/09/06/kubernetes-from-the-ground-up-the-api-server/ Q: Find the error message with the string \u201cSome-error message here\u201d. https://kubernetes.io/docs/concepts/cluster-administration/logging/ see kubectl logs and /var/log for system services Q 17: Create an Ingress resource, Ingress controller and a Service that resolves to cs.rocks.ch. First, create controller and default backend ```BASH kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress/master/controllers/nginx/examples/default-backend.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/nginx-ingress-controller.yaml Second, create service and expose ``` kubectl run ingress-pod \u2013image=nginx \u2013port 80 kubectl expose deployment ingress-pod \u2013port=80 \u2013target-port=80 \u2013type=NodePort Create the ingress ``` cat <<EOF >ingress-cka.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-service spec: rules: - host: \"cs.rocks.ch\" http: paths: - backend: serviceName: ingress-pod servicePort: 80 EOF To test, run a curl pod kubectl run -i --tty client --image=tutum/curl curl -I -L --resolve cs.rocks.ch:80:10.240.0.5 http://cs.rocks.ch/ Q: Run a Jenkins Pod on a specified node only. https://kubernetes.io/docs/tasks/administer-cluster/static-pod/ Create the Pod manifest at the specified location and then edit the systemd service file for kubelet(/etc/systemd/system/kubelet.service) to include --pod-manifest-path=/specified/path . Once done restart the service. Q: Use the utility nslookup to look up the DNS records of the service and pod. From this guide, https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ Look for \u201cQuick Diagnosis\u201d $ kubectl exec -ti busybox \u2013 nslookup mysvc.myns.svc.cluster.local Naming conventions for services and pods: For a regular service, this resolves to the port number and the CNAME: my-svc.my-namespace.svc.cluster.local. For a headless service, this resolves to multiple answers, one for each pod that is backing the service, and contains the port number and a CNAME of the pod of the form auto-generated-name.my-svc.my-namespace.svc.cluster.local When enabled, pods are assigned a DNS A record in the form of pod-ip-address.my-namespace.pod.cluster.local. For example, a pod with IP 1.2.3.4 in the namespace default with a DNS name of cluster.local would have an entry: 1-2-3-4.default.pod.cluster.local Q: Start a pod automatically by keeping manifest in /etc/kubernetes/manifests Refer to https://kubernetes.io/docs/tasks/administer-cluster/static-pod/ Edit kubelet.service on any worker node to contain this flag \u2013pod-manifest-path=/etc/kubernetes/manifests then place the pod manifest at /etc/kubernetes/manifests. Now restart kubelet. Some other Questions: Main container looks for a file and crashes if it doesnt find the file. Write an init container to create the file and make it available for the main container Install and Configure kubelet on a node to run pod on that node without contacting the api server Take backup of etcd cluster rotate TLS certificates 5.rolebinding 6.Troubleshooting - involved identifying failing nodes, pods , services and identifying cpu utilization of pods. General Questions \u00b6 Backup/restore etcd on specific location using certificates. Fix the broken node. You should check kubelet process. Mostly you have to start and enable for permanent change. Create network policy to allow incoming connection from specific namespace , port combination. They might ask from specific pods. Two questions related to jsonpath , you can output the resource in json format and then find the details. They ask to create ingress . Please note ingress controller was already there. One question related to sidecar container. Trick is you have to mount the volume of on the side container. Question related to which pod consume most cpu. You don\u2019t need to install metrics server. Topics to focus on \u00b6 Ingress Role and Role Binding Service Accounts PV and PVC Volumes Network Policy Services Check logs from Pods, Scale Deployment and replica ETCD Backup and restore TLS Bootstraping Question 1 | Contexts Task weight: 1 % You have access to multiple clusters from your main terminal through kubectl contexts. Write all those context names into /opt/course/1/contexts. Next write a command to display the current context into /opt/course/1/context_default_kubectl.sh, the command should use kubectl. Finally write a second command doing the same thing into /opt/course/1/context_default_no_kubectl.sh, but without the use of kubectl. Answer: k config get-contexts # copy manually # OR k config get-contexts -o name > /opt/course/1/contexts # /opt/course/1/context_default_kubectl.sh kubectl config current-context # /opt/course/1/context_default_no_kubectl.sh cat ~/.kube/config | grep current Question 2 | Schedule Pod on Master Node Task weight: 3 % Use context: kubectl config use-context k8s-c1-H Create a single Pod of image httpd:2.4.41-alpine in Namespace default. The Pod should be named pod1 and the container should be named pod1-container. This Pod should only be scheduled on a master node, do not add new labels any nodes. Answer: First we find the master node ( s ) and their taints: k get node # find master node k describe node cluster1-master1 | grep Taint -A3 # get master node taints k get node cluster1-master1 --show-labels # get master node labels # NOTE: In K8s 1.24 master/controlplane nodes have two Taints which means we have to add Tolerations for both. This is done during transitioning from the wording \"master\" to \"controlplane\". Next we create the Pod template: # check the export on the very top of this document so we can use $do k run pod1 --image = httpd:2.4.41-alpine $do > 2 .yaml # vim 2.yaml tolerations : # add - effect : NoSchedule # add key : node-role.kubernetes.io/master # add - effect : NoSchedule # add key : node-role.kubernetes.io/control-plane # add nodeSelector : # add node-role.kubernetes.io/control-plane : \"\" # add # Important here to add the toleration for running on master nodes, but also the nodeSelector to make sure it only runs on master nodes. If we only specify a toleration the Pod can be scheduled on master or worker nodes. Question 3 | Scale down StatefulSet Task weight: 1 % Use context: kubectl config use-context k8s-c1-H There are two Pods named o3db-* in Namespace project-c13. C13 management asked you to scale the Pods down to one replica to save resources. Answer: If we check the Pods we see two replicas: k -n project-c13 get pod | grep o3db # From their name it looks like these are managed by a StatefulSet. But if we're not sure we could also check for the most common resources which manage Pods: k -n project-c13 get deploy,ds,sts | grep o3db #Confirmed, we have to work with a StatefulSet. To find this out we could also look at the Pod labels: k -n project-c13 get pod --show-labels | grep o3db # To fulfil the task we simply run: k -n project-c13 scale sts o3db --replicas 1 Question 4 | Pod Ready if Service is reachable Task weight: 4 % Use context: kubectl config use-context k8s-c1-H # Do the following in Namespace default. Create a single Pod named ready-if-service-ready of image nginx:1.16.1-alpine. Configure a LivenessProbe which simply runs true. Also configure a ReadinessProbe which does check if the url http://service-am-i-ready:80 is reachable, you can use wget -T2 -O- http://service-am-i-ready:80 for this. Start the Pod and confirm it isn't ready because of the ReadinessProbe. Create a second Pod named am-i-ready of image nginx:1.16.1-alpine with label id: cross-server-ready. The already existing Service service-am-i-ready should now have that second Pod as endpoint. Now the first Pod should be in ready state, confirm that. Answer: # It's a bit of an anti-pattern for one Pod to check another Pod for being ready using probes, hence the normally available readinessProbe.httpGet doesn't work for absolute remote urls. Still the workaround requested in this task should show how probes and Pod<->Service communication works. First we create the first Pod: k run ready-if-service-ready --image = nginx:1.16.1-alpine $do > 4_pod1.yaml # And confirm its in a non-ready state: k get pod ready-if-service-ready # We can also check the reason for this using describe: k describe pod ready-if-service-ready # Now we create the second Pod: k run am-i-ready --image = nginx:1.16.1-alpine --labels = \"id=cross-server-ready\" # The already existing Service service-am-i-ready should now have an Endpoint: k describe svc service-am-i-ready k get ep # also possible Which will result in our first Pod being ready, just give it a minute for the Readiness probe to check again: k get pod ready-if-service-ready # 4_pod1.yaml livenessProbe : # add from here exec : command : - 'true' readinessProbe : exec : command : - sh - -c - 'wget -T2 -O- http://service-am-i-ready:80' # to here Question 5 | Kubectl sorting Task weight: 1 % Use context: kubectl config use-context k8s-c1-H There are various Pods in all namespaces. Write a command into /opt/course/5/find_pods.sh which lists all Pods sorted by their AGE ( metadata.creationTimestamp ) . Write a second command into /opt/course/5/find_pods_uid.sh which lists all Pods sorted by field metadata.uid. Use kubectl sorting for both commands. Answer: A good resources here ( and for many other things ) is the kubectl-cheat-sheet. You can reach it fast when searching for \"cheat sheet\" in the Kubernetes docs. # /opt/course/5/find_pods.sh kubectl get pod -A --sort-by = .metadata.creationTimestamp For the second command: # /opt/course/5/find_pods_uid.sh kubectl get pod -A --sort-by = .metadata.uid Question 6 | Storage, PV, PVC, Pod volume Task weight: 8 % Use context: kubectl config use-context k8s-c1-H Create a new PersistentVolume named safari-pv. It should have a capacity of 2Gi, accessMode ReadWriteOnce, hostPath /Volumes/Data and no storageClassName defined. Next create a new PersistentVolumeClaim in Namespace project-tiger named safari-pvc . It should request 2Gi storage, accessMode ReadWriteOnce and should not define a storageClassName. The PVC should bound to the PV correctly. Finally create a new Deployment safari in Namespace project-tiger which mounts that volume at /tmp/safari-data. The Pods of that Deployment should be of image httpd:2.4.41-alpine. Answer Create PV and PV using k8s docs. Next we create a Deployment and mount that volume: k -n project-tiger create deploy safari \\ --image = httpd:2.4.41-alpine $do > 6_dep.yaml We can confirm its mounting correctly: k -n project-tiger describe pod safari-5cbf46d6d-mjhsb | grep -A2 Mounts: # 6_pv.yaml kind : PersistentVolume apiVersion : v1 metadata : name : safari-pv spec : capacity : storage : 2Gi accessModes : - ReadWriteOnce hostPath : path : \"/Volumes/Data\" # 6_pvc.yaml kind : PersistentVolumeClaim apiVersion : v1 metadata : name : safari-pvc namespace : project-tiger spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi # 6_dep.yaml spec : volumes : # add - name : data # add persistentVolumeClaim : # add claimName : safari-pvc # add containers : - image : httpd:2.4.41-alpine name : container volumeMounts : # add - name : data # add mountPath : /tmp/safari-data # add Question 7 | Node and Pod Resource Usage Task weight: 1 % Use context: kubectl config use-context k8s-c1-H The metrics-server has been installed in the cluster. Your college would like to know the kubectl commands to: show Nodes resource usage show Pods and their **containers** resource usage Please write the commands into /opt/course/7/node.sh and /opt/course/7/pod.sh. Answer: The command we need to use here is top: k top -h We create the first file: # /opt/course/7/node.sh kubectl top node For the second file we might need to check the docs again: k top pod -h # /opt/course/7/pod.sh kubectl top pod --containers = true Question 8 | Get Master Information Task weight: 2 % Use context: kubectl config use-context k8s-c1-H # Ssh into the master node with ssh cluster1-master1. Check how the master components kubelet, kube-apiserver, kube-scheduler, kube-controller-manager and etcd are started/installed on the master node. Also find out the name of the DNS application and how it's started/installed on the master node. Write your findings into file /opt/course/8/master-components.txt. The file should be structured like: # /opt/course/8/master-components.txt kubelet: [ TYPE ] kube-apiserver: [ TYPE ] kube-scheduler: [ TYPE ] kube-controller-manager: [ TYPE ] etcd: [ TYPE ] dns: [ TYPE ] [ NAME ] Choices of [ TYPE ] are: not-installed, process, static-pod, pod Answer: We could start by finding processes of the requested components, especially the kubelet at first: ssh cluster1-master1 ps aux | grep kubelet # shows kubelet process We can see which components are controlled via systemd looking at /etc/systemd/system directory: find /etc/systemd/system/ | grep kube find /etc/systemd/system/ | grep etcd # This shows kubelet is controlled via systemd, but no other service named kube nor etcd. It seems that this cluster has been setup using kubeadm, so we check in the default manifests directory: find /etc/kubernetes/manifests/ # (The kubelet could also have a different manifests directory specified via parameter --pod-manifest-path in it's systemd startup config) # This means the main 4 master services are setup as static Pods. Actually, let's check all Pods running on in the kube-system Namespace on the master node: kubectl -n kube-system get pod -o wide | grep master1 # There we see the 5 static pods, with -cluster1-master1 as suffix. # We also see that the dns application seems to be coredns, but how is it controlled? kubectl -n kube-system get ds kubectl -n kube-system get deploy Seems like coredns is controlled via a Deployment. We combine our findings in the requested file: # /opt/course/8/master-components.txt kubelet: process kube-apiserver: static-pod kube-scheduler: static-pod kube-controller-manager: static-pod etcd: static-pod dns: pod coredns Question 9 | Kill Scheduler, Manual Scheduling Task weight: 5 % Use context: kubectl config use-context k8s-c2-AC Ssh into the master node with ssh cluster2-master1. Temporarily stop the kube-scheduler, this means in a way that you can start it again afterwards. Create a single Pod named manual-schedule of image httpd:2.4-alpine, confirm its created but not scheduled on any node. # Now you're the scheduler and have all its power, manually schedule that Pod on node cluster2-master1. Make sure it's running. # Start the kube-scheduler again and confirm its running correctly by creating a second Pod named manual-schedule2 of image httpd:2.4-alpine and check if it's running on cluster2-worker1. Answer: Stop the Scheduler First we find the master node: k get node Then we connect and check if the scheduler is running: ssh cluster2-master1 kubectl -n kube-system get pod | grep schedule Kill the Scheduler ( temporarily ) : cd /etc/kubernetes/manifests/ mv kube-scheduler.yaml .. And it should be stopped: kubectl -n kube-system get pod | grep schedule Create a Pod Now we create the Pod: k run manual-schedule --image = httpd:2.4-alpine # And confirm it has no node assigned: k get pod manual-schedule -o wide Manually schedule the Pod # Let's play the scheduler now: k get pod manual-schedule -o yaml > 9 .yaml nodeName: cluster2-master1 # add the master node name The only thing a scheduler does, is that it sets the nodeName for a Pod declaration. As we cannot kubectl apply or kubectl edit , in this case we need to delete and create or replace: k -f 9 .yaml replace --force k get pod manual-schedule -o wide # It looks like our Pod is running on the master now as requested, although no tolerations were specified. Only the scheduler takes taints/tolerations/affinity into account when finding the correct node name. That's why its still possible to assign Pods manually directly to a master node and skip the scheduler. Start the scheduler again ssh cluster2-master1 cd /etc/kubernetes/manifests/ mv ../kube-scheduler.yaml . Schedule a second test Pod: k run manual-schedule2 --image = httpd:2.4-alpine Question 10 | RBAC ServiceAccount Role RoleBinding Task weight: 6 % Use context: kubectl config use-context k8s-c1-H Create a new ServiceAccount processor in Namespace project-hamster. Create a Role and RoleBinding, both named processor as well. These should allow the new SA to only create Secrets and ConfigMaps in that Namespace. Answer: # Let's talk a little about RBAC resources A ClusterRole | Role defines a set of permissions and where it is available, in the whole cluster or just a single Namespace. A ClusterRoleBinding | RoleBinding connects a set of permissions with an account and defines where it is applied, in the whole cluster or just a single Namespace. Because of this there are 4 different RBAC combinations and 3 valid ones: 1 . Role + RoleBinding ( available in single Namespace, applied in single Namespace ) 2 . ClusterRole + ClusterRoleBinding ( available cluster-wide, applied cluster-wide ) 3 . ClusterRole + RoleBinding ( available cluster-wide, applied in single Namespace ) 4 . Role + ClusterRoleBinding ( NOT POSSIBLE: available in single Namespace, applied cluster-wide ) # To the solution We first create the ServiceAccount: k -n project-hamster create sa processor Then for the Role: k -n project-hamster create role processor \\ --verb = create \\ --resource = secret \\ --resource = configmap \\ --namespace = project-hamster # Now we bind the Role to the ServiceAccount: k -n project-hamster create rolebinding processor \\ --role processor \\ --serviceaccount project-hamster:processor \\ --namespace = project-hamster To test our RBAC setup we can use kubectl auth can-i: k -n project-hamster auth can-i create secret \\ --as system:serviceaccount:project-hamster:processor # yes k -n project-hamster auth can-i create pod \\ --as system:serviceaccount:project-hamster:processor # no Question 11 | DaemonSet on all Nodes Task weight: 4 % Use context: kubectl config use-context k8s-c1-H Use Namespace project-tiger for the following. Create a DaemonSet named ds-important with image httpd:2.4-alpine and labels id = ds-important and uuid = 18426a0b-5f59-4e10-923f-c0e078e82462. The Pods it creates should request 10 millicore cpu and 10 mebibyte memory. The Pods of that DaemonSet should run on all nodes, master and worker. Answer: # As of now we aren't able to create a DaemonSet directly using kubectl, so we create a Deployment and just change it up: k -n project-tiger create deployment --image = httpd:2.4-alpine ds-important --labels = \"id=ds-important,uuid=18426a0b-5f59-4e10-923f-c0e078e82462\" $do > 11 .yaml # NOTE: In K8s 1.24 master/controlplane nodes have two Taints which means we have to add Tolerations for both. This is done during transitioning from the wording \"master\" to \"controlplane\". It was requested that the DaemonSet runs on all nodes, so we need to specify the toleration for this. # 11.yaml apiVersion : apps/v1 kind : DaemonSet # change from Deployment to Daemonset metadata : namespace : project-tiger # important spec : #replicas: 1 # remove #strategy: {} # remove template : spec : containers : tolerations : # add - effect : NoSchedule # add key : node-role.kubernetes.io/master # add - effect : NoSchedule # add key : node-role.kubernetes.io/control-plane # add #status: {} # remove Question 12 | Deployment on all Nodes Task weight: 6 % Use context: kubectl config use-context k8s-c1-H Use Namespace project-tiger for the following. Create a Deployment named deploy-important with label id = very-important ( the Pods should also have this label ) and 3 replicas. It should contain two containers, the first named container1 with image nginx:1.17.6-alpine and the second one named container2 with image kubernetes/pause. # There should be only ever one Pod of that Deployment running on one worker node. We have two worker nodes: cluster1-worker1 and cluster1-worker2. Because the Deployment has three replicas the result should be that on both nodes one Pod is running. The third Pod won't be scheduled, unless a new worker node will be added. # In a way we kind of simulate the behaviour of a DaemonSet here, but using a Deployment and a fixed number of replicas. Answer: There are two possible ways, one using podAntiAffinity and one using topologySpreadConstraint. PodAntiAffinity The idea here is that we create a \"Inter-pod anti-affinity\" which allows us to say a Pod should only be scheduled on a node where another Pod of a specific label ( here the same label ) is not already running. # Let's begin by creating the Deployment template: k -n project-tiger create deployment \\ --image = nginx:1.17.6-alpine deploy-important $do > 12 .yaml # 12.yaml affinity : # add podAntiAffinity : # add requiredDuringSchedulingIgnoredDuringExecution : # add - labelSelector : # add matchExpressions : # add - key : id # add operator : In # add values : # add - very-important # add topologyKey : kubernetes.io/hostname # add # Specify a topologyKey, which is a pre-populated Kubernetes label, you can find this by describing a node. Question 13 | Multi Containers and Pod shared Volume Task weight: 4 % Use context: kubectl config use-context k8s-c1-H # Create a Pod named multi-container-playground in Namespace default with three containers, named c1, c2 and c3. There should be a volume attached to that Pod and mounted into every container, but the volume shouldn't be persisted or shared with other Pods. Container c1 should be of image nginx:1.17.6-alpine and have the name of the node where its Pod is running available as environment variable MY_NODE_NAME. Container c2 should be of image busybox:1.31.1 and write the output of the date command every second in the shared volume into file date.log. You can use while true ; do date >> /your/vol/path/date.log ; sleep 1 ; done for this. Container c3 should be of image busybox:1.31.1 and constantly send the content of file date.log from the shared volume to stdout. You can use tail -f /your/vol/path/date.log for this. Check the logs of container c3 to confirm correct setup. Answer: First we create the Pod template: k run multi-container-playground --image = nginx:1.17.6-alpine $do > 13 .yaml And add the other containers and the commands they should execute: we check if container c1 has the requested node name as env variable: k exec multi-container-playground -c c1 -- env | grep MY And finally we check the logging: k logs multi-container-playground -c c3 # 13.yaml apiVersion : v1 kind : Pod metadata : creationTimestamp : null labels : run : multi-container-playground name : multi-container-playground spec : containers : - image : nginx:1.17.6-alpine name : c1 # change resources : {} env : # add - name : MY_NODE_NAME # add valueFrom : # add fieldRef : # add fieldPath : spec.nodeName # add volumeMounts : # add - name : vol # add mountPath : /vol # add - image : busybox:1.31.1 # add name : c2 # add command : [ \"sh\" , \"-c\" , \"while true; do date >> /vol/date.log; sleep 1; done\" ] # add volumeMounts : # add - name : vol # add mountPath : /vol # add - image : busybox:1.31.1 # add name : c3 # add command : [ \"sh\" , \"-c\" , \"tail -f /vol/date.log\" ] # add volumeMounts : # add - name : vol # add mountPath : /vol # add dnsPolicy : ClusterFirst restartPolicy : Always volumes : # add - name : vol # add emptyDir : {} # add status : {} Question 14 | Find out Cluster Information Task weight: 2 % Use context: kubectl config use-context k8s-c1-H # You're ask to find out following information about the cluster k8s-c1-H: How many master nodes are available? How many worker nodes are available? What is the Service CIDR? Which Networking ( or CNI Plugin ) is configured and where is its config file? Which suffix will static pods have that run on cluster1-worker1? Write your answers into file /opt/course/14/cluster-info, structured like this: # /opt/course/14/cluster-info 1 : [ ANSWER ] 2 : [ ANSWER ] 3 : [ ANSWER ] 4 : [ ANSWER ] 5 : [ ANSWER ] Answer: How many master and worker nodes are available? We see one master and two workers. What is the Service CIDR? ssh cluster1-master1 cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep range Which Networking ( or CNI Plugin ) is configured and where is its config file? find /etc/cni/net.d/ cat /etc/cni/net.d/10-weave.conflist By default the kubelet looks into /etc/cni/net.d to discover the CNI plugins. This will be the same on every master and worker nodes. Which suffix will static pods have that run on cluster1-worker1? The suffix is the node hostname with a leading hyphen. It used to be -static in earlier Kubernetes versions. Result The resulting /opt/course/14/cluster-info could look like: # /opt/course/14/cluster-info # How many master nodes are available? 1 : 1 # How many worker nodes are available? 2 : 2 # What is the Service CIDR? 3 : 10 .96.0.0/12 # Which Networking (or CNI Plugin) is configured and where is its config file? 4 : Weave, /etc/cni/net.d/10-weave.conflist # Which suffix will static pods have that run on cluster1-worker1? 5 : -cluster1-worker1 Question 15 | Cluster Event Logging Task weight: 3 % Use context: kubectl config use-context k8s-c2-AC Write a command into /opt/course/15/cluster_events.sh which shows the latest events in the whole cluster, ordered by time. Use kubectl for it. Now kill the kube-proxy Pod running on node cluster2-worker1 and write the events this caused into /opt/course/15/pod_kill.log. Finally kill the containerd container of the kube-proxy Pod on node cluster2-worker1 and write the events into /opt/course/15/container_kill.log. Do you notice differences in the events both actions caused? Answer: # /opt/course/15/cluster_events.sh kubectl get events -A --sort-by = .metadata.creationTimestamp Now we kill the kube-proxy Pod: k -n kube-system get pod -o wide | grep proxy # find pod running on cluster2-worker1 k -n kube-system delete pod kube-proxy-z64cg Now check the events: sh /opt/course/15/cluster_events.sh Write the events the killing caused into /opt/course/15/pod_kill.log Finally we will try to provoke events by killing the container belonging to the container of the kube-proxy Pod: ssh cluster2-worker1 crictl ps | grep kube-proxy crictl stop 1e020b43c4423 crictl rm 1e020b43c4423 crictl ps | grep kube-proxy We killed the main container ( 1e020b43c4423 ) , but also noticed that a new container ( 0ae4245707910 ) was directly created. Thanks Kubernetes! Now we see if this caused events again and we write those into the second file: sh /opt/course/15/cluster_events.sh # Comparing the events we see that when we deleted the whole Pod there were more things to be done, hence more events. For example was the DaemonSet in the game to re-create the missing Pod. Where when we manually killed the main container of the Pod, the Pod would still exist but only its container needed to be re-created, hence less events. Question 16 | Namespaces and Api Resources Task weight: 2 % Use context: kubectl config use-context k8s-c1-H Create a new Namespace called cka-master. Write the names of all namespaced Kubernetes resources ( like Pod, Secret, ConfigMap... ) into /opt/course/16/resources.txt. Find the project-* Namespace with the highest number of Roles defined in it and write its name and amount of Roles into /opt/course/16/crowded-namespace.txt. Answer: Namespace and Namespaces Resources We create a new Namespace: k create ns cka-master Now we can get a list of all resources like: k api-resources --namespaced -o name > /opt/course/16/resources.txt Namespace with most Roles k -n project-c13 get role --no-headers | wc -l k -n project-c14 get role --no-headers | wc -l # 300 Find all other namespaces Finally we write the name and amount into the file: # /opt/course/16/crowded-namespace.txt project-c14 with 300 resources Question 17 | Find Container of Pod and check info Task weight: 3 % Use context: kubectl config use-context k8s-c1-H In Namespace project-tiger create a Pod named tigers-reunite of image httpd:2.4.41-alpine with labels pod = container and container = pod. Find out on which node the Pod is scheduled. Ssh into that node and find the containerd container belonging to that Pod. Using command crictl: Write the ID of the container and the info.runtimeType into /opt/course/17/pod-container.txt Write the logs of the container into /opt/course/17/pod-container.log Answer: First we create the Pod: k -n project-tiger run tigers-reunite \\ --image = httpd:2.4.41-alpine \\ --labels \"pod=container,container=pod\" # Next we find out the node it's scheduled on: k -n project-tiger get pod -o wide Then we ssh into that node and and check the container info: ssh cluster1-worker2 crictl ps | grep tigers-reunite crictl inspect b01edbe6f89ed | grep runtimeType Then we fill the requested file ( on the main terminal ) : # /opt/course/17/pod-container.txt b01edbe6f89ed io.containerd.runc.v2 Finally we write the container logs in the second file: ssh cluster1-worker2 'crictl logs b01edbe6f89ed' & > /opt/course/17/pod-container.log # The &> in above's command redirects both the standard output and standard error. You could also simply run crictl logs on the node and copy the content manually, if its not a lot. The file should look like: Question 18 | Fix Kubelet Task weight: 8 % Use context: kubectl config use-context k8s-c3-CCC There seems to be an issue with the kubelet not running on cluster3-worker1. Fix it and confirm that cluster has node cluster3-worker1 available in Ready state afterwards. You should be able to schedule a Pod on cluster3-worker1 afterwards. Write the reason of the issue into /opt/course/18/reason.txt. Answer: The procedure on tasks like these should be to check if the kubelet is running, if not start it, then check its logs and correct errors if there are some. Always helpful to check if other clusters already have some of the components defined and running, so you can copy and use existing config files. Though in this case it might not need to be necessary. Check node status: k get node First we check if the kubelet is running: ssh cluster3-worker1 ps aux | grep kubelet Nope, so we check if its configured using systemd as service: service kubelet status # Yes, its configured as a service with config at /etc/systemd/system/kubelet.service.d/10-kubeadm.conf, but we see its inactive. Let's try to start it: service kubelet start We see its trying to execute /usr/local/bin/kubelet with some parameters defined in its service config file. A good way to find errors and get more logs is to run the command manually ( usually also with its parameters ) . /usr/local/bin/kubelet # -bash: /usr/local/bin/kubelet: No such file or directory whereis kubelet # kubelet: /usr/bin/kubelet Another way would be to see the extended logging of a service like using journalctl -u kubelet. Well, there we have it, wrong path specified. Correct the path in file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf and run: systemctl daemon-reload && systemctl restart kubelet Finally we write the reason into the file: # /opt/course/18/reason.txt wrong path to kubelet binary specified in service config Question 19 | Create Secret and mount into Pod Task weight: 3 % NOTE: This task can only be solved if questions 18 or 20 have been successfully implemented and the k8s-c3-CCC cluster has a functioning worker node Use context: kubectl config use-context k8s-c3-CCC Do the following in a new Namespace secret. Create a Pod named secret-pod of image busybox:1.31.1 which should keep running for some time. There is an existing Secret located at /opt/course/19/secret1.yaml, create it in the Namespace secret and mount it readonly into the Pod at /tmp/secret1. # Create a new Secret in Namespace secret called secret2 which should contain user=user1 and pass=1234. These entries should be available inside the Pod's container as environment variables APP_USER and APP_PASS. Confirm everything is working. Answer First we create the Namespace and the requested Secrets in it: k create ns secret cp /opt/course/19/secret1.yaml 19_secret1.yaml k -f 19_secret1.yaml create Next we create the second Secret: k -n secret create secret generic secret2 --from-literal = user = user1 --from-literal = pass = 1234 Now we create the Pod template: k -n secret run secret-pod --image = busybox:1.31.1 $do -- sh -c \"sleep 5d\" > 19 .yaml # It might not be necessary in current K8s versions to specify the readOnly: true because it's the default setting anyways. Finally we check if all is correct: k -n secret exec secret-pod -- env | grep APP k -n secret exec secret-pod -- find /tmp/secret1 env : # add - name : APP_USER # add valueFrom : # add secretKeyRef : # add name : secret2 # add key : user # add - name : APP_PASS # add valueFrom : # add secretKeyRef : # add name : secret2 # add key : pass # add volumeMounts : # add - name : secret1 # add mountPath : /tmp/secret1 # add readOnly : true # add volumes : # add - name : secret1 # add secret : # add secretName : secret1 # add Question 20 | Update Kubernetes Version and join cluster Task weight: 10 % Use context: kubectl config use-context k8s-c3-CCC # Your coworker said node cluster3-worker2 is running an older Kubernetes version and is not even part of the cluster. Update Kubernetes on that node to the exact version that's running on cluster3-master1. Then add this node to the cluster. Use kubeadm for this. Answer: Master node seems to be running Kubernetes 1 .24.1 and cluster3-worker2 is not yet part of the cluster. ssh cluster3-worker2 kubeadm version # kubeadm version matches kubectl version # kubectl version is old kubelet --version # kubelet version is old kubeadm upgrade node This is usually the proper command to upgrade a node. But this error means that this node was never even initialised, so nothing to update here. This will be done later using kubeadm join. For now we can continue with kubelet and kubectl: apt update apt show kubectl -a | grep 1 .24 apt install kubectl = 1 .24.1-00 kubelet = 1 .24.1-00 # Now we're up to date with kubeadm, kubectl and kubelet. Restart the kubelet: systemctl restart kubelet We can ignore the errors and move into next step to generate the join command. # Add cluster3-worker2 to cluster First we log into the master1 and generate a new TLS bootstrap token, also printing out the join command: ssh cluster3-master1 kubeadm token create --print-join-command kubeadm token list Next we connect again to cluster3-worker2 and simply execute the join command: ssh cluster3-worker2 kubeadm join 192 .168.100.31:6443 --token <token> Question 21 | Create a Static Pod and Service Task weight: 2 % Use context: kubectl config use-context k8s-c3-CCC Create a Static Pod named my-static-pod in Namespace default on cluster3-master1. It should be of image nginx:1.16-alpine and have resource requests for 10m CPU and 20Mi memory. Then create a NodePort Service named static-pod-service which exposes that static Pod on port 80 and check if it has Endpoints and if its reachable through the cluster3-master1 internal IP address. You can connect to the internal node IPs from your main terminal. Answer: ssh cluster3-master1 cd /etc/kubernetes/manifests/ kubectl run my-static-pod \\ --image = nginx:1.16-alpine \\ -o yaml --dry-run = client > my-static-pod.yaml And make sure its running: k get pod -A | grep my-static Now we expose that static Pod: k expose pod my-static-pod-cluster3-master1 \\ --name static-pod-service \\ --type = NodePort \\ --port 80 Then run and test: k get svc,ep -l run = my-static-pod # /etc/kubernetes/manifests/my-static-pod.yaml resources : requests : cpu : 10m memory : 20Mi Question 22 | Check how long certificates are valid Task weight: 2 % Use context: kubectl config use-context k8s-c2-AC Check how long the kube-apiserver server certificate is valid on cluster2-master1. Do this with openssl or cfssl. Write the exipiration date into /opt/course/22/expiration. Also run the correct kubeadm command to list the expiration dates and confirm both methods show the same date. Write the correct kubeadm command that would renew the apiserver server certificate into /opt/course/22/kubeadm-renew-certs.sh. Answer: # First let's find that certificate: ssh cluster2-master1 find /etc/kubernetes/pki | grep apiserver Next we use openssl to find out the expiration date: openssl x509 -noout -text -in /etc/kubernetes/pki/apiserver.crt | grep Validity -A2 There we have it, so we write it in the required location on our main terminal: # /opt/course/22/expiration Jan 14 18 :49:40 2022 GMT And we use the feature from kubeadm to get the expiration too: kubeadm certs check-expiration | grep apiserver Looking good. And finally we write the command that would renew all certificates into the requested location: # /opt/course/22/kubeadm-renew-certs.sh kubeadm certs renew apiserver Question 23 | Kubelet client/server cert info Task weight: 2 % Use context: kubectl config use-context k8s-c2-AC Node cluster2-worker1 has been added to the cluster using kubeadm and TLS bootstrapping. Find the \"Issuer\" and \"Extended Key Usage\" values of the cluster2-worker1: kubelet client certificate, the one used for outgoing connections to the kube-apiserver. kubelet server certificate, the one used for incoming connections from the kube-apiserver. Write the information into file /opt/course/23/certificate-info.txt. Compare the \"Issuer\" and \"Extended Key Usage\" fields of both certificates and make sense of these. Answer: To find the correct kubelet certificate directory, we can look for the default value of the --cert-dir parameter for the kubelet. For this search for \"kubelet\" in the Kubernetes docs which will lead to: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet. We can check if another certificate directory has been configured using ps aux or in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf. First we check the kubelet client certificate: ssh cluster2-worker1 openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep Issuer openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep \"Extended Key Usage\" -A1 Next we check the kubelet server certificate: openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet.crt | grep Issuer openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet.crt | grep \"Extended Key Usage\" -A1 We see that the server certificate was generated on the worker node itself and the client certificate was issued by the Kubernetes api. The \"Extended Key Usage\" also shows if its for client or server authentication. More about this: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping Question 24 | NetworkPolicy Task weight: 9 % Use context: kubectl config use-context k8s-c1-H There was a security incident where an intruder was able to access the whole cluster from a single hacked backend Pod. To prevent this create a NetworkPolicy called np-backend in Namespace project-snake. It should allow the backend-* Pods only to: connect to db1-* Pods on port 1111 connect to db2-* Pods on port 2222 Use the app label of Pods in your policy. After implementation, connections from backend-* Pods to vault-* Pods on port 3333 should for example no longer work. Answer: First we look at the existing Pods and their labels: k -n project-snake get pod k -n project-snake get pod -L app We test the current connection situation and see nothing is restricted: k -n project-snake get pod -o wide k -n project-snake exec backend-0 -- curl -s 10 .44.0.25:1111 k -n project-snake exec backend-0 -- curl -s 10 .44.0.23:2222 k -n project-snake exec backend-0 -- curl -s 10 .44.0.22:3333 Now we create the NP by copying and chaning an example from the k8s docs: The NP below has two rules with two conditions each, it can be read as: allow outgoing traffic if : ( destination pod has label app = db1 AND port is 1111 ) OR ( destination pod has label app = db2 AND port is 2222 ) We create the correct NP: k -f 24_np.yaml create And test again: k -n project-snake exec backend-0 -- curl -s 10 .44.0.25:1111 k -n project-snake exec backend-0 -- curl -s 10 .44.0.23:2222 k -n project-snake exec backend-0 -- curl -s 10 .44.0.22:3333 # 24_np.yaml apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : np-backend namespace : project-snake spec : podSelector : matchLabels : app : backend policyTypes : - Egress # policy is only about Egress egress : - # first rule to : # first condition \"to\" - podSelector : matchLabels : app : db1 ports : # second condition \"port\" - protocol : TCP port : 1111 - # second rule to : # first condition \"to\" - podSelector : matchLabels : app : db2 ports : # second condition \"port\" - protocol : TCP port : 2222 Question 25 | Etcd Snapshot Save and Restore Task weight: 8 % Use context: kubectl config use-context k8s-c3-CCC Make a backup of etcd running on cluster3-master1 and save it on the master node at /tmp/etcd-backup.db. Then create a Pod of your kind in the cluster. Finally restore the backup, confirm the cluster is still working and that the created Pod is no longer with us. Etcd Backup First we log into the master and try to create a snapshop of etcd: ssh cluster3-master1 ETCDCTL_API = 3 etcdctl snapshot save /tmp/etcd-backup.db But it fails because we need to authenticate ourselves. For the necessary information we can check the etc manifest: vim /etc/kubernetes/manifests/etcd.yaml # OR But we also know that the api-server is connecting to etcd, so we can check how its manifest is configured: cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep etcd We use the authentication information and pass it to etcdctl: ETCDCTL_API = 3 etcdctl snapshot save /tmp/etcd-backup.db \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key # NOTE: Dont use snapshot status because it can alter the snapshot file and render it invalid Etcd restore Now create a Pod in the cluster and wait for it to be running: kubectl run test --image = nginx # NOTE: If you didn't solve questions 18 or 20 and cluster3 doesn't have a ready worker node then the created pod might stay in a Pending state. This is still ok for this task. Next we stop all controlplane components: cd /etc/kubernetes/manifests/ mv * .. watch crictl ps Now we restore the snapshot into a specific directory: ETCDCTL_API = 3 etcdctl snapshot restore /tmp/etcd-backup.db \\ --data-dir /var/lib/etcd-backup \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key We could specify another host to make the backup from by using etcdctl --endpoints http://IP, but here we just use the default value which is: http://127.0.0.1:2379,http://127.0.0.1:4001. The restored files are located at the new folder /var/lib/etcd-backup, now we have to tell etcd to use that directory: vim /etc/kubernetes/etcd.yaml - hostPath: path: /var/lib/etcd-backup # change Now we move all controlplane yaml again into the manifest directory. Give it some time ( up to several minutes ) for etcd to restart and for the api-server to be reachable again: mv ../*.yaml . watch crictl ps Then we check again for the Pod: kubectl get pod -l run = test Awesome, backup and restore worked as our pod is gone. Extra Question 1 | Find Pods first to be terminated Use context: kubectl config use-context k8s-c1-H Check all available Pods in the Namespace project-c13 and find the names of those that would probably be terminated first if the nodes run out of resources ( cpu or memory ) to schedule all Pods. Write the Pod names into /opt/course/e1/pods-not-stable.txt. Answer: When available cpu or memory resources on the nodes reach their limit, Kubernetes will look for Pods that are using more resources than they requested. These will be the first candidates for termination. If some Pods containers have no resource requests/limits set, then by default those are considered to use more than requested. Kubernetes assigns Quality of Service classes to Pods based on the defined resources and limits, read more here: https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod Hence we should look for Pods without resource requests defined, we can do this with a manual approach: k -n project-c13 describe pod | less -p Requests # describe all pods and highlight Requests k -n project-c13 describe pod | egrep \"^(Name:| Requests:)\" -A1 # We see that the Pods of Deployment c13-3cc-runner-heavy don't have any resources requests specified. Hence our answer would be: Hence our answer would be: # /opt/course/e1/pods-not-stable.txt c13-3cc-runner-heavy-65588d7d6-djtv9map c13-3cc-runner-heavy-65588d7d6-v8kf5map c13-3cc-runner-heavy-65588d7d6-wwpb4map o3db-0 o3db-1 # maybe not existing if already removed via previous scenario To automate this process you could use jsonpath like this: k -n project-c13 get pod \\ -o jsonpath = \"{range .items[*]} {.metadata.name}{.spec.containers[*].resources}{'\\n'}\" Or we look for the Quality of Service classes: k get pods -n project-c13 \\ -o jsonpath = \"{range .items[*]}{.metadata.name} {.status.qosClass}{'\\n'}\" # Here we see three with BestEffort, which Pods get that don't have any memory or cpu limits or requests defined. # A good practice is to always set resource requests and limits. If you don't know the values your containers should have you can find this out using metric tools like Prometheus. You can also use kubectl top pod or even kubectl exec into the container and use top and similar tools. Extra Question 2 | Curl Manually Contact API Use context: kubectl config use-context k8s-c1-H There is an existing ServiceAccount secret-reader in Namespace project-hamster. Create a Pod of image curlimages/curl:7.65.3 named tmp-api-contact which uses this ServiceAccount. Make sure the container keeps running. Exec into the Pod and use curl to access the Kubernetes Api of that cluster manually, listing all available secrets. You can ignore insecure https connection. Write the command ( s ) for this into file /opt/course/e4/list-secrets.sh. Answer: https://kubernetes.io/docs/tasks/run-application/access-api-from-pod # It's important to understand how the Kubernetes API works. For this it helps connecting to the api manually, for example using curl. You can find information fast by search in the Kubernetes docs for \"curl api\" for example. First we create our Pod: k run tmp-api-contact \\ --image = curlimages/curl:7.65.3 $do \\ --command > e2.yaml -- sh -c 'sleep 1d' Add the service account name and Namespace: Then run and exec into: k -f e2.yaml create k -n project-hamster exec tmp-api-contact -it -- sh Once on the container we can try to connect to the api using curl, the api is usually available via the Service named kubernetes in Namespace default ( You should know how dns resolution works across Namespaces. ) . Else we can find the endpoint IP via environment variables running env. So now we can do : curl https://kubernetes.default curl -k https://kubernetes.default # ignore insecure as allowed in ticket description curl -k https://kubernetes.default/api/v1/secrets # should show Forbidden 403 The last command shows 403 forbidden, this is because we are not passing any authorisation information with us. The Kubernetes Api Server thinks we are connecting as system:anonymous. We want to change this and connect using the Pods ServiceAccount named secret-reader. We find the the token in the mounted folder at /var/run/secrets/kubernetes.io/serviceaccount, so we do : TOKEN = $( cat /var/run/secrets/kubernetes.io/serviceaccount/token ) curl -k https://kubernetes.default/api/v1/secrets -H \"Authorization: Bearer ${ TOKEN } \" # Now we're able to list all Secrets, registering as the ServiceAccount secret-reader under which our Pod is running. To use encrypted https connection we can run: CACERT = /var/run/secrets/kubernetes.io/serviceaccount/ca.crt curl --cacert ${ CACERT } https://kubernetes.default/api/v1/secrets -H \"Authorization: Bearer ${ TOKEN } \" For troubleshooting we could also check if the ServiceAccount is actually able to list Secrets using: k auth can-i get secret --as system:serviceaccount:project-hamster:secret-reader # yes Finally write the commands into the requested location: # /opt/course/e4/list-secrets.sh TOKEN = $( cat /var/run/secrets/kubernetes.io/serviceaccount/token ) curl -k https://kubernetes.default/api/v1/secrets -H \"Authorization: Bearer ${ TOKEN } \" Preview Question 1 Use context: kubectl config use-context k8s-c2-AC The cluster admin asked you to find out the following information about etcd running on cluster2-master1: Server private key location Server certificate expiration date Is client certificate authentication enabled Write these information into /opt/course/p1/etcd-info.txt # Finally you're asked to save an etcd snapshot at /etc/etcd-snapshot.db on cluster2-master1 and display its status. Answer: Find out etcd information # Let's check the nodes: k get node ssh cluster2-master1 First we check how etcd is setup in this cluster: kubectl -n kube-system get pod We see its running as a Pod, more specific a static Pod. So we check for the default kubelet directory for static manifests: find /etc/kubernetes/manifests/ vim /etc/kubernetes/manifests/etcd.yaml - command: - etcd - --advertise-client-urls = https://192.168.102.11:2379 - --cert-file = /etc/kubernetes/pki/etcd/server.crt # server certificate - --client-cert-auth = true # enabled - --key-file = /etc/kubernetes/pki/etcd/server.key # server private key # We see that client authentication is enabled and also the requested path to the server private key, now let's find out the expiration of the server certificate: openssl x509 -noout -text -in /etc/kubernetes/pki/etcd/server.crt | grep Validity -A2 # There we have it. Let's write the information into the requested file: # /opt/course/p1/etcd-info.txt Server private key location: /etc/kubernetes/pki/etcd/server.key Server certificate expiration date: Sep 13 13 :01:31 2022 GMT Is client certificate authentication enabled: yes Create etcd snapshot ETCDCTL_API = 3 etcdctl snapshot save /etc/etcd-snapshot.db \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key This worked. Now we can output the status of the backup file: ETCDCTL_API = 3 etcdctl snapshot status /etc/etcd-snapshot.db Preview Question 2 Use context: kubectl config use-context k8s-c1-H # You're asked to confirm that kube-proxy is running correctly on all nodes. For this perform the following in Namespace project-hamster: Create a new Pod named p2-pod with two containers, one of image nginx:1.21.3-alpine and one of image busybox:1.31. Make sure the busybox container keeps running for some time. Create a new Service named p2-service which exposes that Pod internally in the cluster on port 3000 ->80. # Find the kube-proxy container on all nodes cluster1-master1, cluster1-worker1 and cluster1-worker2 and make sure that it's using iptables. Use command crictl for this. Write the iptables rules of all nodes belonging the created Service p2-service into file /opt/course/p2/iptables.txt. Finally delete the Service and confirm that the iptables rules are gone from all nodes. Answer: Create the Pod First we create the Pod: k run p2-pod --image = nginx:1.21.3-alpine $do > p2.yaml # p2.yaml - image: busybox:1.31 # add name: c2 # add command: [ \"sh\" , \"-c\" , \"sleep 1d\" ] # add Create the Service Next we create the Service: k -n project-hamster expose pod p2-pod --name p2-service --port 3000 --target-port 80 We should confirm Pods and Services are connected, hence the Service should have Endpoints. k -n project-hamster get pod,svc,ep Confirm kube-proxy is running and is using iptables First we get nodes in the cluster: k get node The idea here is to log into every node, find the kube-proxy container and check its logs: ssh cluster1-master1 crictl ps | grep kube-proxy crictl logs 27b6a18c0f89c This should be repeated on every node and result in the same output Using iptables Proxier. Check kube-proxy is creating iptables rules Now we check the iptables rules on every node first manually: ssh cluster1-master1 iptables-save | grep p2-service # Great. Now let's write these logs into the requested file: ssh cluster1-master1 iptables-save | grep p2-service >> /opt/course/p2/iptables.txt ssh cluster1-worker1 iptables-save | grep p2-service >> /opt/course/p2/iptables.txt ssh cluster1-worker2 iptables-save | grep p2-service >> /opt/course/p2/iptables.txt Delete the Service and confirm iptables rules are gone Delete the Service: k -n project-hamster delete svc p2-service And confirm the iptables rules are gone: ssh cluster1-master1 iptables-save | grep p2-service # Kubernetes Services are implemented using iptables rules (with default config) on all nodes. Every time a Service has been altered, created, deleted or Endpoints of a Service have changed, the kube-apiserver contacts every node's kube-proxy to update the iptables rules according to the current state. Preview Question 3 Use context: kubectl config use-context k8s-c2-AC Create a Pod named check-ip in Namespace default using image httpd:2.4.41-alpine. Expose it on port 80 as a ClusterIP Service named check-ip-service. Remember/output the IP of that Service. Change the Service CIDR to 11 .96.0.0/12 for the cluster. Then create a second Service named check-ip-service2 pointing to the same Pod to check if your settings did take effect. Finally check if the IP of the first Service has changed. Answer: # Let's create the Pod and expose it: k run check-ip --image = httpd:2.4.41-alpine k expose pod check-ip --name check-ip-service --port 80 And check the Pod and Service ips: k get svc,ep -l run = check-ip Now we change the Service CIDR on the kube-apiserver: ssh cluster2-master1 vim /etc/kubernetes/manifests/kube-apiserver.yaml - --service-cluster-ip-range = 11 .96.0.0/12 # change Give it a bit of time for the kube-apiserver and controller-manager to restart Wait for the api to be up again: kubectl -n kube-system get pod | grep api Now we do the same for the controller manager: vim /etc/kubernetes/manifests/kube-controller-manager.yaml - --service-cluster-ip-range = 11 .96.0.0/12 # change Give it a bit for the controller-manager to restart. We can check if it was restarted using crictl: crictl ps | grep scheduler Checking our existing Pod and Service again: k get pod,svc -l run = check-ip Nothing changed so far. Now we create another Service like before: k expose pod check-ip --name check-ip-service2 --port 80 And check again: k get svc,ep -l run = check-ip There we go, the new Service got an ip of the new specified range assigned. We also see that both Services have our Pod as endpoint.","title":"CKA exam testing"},{"location":"k8s/2-exam/#cka-exam-testing","text":"","title":"CKA exam testing"},{"location":"k8s/2-exam/#preparation","text":"Q: Create a Job that run 60 time with 2 jobs running in parallel https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ Q: Find which Pod is taking max CPU Use kubectl top to find CPU usage per pod Q: List all PersistentVolumes sorted by their name Use kubectl get pv --sort-by= <- this problem is buggy & also by default kubectl give the output sorted by name. Q: Create a NetworkPolicy to allow connect to port 8080 by busybox pod only https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ Make sure to use apiVersion: extensions/v1beta1 which works on both 1.6 and 1.7 Q: fixing broken nodes, see https://kubernetes.io/docs/concepts/architecture/nodes/ Q: etcd backup, see https://kubernetes.io/docs/getting-started-guides/ubuntu/backups/ https://www.mirantis.com/blog/everything-you-ever-wanted-to-know-about-using-etcd-with-kubernetes-v1-6-but-were-afraid-to-ask/ Q: TLS bootstrapping, see https://coreos.com/kubernetes/docs/latest/openssl.html https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/ cloudflare/cfssl Q: You have a Container with a volume mount. Add a init container that creates an empty file in the volume. (only trick is to mount the volume to init-container as well) https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo The app is running! && sleep 3600'] volumeMounts: - mountPath: /cache name: cache-volume initContainers: - name: init-touch-file image: busybox volumeMounts: - mountPath: /data name: cache-volume command: ['sh', '-c', 'echo \"\" > /data/harshal.txt'] volumes: - name: cache-volume emptyDir: {} ```` Q: When running a redis key-value store in your pre-production environments many deployments are incoming from CI and leaving behind a lot of stale cache data in redis which is causing test failures. The CI admin has requested that each time a redis key-value-store is deployed in staging that it not persist its data. Create a pod named non-persistent-redis that specifies a named-volume with name app-cache, and mount path /data/redis. It should launch in the staging namespace and the volume MUST NOT be persistent. Create a Pod with EmptyDir and in the YAML file add namespace: CI Q: Setting up K8s master components with a binaries/from tar balls: Also, convert CRT to PEM: openssl x509 -in abc.crt -out abc.pem - https://coreos.com/kubernetes/docs/latest/openssl.html - https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md - https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/08-bootstrapping-kubernetes-controllers.md - https://gist.github.com/mhausenblas/0e09c448517669ef5ece157fd4a5dc4b - https://kubernetes.io/docs/getting-started-guides/scratch/ - http://alexander.holbreich.org/kubernetes-on-ubuntu/ maybe dashboard? - https://kubernetes.io/docs/getting-started-guides/binary_release/ - http://kamalmarhubi.com/blog/2015/09/06/kubernetes-from-the-ground-up-the-api-server/ Q: Find the error message with the string \u201cSome-error message here\u201d. https://kubernetes.io/docs/concepts/cluster-administration/logging/ see kubectl logs and /var/log for system services Q 17: Create an Ingress resource, Ingress controller and a Service that resolves to cs.rocks.ch. First, create controller and default backend ```BASH kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress/master/controllers/nginx/examples/default-backend.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/nginx-ingress-controller.yaml Second, create service and expose ``` kubectl run ingress-pod \u2013image=nginx \u2013port 80 kubectl expose deployment ingress-pod \u2013port=80 \u2013target-port=80 \u2013type=NodePort Create the ingress ``` cat <<EOF >ingress-cka.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-service spec: rules: - host: \"cs.rocks.ch\" http: paths: - backend: serviceName: ingress-pod servicePort: 80 EOF To test, run a curl pod kubectl run -i --tty client --image=tutum/curl curl -I -L --resolve cs.rocks.ch:80:10.240.0.5 http://cs.rocks.ch/ Q: Run a Jenkins Pod on a specified node only. https://kubernetes.io/docs/tasks/administer-cluster/static-pod/ Create the Pod manifest at the specified location and then edit the systemd service file for kubelet(/etc/systemd/system/kubelet.service) to include --pod-manifest-path=/specified/path . Once done restart the service. Q: Use the utility nslookup to look up the DNS records of the service and pod. From this guide, https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ Look for \u201cQuick Diagnosis\u201d $ kubectl exec -ti busybox \u2013 nslookup mysvc.myns.svc.cluster.local Naming conventions for services and pods: For a regular service, this resolves to the port number and the CNAME: my-svc.my-namespace.svc.cluster.local. For a headless service, this resolves to multiple answers, one for each pod that is backing the service, and contains the port number and a CNAME of the pod of the form auto-generated-name.my-svc.my-namespace.svc.cluster.local When enabled, pods are assigned a DNS A record in the form of pod-ip-address.my-namespace.pod.cluster.local. For example, a pod with IP 1.2.3.4 in the namespace default with a DNS name of cluster.local would have an entry: 1-2-3-4.default.pod.cluster.local Q: Start a pod automatically by keeping manifest in /etc/kubernetes/manifests Refer to https://kubernetes.io/docs/tasks/administer-cluster/static-pod/ Edit kubelet.service on any worker node to contain this flag \u2013pod-manifest-path=/etc/kubernetes/manifests then place the pod manifest at /etc/kubernetes/manifests. Now restart kubelet. Some other Questions: Main container looks for a file and crashes if it doesnt find the file. Write an init container to create the file and make it available for the main container Install and Configure kubelet on a node to run pod on that node without contacting the api server Take backup of etcd cluster rotate TLS certificates 5.rolebinding 6.Troubleshooting - involved identifying failing nodes, pods , services and identifying cpu utilization of pods.","title":"Preparation"},{"location":"k8s/2-exam/#general-questions","text":"Backup/restore etcd on specific location using certificates. Fix the broken node. You should check kubelet process. Mostly you have to start and enable for permanent change. Create network policy to allow incoming connection from specific namespace , port combination. They might ask from specific pods. Two questions related to jsonpath , you can output the resource in json format and then find the details. They ask to create ingress . Please note ingress controller was already there. One question related to sidecar container. Trick is you have to mount the volume of on the side container. Question related to which pod consume most cpu. You don\u2019t need to install metrics server.","title":"General Questions"},{"location":"k8s/2-exam/#topics-to-focus-on","text":"Ingress Role and Role Binding Service Accounts PV and PVC Volumes Network Policy Services Check logs from Pods, Scale Deployment and replica ETCD Backup and restore TLS Bootstraping Question 1 | Contexts Task weight: 1 % You have access to multiple clusters from your main terminal through kubectl contexts. Write all those context names into /opt/course/1/contexts. Next write a command to display the current context into /opt/course/1/context_default_kubectl.sh, the command should use kubectl. Finally write a second command doing the same thing into /opt/course/1/context_default_no_kubectl.sh, but without the use of kubectl. Answer: k config get-contexts # copy manually # OR k config get-contexts -o name > /opt/course/1/contexts # /opt/course/1/context_default_kubectl.sh kubectl config current-context # /opt/course/1/context_default_no_kubectl.sh cat ~/.kube/config | grep current Question 2 | Schedule Pod on Master Node Task weight: 3 % Use context: kubectl config use-context k8s-c1-H Create a single Pod of image httpd:2.4.41-alpine in Namespace default. The Pod should be named pod1 and the container should be named pod1-container. This Pod should only be scheduled on a master node, do not add new labels any nodes. Answer: First we find the master node ( s ) and their taints: k get node # find master node k describe node cluster1-master1 | grep Taint -A3 # get master node taints k get node cluster1-master1 --show-labels # get master node labels # NOTE: In K8s 1.24 master/controlplane nodes have two Taints which means we have to add Tolerations for both. This is done during transitioning from the wording \"master\" to \"controlplane\". Next we create the Pod template: # check the export on the very top of this document so we can use $do k run pod1 --image = httpd:2.4.41-alpine $do > 2 .yaml # vim 2.yaml tolerations : # add - effect : NoSchedule # add key : node-role.kubernetes.io/master # add - effect : NoSchedule # add key : node-role.kubernetes.io/control-plane # add nodeSelector : # add node-role.kubernetes.io/control-plane : \"\" # add # Important here to add the toleration for running on master nodes, but also the nodeSelector to make sure it only runs on master nodes. If we only specify a toleration the Pod can be scheduled on master or worker nodes. Question 3 | Scale down StatefulSet Task weight: 1 % Use context: kubectl config use-context k8s-c1-H There are two Pods named o3db-* in Namespace project-c13. C13 management asked you to scale the Pods down to one replica to save resources. Answer: If we check the Pods we see two replicas: k -n project-c13 get pod | grep o3db # From their name it looks like these are managed by a StatefulSet. But if we're not sure we could also check for the most common resources which manage Pods: k -n project-c13 get deploy,ds,sts | grep o3db #Confirmed, we have to work with a StatefulSet. To find this out we could also look at the Pod labels: k -n project-c13 get pod --show-labels | grep o3db # To fulfil the task we simply run: k -n project-c13 scale sts o3db --replicas 1 Question 4 | Pod Ready if Service is reachable Task weight: 4 % Use context: kubectl config use-context k8s-c1-H # Do the following in Namespace default. Create a single Pod named ready-if-service-ready of image nginx:1.16.1-alpine. Configure a LivenessProbe which simply runs true. Also configure a ReadinessProbe which does check if the url http://service-am-i-ready:80 is reachable, you can use wget -T2 -O- http://service-am-i-ready:80 for this. Start the Pod and confirm it isn't ready because of the ReadinessProbe. Create a second Pod named am-i-ready of image nginx:1.16.1-alpine with label id: cross-server-ready. The already existing Service service-am-i-ready should now have that second Pod as endpoint. Now the first Pod should be in ready state, confirm that. Answer: # It's a bit of an anti-pattern for one Pod to check another Pod for being ready using probes, hence the normally available readinessProbe.httpGet doesn't work for absolute remote urls. Still the workaround requested in this task should show how probes and Pod<->Service communication works. First we create the first Pod: k run ready-if-service-ready --image = nginx:1.16.1-alpine $do > 4_pod1.yaml # And confirm its in a non-ready state: k get pod ready-if-service-ready # We can also check the reason for this using describe: k describe pod ready-if-service-ready # Now we create the second Pod: k run am-i-ready --image = nginx:1.16.1-alpine --labels = \"id=cross-server-ready\" # The already existing Service service-am-i-ready should now have an Endpoint: k describe svc service-am-i-ready k get ep # also possible Which will result in our first Pod being ready, just give it a minute for the Readiness probe to check again: k get pod ready-if-service-ready # 4_pod1.yaml livenessProbe : # add from here exec : command : - 'true' readinessProbe : exec : command : - sh - -c - 'wget -T2 -O- http://service-am-i-ready:80' # to here Question 5 | Kubectl sorting Task weight: 1 % Use context: kubectl config use-context k8s-c1-H There are various Pods in all namespaces. Write a command into /opt/course/5/find_pods.sh which lists all Pods sorted by their AGE ( metadata.creationTimestamp ) . Write a second command into /opt/course/5/find_pods_uid.sh which lists all Pods sorted by field metadata.uid. Use kubectl sorting for both commands. Answer: A good resources here ( and for many other things ) is the kubectl-cheat-sheet. You can reach it fast when searching for \"cheat sheet\" in the Kubernetes docs. # /opt/course/5/find_pods.sh kubectl get pod -A --sort-by = .metadata.creationTimestamp For the second command: # /opt/course/5/find_pods_uid.sh kubectl get pod -A --sort-by = .metadata.uid Question 6 | Storage, PV, PVC, Pod volume Task weight: 8 % Use context: kubectl config use-context k8s-c1-H Create a new PersistentVolume named safari-pv. It should have a capacity of 2Gi, accessMode ReadWriteOnce, hostPath /Volumes/Data and no storageClassName defined. Next create a new PersistentVolumeClaim in Namespace project-tiger named safari-pvc . It should request 2Gi storage, accessMode ReadWriteOnce and should not define a storageClassName. The PVC should bound to the PV correctly. Finally create a new Deployment safari in Namespace project-tiger which mounts that volume at /tmp/safari-data. The Pods of that Deployment should be of image httpd:2.4.41-alpine. Answer Create PV and PV using k8s docs. Next we create a Deployment and mount that volume: k -n project-tiger create deploy safari \\ --image = httpd:2.4.41-alpine $do > 6_dep.yaml We can confirm its mounting correctly: k -n project-tiger describe pod safari-5cbf46d6d-mjhsb | grep -A2 Mounts: # 6_pv.yaml kind : PersistentVolume apiVersion : v1 metadata : name : safari-pv spec : capacity : storage : 2Gi accessModes : - ReadWriteOnce hostPath : path : \"/Volumes/Data\" # 6_pvc.yaml kind : PersistentVolumeClaim apiVersion : v1 metadata : name : safari-pvc namespace : project-tiger spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi # 6_dep.yaml spec : volumes : # add - name : data # add persistentVolumeClaim : # add claimName : safari-pvc # add containers : - image : httpd:2.4.41-alpine name : container volumeMounts : # add - name : data # add mountPath : /tmp/safari-data # add Question 7 | Node and Pod Resource Usage Task weight: 1 % Use context: kubectl config use-context k8s-c1-H The metrics-server has been installed in the cluster. Your college would like to know the kubectl commands to: show Nodes resource usage show Pods and their **containers** resource usage Please write the commands into /opt/course/7/node.sh and /opt/course/7/pod.sh. Answer: The command we need to use here is top: k top -h We create the first file: # /opt/course/7/node.sh kubectl top node For the second file we might need to check the docs again: k top pod -h # /opt/course/7/pod.sh kubectl top pod --containers = true Question 8 | Get Master Information Task weight: 2 % Use context: kubectl config use-context k8s-c1-H # Ssh into the master node with ssh cluster1-master1. Check how the master components kubelet, kube-apiserver, kube-scheduler, kube-controller-manager and etcd are started/installed on the master node. Also find out the name of the DNS application and how it's started/installed on the master node. Write your findings into file /opt/course/8/master-components.txt. The file should be structured like: # /opt/course/8/master-components.txt kubelet: [ TYPE ] kube-apiserver: [ TYPE ] kube-scheduler: [ TYPE ] kube-controller-manager: [ TYPE ] etcd: [ TYPE ] dns: [ TYPE ] [ NAME ] Choices of [ TYPE ] are: not-installed, process, static-pod, pod Answer: We could start by finding processes of the requested components, especially the kubelet at first: ssh cluster1-master1 ps aux | grep kubelet # shows kubelet process We can see which components are controlled via systemd looking at /etc/systemd/system directory: find /etc/systemd/system/ | grep kube find /etc/systemd/system/ | grep etcd # This shows kubelet is controlled via systemd, but no other service named kube nor etcd. It seems that this cluster has been setup using kubeadm, so we check in the default manifests directory: find /etc/kubernetes/manifests/ # (The kubelet could also have a different manifests directory specified via parameter --pod-manifest-path in it's systemd startup config) # This means the main 4 master services are setup as static Pods. Actually, let's check all Pods running on in the kube-system Namespace on the master node: kubectl -n kube-system get pod -o wide | grep master1 # There we see the 5 static pods, with -cluster1-master1 as suffix. # We also see that the dns application seems to be coredns, but how is it controlled? kubectl -n kube-system get ds kubectl -n kube-system get deploy Seems like coredns is controlled via a Deployment. We combine our findings in the requested file: # /opt/course/8/master-components.txt kubelet: process kube-apiserver: static-pod kube-scheduler: static-pod kube-controller-manager: static-pod etcd: static-pod dns: pod coredns Question 9 | Kill Scheduler, Manual Scheduling Task weight: 5 % Use context: kubectl config use-context k8s-c2-AC Ssh into the master node with ssh cluster2-master1. Temporarily stop the kube-scheduler, this means in a way that you can start it again afterwards. Create a single Pod named manual-schedule of image httpd:2.4-alpine, confirm its created but not scheduled on any node. # Now you're the scheduler and have all its power, manually schedule that Pod on node cluster2-master1. Make sure it's running. # Start the kube-scheduler again and confirm its running correctly by creating a second Pod named manual-schedule2 of image httpd:2.4-alpine and check if it's running on cluster2-worker1. Answer: Stop the Scheduler First we find the master node: k get node Then we connect and check if the scheduler is running: ssh cluster2-master1 kubectl -n kube-system get pod | grep schedule Kill the Scheduler ( temporarily ) : cd /etc/kubernetes/manifests/ mv kube-scheduler.yaml .. And it should be stopped: kubectl -n kube-system get pod | grep schedule Create a Pod Now we create the Pod: k run manual-schedule --image = httpd:2.4-alpine # And confirm it has no node assigned: k get pod manual-schedule -o wide Manually schedule the Pod # Let's play the scheduler now: k get pod manual-schedule -o yaml > 9 .yaml nodeName: cluster2-master1 # add the master node name The only thing a scheduler does, is that it sets the nodeName for a Pod declaration. As we cannot kubectl apply or kubectl edit , in this case we need to delete and create or replace: k -f 9 .yaml replace --force k get pod manual-schedule -o wide # It looks like our Pod is running on the master now as requested, although no tolerations were specified. Only the scheduler takes taints/tolerations/affinity into account when finding the correct node name. That's why its still possible to assign Pods manually directly to a master node and skip the scheduler. Start the scheduler again ssh cluster2-master1 cd /etc/kubernetes/manifests/ mv ../kube-scheduler.yaml . Schedule a second test Pod: k run manual-schedule2 --image = httpd:2.4-alpine Question 10 | RBAC ServiceAccount Role RoleBinding Task weight: 6 % Use context: kubectl config use-context k8s-c1-H Create a new ServiceAccount processor in Namespace project-hamster. Create a Role and RoleBinding, both named processor as well. These should allow the new SA to only create Secrets and ConfigMaps in that Namespace. Answer: # Let's talk a little about RBAC resources A ClusterRole | Role defines a set of permissions and where it is available, in the whole cluster or just a single Namespace. A ClusterRoleBinding | RoleBinding connects a set of permissions with an account and defines where it is applied, in the whole cluster or just a single Namespace. Because of this there are 4 different RBAC combinations and 3 valid ones: 1 . Role + RoleBinding ( available in single Namespace, applied in single Namespace ) 2 . ClusterRole + ClusterRoleBinding ( available cluster-wide, applied cluster-wide ) 3 . ClusterRole + RoleBinding ( available cluster-wide, applied in single Namespace ) 4 . Role + ClusterRoleBinding ( NOT POSSIBLE: available in single Namespace, applied cluster-wide ) # To the solution We first create the ServiceAccount: k -n project-hamster create sa processor Then for the Role: k -n project-hamster create role processor \\ --verb = create \\ --resource = secret \\ --resource = configmap \\ --namespace = project-hamster # Now we bind the Role to the ServiceAccount: k -n project-hamster create rolebinding processor \\ --role processor \\ --serviceaccount project-hamster:processor \\ --namespace = project-hamster To test our RBAC setup we can use kubectl auth can-i: k -n project-hamster auth can-i create secret \\ --as system:serviceaccount:project-hamster:processor # yes k -n project-hamster auth can-i create pod \\ --as system:serviceaccount:project-hamster:processor # no Question 11 | DaemonSet on all Nodes Task weight: 4 % Use context: kubectl config use-context k8s-c1-H Use Namespace project-tiger for the following. Create a DaemonSet named ds-important with image httpd:2.4-alpine and labels id = ds-important and uuid = 18426a0b-5f59-4e10-923f-c0e078e82462. The Pods it creates should request 10 millicore cpu and 10 mebibyte memory. The Pods of that DaemonSet should run on all nodes, master and worker. Answer: # As of now we aren't able to create a DaemonSet directly using kubectl, so we create a Deployment and just change it up: k -n project-tiger create deployment --image = httpd:2.4-alpine ds-important --labels = \"id=ds-important,uuid=18426a0b-5f59-4e10-923f-c0e078e82462\" $do > 11 .yaml # NOTE: In K8s 1.24 master/controlplane nodes have two Taints which means we have to add Tolerations for both. This is done during transitioning from the wording \"master\" to \"controlplane\". It was requested that the DaemonSet runs on all nodes, so we need to specify the toleration for this. # 11.yaml apiVersion : apps/v1 kind : DaemonSet # change from Deployment to Daemonset metadata : namespace : project-tiger # important spec : #replicas: 1 # remove #strategy: {} # remove template : spec : containers : tolerations : # add - effect : NoSchedule # add key : node-role.kubernetes.io/master # add - effect : NoSchedule # add key : node-role.kubernetes.io/control-plane # add #status: {} # remove Question 12 | Deployment on all Nodes Task weight: 6 % Use context: kubectl config use-context k8s-c1-H Use Namespace project-tiger for the following. Create a Deployment named deploy-important with label id = very-important ( the Pods should also have this label ) and 3 replicas. It should contain two containers, the first named container1 with image nginx:1.17.6-alpine and the second one named container2 with image kubernetes/pause. # There should be only ever one Pod of that Deployment running on one worker node. We have two worker nodes: cluster1-worker1 and cluster1-worker2. Because the Deployment has three replicas the result should be that on both nodes one Pod is running. The third Pod won't be scheduled, unless a new worker node will be added. # In a way we kind of simulate the behaviour of a DaemonSet here, but using a Deployment and a fixed number of replicas. Answer: There are two possible ways, one using podAntiAffinity and one using topologySpreadConstraint. PodAntiAffinity The idea here is that we create a \"Inter-pod anti-affinity\" which allows us to say a Pod should only be scheduled on a node where another Pod of a specific label ( here the same label ) is not already running. # Let's begin by creating the Deployment template: k -n project-tiger create deployment \\ --image = nginx:1.17.6-alpine deploy-important $do > 12 .yaml # 12.yaml affinity : # add podAntiAffinity : # add requiredDuringSchedulingIgnoredDuringExecution : # add - labelSelector : # add matchExpressions : # add - key : id # add operator : In # add values : # add - very-important # add topologyKey : kubernetes.io/hostname # add # Specify a topologyKey, which is a pre-populated Kubernetes label, you can find this by describing a node. Question 13 | Multi Containers and Pod shared Volume Task weight: 4 % Use context: kubectl config use-context k8s-c1-H # Create a Pod named multi-container-playground in Namespace default with three containers, named c1, c2 and c3. There should be a volume attached to that Pod and mounted into every container, but the volume shouldn't be persisted or shared with other Pods. Container c1 should be of image nginx:1.17.6-alpine and have the name of the node where its Pod is running available as environment variable MY_NODE_NAME. Container c2 should be of image busybox:1.31.1 and write the output of the date command every second in the shared volume into file date.log. You can use while true ; do date >> /your/vol/path/date.log ; sleep 1 ; done for this. Container c3 should be of image busybox:1.31.1 and constantly send the content of file date.log from the shared volume to stdout. You can use tail -f /your/vol/path/date.log for this. Check the logs of container c3 to confirm correct setup. Answer: First we create the Pod template: k run multi-container-playground --image = nginx:1.17.6-alpine $do > 13 .yaml And add the other containers and the commands they should execute: we check if container c1 has the requested node name as env variable: k exec multi-container-playground -c c1 -- env | grep MY And finally we check the logging: k logs multi-container-playground -c c3 # 13.yaml apiVersion : v1 kind : Pod metadata : creationTimestamp : null labels : run : multi-container-playground name : multi-container-playground spec : containers : - image : nginx:1.17.6-alpine name : c1 # change resources : {} env : # add - name : MY_NODE_NAME # add valueFrom : # add fieldRef : # add fieldPath : spec.nodeName # add volumeMounts : # add - name : vol # add mountPath : /vol # add - image : busybox:1.31.1 # add name : c2 # add command : [ \"sh\" , \"-c\" , \"while true; do date >> /vol/date.log; sleep 1; done\" ] # add volumeMounts : # add - name : vol # add mountPath : /vol # add - image : busybox:1.31.1 # add name : c3 # add command : [ \"sh\" , \"-c\" , \"tail -f /vol/date.log\" ] # add volumeMounts : # add - name : vol # add mountPath : /vol # add dnsPolicy : ClusterFirst restartPolicy : Always volumes : # add - name : vol # add emptyDir : {} # add status : {} Question 14 | Find out Cluster Information Task weight: 2 % Use context: kubectl config use-context k8s-c1-H # You're ask to find out following information about the cluster k8s-c1-H: How many master nodes are available? How many worker nodes are available? What is the Service CIDR? Which Networking ( or CNI Plugin ) is configured and where is its config file? Which suffix will static pods have that run on cluster1-worker1? Write your answers into file /opt/course/14/cluster-info, structured like this: # /opt/course/14/cluster-info 1 : [ ANSWER ] 2 : [ ANSWER ] 3 : [ ANSWER ] 4 : [ ANSWER ] 5 : [ ANSWER ] Answer: How many master and worker nodes are available? We see one master and two workers. What is the Service CIDR? ssh cluster1-master1 cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep range Which Networking ( or CNI Plugin ) is configured and where is its config file? find /etc/cni/net.d/ cat /etc/cni/net.d/10-weave.conflist By default the kubelet looks into /etc/cni/net.d to discover the CNI plugins. This will be the same on every master and worker nodes. Which suffix will static pods have that run on cluster1-worker1? The suffix is the node hostname with a leading hyphen. It used to be -static in earlier Kubernetes versions. Result The resulting /opt/course/14/cluster-info could look like: # /opt/course/14/cluster-info # How many master nodes are available? 1 : 1 # How many worker nodes are available? 2 : 2 # What is the Service CIDR? 3 : 10 .96.0.0/12 # Which Networking (or CNI Plugin) is configured and where is its config file? 4 : Weave, /etc/cni/net.d/10-weave.conflist # Which suffix will static pods have that run on cluster1-worker1? 5 : -cluster1-worker1 Question 15 | Cluster Event Logging Task weight: 3 % Use context: kubectl config use-context k8s-c2-AC Write a command into /opt/course/15/cluster_events.sh which shows the latest events in the whole cluster, ordered by time. Use kubectl for it. Now kill the kube-proxy Pod running on node cluster2-worker1 and write the events this caused into /opt/course/15/pod_kill.log. Finally kill the containerd container of the kube-proxy Pod on node cluster2-worker1 and write the events into /opt/course/15/container_kill.log. Do you notice differences in the events both actions caused? Answer: # /opt/course/15/cluster_events.sh kubectl get events -A --sort-by = .metadata.creationTimestamp Now we kill the kube-proxy Pod: k -n kube-system get pod -o wide | grep proxy # find pod running on cluster2-worker1 k -n kube-system delete pod kube-proxy-z64cg Now check the events: sh /opt/course/15/cluster_events.sh Write the events the killing caused into /opt/course/15/pod_kill.log Finally we will try to provoke events by killing the container belonging to the container of the kube-proxy Pod: ssh cluster2-worker1 crictl ps | grep kube-proxy crictl stop 1e020b43c4423 crictl rm 1e020b43c4423 crictl ps | grep kube-proxy We killed the main container ( 1e020b43c4423 ) , but also noticed that a new container ( 0ae4245707910 ) was directly created. Thanks Kubernetes! Now we see if this caused events again and we write those into the second file: sh /opt/course/15/cluster_events.sh # Comparing the events we see that when we deleted the whole Pod there were more things to be done, hence more events. For example was the DaemonSet in the game to re-create the missing Pod. Where when we manually killed the main container of the Pod, the Pod would still exist but only its container needed to be re-created, hence less events. Question 16 | Namespaces and Api Resources Task weight: 2 % Use context: kubectl config use-context k8s-c1-H Create a new Namespace called cka-master. Write the names of all namespaced Kubernetes resources ( like Pod, Secret, ConfigMap... ) into /opt/course/16/resources.txt. Find the project-* Namespace with the highest number of Roles defined in it and write its name and amount of Roles into /opt/course/16/crowded-namespace.txt. Answer: Namespace and Namespaces Resources We create a new Namespace: k create ns cka-master Now we can get a list of all resources like: k api-resources --namespaced -o name > /opt/course/16/resources.txt Namespace with most Roles k -n project-c13 get role --no-headers | wc -l k -n project-c14 get role --no-headers | wc -l # 300 Find all other namespaces Finally we write the name and amount into the file: # /opt/course/16/crowded-namespace.txt project-c14 with 300 resources Question 17 | Find Container of Pod and check info Task weight: 3 % Use context: kubectl config use-context k8s-c1-H In Namespace project-tiger create a Pod named tigers-reunite of image httpd:2.4.41-alpine with labels pod = container and container = pod. Find out on which node the Pod is scheduled. Ssh into that node and find the containerd container belonging to that Pod. Using command crictl: Write the ID of the container and the info.runtimeType into /opt/course/17/pod-container.txt Write the logs of the container into /opt/course/17/pod-container.log Answer: First we create the Pod: k -n project-tiger run tigers-reunite \\ --image = httpd:2.4.41-alpine \\ --labels \"pod=container,container=pod\" # Next we find out the node it's scheduled on: k -n project-tiger get pod -o wide Then we ssh into that node and and check the container info: ssh cluster1-worker2 crictl ps | grep tigers-reunite crictl inspect b01edbe6f89ed | grep runtimeType Then we fill the requested file ( on the main terminal ) : # /opt/course/17/pod-container.txt b01edbe6f89ed io.containerd.runc.v2 Finally we write the container logs in the second file: ssh cluster1-worker2 'crictl logs b01edbe6f89ed' & > /opt/course/17/pod-container.log # The &> in above's command redirects both the standard output and standard error. You could also simply run crictl logs on the node and copy the content manually, if its not a lot. The file should look like: Question 18 | Fix Kubelet Task weight: 8 % Use context: kubectl config use-context k8s-c3-CCC There seems to be an issue with the kubelet not running on cluster3-worker1. Fix it and confirm that cluster has node cluster3-worker1 available in Ready state afterwards. You should be able to schedule a Pod on cluster3-worker1 afterwards. Write the reason of the issue into /opt/course/18/reason.txt. Answer: The procedure on tasks like these should be to check if the kubelet is running, if not start it, then check its logs and correct errors if there are some. Always helpful to check if other clusters already have some of the components defined and running, so you can copy and use existing config files. Though in this case it might not need to be necessary. Check node status: k get node First we check if the kubelet is running: ssh cluster3-worker1 ps aux | grep kubelet Nope, so we check if its configured using systemd as service: service kubelet status # Yes, its configured as a service with config at /etc/systemd/system/kubelet.service.d/10-kubeadm.conf, but we see its inactive. Let's try to start it: service kubelet start We see its trying to execute /usr/local/bin/kubelet with some parameters defined in its service config file. A good way to find errors and get more logs is to run the command manually ( usually also with its parameters ) . /usr/local/bin/kubelet # -bash: /usr/local/bin/kubelet: No such file or directory whereis kubelet # kubelet: /usr/bin/kubelet Another way would be to see the extended logging of a service like using journalctl -u kubelet. Well, there we have it, wrong path specified. Correct the path in file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf and run: systemctl daemon-reload && systemctl restart kubelet Finally we write the reason into the file: # /opt/course/18/reason.txt wrong path to kubelet binary specified in service config Question 19 | Create Secret and mount into Pod Task weight: 3 % NOTE: This task can only be solved if questions 18 or 20 have been successfully implemented and the k8s-c3-CCC cluster has a functioning worker node Use context: kubectl config use-context k8s-c3-CCC Do the following in a new Namespace secret. Create a Pod named secret-pod of image busybox:1.31.1 which should keep running for some time. There is an existing Secret located at /opt/course/19/secret1.yaml, create it in the Namespace secret and mount it readonly into the Pod at /tmp/secret1. # Create a new Secret in Namespace secret called secret2 which should contain user=user1 and pass=1234. These entries should be available inside the Pod's container as environment variables APP_USER and APP_PASS. Confirm everything is working. Answer First we create the Namespace and the requested Secrets in it: k create ns secret cp /opt/course/19/secret1.yaml 19_secret1.yaml k -f 19_secret1.yaml create Next we create the second Secret: k -n secret create secret generic secret2 --from-literal = user = user1 --from-literal = pass = 1234 Now we create the Pod template: k -n secret run secret-pod --image = busybox:1.31.1 $do -- sh -c \"sleep 5d\" > 19 .yaml # It might not be necessary in current K8s versions to specify the readOnly: true because it's the default setting anyways. Finally we check if all is correct: k -n secret exec secret-pod -- env | grep APP k -n secret exec secret-pod -- find /tmp/secret1 env : # add - name : APP_USER # add valueFrom : # add secretKeyRef : # add name : secret2 # add key : user # add - name : APP_PASS # add valueFrom : # add secretKeyRef : # add name : secret2 # add key : pass # add volumeMounts : # add - name : secret1 # add mountPath : /tmp/secret1 # add readOnly : true # add volumes : # add - name : secret1 # add secret : # add secretName : secret1 # add Question 20 | Update Kubernetes Version and join cluster Task weight: 10 % Use context: kubectl config use-context k8s-c3-CCC # Your coworker said node cluster3-worker2 is running an older Kubernetes version and is not even part of the cluster. Update Kubernetes on that node to the exact version that's running on cluster3-master1. Then add this node to the cluster. Use kubeadm for this. Answer: Master node seems to be running Kubernetes 1 .24.1 and cluster3-worker2 is not yet part of the cluster. ssh cluster3-worker2 kubeadm version # kubeadm version matches kubectl version # kubectl version is old kubelet --version # kubelet version is old kubeadm upgrade node This is usually the proper command to upgrade a node. But this error means that this node was never even initialised, so nothing to update here. This will be done later using kubeadm join. For now we can continue with kubelet and kubectl: apt update apt show kubectl -a | grep 1 .24 apt install kubectl = 1 .24.1-00 kubelet = 1 .24.1-00 # Now we're up to date with kubeadm, kubectl and kubelet. Restart the kubelet: systemctl restart kubelet We can ignore the errors and move into next step to generate the join command. # Add cluster3-worker2 to cluster First we log into the master1 and generate a new TLS bootstrap token, also printing out the join command: ssh cluster3-master1 kubeadm token create --print-join-command kubeadm token list Next we connect again to cluster3-worker2 and simply execute the join command: ssh cluster3-worker2 kubeadm join 192 .168.100.31:6443 --token <token> Question 21 | Create a Static Pod and Service Task weight: 2 % Use context: kubectl config use-context k8s-c3-CCC Create a Static Pod named my-static-pod in Namespace default on cluster3-master1. It should be of image nginx:1.16-alpine and have resource requests for 10m CPU and 20Mi memory. Then create a NodePort Service named static-pod-service which exposes that static Pod on port 80 and check if it has Endpoints and if its reachable through the cluster3-master1 internal IP address. You can connect to the internal node IPs from your main terminal. Answer: ssh cluster3-master1 cd /etc/kubernetes/manifests/ kubectl run my-static-pod \\ --image = nginx:1.16-alpine \\ -o yaml --dry-run = client > my-static-pod.yaml And make sure its running: k get pod -A | grep my-static Now we expose that static Pod: k expose pod my-static-pod-cluster3-master1 \\ --name static-pod-service \\ --type = NodePort \\ --port 80 Then run and test: k get svc,ep -l run = my-static-pod # /etc/kubernetes/manifests/my-static-pod.yaml resources : requests : cpu : 10m memory : 20Mi Question 22 | Check how long certificates are valid Task weight: 2 % Use context: kubectl config use-context k8s-c2-AC Check how long the kube-apiserver server certificate is valid on cluster2-master1. Do this with openssl or cfssl. Write the exipiration date into /opt/course/22/expiration. Also run the correct kubeadm command to list the expiration dates and confirm both methods show the same date. Write the correct kubeadm command that would renew the apiserver server certificate into /opt/course/22/kubeadm-renew-certs.sh. Answer: # First let's find that certificate: ssh cluster2-master1 find /etc/kubernetes/pki | grep apiserver Next we use openssl to find out the expiration date: openssl x509 -noout -text -in /etc/kubernetes/pki/apiserver.crt | grep Validity -A2 There we have it, so we write it in the required location on our main terminal: # /opt/course/22/expiration Jan 14 18 :49:40 2022 GMT And we use the feature from kubeadm to get the expiration too: kubeadm certs check-expiration | grep apiserver Looking good. And finally we write the command that would renew all certificates into the requested location: # /opt/course/22/kubeadm-renew-certs.sh kubeadm certs renew apiserver Question 23 | Kubelet client/server cert info Task weight: 2 % Use context: kubectl config use-context k8s-c2-AC Node cluster2-worker1 has been added to the cluster using kubeadm and TLS bootstrapping. Find the \"Issuer\" and \"Extended Key Usage\" values of the cluster2-worker1: kubelet client certificate, the one used for outgoing connections to the kube-apiserver. kubelet server certificate, the one used for incoming connections from the kube-apiserver. Write the information into file /opt/course/23/certificate-info.txt. Compare the \"Issuer\" and \"Extended Key Usage\" fields of both certificates and make sense of these. Answer: To find the correct kubelet certificate directory, we can look for the default value of the --cert-dir parameter for the kubelet. For this search for \"kubelet\" in the Kubernetes docs which will lead to: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet. We can check if another certificate directory has been configured using ps aux or in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf. First we check the kubelet client certificate: ssh cluster2-worker1 openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep Issuer openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep \"Extended Key Usage\" -A1 Next we check the kubelet server certificate: openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet.crt | grep Issuer openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet.crt | grep \"Extended Key Usage\" -A1 We see that the server certificate was generated on the worker node itself and the client certificate was issued by the Kubernetes api. The \"Extended Key Usage\" also shows if its for client or server authentication. More about this: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping Question 24 | NetworkPolicy Task weight: 9 % Use context: kubectl config use-context k8s-c1-H There was a security incident where an intruder was able to access the whole cluster from a single hacked backend Pod. To prevent this create a NetworkPolicy called np-backend in Namespace project-snake. It should allow the backend-* Pods only to: connect to db1-* Pods on port 1111 connect to db2-* Pods on port 2222 Use the app label of Pods in your policy. After implementation, connections from backend-* Pods to vault-* Pods on port 3333 should for example no longer work. Answer: First we look at the existing Pods and their labels: k -n project-snake get pod k -n project-snake get pod -L app We test the current connection situation and see nothing is restricted: k -n project-snake get pod -o wide k -n project-snake exec backend-0 -- curl -s 10 .44.0.25:1111 k -n project-snake exec backend-0 -- curl -s 10 .44.0.23:2222 k -n project-snake exec backend-0 -- curl -s 10 .44.0.22:3333 Now we create the NP by copying and chaning an example from the k8s docs: The NP below has two rules with two conditions each, it can be read as: allow outgoing traffic if : ( destination pod has label app = db1 AND port is 1111 ) OR ( destination pod has label app = db2 AND port is 2222 ) We create the correct NP: k -f 24_np.yaml create And test again: k -n project-snake exec backend-0 -- curl -s 10 .44.0.25:1111 k -n project-snake exec backend-0 -- curl -s 10 .44.0.23:2222 k -n project-snake exec backend-0 -- curl -s 10 .44.0.22:3333 # 24_np.yaml apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : np-backend namespace : project-snake spec : podSelector : matchLabels : app : backend policyTypes : - Egress # policy is only about Egress egress : - # first rule to : # first condition \"to\" - podSelector : matchLabels : app : db1 ports : # second condition \"port\" - protocol : TCP port : 1111 - # second rule to : # first condition \"to\" - podSelector : matchLabels : app : db2 ports : # second condition \"port\" - protocol : TCP port : 2222 Question 25 | Etcd Snapshot Save and Restore Task weight: 8 % Use context: kubectl config use-context k8s-c3-CCC Make a backup of etcd running on cluster3-master1 and save it on the master node at /tmp/etcd-backup.db. Then create a Pod of your kind in the cluster. Finally restore the backup, confirm the cluster is still working and that the created Pod is no longer with us. Etcd Backup First we log into the master and try to create a snapshop of etcd: ssh cluster3-master1 ETCDCTL_API = 3 etcdctl snapshot save /tmp/etcd-backup.db But it fails because we need to authenticate ourselves. For the necessary information we can check the etc manifest: vim /etc/kubernetes/manifests/etcd.yaml # OR But we also know that the api-server is connecting to etcd, so we can check how its manifest is configured: cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep etcd We use the authentication information and pass it to etcdctl: ETCDCTL_API = 3 etcdctl snapshot save /tmp/etcd-backup.db \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key # NOTE: Dont use snapshot status because it can alter the snapshot file and render it invalid Etcd restore Now create a Pod in the cluster and wait for it to be running: kubectl run test --image = nginx # NOTE: If you didn't solve questions 18 or 20 and cluster3 doesn't have a ready worker node then the created pod might stay in a Pending state. This is still ok for this task. Next we stop all controlplane components: cd /etc/kubernetes/manifests/ mv * .. watch crictl ps Now we restore the snapshot into a specific directory: ETCDCTL_API = 3 etcdctl snapshot restore /tmp/etcd-backup.db \\ --data-dir /var/lib/etcd-backup \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key We could specify another host to make the backup from by using etcdctl --endpoints http://IP, but here we just use the default value which is: http://127.0.0.1:2379,http://127.0.0.1:4001. The restored files are located at the new folder /var/lib/etcd-backup, now we have to tell etcd to use that directory: vim /etc/kubernetes/etcd.yaml - hostPath: path: /var/lib/etcd-backup # change Now we move all controlplane yaml again into the manifest directory. Give it some time ( up to several minutes ) for etcd to restart and for the api-server to be reachable again: mv ../*.yaml . watch crictl ps Then we check again for the Pod: kubectl get pod -l run = test Awesome, backup and restore worked as our pod is gone. Extra Question 1 | Find Pods first to be terminated Use context: kubectl config use-context k8s-c1-H Check all available Pods in the Namespace project-c13 and find the names of those that would probably be terminated first if the nodes run out of resources ( cpu or memory ) to schedule all Pods. Write the Pod names into /opt/course/e1/pods-not-stable.txt. Answer: When available cpu or memory resources on the nodes reach their limit, Kubernetes will look for Pods that are using more resources than they requested. These will be the first candidates for termination. If some Pods containers have no resource requests/limits set, then by default those are considered to use more than requested. Kubernetes assigns Quality of Service classes to Pods based on the defined resources and limits, read more here: https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod Hence we should look for Pods without resource requests defined, we can do this with a manual approach: k -n project-c13 describe pod | less -p Requests # describe all pods and highlight Requests k -n project-c13 describe pod | egrep \"^(Name:| Requests:)\" -A1 # We see that the Pods of Deployment c13-3cc-runner-heavy don't have any resources requests specified. Hence our answer would be: Hence our answer would be: # /opt/course/e1/pods-not-stable.txt c13-3cc-runner-heavy-65588d7d6-djtv9map c13-3cc-runner-heavy-65588d7d6-v8kf5map c13-3cc-runner-heavy-65588d7d6-wwpb4map o3db-0 o3db-1 # maybe not existing if already removed via previous scenario To automate this process you could use jsonpath like this: k -n project-c13 get pod \\ -o jsonpath = \"{range .items[*]} {.metadata.name}{.spec.containers[*].resources}{'\\n'}\" Or we look for the Quality of Service classes: k get pods -n project-c13 \\ -o jsonpath = \"{range .items[*]}{.metadata.name} {.status.qosClass}{'\\n'}\" # Here we see three with BestEffort, which Pods get that don't have any memory or cpu limits or requests defined. # A good practice is to always set resource requests and limits. If you don't know the values your containers should have you can find this out using metric tools like Prometheus. You can also use kubectl top pod or even kubectl exec into the container and use top and similar tools. Extra Question 2 | Curl Manually Contact API Use context: kubectl config use-context k8s-c1-H There is an existing ServiceAccount secret-reader in Namespace project-hamster. Create a Pod of image curlimages/curl:7.65.3 named tmp-api-contact which uses this ServiceAccount. Make sure the container keeps running. Exec into the Pod and use curl to access the Kubernetes Api of that cluster manually, listing all available secrets. You can ignore insecure https connection. Write the command ( s ) for this into file /opt/course/e4/list-secrets.sh. Answer: https://kubernetes.io/docs/tasks/run-application/access-api-from-pod # It's important to understand how the Kubernetes API works. For this it helps connecting to the api manually, for example using curl. You can find information fast by search in the Kubernetes docs for \"curl api\" for example. First we create our Pod: k run tmp-api-contact \\ --image = curlimages/curl:7.65.3 $do \\ --command > e2.yaml -- sh -c 'sleep 1d' Add the service account name and Namespace: Then run and exec into: k -f e2.yaml create k -n project-hamster exec tmp-api-contact -it -- sh Once on the container we can try to connect to the api using curl, the api is usually available via the Service named kubernetes in Namespace default ( You should know how dns resolution works across Namespaces. ) . Else we can find the endpoint IP via environment variables running env. So now we can do : curl https://kubernetes.default curl -k https://kubernetes.default # ignore insecure as allowed in ticket description curl -k https://kubernetes.default/api/v1/secrets # should show Forbidden 403 The last command shows 403 forbidden, this is because we are not passing any authorisation information with us. The Kubernetes Api Server thinks we are connecting as system:anonymous. We want to change this and connect using the Pods ServiceAccount named secret-reader. We find the the token in the mounted folder at /var/run/secrets/kubernetes.io/serviceaccount, so we do : TOKEN = $( cat /var/run/secrets/kubernetes.io/serviceaccount/token ) curl -k https://kubernetes.default/api/v1/secrets -H \"Authorization: Bearer ${ TOKEN } \" # Now we're able to list all Secrets, registering as the ServiceAccount secret-reader under which our Pod is running. To use encrypted https connection we can run: CACERT = /var/run/secrets/kubernetes.io/serviceaccount/ca.crt curl --cacert ${ CACERT } https://kubernetes.default/api/v1/secrets -H \"Authorization: Bearer ${ TOKEN } \" For troubleshooting we could also check if the ServiceAccount is actually able to list Secrets using: k auth can-i get secret --as system:serviceaccount:project-hamster:secret-reader # yes Finally write the commands into the requested location: # /opt/course/e4/list-secrets.sh TOKEN = $( cat /var/run/secrets/kubernetes.io/serviceaccount/token ) curl -k https://kubernetes.default/api/v1/secrets -H \"Authorization: Bearer ${ TOKEN } \" Preview Question 1 Use context: kubectl config use-context k8s-c2-AC The cluster admin asked you to find out the following information about etcd running on cluster2-master1: Server private key location Server certificate expiration date Is client certificate authentication enabled Write these information into /opt/course/p1/etcd-info.txt # Finally you're asked to save an etcd snapshot at /etc/etcd-snapshot.db on cluster2-master1 and display its status. Answer: Find out etcd information # Let's check the nodes: k get node ssh cluster2-master1 First we check how etcd is setup in this cluster: kubectl -n kube-system get pod We see its running as a Pod, more specific a static Pod. So we check for the default kubelet directory for static manifests: find /etc/kubernetes/manifests/ vim /etc/kubernetes/manifests/etcd.yaml - command: - etcd - --advertise-client-urls = https://192.168.102.11:2379 - --cert-file = /etc/kubernetes/pki/etcd/server.crt # server certificate - --client-cert-auth = true # enabled - --key-file = /etc/kubernetes/pki/etcd/server.key # server private key # We see that client authentication is enabled and also the requested path to the server private key, now let's find out the expiration of the server certificate: openssl x509 -noout -text -in /etc/kubernetes/pki/etcd/server.crt | grep Validity -A2 # There we have it. Let's write the information into the requested file: # /opt/course/p1/etcd-info.txt Server private key location: /etc/kubernetes/pki/etcd/server.key Server certificate expiration date: Sep 13 13 :01:31 2022 GMT Is client certificate authentication enabled: yes Create etcd snapshot ETCDCTL_API = 3 etcdctl snapshot save /etc/etcd-snapshot.db \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key This worked. Now we can output the status of the backup file: ETCDCTL_API = 3 etcdctl snapshot status /etc/etcd-snapshot.db Preview Question 2 Use context: kubectl config use-context k8s-c1-H # You're asked to confirm that kube-proxy is running correctly on all nodes. For this perform the following in Namespace project-hamster: Create a new Pod named p2-pod with two containers, one of image nginx:1.21.3-alpine and one of image busybox:1.31. Make sure the busybox container keeps running for some time. Create a new Service named p2-service which exposes that Pod internally in the cluster on port 3000 ->80. # Find the kube-proxy container on all nodes cluster1-master1, cluster1-worker1 and cluster1-worker2 and make sure that it's using iptables. Use command crictl for this. Write the iptables rules of all nodes belonging the created Service p2-service into file /opt/course/p2/iptables.txt. Finally delete the Service and confirm that the iptables rules are gone from all nodes. Answer: Create the Pod First we create the Pod: k run p2-pod --image = nginx:1.21.3-alpine $do > p2.yaml # p2.yaml - image: busybox:1.31 # add name: c2 # add command: [ \"sh\" , \"-c\" , \"sleep 1d\" ] # add Create the Service Next we create the Service: k -n project-hamster expose pod p2-pod --name p2-service --port 3000 --target-port 80 We should confirm Pods and Services are connected, hence the Service should have Endpoints. k -n project-hamster get pod,svc,ep Confirm kube-proxy is running and is using iptables First we get nodes in the cluster: k get node The idea here is to log into every node, find the kube-proxy container and check its logs: ssh cluster1-master1 crictl ps | grep kube-proxy crictl logs 27b6a18c0f89c This should be repeated on every node and result in the same output Using iptables Proxier. Check kube-proxy is creating iptables rules Now we check the iptables rules on every node first manually: ssh cluster1-master1 iptables-save | grep p2-service # Great. Now let's write these logs into the requested file: ssh cluster1-master1 iptables-save | grep p2-service >> /opt/course/p2/iptables.txt ssh cluster1-worker1 iptables-save | grep p2-service >> /opt/course/p2/iptables.txt ssh cluster1-worker2 iptables-save | grep p2-service >> /opt/course/p2/iptables.txt Delete the Service and confirm iptables rules are gone Delete the Service: k -n project-hamster delete svc p2-service And confirm the iptables rules are gone: ssh cluster1-master1 iptables-save | grep p2-service # Kubernetes Services are implemented using iptables rules (with default config) on all nodes. Every time a Service has been altered, created, deleted or Endpoints of a Service have changed, the kube-apiserver contacts every node's kube-proxy to update the iptables rules according to the current state. Preview Question 3 Use context: kubectl config use-context k8s-c2-AC Create a Pod named check-ip in Namespace default using image httpd:2.4.41-alpine. Expose it on port 80 as a ClusterIP Service named check-ip-service. Remember/output the IP of that Service. Change the Service CIDR to 11 .96.0.0/12 for the cluster. Then create a second Service named check-ip-service2 pointing to the same Pod to check if your settings did take effect. Finally check if the IP of the first Service has changed. Answer: # Let's create the Pod and expose it: k run check-ip --image = httpd:2.4.41-alpine k expose pod check-ip --name check-ip-service --port 80 And check the Pod and Service ips: k get svc,ep -l run = check-ip Now we change the Service CIDR on the kube-apiserver: ssh cluster2-master1 vim /etc/kubernetes/manifests/kube-apiserver.yaml - --service-cluster-ip-range = 11 .96.0.0/12 # change Give it a bit of time for the kube-apiserver and controller-manager to restart Wait for the api to be up again: kubectl -n kube-system get pod | grep api Now we do the same for the controller manager: vim /etc/kubernetes/manifests/kube-controller-manager.yaml - --service-cluster-ip-range = 11 .96.0.0/12 # change Give it a bit for the controller-manager to restart. We can check if it was restarted using crictl: crictl ps | grep scheduler Checking our existing Pod and Service again: k get pod,svc -l run = check-ip Nothing changed so far. Now we create another Service like before: k expose pod check-ip --name check-ip-service2 --port 80 And check again: k get svc,ep -l run = check-ip There we go, the new Service got an ip of the new specified range assigned. We also see that both Services have our Pod as endpoint.","title":"Topics to focus on"},{"location":"k8s/3-commands/","text":"Useful subcommand \u00b6 kubectl set # k set --help will tell you what parameters can be set kubectl label kubectl scale kubectl edit kns default # alias to set context to default kubectl create deployment examplehttpapp --image = katacoda/docker-http-server --replicas = 2 kubectl get deployments kubectl get pods -o wide kubectl get pods -L labels # L adds a custom column called labels in tablular output # Set # Changing images on both the pods at the same time kubectl set image deploy examplehttpapp * = nginx:1.19 # Check if image has been updated kubectl describe po <podname> | grep -i image # -i for insensitive search kubectl describe po <podname> | grep -i image -A 2 # -A for showing next 2 lines 'A'fter match kubectl create ns testns kns testns # alias to set context to testns kubectl create deployment namespacedeg -n testns --image = katacoda/docker-http-server kubectl get pods -n testns # Scaling kns default # alias to set context to default kubectl scale deployment examplehttpapp --replicas = 5 --record # record is important for scaling kubectl --record = true set image deployment examplehttpapp docker-http-server = katacoda/docker-http-server:v2 kubectl rollout status deployment examplehttpapp # It will show 2 change cause records kubectl rollout undo deployment examplehttpapp # Incase you want to undo the last change # Labels # Edit kubectl edit deploy examplehttpapp # Change the image, another way to do instead of set kubectl expose deployment examplehttpapp --port 80 kubectl get svc -o wide kubectl describe svc examplehttpapp # Incase you just want to see the labels on the svc, instead of describe kubectl get svc --show-labels kubectl get services -l app = examplehttpapp -o go-template = '{{(index .items 0).spec.clusterIP}}' curl $( kubectl get services -l app = examplehttpapp -o go-template = '{{(index .items 0).spec.clusterIP}}' ) kubectl logs $( kubectl get pods -l app = examplehttpapp -o go-template = '{{(index .items 0).metadata.name}}' ) Create a temporary Pod and execute the wget command inside of its container using the IP address and the container port. kubectl run busybox --image = busybox --rm -it --restart = Never -n ckad -- wget 10 .244.1.2:80 All Commands \u00b6 ################# # How to navigate through your cluster. ################# # To see only the Kubernetes kubectl client version and not the Kubernetes version kubectl version --client = true # You might need to gather information about the endpoints of the master and services in the cluster. This information will come in handy if you have to troubleshoot your cluster. kubectl cluster-info # Command autocompletion help and options kubectl completion -h # setup is using the bash shell source < ( kubectl completion bash ) # check the status of the Nodes kubectl get no # various Namespaces that are critical to Kubernetes operations kubectl get ns # look at the Pods in the cluster kubectl get po kubectl get po -n kube-system # see the Pods in all the Namespaces kubectl get pods -A # see deployments kubectl get deploy # Shows key information about the Deployment such as: # Labels, Number of Replicas, Annotations, Deployment Strategy Type, Events kubectl describe deploy nginx # Get additional information on the Pods kubectl get pod -o wide # YAML output format kubectl get pod -o yaml # sort the output of queries, you can use the --sort-by flag # sort by various data points for pod specs like # Pod Ip, Pod name, Pod nodeName, Pod hostname, Pod volumes kubectl get pod -o wide --sort-by = .status.podIP kubectl get pod -o wide --sort-by = .spec.podName # Get the documentation for Kubernetes resources such as Pods or Services: kubectl explain deployment # View all the supported resource types: kubectl api-resources # View all the resources kubectl get all ################# # How to switch between contexts and namespaces # How to find resources and format their output # How to update, patch, delete, and scale resources ################# # View kubectl configuration kubectl view config # list of all our configured clusters kubectl config get-clusters # From above 2 commands, you can see for example - there is currently one cluster named kubernetes and one user named kubernetes-admin. # Same can be seen in one line with below command. In this case, the context is kubernetes-admin@kubernetes. kubectl config current-context # create a new kubectl context using the existing kubernetes-admin user. kubectl config set-context dev-context --cluster kubernetes --user = kubernetes-admin kubectl config get-contexts # Notice that the current active context is set to kubernetes-admin@kubernetes # To switch to the dev-context context kubectl config use-context dev-context # Let's switch back to the kubernetes-admin@kubernetes context kubectl config use-context kubernetes-admin@kubernetes # As you can see, switching back and forth between contexts with the above kubectl command can be very tedious especially when dealing with multiple contexts. # The alternative would be to install kubectx. # When the installion is done, type the kubectx command to list all the contexts: kubectx # Notice that the current active context kubernetes-admin@kubernetes is highlighted in yellow # To switch between contexts, you can now type kubectx CONTEXT NAME. kubectx dev-context # Go ahead and switch back to the kubernetes-admin@kubernetes context: kubectx kubernetes-admin@kubernetes # to create the namespace: kubectl create ns frontend # Declarative method kubectl create namespace backend -o yaml --dry-run = client > ns-backend.yaml kubectl apply -f ns-backend.yaml # deploy a single redis container kubectl run redis --image = quay.io/quay/redis -n backend kubectl run nginx --image = quay.io/bitnami/nginx -n frontend kubectl get pods -n frontend # Switch to the appropriate namespace context where the resources live without having to specify ns kubectl config set-context --current --namespace = frontend # If you are constantly switching between namespaces and want to avoid using the long kubectl command above, then the kubens plugin becomes handy. # Let's list all the namespaces using the kubens command. kubens # Notice again, the current active namespace default is highlighted in yellow. # let's switch to the frontend namespace kubens frontend # switch back to the default namespace: kubens default # You can use the --selector flag to filter and find resources based on their assigned labels. # Use the deployment yaml mentioned below kubectl create -f ~/label-deploy.yaml kubectl get pods -n frontend --show-labels # find all the pods that have the label app: web in the frontend namespace: kubectl get pods -n frontend --selector = app = web # You can also use the -l flag, which represents label and is equivalent to the --selector flag. kubectl get pods -n frontend -l app = haproxy # let's find nodes within our cluster that do NOT have the taint label: node-role.kubernetes.io/master kubectl get nodes --selector = '!node-role.kubernetes.io/master' # As you can see, the --selector or -l flags could come in very handy when identifying thousands of kubernetes resources with differing labels. # Use jsonpath to find/filter resources # The -o=jsonpath flag with the kubectl command allows you to filter resources and display them in the way you desire. # Let's say we want to find the names of all the kubernetes nodes along with their CPU resources. kubectl get nodes -o = jsonpath = '{.items[*].metadata.name} {.items[*].status.capacity.cpu}' # As you may notice, the output does not look pretty. What if we add a \\n (newline character) between the two JSONPath pairs as: kubectl get nodes -o = jsonpath = '{.items[*].metadata.name}{\"\\n\"}{.items[*].status.capacity.cpu}{\"\\n\"}' # we wanted to get an output that is formatted as the output shown below: # master 2 # node01 4 # To achieve this, we would use the range JSONPath operator to iterate through each item (nodes in this case) and use tabulation \\t as well as new line \\n characters to achieve the desired output. # To do this in JSONPath, we would use the range and end operators kubectl get nodes -o = jsonpath = '{range .items[*]}{.metadata.name}{\"\\t\"}{.status.capacity.cpu}{\"\\n\"}{end}' # Format output with custom-columns # we want to get all nodes within our cluster and nicely format the output with a column header called NAME. kubectl get nodes -o = custom-columns = NAME:.metadata.name # You can add additional columns to the above command by adding JSONPath pairs (COLUMN HEADER:.metadata) separated by a comma. kubectl get nodes -o = custom-columns = NAME:.metadata.name,CPU:.status.capacity.cpu # let's find all the pods that were deployed and output them in a tabulated format with column headers POD_NAME and IMAGE_VER: kubectl get pods -n frontend -o custom-columns = POD_NAME:.metadata.name,IMAGE_VER:.spec.containers [ * ] .image # Scale resources kubectl create deployment nginx-deployment --image = quay.io/bitnami/nginx:1.20 kubectl scale deploy nginx-deployment --replicas = 5 # scale the deployment down to 1 replica kubectl scale deploy/nginx-deployment --replicas = 1 # Update resources # we are going to update the nginx image from nginx:1.20 to nginx:1.21 with no downtime. kubectl set image deployment/nginx-deployment nginx = quay.io/bitnami/nginx:1.21 --record # watch the status of the nginx-deployment deployment's rollingUpdate changes until completion. kubectl rollout status -w deployment/nginx-deployment # output of the rollout history kubectl rollout history deployment/nginx-deployment # To undo the update kubectl rollout undo deployment/nginx-deployment # Let's change the image version back to nginx:1.21 kubectl rollout undo deployment/nginx-deployment --to-revision = 2 # Patch and label resources # Patching can be used to partially update any kubernetes resources such as nodes, pods, deployments, etc. # we are going to deploy an nginx pod with a label of env: prod kubectl run nginx --image = quay.io/bitnami/nginx --labels = env = prod kubectl get pod nginx --show-labels # let's update the label to env=dev using the patch command: kubectl patch pod nginx -p '{\"metadata\":{\"labels\":{\"env\":\"dev\"}}}' kubectl get pod nginx --show-labels # We can also use the kubectl label command to add a label, update an existing label, or delete a label. kubectl label pod nginx env = prod --overwrite # Note: the --overwrite flag is used when the label already exists. # To delete the label, append the - to env , which is the value of the label's key. # Alternatively, use the kubectl edit pod nginx command and manually edit the .metadata.label.env and save your changes. # Delete resources kubectl delete pod nginx ################# # Advanced kubeclt commands that can be used in the field as a cluster operator/administrator. # > krew a kubectl plugin manager # > Interaction with pods # kubectl logs # kubectl cp # kubectl exec # > Interacting with nodes: # kubectl taint # Pod's Tolerations # kubectl cordon/uncordon # kubectl drain # kubectl top ################# # krew is a plugin manager for kubectl. # We will be using the following plugins: # access-matrix - shows an RBAC (role based access control) access matrix for server resources # ns - view or change switch namespace contexts # ctx - switch between Kubernetes cluster contexts # Let's discover some of these plugins: kubectl krew search # Install plugins via krew cat > ~/plugins <<EOF access-matrix ca-cert ctx get-all iexec images ns pod-dive pod-logs whoami who-can EOF # Install the plugins for plugin in $( cat ~/plugins ) ; do echo -en $( kubectl krew install $plugin ) ; done # Verify and list the installed plugins: kubectl krew list # You can also list the installed plugins: kubectl plugin list # We can begin by listing who the current authenticated user is: kubectl whoami # Let's also look at the who-can plugin, which is equivalent to the kubectl auth can-i VERB [TYPE/NAME]: kubectl who-can create nodes kubectl who-can '*' pods # list the namespaces: kubectl ns # Let's get the name of the first pod, assign it to a variable and run the pod-dive plugin: POD = $( kubectl get pods -o = jsonpath = '{.items[0].metadata.name}' ) && echo $POD kubectl pod-dive $POD # The above output shows a nice pod resource tree (node, namespace, type of resource, etc.). # display all the images in all namespaces: kubectl images -A # access-matrix plug-in, which is handy when looking for a RBAC Access matrix for Kubernetes resources: kubectl access-matrix # Interacting with pods # Let's switch to the kube-system namespace and access some logs: kubectl ns kube-system # Use the pod-logs plugin to get the weave pods logs: kubectl pod-logs # The pod-logs plug-in does not allow output redirection. Therefore, if you want to redirect the output use kubectl logs as such: kubectl logs POD -c CONTAINER > logsfile # kubectl exec /iexec # Let's create a single container pod called test with an nginx image: kubectl run test --image = quay.io/bitnami/nginx # Let's get the output of the date command from the running test container without logging into it: kubectl exec test -- date # Using the iexec plug-in, let's get the content of the /etc/resolv.conf/ file from the running test container: kubectl iexec test cat /etc/resolv.conf # To login and interact with the container's shell kubectl iexec test # Alternatively, you can use the below command: kubectl exec test -it -- /bin/sh # kubectl cp # The cp command can be used to copy files and directories to and from containers within a pod. # let's copy the content of the krew-install directory to the test container's /tmp directory kubectl cp ~/krew-install test:/tmp # Let's verify whether the directory has been copied. kubectl iexec test ls /tmp/krew-install # Now, let's copy the welcome.txt file from the test container to the master server's /tmp directory: kubectl cp test:/tmp/welcome.txt /tmp/welcome.txt # kubectl taint # A taint consist of a key, value, and effect. As an argument, it is expressed as key=value:effect. # The effect should be one these values: NoShedule, PreferNoSchedule, or NoExecute # Here is how it is used with the kubectl command: kubectl taint NODE NAME KEY_1 = VAL_1:TAINT_EFFECT # Let's taint node01 as dedicated to the devops-group only kubectl taint node node01 dedicated = devops-group:NoSchedule # Verify that node01 is tainted: kubectl describe node node01 | grep -i taints # You can also check the taints on all nodes: kubectl get nodes -o custom-columns = NAME:.metadata.name,TAINTS:.spec.taints [ * ] .key # Let's now try to deploy a single pod: kubectl run my-app --image = quay.io/bitnami/nginx # The newly deployed pod will be in a pending state, because it will not tolerate the taints applied to both nodes. Therefore, it will not be scheduled. # To see the error, type the below command and check under the events section: kubectl describe pod my-app # Alternatively, you can run the below command: kubectl get events # There are 2 ways to solve this issue. We can add a toleration matching the taint that was applied to the nodes, or remove the taint from the nodes. For now, let's remove the taint on node01: kubectl taint node node01 dedicated- # Note: to remove a taint, append the - to the value of the key. # And by default, the control node is tainted with node-role.kubernetes.io/master, therefore, any pod that does not have a toleration matching the node's taint cannot be deployed onto the control node. # kubectl cordon # Let's now try to get one of the pods that are deployed on node01 and assign its name to a variable: APOD = $( kubectl get pods -ojsonpath = '{.items[?(@.spec.nodeName == \"node01\")].metadata.name}' ) && echo $APOD # let's run the pod-dive plugin: kubectl pod-dive $APOD # Before we drain the node, we will cordon it first. cordon means ensuring that no pod can be scheduled on the particular node. kubectl cordon node01 # If you list the nodes now, you will find the status of node01 set to Ready,SchedulingDisabled kubectl get nodes # kubectl drain # Draining a node means removing all running pods from the node, typically performed for maintenance activities. # Open a second terminal and run the below command to watch the output in Terminal 2: watch -d kubectl get pods -o wide # Run the below command to drain node01: kubectl drain node01 --ignore-daemonsets # you will observe, how the pods in node01 are being terminated and re-deployed on the controlplane node. # Now, let's uncordon node01: kubectl uncordon node01 # In Terminal 2, you will notice that the pods have not been moved back to node01. These Pods will not be rescheduled automatically to the new nodes. # let's try to scale up the deployment to 8 replicas. kubectl scale deployment/nginx-deployment --replicas = 8 # Note: The --ignore-daemonsets flag in the kubectl drain command is required because DaemonSet pods are required to run on each node when deployed. This allows pods that are not part of a DaemonSet to be re-deployed on another available node # kubectl top # The kubectl top allows you to see the resource consumption for nodes or pods. However, in order to use the top command, we have to install a metrics server. git clone https://github.com/mbahvw/kubernetes-metrics-server.git kubectl apply -f kubernetes-metrics-server/ # Let's verify that we are getting a response from the metric server API: kubectl get --raw /apis/metrics.k8s.io/ # Let's get the CPU and memory utilization for all nodes in the cluster: kubectl top nodes # let's try to get the memory and CPU utilization of pods in all namespaces: kubectl top pods --all-namespaces # We can also gather the metrics of all the pods in the kube-system namespace: kubectl top pods -n kube-system # cat ~/label-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : frontend-deployment labels : app : web tier : frontend namespace : frontend spec : replicas : 3 selector : matchLabels : app : web template : metadata : labels : app : web spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : haproxy-deployment labels : app : haproxy tier : frontend namespace : frontend spec : replicas : 3 selector : matchLabels : app : haproxy template : metadata : labels : app : haproxy spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80 Install kubectx \u00b6 cat ~/kubectx.sh #!/bin/bash cd ~/ echo \"You are on the $PWD directory\" sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens Install Krew \u00b6 # cat ~/krew-install/install-krew.sh #!/bin/bash #Downloading krew from repo\" echo -en \"Downloading and installing krew\\n\" set -x ; cd \" $( mktemp -d ) \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/download/v0.3.4/krew.{tar.gz,yaml}\" && tar zxvf krew.tar.gz && KREW = ./krew- \" $( uname | tr '[:upper:]' '[:lower:]' ) _amd64\" && \" $KREW \" install --manifest = krew.yaml --archive = krew.tar.gz && \" $KREW \" update #Adding it to home user ~/.bashrc file echo 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' >>~/.bashrc #source the bashrc and restart bash source ~/.bashrc exec bash # Automation ideas # cat kc_step3.sh #!/bin/bash kubectl config set-context test-context --cluster kubernetes --user = kubernetes-admin for x in developers admins dbadmins ; do kubectl create namespace $x ; done cd ~/deployment kubectl create -f explore-deploy.yaml # cat kc_step4.sh #!/bin/bash kubectl ns default cd ~/deployment kubectl delete -f explore-deploy.yaml kubectl delete namespace developers kubectl delete namespace dbadmins kubectl delete namespace admins # cat explore-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : frontend-deployment labels : app : webapp tier : devops namespace : developers spec : replicas : 3 selector : matchLabels : app : webapp template : metadata : labels : app : webapp spec : containers : - name : nginx image : quay.io/bitnami/nginx:latest ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : haproxy-deployment labels : app : haproxy tier : admins namespace : admins spec : replicas : 3 selector : matchLabels : app : haproxy template : metadata : labels : app : haproxy spec : containers : - name : nginx image : quay.io/bitnami/nginx:latest ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : db-deployment labels : app : redis tier : dbadmins namespace : dbadmins spec : replicas : 3 selector : matchLabels : app : redis template : metadata : labels : app : redis spec : containers : - name : redis image : quay.io/quay/redis # Deploy pod on master node as pod taints match master node # cat nginx-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : web tier : frontend namespace : default spec : replicas : 4 selector : matchLabels : app : web template : metadata : labels : app : web spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80 tolerations : - key : \"node-role.kubernetes.io/master\" operator : Exists effect : NoSchedule","title":"Useful subcommand"},{"location":"k8s/3-commands/#useful-subcommand","text":"kubectl set # k set --help will tell you what parameters can be set kubectl label kubectl scale kubectl edit kns default # alias to set context to default kubectl create deployment examplehttpapp --image = katacoda/docker-http-server --replicas = 2 kubectl get deployments kubectl get pods -o wide kubectl get pods -L labels # L adds a custom column called labels in tablular output # Set # Changing images on both the pods at the same time kubectl set image deploy examplehttpapp * = nginx:1.19 # Check if image has been updated kubectl describe po <podname> | grep -i image # -i for insensitive search kubectl describe po <podname> | grep -i image -A 2 # -A for showing next 2 lines 'A'fter match kubectl create ns testns kns testns # alias to set context to testns kubectl create deployment namespacedeg -n testns --image = katacoda/docker-http-server kubectl get pods -n testns # Scaling kns default # alias to set context to default kubectl scale deployment examplehttpapp --replicas = 5 --record # record is important for scaling kubectl --record = true set image deployment examplehttpapp docker-http-server = katacoda/docker-http-server:v2 kubectl rollout status deployment examplehttpapp # It will show 2 change cause records kubectl rollout undo deployment examplehttpapp # Incase you want to undo the last change # Labels # Edit kubectl edit deploy examplehttpapp # Change the image, another way to do instead of set kubectl expose deployment examplehttpapp --port 80 kubectl get svc -o wide kubectl describe svc examplehttpapp # Incase you just want to see the labels on the svc, instead of describe kubectl get svc --show-labels kubectl get services -l app = examplehttpapp -o go-template = '{{(index .items 0).spec.clusterIP}}' curl $( kubectl get services -l app = examplehttpapp -o go-template = '{{(index .items 0).spec.clusterIP}}' ) kubectl logs $( kubectl get pods -l app = examplehttpapp -o go-template = '{{(index .items 0).metadata.name}}' ) Create a temporary Pod and execute the wget command inside of its container using the IP address and the container port. kubectl run busybox --image = busybox --rm -it --restart = Never -n ckad -- wget 10 .244.1.2:80","title":"Useful subcommand"},{"location":"k8s/3-commands/#all-commands","text":"################# # How to navigate through your cluster. ################# # To see only the Kubernetes kubectl client version and not the Kubernetes version kubectl version --client = true # You might need to gather information about the endpoints of the master and services in the cluster. This information will come in handy if you have to troubleshoot your cluster. kubectl cluster-info # Command autocompletion help and options kubectl completion -h # setup is using the bash shell source < ( kubectl completion bash ) # check the status of the Nodes kubectl get no # various Namespaces that are critical to Kubernetes operations kubectl get ns # look at the Pods in the cluster kubectl get po kubectl get po -n kube-system # see the Pods in all the Namespaces kubectl get pods -A # see deployments kubectl get deploy # Shows key information about the Deployment such as: # Labels, Number of Replicas, Annotations, Deployment Strategy Type, Events kubectl describe deploy nginx # Get additional information on the Pods kubectl get pod -o wide # YAML output format kubectl get pod -o yaml # sort the output of queries, you can use the --sort-by flag # sort by various data points for pod specs like # Pod Ip, Pod name, Pod nodeName, Pod hostname, Pod volumes kubectl get pod -o wide --sort-by = .status.podIP kubectl get pod -o wide --sort-by = .spec.podName # Get the documentation for Kubernetes resources such as Pods or Services: kubectl explain deployment # View all the supported resource types: kubectl api-resources # View all the resources kubectl get all ################# # How to switch between contexts and namespaces # How to find resources and format their output # How to update, patch, delete, and scale resources ################# # View kubectl configuration kubectl view config # list of all our configured clusters kubectl config get-clusters # From above 2 commands, you can see for example - there is currently one cluster named kubernetes and one user named kubernetes-admin. # Same can be seen in one line with below command. In this case, the context is kubernetes-admin@kubernetes. kubectl config current-context # create a new kubectl context using the existing kubernetes-admin user. kubectl config set-context dev-context --cluster kubernetes --user = kubernetes-admin kubectl config get-contexts # Notice that the current active context is set to kubernetes-admin@kubernetes # To switch to the dev-context context kubectl config use-context dev-context # Let's switch back to the kubernetes-admin@kubernetes context kubectl config use-context kubernetes-admin@kubernetes # As you can see, switching back and forth between contexts with the above kubectl command can be very tedious especially when dealing with multiple contexts. # The alternative would be to install kubectx. # When the installion is done, type the kubectx command to list all the contexts: kubectx # Notice that the current active context kubernetes-admin@kubernetes is highlighted in yellow # To switch between contexts, you can now type kubectx CONTEXT NAME. kubectx dev-context # Go ahead and switch back to the kubernetes-admin@kubernetes context: kubectx kubernetes-admin@kubernetes # to create the namespace: kubectl create ns frontend # Declarative method kubectl create namespace backend -o yaml --dry-run = client > ns-backend.yaml kubectl apply -f ns-backend.yaml # deploy a single redis container kubectl run redis --image = quay.io/quay/redis -n backend kubectl run nginx --image = quay.io/bitnami/nginx -n frontend kubectl get pods -n frontend # Switch to the appropriate namespace context where the resources live without having to specify ns kubectl config set-context --current --namespace = frontend # If you are constantly switching between namespaces and want to avoid using the long kubectl command above, then the kubens plugin becomes handy. # Let's list all the namespaces using the kubens command. kubens # Notice again, the current active namespace default is highlighted in yellow. # let's switch to the frontend namespace kubens frontend # switch back to the default namespace: kubens default # You can use the --selector flag to filter and find resources based on their assigned labels. # Use the deployment yaml mentioned below kubectl create -f ~/label-deploy.yaml kubectl get pods -n frontend --show-labels # find all the pods that have the label app: web in the frontend namespace: kubectl get pods -n frontend --selector = app = web # You can also use the -l flag, which represents label and is equivalent to the --selector flag. kubectl get pods -n frontend -l app = haproxy # let's find nodes within our cluster that do NOT have the taint label: node-role.kubernetes.io/master kubectl get nodes --selector = '!node-role.kubernetes.io/master' # As you can see, the --selector or -l flags could come in very handy when identifying thousands of kubernetes resources with differing labels. # Use jsonpath to find/filter resources # The -o=jsonpath flag with the kubectl command allows you to filter resources and display them in the way you desire. # Let's say we want to find the names of all the kubernetes nodes along with their CPU resources. kubectl get nodes -o = jsonpath = '{.items[*].metadata.name} {.items[*].status.capacity.cpu}' # As you may notice, the output does not look pretty. What if we add a \\n (newline character) between the two JSONPath pairs as: kubectl get nodes -o = jsonpath = '{.items[*].metadata.name}{\"\\n\"}{.items[*].status.capacity.cpu}{\"\\n\"}' # we wanted to get an output that is formatted as the output shown below: # master 2 # node01 4 # To achieve this, we would use the range JSONPath operator to iterate through each item (nodes in this case) and use tabulation \\t as well as new line \\n characters to achieve the desired output. # To do this in JSONPath, we would use the range and end operators kubectl get nodes -o = jsonpath = '{range .items[*]}{.metadata.name}{\"\\t\"}{.status.capacity.cpu}{\"\\n\"}{end}' # Format output with custom-columns # we want to get all nodes within our cluster and nicely format the output with a column header called NAME. kubectl get nodes -o = custom-columns = NAME:.metadata.name # You can add additional columns to the above command by adding JSONPath pairs (COLUMN HEADER:.metadata) separated by a comma. kubectl get nodes -o = custom-columns = NAME:.metadata.name,CPU:.status.capacity.cpu # let's find all the pods that were deployed and output them in a tabulated format with column headers POD_NAME and IMAGE_VER: kubectl get pods -n frontend -o custom-columns = POD_NAME:.metadata.name,IMAGE_VER:.spec.containers [ * ] .image # Scale resources kubectl create deployment nginx-deployment --image = quay.io/bitnami/nginx:1.20 kubectl scale deploy nginx-deployment --replicas = 5 # scale the deployment down to 1 replica kubectl scale deploy/nginx-deployment --replicas = 1 # Update resources # we are going to update the nginx image from nginx:1.20 to nginx:1.21 with no downtime. kubectl set image deployment/nginx-deployment nginx = quay.io/bitnami/nginx:1.21 --record # watch the status of the nginx-deployment deployment's rollingUpdate changes until completion. kubectl rollout status -w deployment/nginx-deployment # output of the rollout history kubectl rollout history deployment/nginx-deployment # To undo the update kubectl rollout undo deployment/nginx-deployment # Let's change the image version back to nginx:1.21 kubectl rollout undo deployment/nginx-deployment --to-revision = 2 # Patch and label resources # Patching can be used to partially update any kubernetes resources such as nodes, pods, deployments, etc. # we are going to deploy an nginx pod with a label of env: prod kubectl run nginx --image = quay.io/bitnami/nginx --labels = env = prod kubectl get pod nginx --show-labels # let's update the label to env=dev using the patch command: kubectl patch pod nginx -p '{\"metadata\":{\"labels\":{\"env\":\"dev\"}}}' kubectl get pod nginx --show-labels # We can also use the kubectl label command to add a label, update an existing label, or delete a label. kubectl label pod nginx env = prod --overwrite # Note: the --overwrite flag is used when the label already exists. # To delete the label, append the - to env , which is the value of the label's key. # Alternatively, use the kubectl edit pod nginx command and manually edit the .metadata.label.env and save your changes. # Delete resources kubectl delete pod nginx ################# # Advanced kubeclt commands that can be used in the field as a cluster operator/administrator. # > krew a kubectl plugin manager # > Interaction with pods # kubectl logs # kubectl cp # kubectl exec # > Interacting with nodes: # kubectl taint # Pod's Tolerations # kubectl cordon/uncordon # kubectl drain # kubectl top ################# # krew is a plugin manager for kubectl. # We will be using the following plugins: # access-matrix - shows an RBAC (role based access control) access matrix for server resources # ns - view or change switch namespace contexts # ctx - switch between Kubernetes cluster contexts # Let's discover some of these plugins: kubectl krew search # Install plugins via krew cat > ~/plugins <<EOF access-matrix ca-cert ctx get-all iexec images ns pod-dive pod-logs whoami who-can EOF # Install the plugins for plugin in $( cat ~/plugins ) ; do echo -en $( kubectl krew install $plugin ) ; done # Verify and list the installed plugins: kubectl krew list # You can also list the installed plugins: kubectl plugin list # We can begin by listing who the current authenticated user is: kubectl whoami # Let's also look at the who-can plugin, which is equivalent to the kubectl auth can-i VERB [TYPE/NAME]: kubectl who-can create nodes kubectl who-can '*' pods # list the namespaces: kubectl ns # Let's get the name of the first pod, assign it to a variable and run the pod-dive plugin: POD = $( kubectl get pods -o = jsonpath = '{.items[0].metadata.name}' ) && echo $POD kubectl pod-dive $POD # The above output shows a nice pod resource tree (node, namespace, type of resource, etc.). # display all the images in all namespaces: kubectl images -A # access-matrix plug-in, which is handy when looking for a RBAC Access matrix for Kubernetes resources: kubectl access-matrix # Interacting with pods # Let's switch to the kube-system namespace and access some logs: kubectl ns kube-system # Use the pod-logs plugin to get the weave pods logs: kubectl pod-logs # The pod-logs plug-in does not allow output redirection. Therefore, if you want to redirect the output use kubectl logs as such: kubectl logs POD -c CONTAINER > logsfile # kubectl exec /iexec # Let's create a single container pod called test with an nginx image: kubectl run test --image = quay.io/bitnami/nginx # Let's get the output of the date command from the running test container without logging into it: kubectl exec test -- date # Using the iexec plug-in, let's get the content of the /etc/resolv.conf/ file from the running test container: kubectl iexec test cat /etc/resolv.conf # To login and interact with the container's shell kubectl iexec test # Alternatively, you can use the below command: kubectl exec test -it -- /bin/sh # kubectl cp # The cp command can be used to copy files and directories to and from containers within a pod. # let's copy the content of the krew-install directory to the test container's /tmp directory kubectl cp ~/krew-install test:/tmp # Let's verify whether the directory has been copied. kubectl iexec test ls /tmp/krew-install # Now, let's copy the welcome.txt file from the test container to the master server's /tmp directory: kubectl cp test:/tmp/welcome.txt /tmp/welcome.txt # kubectl taint # A taint consist of a key, value, and effect. As an argument, it is expressed as key=value:effect. # The effect should be one these values: NoShedule, PreferNoSchedule, or NoExecute # Here is how it is used with the kubectl command: kubectl taint NODE NAME KEY_1 = VAL_1:TAINT_EFFECT # Let's taint node01 as dedicated to the devops-group only kubectl taint node node01 dedicated = devops-group:NoSchedule # Verify that node01 is tainted: kubectl describe node node01 | grep -i taints # You can also check the taints on all nodes: kubectl get nodes -o custom-columns = NAME:.metadata.name,TAINTS:.spec.taints [ * ] .key # Let's now try to deploy a single pod: kubectl run my-app --image = quay.io/bitnami/nginx # The newly deployed pod will be in a pending state, because it will not tolerate the taints applied to both nodes. Therefore, it will not be scheduled. # To see the error, type the below command and check under the events section: kubectl describe pod my-app # Alternatively, you can run the below command: kubectl get events # There are 2 ways to solve this issue. We can add a toleration matching the taint that was applied to the nodes, or remove the taint from the nodes. For now, let's remove the taint on node01: kubectl taint node node01 dedicated- # Note: to remove a taint, append the - to the value of the key. # And by default, the control node is tainted with node-role.kubernetes.io/master, therefore, any pod that does not have a toleration matching the node's taint cannot be deployed onto the control node. # kubectl cordon # Let's now try to get one of the pods that are deployed on node01 and assign its name to a variable: APOD = $( kubectl get pods -ojsonpath = '{.items[?(@.spec.nodeName == \"node01\")].metadata.name}' ) && echo $APOD # let's run the pod-dive plugin: kubectl pod-dive $APOD # Before we drain the node, we will cordon it first. cordon means ensuring that no pod can be scheduled on the particular node. kubectl cordon node01 # If you list the nodes now, you will find the status of node01 set to Ready,SchedulingDisabled kubectl get nodes # kubectl drain # Draining a node means removing all running pods from the node, typically performed for maintenance activities. # Open a second terminal and run the below command to watch the output in Terminal 2: watch -d kubectl get pods -o wide # Run the below command to drain node01: kubectl drain node01 --ignore-daemonsets # you will observe, how the pods in node01 are being terminated and re-deployed on the controlplane node. # Now, let's uncordon node01: kubectl uncordon node01 # In Terminal 2, you will notice that the pods have not been moved back to node01. These Pods will not be rescheduled automatically to the new nodes. # let's try to scale up the deployment to 8 replicas. kubectl scale deployment/nginx-deployment --replicas = 8 # Note: The --ignore-daemonsets flag in the kubectl drain command is required because DaemonSet pods are required to run on each node when deployed. This allows pods that are not part of a DaemonSet to be re-deployed on another available node # kubectl top # The kubectl top allows you to see the resource consumption for nodes or pods. However, in order to use the top command, we have to install a metrics server. git clone https://github.com/mbahvw/kubernetes-metrics-server.git kubectl apply -f kubernetes-metrics-server/ # Let's verify that we are getting a response from the metric server API: kubectl get --raw /apis/metrics.k8s.io/ # Let's get the CPU and memory utilization for all nodes in the cluster: kubectl top nodes # let's try to get the memory and CPU utilization of pods in all namespaces: kubectl top pods --all-namespaces # We can also gather the metrics of all the pods in the kube-system namespace: kubectl top pods -n kube-system # cat ~/label-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : frontend-deployment labels : app : web tier : frontend namespace : frontend spec : replicas : 3 selector : matchLabels : app : web template : metadata : labels : app : web spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : haproxy-deployment labels : app : haproxy tier : frontend namespace : frontend spec : replicas : 3 selector : matchLabels : app : haproxy template : metadata : labels : app : haproxy spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80","title":"All Commands"},{"location":"k8s/3-commands/#install-kubectx","text":"cat ~/kubectx.sh #!/bin/bash cd ~/ echo \"You are on the $PWD directory\" sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens","title":"Install kubectx"},{"location":"k8s/3-commands/#install-krew","text":"# cat ~/krew-install/install-krew.sh #!/bin/bash #Downloading krew from repo\" echo -en \"Downloading and installing krew\\n\" set -x ; cd \" $( mktemp -d ) \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/download/v0.3.4/krew.{tar.gz,yaml}\" && tar zxvf krew.tar.gz && KREW = ./krew- \" $( uname | tr '[:upper:]' '[:lower:]' ) _amd64\" && \" $KREW \" install --manifest = krew.yaml --archive = krew.tar.gz && \" $KREW \" update #Adding it to home user ~/.bashrc file echo 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' >>~/.bashrc #source the bashrc and restart bash source ~/.bashrc exec bash # Automation ideas # cat kc_step3.sh #!/bin/bash kubectl config set-context test-context --cluster kubernetes --user = kubernetes-admin for x in developers admins dbadmins ; do kubectl create namespace $x ; done cd ~/deployment kubectl create -f explore-deploy.yaml # cat kc_step4.sh #!/bin/bash kubectl ns default cd ~/deployment kubectl delete -f explore-deploy.yaml kubectl delete namespace developers kubectl delete namespace dbadmins kubectl delete namespace admins # cat explore-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : frontend-deployment labels : app : webapp tier : devops namespace : developers spec : replicas : 3 selector : matchLabels : app : webapp template : metadata : labels : app : webapp spec : containers : - name : nginx image : quay.io/bitnami/nginx:latest ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : haproxy-deployment labels : app : haproxy tier : admins namespace : admins spec : replicas : 3 selector : matchLabels : app : haproxy template : metadata : labels : app : haproxy spec : containers : - name : nginx image : quay.io/bitnami/nginx:latest ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : db-deployment labels : app : redis tier : dbadmins namespace : dbadmins spec : replicas : 3 selector : matchLabels : app : redis template : metadata : labels : app : redis spec : containers : - name : redis image : quay.io/quay/redis # Deploy pod on master node as pod taints match master node # cat nginx-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : web tier : frontend namespace : default spec : replicas : 4 selector : matchLabels : app : web template : metadata : labels : app : web spec : containers : - name : nginx image : quay.io/bitnami/nginx:1.20 ports : - containerPort : 80 tolerations : - key : \"node-role.kubernetes.io/master\" operator : Exists effect : NoSchedule","title":"Install Krew"},{"location":"k8s/3-k8s/","text":"To get the version of K8s , run kubectl get nodes cat /etc/*release* - Shows the OS version Hit Ctrl + D to exit out of SSH session when moving to Nodes and coming back or type exit journalctl -u etcd.service -l - List the service logs ETCD \u00b6 ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3. By default its set to use Version 2. Each version has different sets of commands. To set the right version of API set the environment variable ETCDCTL_API command export ETCDCTL_API=3 When API version is not set, it is assumed to be set to version 2. And version 3 commands listed below don't work. When API version is set to version 3, version 2 commands listed below don't work. # ETCDCTL version 2 etcdctl backup etcdctl cluster-health etcdctl mk etcdctl mkdir etcdctl set # ETCDCTL version 3 etcdctl snapshot save etcdctl endpoint health etcdctl get etcdctl put # This command sets the ETCD version to 3 and then shows all the keys in ETCD database and also sets the certificates kubectl exec etcd-master -n kube-system -- sh -c \"ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key\" API Server \u00b6 Api Server Configuration is stored Using Kubeadm - Inside API Server Pod - /etc/kubernetes/manifests/kube-apiserver.yaml As a Service - Inside the Master Node - /etc/systemd/system/kube-apiserver.service ps -ef | grep kube-apiserver # To see all kube apiserver configuration Kube Controller Manager \u00b6 Watch Status Remediate Situation Node Controller Replicaton Controller Api Server Configuration is stored Using Kubeadm - Inside API Server Pod - /etc/kubernetes/manifests/kube-controller-manager.yaml As a Service - Inside the Master Node - /etc/systemd/system/kube-controller-manager.service ps -ef | grep kube-controller-manager # To see all kube controller-manager configuration Kube Scheduler \u00b6 Assigning a Pod to a Node: Filter Nodes Rank Nodes Using Kubeadm - Inside API Server Pod - /etc/kubernetes/manifests/kube-scheduler.yaml As a Service - Inside the Master Node - /etc/systemd/system/kube-scheduler.service ps -ef | grep kube-scheduler # To see all kube scheduler configuration Kubelet \u00b6 NOTE : Kubeadm does not install kubelet. Always install kubelet manually on the worker nodes. Always runs as a service on the worker nodes. ps -ef | grep kubelet # To see all kubelet configuration Kube Proxy \u00b6 Process running on the worker node as a service. Manual Scheduling \u00b6 Add nodeName property in the pod definition to schedule a pod at creation time if there is no scheduler. Explain Commands \u00b6 # When is it useful: sometimes when editing/creating yaml files, it is not clear where exactly rsource should be placed (indented) in the file. Using this command gives a quick overview of resources structure as well as helpful explaination. Sometimes this is faster then looking up in k8s docs. kubectl explian pods --recursive | grep envFrom -A3 # Prints lines after a match is found # Output would be envFrom < [] Object> # This is an array of Objects, so the next line will be start with - configMapRef <Object> # This is an Object name <string> # This is a dictionary optional <boolean> # kubectl explain cronjob.spec.jobTemplate --recursive | less kubectl explain pods.spec.containers --recursive | less Pods \u00b6 kubetcl get pods -o wide # Get the Node on which pod is running - Remember : You CANNOT edit specifications of an existing POD other than the below. 1. spec.containers[ ].image 1. spec.initContainers[ ].image 1. spec.activeDeadlineSeconds 1. spec.tolerations - For example: when you edit a pod in vi editor for environment variables, service accounts, resource limits. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable. 1. A copy of the file with your changes is saved in a temporary location when it fails. You can then delete the existing pod. Then create a new pod with your changes using the temporary file which was saved earlier in /tmp . 2. The second option is to extract the pod definition in YAML format to a file. Then make the changes to the exported file using an editor and save the file. Then delete the existing pod. Then create a new pod with the edited file. 3. Force replace the pod using kubectl replace -f <filename> --force Replicasets \u00b6 selector is the difference between ReplicaSet and ReplicationController apart from apiVersion. # Scaling RS kubectl replace -f rs.yaml kubectl scale --replicas = 6 rs myapp-rs # <Type> <Name of RS> format kubectl delete rs myapp-rs # Also deletes the underlying pods ### IMPORTANT # Either delete and recreate the ReplicaSet or Update the existing ReplicaSet and then delete all PODs, so new ones with the correct image will be created. Deployments \u00b6 kubectl create deployment mydeploy --image = nginx --replicas = 3 kubectl scale deployment mydeploy --replicas = 6 - Edit Deployments - With Deployments you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification, with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment. Formatting kubectl Output \u00b6 -o json # Output a JSON formatted API object. -o name # Print only the resource name and nothing else. -o wide # Output in the plain-text format with any additional information. -o yaml # Output a YAML formatted API object. Namespaces \u00b6 kubectl config set-context $( kubectl config current-context ) --namespace = test # OR alias kns = kubectl config set-context --current --namespace kns test # Testing services # DNS resolution <svc name>.<namespace>.svc.cluster.local:<svc port> # cluster.local - Domain name, svc - subdomain name Imperative Commands \u00b6 ` --dry-run ` : # By default as soon as the command is run, the resource will be created. ` -dry-run = client ` : # This will not create the resource, instead, tell you whether the resource can be created and if your command is right. # Generate POD Manifest kubectl run nginx --image = nginx --dry-run = client -o yaml > pod.yaml # Generate Deployment with 4 Replicas kubectl create deployment --image = nginx nginx --replicas = 4 --dry-run = client -o yaml > deploy.yaml # Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379 # This will automatically use the pod's labels as selectors kubectl expose pod redis --port = 6379 --name redis-service --dry-run = client -o yaml > svc.yaml # OR # This will not use the pods labels as selectors, instead it will assume selectors as app=redis. kubectl create service clusterip redis --tcp = 6379 :6379 --dry-run = client -o yaml # So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service # Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes # This will automatically use the pod's labels as selectors, but you cannot specify the node port. kubectl expose pod nginx --port = 80 --name nginx-service --type = NodePort --dry-run = client -o yaml # OR # This will not use the labels as selectors kubectl create service nodeport nginx --tcp = 80 :80 --node-port = 30080 --dry-run = client -o yaml # I would recommend going with the `kubectl expose` command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service. # Create Pod and Svc in one command kubetcl run nginx --image = nginx --port = 8080 --expose Docker Commands \u00b6 CMD [ \"command\" , \"parameters\" ] # Process which is executed in Docker container continuously # example CMD [ \"sleep\" , \"5\" ] # Sleep is executed every 5 secs # What is you want to pass parameters to Docker during execution, Use ENTRYPOINT ENTRYPOINT [ \"sleep\" ] # Process invoked at startup # To execute docker run sleeper-image 10 # This will pass 10 to the Sleep process # If no parameter is passed to Docker command, it will fail. # Passing default parameter when no parameter is passed, Use ENTRYPOINT and CMD both in Dockerfile ENTRYPOINT [ \"sleep\" ] # Process invoked at startup CMD [ \"5\" ] # Default parameter passed to sleep, if not given during docker execution # Suppose you want to override the default sleep process during execution docker run --entrypoint ping sleeper-image 8 .8.8.8 # Override sleep with ping process K8s Commands and Arguments \u00b6 # Docker # K8s #---------------------------# # ENTRYPOINT --> command # CMD --> args # Example # In Dockerfile ENTRYPOINT [ \"python\" , \"app.py\" ] CMD [ \"--color\" , \"red\" ] # In K8s, args is overriding the input command: [ \"python\" , \"app.py\" ] args: [ \"--color\" , \"pink\" ] Environment Variables \u00b6 Passed as an array in key value format 3 Types of setting Env variables Direct Config Map Secrets # Direct env: - name: APP_COLOR value: pink # ConfigMap env: - name: APP_COLOR valueFrom: configMapKeyRef: # Secret env: - name: APP_COLOR valueFrom: secretKeyRef: ConfigMap \u00b6 # From Literal kubectl create configmap <config-map name> --from-literal = <key>-<value> # From File kubectl create configmap <config-map name> --from-file = <path-to-file> # To reference a configMap file in pod definition # ConfigMap with apiVersion etc defined envFrom: - configMapRef: name: <configMap Name in Metadata> # To reference a configMap volume in pod definition volumes: - name: app-config-volume configMap: name: <configMap Name in Metadata> Secrets \u00b6 The way kubernetes handles secrets. Such as: A secret is only sent to a node if a pod on that node requires it. Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage. Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well. Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault. # From Literal kubectl create secret generic <secret name> --from-literal = <key>-<value> # Note: generic is added # From File kubectl create secret generic <secret name> --from-file = <path-to-file> # To encode text to base64 echo -n \"Hello\" | base64 # To view the secret information kubectl get secret app-secret -o yaml # Output in yaml, then decode using base64 echo -n \"aTsfgs*#\" | base64 -d # To reference a secret in pod definition envFrom: - secretRef: name: <secret Name in Metadata> # To reference a secret in volume definition volumes: - name: app-secret-volume secret: secretName: <secret Name in Metadata> # Note the change of Key Security Context \u00b6 Security Context can be added at Pod and Container level. If defined at both levels, container configuration overrides the security context defined at pod level. Note : Capabilities are only supported at container level and NOT at Pod level. # Adding additional Linux capability during container execution in Docker docker run --cap-add MAC_ADMIN ubuntu # Adds additional capability to the container apart from defaults # Adding security context to Pod in container section securityContext: runAsUser: 1000 capabilities: add: [ \"MAC_ADMIN\" ] Service Accounts \u00b6 Service Accounts are used by applications or services and not by humans. Note: SA cannot be added to existing Pod. Always Delete and add SA to Pod definition to recreate. K8s automatically mounts the default namespace SA. To override this behavior, set automountServiceAccountToken: false in the Pod definition. kubectl create sa dashboard-sa # Create a SA kubectl get secret dashboard-sa-token-kdbm # K8s creates a secret to store the token to auth the service # You can use the token to run K8s API calls curl https://192.168.0.10:6443/api -insecure --header \"Authorization: Bearer <sa token>\" Resource Requirments \u00b6 1 Gi - Gibibyte 1 Mi - Mebibyte 1 Ki - Kibibyte 1 CPU = 1 vCPU or 1 Hyperthread # If a Pod uses more CPU than its limit, it will be throttled. # If a Pod used more Mem than its limit, it will be terminated. - When a pod is created the containers are assigned a default CPU request of .5 and memory of 256Mi\". For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a LimitRange in that namespace. apiVersion : v1 kind : LimitRange metadata : name : mem-limit-range spec : limits : - default : memory : 512Mi defaultRequest : memory : 256Mi type : Container --- apiVersion : v1 kind : LimitRange metadata : name : cpu-limit-range spec : limits : - default : cpu : 1 defaultRequest : cpu : 0.5 type : Container Taints and Tolerations \u00b6 If a taint is placed, by default no pods will be scheduled on the Node. Only when a Pod has tolerations matching the taint, will the K8s scheduler place the pod on the tainted node. 3 taint-effects: NoSchedule PreferNoSchedule NoExecute Important : Taints does not neccessarily mean that Pods matching the tolerations will be placed always on that tainted node. It can be placed on another Node which is not tainted. Taints and Tolerations is used only for restricting certain pods from being placed in it. To always place a pod on a tainted node, use Node Affinity . # Node Taint kubectl taint nodes <node name> key = value:taint-effect # Example kubectl taint nodes node1 app = blue:NoSchedule # Tolerations added to Pod definition tolerations: - key: \"app\" # Note: This is placed under Pod not containers section operator: \"Equal\" # Note: All values should be doube quoted value: \"blue\" effect: \"NoSchedule\" # By default, master Node is always tainted. To see the taint kubectl describe node kubemaster | grep Taint # To remove the taint from a node, add a minus (-) symbol at the end of the taint with NO spaces in between kubectl taint nodes node1 app = blue:NoSchedule- # Note the - with no spaces Node Selectors \u00b6 Labels placed on the Nodes which help scheduler place the pods matching the labels # Adding the lable to the node kubectl label node <node name> key = value # Example kubectl label node node1 size = Large # NodeSelectors added to Pod definition nodeSelector: size: Large # Important: Labels are simple and can't be used for complex selection using OR or NOT operators. # For example: Place Pods in Large or Medium Nodes. OR Place Pods in Nodes which are not Small. # OR # Example kubectl label node node1 size = # NodeSelectors added to Pod definition nodeSelector: size: \"\" Node Affinity \u00b6 To overcome NodeSelector limitations, Affinity and Anti-Affinity is used. Node Affinity Types: requiredDuringSchedulingIgnoredDuringExecution : The scheduler can't schedule the Pod unless the rule is met. This functions like nodeSelector, but with a more expressive syntax. preferredDuringSchedulingIgnoredDuringExecution : The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod. 2 Types of Operators - In and Exists Important : Use a combination of taints and tolerations to Deny Pods from being placed on to it. Then Add labels to the Nodes. After that add NodeAffinity to ensure the matching Pod goes to the correct Node. affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/os operator : In values : - linux Multi-Container Pods \u00b6 Design Patterns Side car - Example is a logging container which ships the logs to a central logging service Adapter - Example is a logging container which converts the logs to a standard format before shipping Ambassador - Outsourcing the database connection to a separate container based on environments which acts as a proxy to the database service. The application always refers to the database using a standard dns name. Init Pods \u00b6 In a multi-container pod, each container is expected to run a process that stays alive as long as the POD's lifecycle. For example in the multi-container pod that we talked about earlier that has a web application and logging agent, both the containers are expected to stay alive at all times. The process running in the log agent container is expected to stay alive as long as the web application is running. If any of them fails, the POD restarts. But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or binary from a repository that will be used by the main web application. That is a task that will be run only one time when the pod is first created. Or a process that waits for an external service or database to be up before the actual application starts. That's where initContainers comes in. When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts. You can configure multiple such initContainers as well, like how we did for multi-pod containers. In that case each init container is run one at a time in sequential order . If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds. containers : - name : myapp-container image : busybox:1.28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] initContainers : - name : init-myservice image : busybox command : [ 'sh' , '-c' , 'git clone <some-repository-that-will-be-used-by-application> ;' ] # Another Example of Sequential execution of Init Containers containers : - name : myapp-container image : busybox:1.28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] initContainers : - name : init-myservice image : busybox:1.28 command : [ 'sh' , '-c' , 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;' ] - name : init-mydb image : busybox:1.28 command : [ 'sh' , '-c' , 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;' ] # To debug the Init container logs kubectl logs -c <Init container name> # Shows the exact error Readiness and Liveness Probes \u00b6 Status of a Pod Lifecycle: PodScheduled --> Initialized --> ConatinersReady --> Ready Liveness - Test for checking if your application is working Liveness and Readiness Probes have the same configuration. # Readiness probes based on the protocol # Http readinessProbe: httpGet: path: /api/ready port: 8080 initialDelaySeconds: 10 # Tells to wait before checking periodSeconds: 5 # Interval between each attempt failureThreshold: 8 # How many attempts # TCP readinessProbe: tcpSocket: port: 3306 # Exec readinesProbe: exec: command: - cat - /app/is_ready Container Logging \u00b6 # Docker Logs docker run -d kodekloud /event-simulator # Logs are not streamed as its running in detached mode docker logs -f <container id> # Shows the container logs # K8s logs kubectl logs -f event-simulator-pod # -f = Live streaming of logs # Multiple containers in pod # Get pods and see if there are more than 1 containers and then after -c do a tab to see the container names kubectl logs -f event-simulator-pod -c event-simulator # You can skip -c and directy mention container name Monitor and Debug Aplications \u00b6 Open sources projects to monitor clusters, Metrics server, Prometheus You can have 1 Metrics server per cluster . It is a In-memory solution and does not store data in disk. Kubelet agent has cAdvisor (container Advisor) component which extracts performance metrics. cAdvisor then makes this data available via the K8s API to the Metrics server. # Download the Metrics server from Github git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git # Apply the metric server components kubectl apply -f . # OR kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml # After metrics is collected after some time lag, run the commands # To wait for the output to come watch \"kubectl top node\" # Note: the command in double quote. Ctrl + c to exit kubectl top node # Get the CPU and memory consumption of each node kubectl top pod # Pod performance Labels, Selectors and Annotations \u00b6 kubectl get pods --show-labels # List labels kubectl get pods -l env = dev # Filter Labels using short form kubectl get all --selector = env = prod --no-headers | wc -l # Filter all objects and remove headers kubectl get pods --selector = env = prod,bu = finance,tier = frontend # Logical AND # Tip # In ReplicaSet or Service, the matchLabels in the spec.selector section should always match the pod labels in the spec.template.metadata.labels. # Error: \"selector\" does not match template 'labels' Update and Rollback Deployments \u00b6 2 Deployment strategy - RollingUpdate (default) and Recreate # Create Deployments kubectl create -f deployments-definition.yml # Using yaml format # OR kubectl create deployment my-app-deployment --image = nginx kubectl apply -f deployments-definition.yml # To update a deployment # OR Without changing definition file, updating parameters, # NOTE nginx is the container name in existing pod/deployment kubectl set image deployment/my-app-deployment nginx = nginx:1.9 # Image is upgraded # Roll-out strategy kubectl rollout status deployment/my-app-deployment # Shows rollout status kubectl rollout history deployment/my-app-deployment # Shows rollout history and revisions # You can check the status of each revision individually by using the --revision flag: kubectl rollout history deployment/my-app-deployment --revision = 1 # Shows detailed history # We can use the --record flag to save the command used to create/update a deployment against the revision number. Change is recorded as annotation in the deployment as \"change-cause\". kubectl set image deployment/my-app-deployment nginx = nginx:1.7 --record # OR kubectl edit deployments my-app-deployment --record kubectl rollout undo deployment/my-app-deployment # Rolls back to previous version DaemonSets \u00b6 kubectl get daemonsets # TIP: There is no kubectl create daemonset, so do a create deployment, get this into a yaml. # Format the yaml for Kind, Remove replicas, strategy and save the file. Static Pods \u00b6 When there is no Master Node and its components, you can create Pods on standalone Worker Node. Such a pod is called Static Pod To inspect the path where the definition is defined, look at the kubelet.service. It could be in 2 places: \u2013config= - kubeadm installation \u2013pod-manifest-path= - manual installation The pod definitions have to be placed for example in /etc/kubernetes/manifests in a yaml file. Deleting the yaml file, removes the pod from the node. To identify static pods, pod name will have the node name appended at the end. kubeadm deploys the cluster components as static pods, which have controlplane appended in the pod name. Another way to identify static pod is to get the yaml of the pod and then searching for ownerReferences . In that if kind: Node then its a static pod. # To view the pods after creation, as kubectl will not work docker ps # To kill the pod docker container rm <id> # Kubelet config /var/lib/kubelet/config.yaml Jobs \u00b6 # Docker execution of mathematical problem docker run ubuntu expr 3 + 2 # Task is completed and container exits docker ps -a # Shows the exit status of the container # For batch processing Jobs are used in K8s kubectl create job throw-dice --image = kodekloud/throw-dice --dry-run = client -o yaml > job.yml # NOTE: Add backofflimit parameter if its not in the generated template to avoid job from quiting before it succeeds kubectl get jobs # list the jobs kubectl get pods # lists the pods created by the job kubectl logs <pod-name> # shows the pod output # Running multiple pods in sequence, add completions parameter to the Job spec. # NOTE: This is the successful pod completion count, it will keep on recreating pods till this number matches. # Running multiple pods in parallel, add parallelism parameter to the Job spec along with completion. Cronjobs \u00b6 # Min-Hour-DOM-Month-Day of Week (0-6) # Sun - 0 & 7 both, Sat -6 # spec.schedule is the additional parameter added. # NOTE: schedule is at the first spec kubectl create job throw-dice --image = kodekloud/throw-dice --schedule = \"30 21 * * *\" --dry-run = client -o yaml > cronjob.yml kubectl get cronjob # list the cronjob Services \u00b6 3 Types to access a service NodePort: Mapping a port on the Node to a port on the pod ClusterIP: Internal Virtual IP not exposed out of the Node LoadBalancer: External IP which load balances multiple ports on the Node NodePort: K8s takes care of deploying the service across all nodes, even though the pod is not on those nodes. This helps in getting the same Nodeport exposed on all the Nodes. When a http call hits a nodeport on a node, K8s will route traffic internally to the Pod on the correct Node. # NodePort (Node) --> Port (Svc) --> TargetPort (Pod) # Target Port is the Pod port where the service forwards requests to # Port is on the Service kubectl create deployment frontend --replicas = 2 \\ --labels = run = load-balancer-example --image = busybox --port = 8080 kubectl expose deployment frontend --type = NodePort --name = frontend-service --port = 6262 --target-port = 8080 --dry-run = client -o yaml > svc.yml # This is because we cant control the value of NodePort in imperative command. Edit the yaml file and add the nodePort parameter under spec.ports.port.nodePort kubectl get services # List services # Take the NodePort and use the Node IP to hit the service from outside the machine Multiple Kube Schedulers \u00b6 Advanced Scheduling https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/ https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/ https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work OS Upgrades \u00b6 kubectl drain node1 # Gracefully evict the pods kubectl cordon node1 # Makes the node unschedulable kubectl uncordon node1 # Makes its schedulable after maintenance kubectl drain node01 --ignore-daemonsets # Incase you get the ds exist error # Even with ignore ds options, you can get error when there are pods which are not managed by a replicaSet # error: unable to drain node \"node01\" due to error:cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override) # In this case, copy the pod data into yaml and apply it again after draining the nodes using --force K8s Release Strategy \u00b6 https://kubernetes.io/docs/concepts/overview/kubernetes-api/ https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md Cluster Upgrade \u00b6 First upgrade master, then the nodes Kubeadm does not update the kubelet, so it needs to be updated manually cat /etc/*release* # Shows the OS version # When you do apt-cache update, it will show which kubeadm minor version is present # TIP: Copy the upgrade commands to notepad, before updating versions and pasting kubeadm upgrade plan # Shows the upgrade plan apt-get upgrade -y kubeadm = 1 .12.0-00 # 1st Update kubeadm as per plan kubeadm upgrade apply v1.12.0 # Upgrade the cluster controlplane # NOTE: Master will show version of kubelet, which has not been updated yet. kubectl get nodes # Master is still shown with V1.11 version # Drain the master and then perform kubelet upgrade apt-get upgrade -y kubelet = 1 .12.0-00 # Upgrade the kubelet systemctl restart kubelet # Make the change permanent kubectl get nodes # Master is now shown with V1.12 version # Now follow the process of upgrading Nodes using drain and cordon # IMPORTANT: the Node Drain command needs to be run on the master and not Node01. .e. all kubelet commands like drain and uncordon needs to be run on master # Follow the upgrade process, upgrade kubeadm, upgrade kubelet , upgrade node and then restart kubelet # Uncordon the Node # NOTE: the Node upgrade command kubeadm upgrade node config --kubelet-version v1.12.0 # Upgrade the node Backup and Restore \u00b6 To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3. export ETCDCTL_API=3 For example, if you want to take a snapshot of etcd, use: etcdctl snapshot save -h and keep a note of the mandatory global options. Since our ETCD database is TLS-Enabled, the following options are mandatory: --cacert - verify certificates of TLS-enabled secure servers using this CA bundle --cert - identify secure client using this TLS certificate file --endpoints=https://127.0.0.1:2379 - This is the default as ETCD is running on master node and exposed on localhost 2379. --key - identify secure client using this TLS key file Similarly use the help option for snapshot restore to see all available options for restoring the backup. etcdctl snapshot restore -h # Manual Backup of applications kubectl get all --all-namespaces -o yaml > all-deployed-services.yaml # Backup ETCD ETCDCTL_API = 3 etcdctl snapshot save snapshot.db ETCDCTL_API = 3 etcdctl snapshot status snapshot.db # View backup status # To restore ETCD # First stop the kube-apiserver service kube-apiserver stop # Execute restore from the saved state ETCDCTL_API = 3 etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup # Here the data-dir is a new dir where the ETCD will start storing the data. # This is done so that the existing data is not overwritten, in case of any issues during restore # Reload the service daemon and restart etcd service systemctl daemon-reload service etcd restart # Finally start the kube-apiserver service kube-apiserver start # Tips: To check the version of etcd, check the etcd pod logs or the image version in the etcd pod kubectl describe pod etcd-controlplane -n kube-system # Get the below 4 parameters after describe etcd pod ETCDCTL_API = 3 etcdctl snapshot save /opt/snapshot-pre-boot.db \\ --cacert = \"/etc/kubernetes/pki/etcd/ca.crt\" \\ --cert = \"/etc/kubernetes/pki/etcd/server.crt\" \\ --endpoints = https://127.0.0.1:2379 \\ --key = \"/etc/kubernetes/pki/etcd/server.key\" # Restore the etcd to a new directory from the snapshot # So move the backed up data to a new dir ETCDCTL_API = 3 etcdctl snapshot restore /opt/snapshot-pre-boot.db \\ --data-dir = \"/var/lib/backup-from-etcd\" # Certificate details are not required in restore as the file in in local directory. # Next, update the /etc/kubernetes/manifests/etcd.yaml: # We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory (/var/lib/etcd-from-backup). volumes: - hostPath: path: /var/lib/etcd-from-backup type: DirectoryOrCreate name: etcd-data https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md https://www.youtube.com/watch?v=qRPNuT080Hk Security \u00b6 TLS \u00b6 There are 2 types of Encryption mechanisms: Symmetric and Asymmetric For TLS both the mechanims are used: Symmetric - To encrypt the client data Asymmetric - To encrypt the symmetric key after server auth In symmetric, one key is used to encrypt and decrypt the data, while in asymmetic 2 keys are used. Public Key (will be refered as Lock). You can encrypt data with either private or public key. NOTE : Decryption is only done using the opposite key. For example, if you encrypt the data with public key, you cannot decrypt it using public key, you MUST using the private key. Keys have naming convention, private key will always have key in the name to identify it as private key. There are 3 usecases of using Asymmetric Encyption in the TLS process Using SSH to login to any server. In this case, user private key is used to unlock the server access having the user's public key stored on the server. Using asymmetric keys to transfer client's symetric key over the Internet. These are called Server Certificates . This should ensure no hacker is allowed to decrypt the data in transit. Using asymmetric keys to authenticate the website. These are called Root Certificates . In this case, the website presents a CA certificate which is having the CA pulic key embedded. The client presents this via the browser which has CA's public keys installed, which decrypts the certificate using the public key to authenticate the website. TLS Overview A system admin generates private and public key to enforce SSH authenication. The public key is stored in the server. Web Server generates private and public key to encrypt HTTPS trafic. For this, the web server generates a certificate signing request using its public key. This CSR (which has the public key of the server) is sent to the CA for signing the certificate. The CA signs the certificate using its private key and sends it back to the server after completing its validation process. When a client request comes, the web server sends its certificate having its encrypted public key back. The client presents the certificate to the browser. The browser using the CA public key, decrypts the certificate, thus authenticating the web server and thus the website. The decrypted certificate has the server's public key. The client generates its symmetric key and then encrypts this using the web server's public key and sends the request back to the web server. The web server decrypts the request using its private key and gets access to the client's symmetric key. Now the communication between the client and web server will continue happening using this symmetric key. TLS in Kubernetes \u00b6 There are 2 types of certificates used in Kubernetes components Server Certificates - Used by the server components Client Certificates - These are certificates used by Users or process to authenticate themselves. Types of Clients for the API server: Admin Users Kube Scheduler Kube Controller Manager Kube Proxy There is only one client for the ETCD server: API Server There is also one client for the Kubelet server: API Server All the certificates (Server and Client) need to be signed by a Root certificate that is issued by a CA. Certificate Creation \u00b6 # We require only Private Keys and Certificates # Generate CA certificates openssl genrsa -out ca.key 2048 openssl req -new -key ca.key -subj \"/CN=KUBERNETES-CA\" -out ca.csr # Sign the CSR to generate the Root Cert openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt # Generating Client Key and Certificates openssl genrsa -out admin.key 2048 openssl req -new -key admin.key -subj \"/CN=kube-admin/O=system:masters\" -out admin.csr # Remember CN will the user name that is used to login to API server # O parameter should link to the user account, in this case its the admin group openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt # Here the CA crt and CA key is used to sign the user csr # Once the cluster is configured, you an use the certificates in Rest API calls curl https://kube-apiserver:6443/api/v1/pods \\ --key admin.key --cert admin.crt --cacert ca.crt # Generating Server Key and Certificates openssl genrsa -out apiserver.key 2048 openssl req -new -key apiserver.key -subj \"/CN=kube-apiserver\" -out apiserver.csr -config openssl.cnf # Additional openssl.cnf contains DNS Alias or IP address which refers back to the api-server openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -out apiserver.crt Debugging Certificates \u00b6 # View existing certificates openssl x509 -req -in /etc/kubernetes/pki/apiserver.crt -text -noout # Verify the Subject, Subject Alternative Names (Alias), Not After (to check validity), Issuer (CA) # Debugging Certificate Issues # When installed as service journalctl -u etcd.service -l # List the service logs # When installed using kubeadm kubectl logs etcd-master ## Failure logs - Failed to dial 127.0.0.1:2379: connection error # If ETCD or Apiserver is down, use docker docker ps -a # To view the containers docker logs <container id> # To view the logs Certificates API \u00b6 - NOTE : To view the CA certificates, as its stored on the master look at the Controller Manager configuration and check the path mentioned under --cluster-signing-cert-file and --cluster-signing-key-file kubectl get csr # Get pending CSR kubectl certificate approve jane # Approve CSR kubectl get csr jane -o yaml # View the CSR echo \"data\" | base64 --decode # Extract the Certificate from CSR after it is approved and send to user jane # Now she will be able to login to the cluster Image Security \u00b6 # Create a Docker Registry secret in the cluster for Kubelet to pass this to the Docker Runtime on the worker nodes kubectl create secret docker-registry regcred \\ --docker-server = <URL> \\ --docker-username = <> \\ --docker-password = <> \\ --docker-email = <email> # Once the secret is created, use this in the Pod definition imagePullSecrets: - name: regcred Deployment Strategies \u00b6 Recreate RollingUpdate The above 2 deployments are native to K8s. While the below 2 needs several steps to complete. Blue Green Deployment Switch is done in the service object before the blue deployment is killed completely. Canary Deployments Reduce the number of replicas in the canary deployment initially, then scale up after testing Docker Volume \u00b6 Docker creates Read Only layers for the image For the container a transient Read-Write layer is created. When a volume is created and then it is mounted using the -v option, its is called Volume Mount . If a volume is not present, but a volume is mounted, Docker will create the volume at run time. When a host directory is mounted, it is called Bind Mount Use the new convention --mount instead of -v in the new version of Docker. Container Storage Interface \u00b6 Volumes \u00b6 Persistent Volumes and Claims \u00b6 - Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this: apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : myfrontend image : nginx volumeMounts : - mountPath : \"/var/www/html\" name : mypd volumes : - name : mypd persistentVolumeClaim : claimName : myclaim kubectl get persistentvolume kubectl get persistentvolumeclaim Storage Class \u00b6 When we create PV, then it is called Static Provisioning of Storage. In cloud, there is need to create storage dynamically when PVC is used. In this case, only Storage Class Objects are created. When a claim is made wihich references the Storage class, a PV is dynamically created. This is Dynamic Provisioning of Storage. The Storage Class makes use of VolumeBindingMode set to WaitForFirstConsumer . This will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created. Till then the PVC will be in Pending State . kubectl get storageclass Networking Basics \u00b6 Switching allows to configure machines to talk to each other in the same network Routing allows machines to talk across 2 or more networks Gateway allows machines to reach other network using a single entry point made in the routing table of each machine When a machine wants to reach to the internet, manual routes can be made to reach the destination on each machine To avoid the hassle of making entry for each IP address available on the Internet on each machine, a default route entry is made. When a route does not match, it goes through the default route. To separate Internal and External routing, you can make separate entries in the routing table of the machine. Networking Native Commands \u00b6 What is the network interface configured for cluster connectivity on the controlplane node? Run the ip a / ip link command and identify the interface. What is the MAC address of the interface on the controlplane node? Run the command: ip link show eth0 What is the MAC address assigned to node01? Run the command: arp node01 on the controlplane node. What is the state of the interface docker0? Run the command: ip link show docker0 and look for the state. If you were to ping google from the controlplane node, which route does it take? What is the IP address of the Default Gateway? Run the command: ip route show default and look at for default gateway. What is the port the kube-scheduler is listening on in the controlplane node ? Use the command: netstat -tunlp Notice that ETCD is listening on two ports. Which of these have more client connections established? Run the command: netstat -anp | grep etcd . That's because 2379 is the port of ETCD to which all control plane components connect to. 2380 is only for etcd peer-to-peer connectivity. When you have multiple controlplane nodes. CNI \u00b6 ps aux | grep kubelet # Show kubelet configuration for CNI ls /opt/cni/bin # Shows all CNI executables ls /etc/cni/net.d # Shows the configuration files kubectl exec busybox -- ip route # To show the routes a pod can take. Deploy a busybox pod first - What binary executable file will be run by kubelet after a container and its associated namespace are created. Look at the type field in file / etc/cni/net.d/10-flannel.conflist . Deploy Weave for Pod Networking \u00b6 # Deploy Weave CNI, use the bookmark to find the command kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \" # To Trouble shoot the deployment kubectl logs -n kube-system <weave pod name> -c weave # If CNI is not deployed you get the following error in App Pod # failed to set up sandbox container network for pod \"app\": networkPlugin cni failed to set up pod \"app_default\" network: unable to allocate IP address ip a | grep eth0 # Identify the Host Network kubectl logs -n kube-system weave-net # Show the current failure logs # If error says \"Network 10.232.0.0/12 overlaps with the existing route\" of the host network, you need to allocate Weave Network to a differentIP range. # In this case, add &env.IPALLOC_RANGE=10.50.0.0/16 in the kubectl apply command. #NOTE: this env should be inside the quotes. # Once deployed, you can view the interface that weave creates, by ip link show weave # The network is configured with weave. Check the weave pods logs using command kubectl logs <weave-pod-name> weave -n kube-system and look for ipalloc-range # The above command will show POD IP address range configured by weave # What is the default gateway configured on the PODs scheduled on node01 by weave? kubectl run busybox --image = busybox --restart = Never --dry-run = client -o yaml -- sleep 1000 > po.yaml # Add nodeName: node01 to install the pod in Node01 and then exec to find the default gateway used by the Pod kubectl exec busybox -- route # OR kubectl exec busybox -- ip route Service Networking \u00b6 Kube-Proxy assigns service ip and port combination to the iptables to enable networking. ps aux | grep kube-api-server # API server has the service ip range configured # search for --service-cluster-ip-range value to get the service ip range. Pod ip will come from the CNI plugin # OR cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range kubectl get service # Shows the service ip iptables -L -t nat | grep <service-name> # Shows the IP table rules for the service # Check the kube-proxy logs on the node cat /var/log/kube-proxy.log # Shows the logs of what iptable proxy it uses and also when a service entry is made to iptables DNS \u00b6 You need a DNS server and it can help manage name resolution in large environments with many hostnames and Ips and then configure your hosts to point to a DNS server. Domain names and the IP address of all the host in the network are added to the /etc/hosts file of the DNS server. host file of the individual server is made to point to the DNS server for getting server names resolved to IP addresses. When the servers need to resolve a name which is not part of the internal network, the internal DNS server can point to an external DNS server to handle the name resolution. In this case, ping to facebook.com will get correctly resolved using Google DNS. High level domain names are categorized based on their functions Domains are further sub-divided into sub domains. Root (.) being the top most level. DNS resolution request originating from an internal company DNS traverses through the Root domain servers till it gets a match and then the IP address is sent back to the calling DNS, where the response is cached based on Time To Live parameter. A records are stored which maps the DNS to the IP address. When you want yone DNS to map to multiple alias, you use the CNAME record. Using search inside an organization you can alias your domains. In this way, you can address your servers with only the sub-domain. search will append the domain name and then resolve the IP address Cluster DNS \u00b6 - Pod IP is replaced by - to derive its POD name in the DNS CoreDNS \u00b6 # Where is the configuration file located for configuring the CoreDNS service? Inspect the Args field of the coredns deployment and check the file used. cat /etc/coredns/Corefile # Shows the plugins configured and the error handling kubectl get svc -n kube-system # Shows the core dns service which is made available to all the pods # The IP address shown as the Cluster-IP of this service is the Nameservice that is configured inside all the pods `/etc/resolv.conf` file which then knows which DNS they need to point to. # Kubelet makes sure this entry is made inside all pods. # View the kubelet service config yaml to see the DNS config cat /var/lib/kubelet/config.yaml # clusterDNS section points to the coreDNS service cluster IP # How is the Corefile passed in to the CoreDNS POD? Use the kubectl get configmap command for kube-system namespace and inspect the correct ConfigMap. It will be passed as a Config Volume # What is the root domain/zone configured for this kubernetes cluster? Run the command: kubectl describe configmap coredns -n kube-system and look for the entry after kubernetes. This is where cluster.local as root domain is configured # Check the FQDN of any service host web-service # DNS will return the FQDN # Checking the FQDN of pod is not possible, you need to specify the FQDN host 10 -244-4-5.default.pod.cluster.local # hr app POD is hr namespace and mysql Service is in payroll namespace. From the hr pod nslookup the mysql service and redirect the output to a file /root/CKA/nslookup.out kubectl exec -it hr -- nslookup mysql.payroll > /root/CKA/nslookup.out # NOTE: the DNS name of the service is used along with namespace Ingress Networking \u00b6 - Create a default-backend deployment to handle routes that are not managed. - Create a service default-backend-service to manage 404 error handling and link to the ingress resource. - NOTE : Ingress needs to be deployed in the same namespace as the deployment & service object. # Imperative command from K8s 1.20 kubectl create ingress <ingress-name> --rule = \"host/path=service:port\" # Example of Imperative kubectl create ingress ingress-test --rule = \"wear.my-online-store.com/wear*=wear-service:80\" kubectl get ingress # list the ingress Rewrite Target Option \u00b6 Our watch app displays the video streaming webpage at http://<watch-service>:<port>/ Our wear app displays the apparel webpage at http://<wear-service>:<port>/ We must configure Ingress to achieve the below. When user visits the URL on the left, his request should be forwarded internally to the URL on the right. Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forwarded users to the appropriate application in the backend. http:// < ingress-service > : < ingress-port > /watch` --> http:// < watch-service > : < port > / http:// < ingress-service > : < ingress-port > /wear --> http:// < wear-service > : < port > / Without the rewrite-target option, this is what would happen: http:// < ingress-service > : < ingress-port > /watch --> http:// < watch-service > : < port > /watch http:// < ingress-service > : < ingress-port > /wear --> http:// < wear-service > : < port > /wear Notice watch and wear at the end of the target URLs. The target applications are not configured with /watch or /wear paths. They are different applications built specifically for their purpose, so they don't expect /watch or /wear in the URLs. And as such the requests would fail and throw a 404 not found error. To fix that we want to \"ReWrite\" the URL when the request is passed on to the watch or wear applications. We don't want to pass in the same path that user typed in. So we specify the rewrite-target option. This rewrites the URL by replacing whatever is under rules->http->paths->path which happens to be /pay in this case with the value in rewrite-target. This works just like a search and replace function . For example: replace ( path, rewrite-target ) In our case : replace ( \"/path\" , \"/\" ) apiVersion : extensions/v1beta1 kind : Ingress metadata : name : test-ingress namespace : critical-space annotations : nginx.ingress.kubernetes.io/rewrite-target : / spec : rules : - http : paths : - path : /pay backend : serviceName : pay-service servicePort : 8282 replace ( \"/something(/| $ )(.*)\" , \"/ $2 \" ) In this ingress definition, any characters captured by (.*) will be assigned to the placeholder $2, which is then used as a parameter in the rewrite-target annotation. rewrite.bar.com/something rewrites to rewrite.bar.com/ rewrite.bar.com/something/new rewrites to rewrite.bar.com/new apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : nginx.ingress.kubernetes.io/rewrite-target : /$2 name : rewrite namespace : default spec : rules : - host : rewrite.bar.com http : paths : - backend : serviceName : http-svc servicePort : 80 path : /something(/|$)(.*) Network Policies \u00b6 - Important : Always look at Network policy from the perspective of the Pod. For Rules - Always pay attention to the Request (Ingress) and not the Response (Egress) as that may be already blocked due to cluster wide network policy. - Example: If DB pod needs to be accessed by API pod, then in DB pod the traffic is Ingress. - Flannel does not support Network Policy. If Network policy is still applied to this network, it will not have any effect. - Usecase 1: Rule - Apply Ingress policy using Pod Labels - Usecase 2: Rule - Apply Ingress policy using Pod Labels and Namespaces - This is a Logical AND operation where pods have same labels in other namespaces are ignored. - Usecase 3: Rule - Apply Ingress policy using Namespace only - Usecase 4: Rule - Apply Ingress policy using External IP - This is required, where the service resides outside the cluster and the service needs to connect to it. - Along with the Pod label AND Namesace, External service is an OR. So the Pod should have the correct label AND in the namespace OR extrenal service should have the IP. Either of these 2 rule matches traffic will be allowed. - Usecase 5: Rule - Apply Ingress policy using 3 Rules and OR operation - NOTE : There is a hypen to `namespaceSelector. This makes it as a separate rule. - Usecase 6: Rule - Apply Egress policy using External Service - The traffic is allowed from pod to the external service kubectl get networkpolicy Kubeconfig \u00b6 kubectl view config # $HOME/.kube/config file which is default is read kubectl view config --kubeconfig = my-custom-config # Pass a config file not in .kube dir kubectl config use-context prod-user@production # Sets the current context API Versions \u00b6 - Important : kube proxy != kubectl proxy. # To access the K8s API from a local server, start the kubectl proxy. # This proxy will use the kubeconfig data in the default kubeconfig kubectl proxy # start the proxy service on port 8001. Use 8001 instead of 6443 kubectl http://localhost:8001 -k # Shows all the API groups kubectl http://localhost:8001 -k | grep \"name\" # Shows the named API groups Deprecated AP Versions \u00b6 # Convert an old API formatted file to a new stable version kubectl convert -f <old-file> --output-version <new api version> kubectl convert -f nginx.yaml --output-version apps/v1 Authentication and Authorization \u00b6 Authentication Authorization Roles and Rolebindings \u00b6 - IMPORTANT : Roles and RoleBindings are namespaced kubectl get roles kubectl get rolebindngs # Check access kubect auth can-i create deployments # As current user kubectl auth can-i delete nodes --as dev-user # As an Admin user, you can impersonate and test for another user kubectl auth can-i delete pods --as system:serviceaccount:<namespace>:<sa name>> # As an service account in a namespace Cluster Roles and Cluster RoleBindings \u00b6 kubectl api-resources --namespaced = true # Get resources which can be added to roles kubectl api-resources --namespaced = false # Get resources which can be added to clusterroles kubectl get clusterroles kubectl get clusterrolebindngs Admission Controllers \u00b6 - Helps implement better security measures. - Validates configuration. - Performs additional operations before a pod is created. - Note : The NamespaceExists and NamespaceAutoProvision admission controllers are deprecated and now replaced by NamespaceLifecycle admission controller. - The NamespaceLifecycle admission controller will make sure that requests to a non-existent namespace is rejected and that the default namespaces such as default , kube-system and kube-public cannot be deleted. ##NOTE: # Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins. ps -ef | grep kube-apiserver | grep admission-plugins # To check all the values that are valid for kube-apiserver kube-apiserver -h | grep enable-admission-plugins # Shows enabled admin controllers # Incase kube-apiserver is running as a pod managed by kubeadm kubectl exec kube-apiserver-controlplane -n kube-system \\ # Exec into pod -- kube-apiserver -h | grep enable-admission-plugins # NOTE: -- for command execution Validating and Mutating Addmission Controllers \u00b6 Validating AC : Are the controllers which validate the request that is submitted to the API server Mutating AC : Are the controllers which change or mutate the request that is submitted to the API server if it does not meet the standards defined. Mutating AC are always invoked before Validating AC otherwise some requests may be rejected otherwise. # A pod with a conflicting securityContext setting: it has to run as a non-root # user, but we explicitly request a user id of 0 (root). # Without the webhook, the pod could be created, but would be unable to launch # due to an unenforceable security context leading to it being stuck in a # 'CreateContainerConfigError' status. With the webhook, the creation of # the pod is outright rejected. apiVersion : v1 kind : Pod metadata : name : pod-with-conflict labels : app : pod-with-conflict spec : restartPolicy : OnFailure securityContext : runAsNonRoot : true runAsUser : 0 containers : - name : busybox image : busybox command : [ \"sh\" , \"-c\" , \"echo I am running as user $(id -u)\" ] Helm \u00b6 # Install helm sudo snap install helm # OR sudo snap install helm --classic # This is the default helm repo helm install wordpress helm upgrade wordpress helm rollback wordpress helm uninstall wordpress # To work with a custom helm repo helm repo add bitnami https://charts.bitnami.com/bitnami # Add a custom helm repository helm search repo wordpress # Search for a chart in a named repository helm install release-1 bitnami/wordpress helm list helm uninstall release-1 helm pull --untar bitnami/wordpress # Just downloads the chart, does not install it ls wordpress # to check the chart contents helm install release-2 ./wordpress Troubleshooting \u00b6 Application Troubleshooting \u00b6 # Check the Front End app service if that is reachable kubectl describe web-service # Get the Endpoint and Nodeport curl http://web-service:30080 # OR If you are in the same machine curl https://localhost:30080 # NOTE: When you describe the service, there should be an ENDPOINTS which is detected. If its None, then selector is not correct # If there is no response, check the labels on the service and the pod that is its serving traffic to # Check the Pod restarts and why it is failing kubectl logs web -f # -f to tail the live logs and wait for it to fail # OR kubectl logs web -f --previous # --previous to get the last pod's logss Cluster Troubleshooting \u00b6 # Check Node Status kubectl get nodes # Check Control plane pods kubectl get pods -n kube-system # Check Service Health of Control plane service kube-apiserver status service kube-controller-manager status service kube-scheduler status # Check Service Health of Workers service kubelet status service kube-proxy status # Check the logs kubectl logs -n kube-system kube-apiserver-master # Check the service logs sudo journalctl -u kube-apiserver -l # Remember this as its native # NOTE: The certificates inside the cluster components are mapped as Volumes from the host. The path needs to be correct Worker Troubleshooting \u00b6 # Check Node Status kubectl get nodes # If the Nodes are not Ready, describe the node to get the error reason kubectl describe nodes node01 # Check for memory and disk space on the node top df -h # Check the service logs for kubelet sudo journalctl -u kubelet -f # -f to follow live logs # Start the service without change node ssh node01 \"service kubelet start\" # OR SSH to node01 and then run systemctl start kubelet # Check the kubelet certificates, make sure they are not expired and are of the right group and issued by the right CA openssl x509 -text -in /var/lib/kubelet/worker-1.crt # Kubelet Configuration Mismatch # Another issue from the worker node, watch the logs for kubelet # Check the kubelet.conf file at /etc/kubernetes/kubelet.conf on the worker node. # kubelet is trying to connect to the API server on the controlplane node on port 6553. This is incorrect. # Update the conf to port 6443. Restart kubelet # Kubelet Service Configuration mismatch # There appears to be a mistake path used for the CA certificate in the kubelet configuration. This can be corrected by updating the file /var/lib/kubelet/config.yaml. # Once this is fixed, restart the kubelet service. Network Troubleshooting \u00b6 Kubernetes uses CNI plugins to setup network. The kubelet is responsible for executing plugins as we mention the following parameters in kubelet configuration. - cni-bin-dir: Kubelet probes this directory for plugins on startup - network-plugin: The network plugin to use from cni-bin-dir. It must match the name reported by a plugin probed from the plugin directory. Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order. # DNS in Kubernetes # Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. # In large scale Kubernetes clusters, CoreDNS's memory usage is predominantly affected by the number of Pods and Services in the cluster. Other factors include the size of the filled DNS answer cache, and the rate of queries received (QPS) per CoreDNS instance. Kubernetes resources for coreDNS are: 1 . a service account named coredns, 2 . cluster-roles named coredns and kube-dns 3 . clusterrolebindings named coredns and kube-dns, 4 . a deployment named coredns, 5 . a configmap named coredns and a 6 . service named kube-dns. While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap. This is the backend to k8s for cluster.local and reverse domains. proxy . /etc/resolv.conf Forward out of cluster domains directly to right authoritative DNS server. 1 . If you find CoreDNS pods in pending state first check network plugin is installed. 2 . coredns pods have CrashLoopBackOff or Error state If you have nodes that are running SELinux with an older version of Docker you might experience a scenario where the coredns pods are not starting. To solve that you can try one of the following options: a ) Upgrade to a newer version of Docker. b ) Disable SELinux. c ) Modify the coredns deployment to set allowPrivilegeEscalation to true: kubectl -n kube-system get deployment coredns -o yaml | \\ sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \\ kubectl apply -f - d ) Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop. There are many ways to work around this issue, some are listed here: - Add the following to your kubelet config yaml: resolvConf: <path-to-your-real-resolv-conf-file> This flag tells kubelet to pass an alternate resolv.conf to Pods. For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the \"real\" resolv.conf, although this can be different depending on your distribution. - Disable the local DNS cache on host nodes, and restore /etc/resolv.conf to the original. 3 . If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints. kubectl -n kube-system get ep kube-dns If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports. kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. These network rules allow network communication to the Pods from network sessions inside or outside of the cluster. In a cluster configured with kubeadm, you can find kube-proxy as a daemonset. kubeproxy is responsible for watching services and endpoint associated with each service. When the client is going to connect to the service using the virtual IP the kubeproxy is responsible for sending traffic to actual pods. If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following command inside the kube-proxy container. Command: /usr/local/bin/kube-proxy --config = /var/lib/kube-proxy/config.conf --hostname-override = $( NODE_NAME ) So it fetches the configuration from a configuration file ie, /var/lib/kube-proxy/config.conf and we can override the hostname with the node name of at which the pod is running. 1 . Check kube-proxy pod in the kube-system namespace is running. 2 . Check kube-proxy logs. 3 . Check configmap is correctly defined and the config file for running kube-proxy binary is correct. 4 . kube-config is defined in the config map. 5 . check kube-proxy is running inside the container netstat -plan | grep kube-proxy JSONPath \u00b6 NOTE: use jq to see the data in proper format kubectl get nodes -o json | jq -c 'paths' | grep Output of this can be used in jsonpath search query jsonpath data can be filtered using jq # Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json. # Retrieve just the first 2 columns of pv output and store it in /opt/outputs/pv-and-capacity-sorted.txt. # The columns should be named NAME and CAPACITY. Use the custom-columns option and remember, it should still be sorted based on storage capacity. # Use the command kubectl get pv --sort-by = .spec.capacity.storage -o = custom-columns = NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt # Use a JSON PATH query to identify the context configured for the aws-user in the my-kube-config context file and store the result in /opt/outputs/aws-context-name. kubectl config view --kubeconfig = my-kube-config -o jsonpath = \"{.contexts[?(@.context.user=='aws-user')].name}\" > /opt/outputs/aws-context-name","title":"3 k8s"},{"location":"k8s/3-k8s/#etcd","text":"ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3. By default its set to use Version 2. Each version has different sets of commands. To set the right version of API set the environment variable ETCDCTL_API command export ETCDCTL_API=3 When API version is not set, it is assumed to be set to version 2. And version 3 commands listed below don't work. When API version is set to version 3, version 2 commands listed below don't work. # ETCDCTL version 2 etcdctl backup etcdctl cluster-health etcdctl mk etcdctl mkdir etcdctl set # ETCDCTL version 3 etcdctl snapshot save etcdctl endpoint health etcdctl get etcdctl put # This command sets the ETCD version to 3 and then shows all the keys in ETCD database and also sets the certificates kubectl exec etcd-master -n kube-system -- sh -c \"ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key\"","title":"ETCD"},{"location":"k8s/3-k8s/#api-server","text":"Api Server Configuration is stored Using Kubeadm - Inside API Server Pod - /etc/kubernetes/manifests/kube-apiserver.yaml As a Service - Inside the Master Node - /etc/systemd/system/kube-apiserver.service ps -ef | grep kube-apiserver # To see all kube apiserver configuration","title":"API Server"},{"location":"k8s/3-k8s/#kube-controller-manager","text":"Watch Status Remediate Situation Node Controller Replicaton Controller Api Server Configuration is stored Using Kubeadm - Inside API Server Pod - /etc/kubernetes/manifests/kube-controller-manager.yaml As a Service - Inside the Master Node - /etc/systemd/system/kube-controller-manager.service ps -ef | grep kube-controller-manager # To see all kube controller-manager configuration","title":"Kube Controller Manager"},{"location":"k8s/3-k8s/#kube-scheduler","text":"Assigning a Pod to a Node: Filter Nodes Rank Nodes Using Kubeadm - Inside API Server Pod - /etc/kubernetes/manifests/kube-scheduler.yaml As a Service - Inside the Master Node - /etc/systemd/system/kube-scheduler.service ps -ef | grep kube-scheduler # To see all kube scheduler configuration","title":"Kube Scheduler"},{"location":"k8s/3-k8s/#kubelet","text":"NOTE : Kubeadm does not install kubelet. Always install kubelet manually on the worker nodes. Always runs as a service on the worker nodes. ps -ef | grep kubelet # To see all kubelet configuration","title":"Kubelet"},{"location":"k8s/3-k8s/#kube-proxy","text":"Process running on the worker node as a service.","title":"Kube Proxy"},{"location":"k8s/3-k8s/#manual-scheduling","text":"Add nodeName property in the pod definition to schedule a pod at creation time if there is no scheduler.","title":"Manual Scheduling"},{"location":"k8s/3-k8s/#explain-commands","text":"# When is it useful: sometimes when editing/creating yaml files, it is not clear where exactly rsource should be placed (indented) in the file. Using this command gives a quick overview of resources structure as well as helpful explaination. Sometimes this is faster then looking up in k8s docs. kubectl explian pods --recursive | grep envFrom -A3 # Prints lines after a match is found # Output would be envFrom < [] Object> # This is an array of Objects, so the next line will be start with - configMapRef <Object> # This is an Object name <string> # This is a dictionary optional <boolean> # kubectl explain cronjob.spec.jobTemplate --recursive | less kubectl explain pods.spec.containers --recursive | less","title":"Explain Commands"},{"location":"k8s/3-k8s/#pods","text":"kubetcl get pods -o wide # Get the Node on which pod is running - Remember : You CANNOT edit specifications of an existing POD other than the below. 1. spec.containers[ ].image 1. spec.initContainers[ ].image 1. spec.activeDeadlineSeconds 1. spec.tolerations - For example: when you edit a pod in vi editor for environment variables, service accounts, resource limits. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable. 1. A copy of the file with your changes is saved in a temporary location when it fails. You can then delete the existing pod. Then create a new pod with your changes using the temporary file which was saved earlier in /tmp . 2. The second option is to extract the pod definition in YAML format to a file. Then make the changes to the exported file using an editor and save the file. Then delete the existing pod. Then create a new pod with the edited file. 3. Force replace the pod using kubectl replace -f <filename> --force","title":"Pods"},{"location":"k8s/3-k8s/#replicasets","text":"selector is the difference between ReplicaSet and ReplicationController apart from apiVersion. # Scaling RS kubectl replace -f rs.yaml kubectl scale --replicas = 6 rs myapp-rs # <Type> <Name of RS> format kubectl delete rs myapp-rs # Also deletes the underlying pods ### IMPORTANT # Either delete and recreate the ReplicaSet or Update the existing ReplicaSet and then delete all PODs, so new ones with the correct image will be created.","title":"Replicasets"},{"location":"k8s/3-k8s/#deployments","text":"kubectl create deployment mydeploy --image = nginx --replicas = 3 kubectl scale deployment mydeploy --replicas = 6 - Edit Deployments - With Deployments you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification, with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment.","title":"Deployments"},{"location":"k8s/3-k8s/#formatting-kubectl-output","text":"-o json # Output a JSON formatted API object. -o name # Print only the resource name and nothing else. -o wide # Output in the plain-text format with any additional information. -o yaml # Output a YAML formatted API object.","title":"Formatting kubectl Output"},{"location":"k8s/3-k8s/#namespaces","text":"kubectl config set-context $( kubectl config current-context ) --namespace = test # OR alias kns = kubectl config set-context --current --namespace kns test # Testing services # DNS resolution <svc name>.<namespace>.svc.cluster.local:<svc port> # cluster.local - Domain name, svc - subdomain name","title":"Namespaces"},{"location":"k8s/3-k8s/#imperative-commands","text":"` --dry-run ` : # By default as soon as the command is run, the resource will be created. ` -dry-run = client ` : # This will not create the resource, instead, tell you whether the resource can be created and if your command is right. # Generate POD Manifest kubectl run nginx --image = nginx --dry-run = client -o yaml > pod.yaml # Generate Deployment with 4 Replicas kubectl create deployment --image = nginx nginx --replicas = 4 --dry-run = client -o yaml > deploy.yaml # Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379 # This will automatically use the pod's labels as selectors kubectl expose pod redis --port = 6379 --name redis-service --dry-run = client -o yaml > svc.yaml # OR # This will not use the pods labels as selectors, instead it will assume selectors as app=redis. kubectl create service clusterip redis --tcp = 6379 :6379 --dry-run = client -o yaml # So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service # Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes # This will automatically use the pod's labels as selectors, but you cannot specify the node port. kubectl expose pod nginx --port = 80 --name nginx-service --type = NodePort --dry-run = client -o yaml # OR # This will not use the labels as selectors kubectl create service nodeport nginx --tcp = 80 :80 --node-port = 30080 --dry-run = client -o yaml # I would recommend going with the `kubectl expose` command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service. # Create Pod and Svc in one command kubetcl run nginx --image = nginx --port = 8080 --expose","title":"Imperative Commands"},{"location":"k8s/3-k8s/#docker-commands","text":"CMD [ \"command\" , \"parameters\" ] # Process which is executed in Docker container continuously # example CMD [ \"sleep\" , \"5\" ] # Sleep is executed every 5 secs # What is you want to pass parameters to Docker during execution, Use ENTRYPOINT ENTRYPOINT [ \"sleep\" ] # Process invoked at startup # To execute docker run sleeper-image 10 # This will pass 10 to the Sleep process # If no parameter is passed to Docker command, it will fail. # Passing default parameter when no parameter is passed, Use ENTRYPOINT and CMD both in Dockerfile ENTRYPOINT [ \"sleep\" ] # Process invoked at startup CMD [ \"5\" ] # Default parameter passed to sleep, if not given during docker execution # Suppose you want to override the default sleep process during execution docker run --entrypoint ping sleeper-image 8 .8.8.8 # Override sleep with ping process","title":"Docker Commands"},{"location":"k8s/3-k8s/#k8s-commands-and-arguments","text":"# Docker # K8s #---------------------------# # ENTRYPOINT --> command # CMD --> args # Example # In Dockerfile ENTRYPOINT [ \"python\" , \"app.py\" ] CMD [ \"--color\" , \"red\" ] # In K8s, args is overriding the input command: [ \"python\" , \"app.py\" ] args: [ \"--color\" , \"pink\" ]","title":"K8s Commands and Arguments"},{"location":"k8s/3-k8s/#environment-variables","text":"Passed as an array in key value format 3 Types of setting Env variables Direct Config Map Secrets # Direct env: - name: APP_COLOR value: pink # ConfigMap env: - name: APP_COLOR valueFrom: configMapKeyRef: # Secret env: - name: APP_COLOR valueFrom: secretKeyRef:","title":"Environment Variables"},{"location":"k8s/3-k8s/#configmap","text":"# From Literal kubectl create configmap <config-map name> --from-literal = <key>-<value> # From File kubectl create configmap <config-map name> --from-file = <path-to-file> # To reference a configMap file in pod definition # ConfigMap with apiVersion etc defined envFrom: - configMapRef: name: <configMap Name in Metadata> # To reference a configMap volume in pod definition volumes: - name: app-config-volume configMap: name: <configMap Name in Metadata>","title":"ConfigMap"},{"location":"k8s/3-k8s/#secrets","text":"The way kubernetes handles secrets. Such as: A secret is only sent to a node if a pod on that node requires it. Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage. Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well. Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault. # From Literal kubectl create secret generic <secret name> --from-literal = <key>-<value> # Note: generic is added # From File kubectl create secret generic <secret name> --from-file = <path-to-file> # To encode text to base64 echo -n \"Hello\" | base64 # To view the secret information kubectl get secret app-secret -o yaml # Output in yaml, then decode using base64 echo -n \"aTsfgs*#\" | base64 -d # To reference a secret in pod definition envFrom: - secretRef: name: <secret Name in Metadata> # To reference a secret in volume definition volumes: - name: app-secret-volume secret: secretName: <secret Name in Metadata> # Note the change of Key","title":"Secrets"},{"location":"k8s/3-k8s/#security-context","text":"Security Context can be added at Pod and Container level. If defined at both levels, container configuration overrides the security context defined at pod level. Note : Capabilities are only supported at container level and NOT at Pod level. # Adding additional Linux capability during container execution in Docker docker run --cap-add MAC_ADMIN ubuntu # Adds additional capability to the container apart from defaults # Adding security context to Pod in container section securityContext: runAsUser: 1000 capabilities: add: [ \"MAC_ADMIN\" ]","title":"Security Context"},{"location":"k8s/3-k8s/#service-accounts","text":"Service Accounts are used by applications or services and not by humans. Note: SA cannot be added to existing Pod. Always Delete and add SA to Pod definition to recreate. K8s automatically mounts the default namespace SA. To override this behavior, set automountServiceAccountToken: false in the Pod definition. kubectl create sa dashboard-sa # Create a SA kubectl get secret dashboard-sa-token-kdbm # K8s creates a secret to store the token to auth the service # You can use the token to run K8s API calls curl https://192.168.0.10:6443/api -insecure --header \"Authorization: Bearer <sa token>\"","title":"Service Accounts"},{"location":"k8s/3-k8s/#resource-requirments","text":"1 Gi - Gibibyte 1 Mi - Mebibyte 1 Ki - Kibibyte 1 CPU = 1 vCPU or 1 Hyperthread # If a Pod uses more CPU than its limit, it will be throttled. # If a Pod used more Mem than its limit, it will be terminated. - When a pod is created the containers are assigned a default CPU request of .5 and memory of 256Mi\". For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a LimitRange in that namespace. apiVersion : v1 kind : LimitRange metadata : name : mem-limit-range spec : limits : - default : memory : 512Mi defaultRequest : memory : 256Mi type : Container --- apiVersion : v1 kind : LimitRange metadata : name : cpu-limit-range spec : limits : - default : cpu : 1 defaultRequest : cpu : 0.5 type : Container","title":"Resource Requirments"},{"location":"k8s/3-k8s/#taints-and-tolerations","text":"If a taint is placed, by default no pods will be scheduled on the Node. Only when a Pod has tolerations matching the taint, will the K8s scheduler place the pod on the tainted node. 3 taint-effects: NoSchedule PreferNoSchedule NoExecute Important : Taints does not neccessarily mean that Pods matching the tolerations will be placed always on that tainted node. It can be placed on another Node which is not tainted. Taints and Tolerations is used only for restricting certain pods from being placed in it. To always place a pod on a tainted node, use Node Affinity . # Node Taint kubectl taint nodes <node name> key = value:taint-effect # Example kubectl taint nodes node1 app = blue:NoSchedule # Tolerations added to Pod definition tolerations: - key: \"app\" # Note: This is placed under Pod not containers section operator: \"Equal\" # Note: All values should be doube quoted value: \"blue\" effect: \"NoSchedule\" # By default, master Node is always tainted. To see the taint kubectl describe node kubemaster | grep Taint # To remove the taint from a node, add a minus (-) symbol at the end of the taint with NO spaces in between kubectl taint nodes node1 app = blue:NoSchedule- # Note the - with no spaces","title":"Taints and Tolerations"},{"location":"k8s/3-k8s/#node-selectors","text":"Labels placed on the Nodes which help scheduler place the pods matching the labels # Adding the lable to the node kubectl label node <node name> key = value # Example kubectl label node node1 size = Large # NodeSelectors added to Pod definition nodeSelector: size: Large # Important: Labels are simple and can't be used for complex selection using OR or NOT operators. # For example: Place Pods in Large or Medium Nodes. OR Place Pods in Nodes which are not Small. # OR # Example kubectl label node node1 size = # NodeSelectors added to Pod definition nodeSelector: size: \"\"","title":"Node Selectors"},{"location":"k8s/3-k8s/#node-affinity","text":"To overcome NodeSelector limitations, Affinity and Anti-Affinity is used. Node Affinity Types: requiredDuringSchedulingIgnoredDuringExecution : The scheduler can't schedule the Pod unless the rule is met. This functions like nodeSelector, but with a more expressive syntax. preferredDuringSchedulingIgnoredDuringExecution : The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod. 2 Types of Operators - In and Exists Important : Use a combination of taints and tolerations to Deny Pods from being placed on to it. Then Add labels to the Nodes. After that add NodeAffinity to ensure the matching Pod goes to the correct Node. affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/os operator : In values : - linux","title":"Node Affinity"},{"location":"k8s/3-k8s/#multi-container-pods","text":"Design Patterns Side car - Example is a logging container which ships the logs to a central logging service Adapter - Example is a logging container which converts the logs to a standard format before shipping Ambassador - Outsourcing the database connection to a separate container based on environments which acts as a proxy to the database service. The application always refers to the database using a standard dns name.","title":"Multi-Container Pods"},{"location":"k8s/3-k8s/#init-pods","text":"In a multi-container pod, each container is expected to run a process that stays alive as long as the POD's lifecycle. For example in the multi-container pod that we talked about earlier that has a web application and logging agent, both the containers are expected to stay alive at all times. The process running in the log agent container is expected to stay alive as long as the web application is running. If any of them fails, the POD restarts. But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or binary from a repository that will be used by the main web application. That is a task that will be run only one time when the pod is first created. Or a process that waits for an external service or database to be up before the actual application starts. That's where initContainers comes in. When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts. You can configure multiple such initContainers as well, like how we did for multi-pod containers. In that case each init container is run one at a time in sequential order . If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds. containers : - name : myapp-container image : busybox:1.28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] initContainers : - name : init-myservice image : busybox command : [ 'sh' , '-c' , 'git clone <some-repository-that-will-be-used-by-application> ;' ] # Another Example of Sequential execution of Init Containers containers : - name : myapp-container image : busybox:1.28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] initContainers : - name : init-myservice image : busybox:1.28 command : [ 'sh' , '-c' , 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;' ] - name : init-mydb image : busybox:1.28 command : [ 'sh' , '-c' , 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;' ] # To debug the Init container logs kubectl logs -c <Init container name> # Shows the exact error","title":"Init Pods"},{"location":"k8s/3-k8s/#readiness-and-liveness-probes","text":"Status of a Pod Lifecycle: PodScheduled --> Initialized --> ConatinersReady --> Ready Liveness - Test for checking if your application is working Liveness and Readiness Probes have the same configuration. # Readiness probes based on the protocol # Http readinessProbe: httpGet: path: /api/ready port: 8080 initialDelaySeconds: 10 # Tells to wait before checking periodSeconds: 5 # Interval between each attempt failureThreshold: 8 # How many attempts # TCP readinessProbe: tcpSocket: port: 3306 # Exec readinesProbe: exec: command: - cat - /app/is_ready","title":"Readiness and Liveness Probes"},{"location":"k8s/3-k8s/#container-logging","text":"# Docker Logs docker run -d kodekloud /event-simulator # Logs are not streamed as its running in detached mode docker logs -f <container id> # Shows the container logs # K8s logs kubectl logs -f event-simulator-pod # -f = Live streaming of logs # Multiple containers in pod # Get pods and see if there are more than 1 containers and then after -c do a tab to see the container names kubectl logs -f event-simulator-pod -c event-simulator # You can skip -c and directy mention container name","title":"Container Logging"},{"location":"k8s/3-k8s/#monitor-and-debug-aplications","text":"Open sources projects to monitor clusters, Metrics server, Prometheus You can have 1 Metrics server per cluster . It is a In-memory solution and does not store data in disk. Kubelet agent has cAdvisor (container Advisor) component which extracts performance metrics. cAdvisor then makes this data available via the K8s API to the Metrics server. # Download the Metrics server from Github git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git # Apply the metric server components kubectl apply -f . # OR kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml # After metrics is collected after some time lag, run the commands # To wait for the output to come watch \"kubectl top node\" # Note: the command in double quote. Ctrl + c to exit kubectl top node # Get the CPU and memory consumption of each node kubectl top pod # Pod performance","title":"Monitor and Debug Aplications"},{"location":"k8s/3-k8s/#labels-selectors-and-annotations","text":"kubectl get pods --show-labels # List labels kubectl get pods -l env = dev # Filter Labels using short form kubectl get all --selector = env = prod --no-headers | wc -l # Filter all objects and remove headers kubectl get pods --selector = env = prod,bu = finance,tier = frontend # Logical AND # Tip # In ReplicaSet or Service, the matchLabels in the spec.selector section should always match the pod labels in the spec.template.metadata.labels. # Error: \"selector\" does not match template 'labels'","title":"Labels, Selectors and Annotations"},{"location":"k8s/3-k8s/#update-and-rollback-deployments","text":"2 Deployment strategy - RollingUpdate (default) and Recreate # Create Deployments kubectl create -f deployments-definition.yml # Using yaml format # OR kubectl create deployment my-app-deployment --image = nginx kubectl apply -f deployments-definition.yml # To update a deployment # OR Without changing definition file, updating parameters, # NOTE nginx is the container name in existing pod/deployment kubectl set image deployment/my-app-deployment nginx = nginx:1.9 # Image is upgraded # Roll-out strategy kubectl rollout status deployment/my-app-deployment # Shows rollout status kubectl rollout history deployment/my-app-deployment # Shows rollout history and revisions # You can check the status of each revision individually by using the --revision flag: kubectl rollout history deployment/my-app-deployment --revision = 1 # Shows detailed history # We can use the --record flag to save the command used to create/update a deployment against the revision number. Change is recorded as annotation in the deployment as \"change-cause\". kubectl set image deployment/my-app-deployment nginx = nginx:1.7 --record # OR kubectl edit deployments my-app-deployment --record kubectl rollout undo deployment/my-app-deployment # Rolls back to previous version","title":"Update and Rollback Deployments"},{"location":"k8s/3-k8s/#daemonsets","text":"kubectl get daemonsets # TIP: There is no kubectl create daemonset, so do a create deployment, get this into a yaml. # Format the yaml for Kind, Remove replicas, strategy and save the file.","title":"DaemonSets"},{"location":"k8s/3-k8s/#static-pods","text":"When there is no Master Node and its components, you can create Pods on standalone Worker Node. Such a pod is called Static Pod To inspect the path where the definition is defined, look at the kubelet.service. It could be in 2 places: \u2013config= - kubeadm installation \u2013pod-manifest-path= - manual installation The pod definitions have to be placed for example in /etc/kubernetes/manifests in a yaml file. Deleting the yaml file, removes the pod from the node. To identify static pods, pod name will have the node name appended at the end. kubeadm deploys the cluster components as static pods, which have controlplane appended in the pod name. Another way to identify static pod is to get the yaml of the pod and then searching for ownerReferences . In that if kind: Node then its a static pod. # To view the pods after creation, as kubectl will not work docker ps # To kill the pod docker container rm <id> # Kubelet config /var/lib/kubelet/config.yaml","title":"Static Pods"},{"location":"k8s/3-k8s/#jobs","text":"# Docker execution of mathematical problem docker run ubuntu expr 3 + 2 # Task is completed and container exits docker ps -a # Shows the exit status of the container # For batch processing Jobs are used in K8s kubectl create job throw-dice --image = kodekloud/throw-dice --dry-run = client -o yaml > job.yml # NOTE: Add backofflimit parameter if its not in the generated template to avoid job from quiting before it succeeds kubectl get jobs # list the jobs kubectl get pods # lists the pods created by the job kubectl logs <pod-name> # shows the pod output # Running multiple pods in sequence, add completions parameter to the Job spec. # NOTE: This is the successful pod completion count, it will keep on recreating pods till this number matches. # Running multiple pods in parallel, add parallelism parameter to the Job spec along with completion.","title":"Jobs"},{"location":"k8s/3-k8s/#cronjobs","text":"# Min-Hour-DOM-Month-Day of Week (0-6) # Sun - 0 & 7 both, Sat -6 # spec.schedule is the additional parameter added. # NOTE: schedule is at the first spec kubectl create job throw-dice --image = kodekloud/throw-dice --schedule = \"30 21 * * *\" --dry-run = client -o yaml > cronjob.yml kubectl get cronjob # list the cronjob","title":"Cronjobs"},{"location":"k8s/3-k8s/#services","text":"3 Types to access a service NodePort: Mapping a port on the Node to a port on the pod ClusterIP: Internal Virtual IP not exposed out of the Node LoadBalancer: External IP which load balances multiple ports on the Node NodePort: K8s takes care of deploying the service across all nodes, even though the pod is not on those nodes. This helps in getting the same Nodeport exposed on all the Nodes. When a http call hits a nodeport on a node, K8s will route traffic internally to the Pod on the correct Node. # NodePort (Node) --> Port (Svc) --> TargetPort (Pod) # Target Port is the Pod port where the service forwards requests to # Port is on the Service kubectl create deployment frontend --replicas = 2 \\ --labels = run = load-balancer-example --image = busybox --port = 8080 kubectl expose deployment frontend --type = NodePort --name = frontend-service --port = 6262 --target-port = 8080 --dry-run = client -o yaml > svc.yml # This is because we cant control the value of NodePort in imperative command. Edit the yaml file and add the nodePort parameter under spec.ports.port.nodePort kubectl get services # List services # Take the NodePort and use the Node IP to hit the service from outside the machine","title":"Services"},{"location":"k8s/3-k8s/#multiple-kube-schedulers","text":"Advanced Scheduling https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/ https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/ https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work","title":"Multiple Kube Schedulers"},{"location":"k8s/3-k8s/#os-upgrades","text":"kubectl drain node1 # Gracefully evict the pods kubectl cordon node1 # Makes the node unschedulable kubectl uncordon node1 # Makes its schedulable after maintenance kubectl drain node01 --ignore-daemonsets # Incase you get the ds exist error # Even with ignore ds options, you can get error when there are pods which are not managed by a replicaSet # error: unable to drain node \"node01\" due to error:cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override) # In this case, copy the pod data into yaml and apply it again after draining the nodes using --force","title":"OS Upgrades"},{"location":"k8s/3-k8s/#k8s-release-strategy","text":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/ https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md","title":"K8s Release Strategy"},{"location":"k8s/3-k8s/#cluster-upgrade","text":"First upgrade master, then the nodes Kubeadm does not update the kubelet, so it needs to be updated manually cat /etc/*release* # Shows the OS version # When you do apt-cache update, it will show which kubeadm minor version is present # TIP: Copy the upgrade commands to notepad, before updating versions and pasting kubeadm upgrade plan # Shows the upgrade plan apt-get upgrade -y kubeadm = 1 .12.0-00 # 1st Update kubeadm as per plan kubeadm upgrade apply v1.12.0 # Upgrade the cluster controlplane # NOTE: Master will show version of kubelet, which has not been updated yet. kubectl get nodes # Master is still shown with V1.11 version # Drain the master and then perform kubelet upgrade apt-get upgrade -y kubelet = 1 .12.0-00 # Upgrade the kubelet systemctl restart kubelet # Make the change permanent kubectl get nodes # Master is now shown with V1.12 version # Now follow the process of upgrading Nodes using drain and cordon # IMPORTANT: the Node Drain command needs to be run on the master and not Node01. .e. all kubelet commands like drain and uncordon needs to be run on master # Follow the upgrade process, upgrade kubeadm, upgrade kubelet , upgrade node and then restart kubelet # Uncordon the Node # NOTE: the Node upgrade command kubeadm upgrade node config --kubelet-version v1.12.0 # Upgrade the node","title":"Cluster Upgrade"},{"location":"k8s/3-k8s/#backup-and-restore","text":"To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3. export ETCDCTL_API=3 For example, if you want to take a snapshot of etcd, use: etcdctl snapshot save -h and keep a note of the mandatory global options. Since our ETCD database is TLS-Enabled, the following options are mandatory: --cacert - verify certificates of TLS-enabled secure servers using this CA bundle --cert - identify secure client using this TLS certificate file --endpoints=https://127.0.0.1:2379 - This is the default as ETCD is running on master node and exposed on localhost 2379. --key - identify secure client using this TLS key file Similarly use the help option for snapshot restore to see all available options for restoring the backup. etcdctl snapshot restore -h # Manual Backup of applications kubectl get all --all-namespaces -o yaml > all-deployed-services.yaml # Backup ETCD ETCDCTL_API = 3 etcdctl snapshot save snapshot.db ETCDCTL_API = 3 etcdctl snapshot status snapshot.db # View backup status # To restore ETCD # First stop the kube-apiserver service kube-apiserver stop # Execute restore from the saved state ETCDCTL_API = 3 etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup # Here the data-dir is a new dir where the ETCD will start storing the data. # This is done so that the existing data is not overwritten, in case of any issues during restore # Reload the service daemon and restart etcd service systemctl daemon-reload service etcd restart # Finally start the kube-apiserver service kube-apiserver start # Tips: To check the version of etcd, check the etcd pod logs or the image version in the etcd pod kubectl describe pod etcd-controlplane -n kube-system # Get the below 4 parameters after describe etcd pod ETCDCTL_API = 3 etcdctl snapshot save /opt/snapshot-pre-boot.db \\ --cacert = \"/etc/kubernetes/pki/etcd/ca.crt\" \\ --cert = \"/etc/kubernetes/pki/etcd/server.crt\" \\ --endpoints = https://127.0.0.1:2379 \\ --key = \"/etc/kubernetes/pki/etcd/server.key\" # Restore the etcd to a new directory from the snapshot # So move the backed up data to a new dir ETCDCTL_API = 3 etcdctl snapshot restore /opt/snapshot-pre-boot.db \\ --data-dir = \"/var/lib/backup-from-etcd\" # Certificate details are not required in restore as the file in in local directory. # Next, update the /etc/kubernetes/manifests/etcd.yaml: # We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory (/var/lib/etcd-from-backup). volumes: - hostPath: path: /var/lib/etcd-from-backup type: DirectoryOrCreate name: etcd-data https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md https://www.youtube.com/watch?v=qRPNuT080Hk","title":"Backup and Restore"},{"location":"k8s/3-k8s/#security","text":"","title":"Security"},{"location":"k8s/3-k8s/#tls","text":"There are 2 types of Encryption mechanisms: Symmetric and Asymmetric For TLS both the mechanims are used: Symmetric - To encrypt the client data Asymmetric - To encrypt the symmetric key after server auth In symmetric, one key is used to encrypt and decrypt the data, while in asymmetic 2 keys are used. Public Key (will be refered as Lock). You can encrypt data with either private or public key. NOTE : Decryption is only done using the opposite key. For example, if you encrypt the data with public key, you cannot decrypt it using public key, you MUST using the private key. Keys have naming convention, private key will always have key in the name to identify it as private key. There are 3 usecases of using Asymmetric Encyption in the TLS process Using SSH to login to any server. In this case, user private key is used to unlock the server access having the user's public key stored on the server. Using asymmetric keys to transfer client's symetric key over the Internet. These are called Server Certificates . This should ensure no hacker is allowed to decrypt the data in transit. Using asymmetric keys to authenticate the website. These are called Root Certificates . In this case, the website presents a CA certificate which is having the CA pulic key embedded. The client presents this via the browser which has CA's public keys installed, which decrypts the certificate using the public key to authenticate the website. TLS Overview A system admin generates private and public key to enforce SSH authenication. The public key is stored in the server. Web Server generates private and public key to encrypt HTTPS trafic. For this, the web server generates a certificate signing request using its public key. This CSR (which has the public key of the server) is sent to the CA for signing the certificate. The CA signs the certificate using its private key and sends it back to the server after completing its validation process. When a client request comes, the web server sends its certificate having its encrypted public key back. The client presents the certificate to the browser. The browser using the CA public key, decrypts the certificate, thus authenticating the web server and thus the website. The decrypted certificate has the server's public key. The client generates its symmetric key and then encrypts this using the web server's public key and sends the request back to the web server. The web server decrypts the request using its private key and gets access to the client's symmetric key. Now the communication between the client and web server will continue happening using this symmetric key.","title":"TLS"},{"location":"k8s/3-k8s/#tls-in-kubernetes","text":"There are 2 types of certificates used in Kubernetes components Server Certificates - Used by the server components Client Certificates - These are certificates used by Users or process to authenticate themselves. Types of Clients for the API server: Admin Users Kube Scheduler Kube Controller Manager Kube Proxy There is only one client for the ETCD server: API Server There is also one client for the Kubelet server: API Server All the certificates (Server and Client) need to be signed by a Root certificate that is issued by a CA.","title":"TLS in Kubernetes"},{"location":"k8s/3-k8s/#certificate-creation","text":"# We require only Private Keys and Certificates # Generate CA certificates openssl genrsa -out ca.key 2048 openssl req -new -key ca.key -subj \"/CN=KUBERNETES-CA\" -out ca.csr # Sign the CSR to generate the Root Cert openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt # Generating Client Key and Certificates openssl genrsa -out admin.key 2048 openssl req -new -key admin.key -subj \"/CN=kube-admin/O=system:masters\" -out admin.csr # Remember CN will the user name that is used to login to API server # O parameter should link to the user account, in this case its the admin group openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt # Here the CA crt and CA key is used to sign the user csr # Once the cluster is configured, you an use the certificates in Rest API calls curl https://kube-apiserver:6443/api/v1/pods \\ --key admin.key --cert admin.crt --cacert ca.crt # Generating Server Key and Certificates openssl genrsa -out apiserver.key 2048 openssl req -new -key apiserver.key -subj \"/CN=kube-apiserver\" -out apiserver.csr -config openssl.cnf # Additional openssl.cnf contains DNS Alias or IP address which refers back to the api-server openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -out apiserver.crt","title":"Certificate Creation"},{"location":"k8s/3-k8s/#debugging-certificates","text":"# View existing certificates openssl x509 -req -in /etc/kubernetes/pki/apiserver.crt -text -noout # Verify the Subject, Subject Alternative Names (Alias), Not After (to check validity), Issuer (CA) # Debugging Certificate Issues # When installed as service journalctl -u etcd.service -l # List the service logs # When installed using kubeadm kubectl logs etcd-master ## Failure logs - Failed to dial 127.0.0.1:2379: connection error # If ETCD or Apiserver is down, use docker docker ps -a # To view the containers docker logs <container id> # To view the logs","title":"Debugging Certificates"},{"location":"k8s/3-k8s/#certificates-api","text":"- NOTE : To view the CA certificates, as its stored on the master look at the Controller Manager configuration and check the path mentioned under --cluster-signing-cert-file and --cluster-signing-key-file kubectl get csr # Get pending CSR kubectl certificate approve jane # Approve CSR kubectl get csr jane -o yaml # View the CSR echo \"data\" | base64 --decode # Extract the Certificate from CSR after it is approved and send to user jane # Now she will be able to login to the cluster","title":"Certificates API"},{"location":"k8s/3-k8s/#image-security","text":"# Create a Docker Registry secret in the cluster for Kubelet to pass this to the Docker Runtime on the worker nodes kubectl create secret docker-registry regcred \\ --docker-server = <URL> \\ --docker-username = <> \\ --docker-password = <> \\ --docker-email = <email> # Once the secret is created, use this in the Pod definition imagePullSecrets: - name: regcred","title":"Image Security"},{"location":"k8s/3-k8s/#deployment-strategies","text":"Recreate RollingUpdate The above 2 deployments are native to K8s. While the below 2 needs several steps to complete. Blue Green Deployment Switch is done in the service object before the blue deployment is killed completely. Canary Deployments Reduce the number of replicas in the canary deployment initially, then scale up after testing","title":"Deployment Strategies"},{"location":"k8s/3-k8s/#docker-volume","text":"Docker creates Read Only layers for the image For the container a transient Read-Write layer is created. When a volume is created and then it is mounted using the -v option, its is called Volume Mount . If a volume is not present, but a volume is mounted, Docker will create the volume at run time. When a host directory is mounted, it is called Bind Mount Use the new convention --mount instead of -v in the new version of Docker.","title":"Docker Volume"},{"location":"k8s/3-k8s/#container-storage-interface","text":"","title":"Container Storage Interface"},{"location":"k8s/3-k8s/#volumes","text":"","title":"Volumes"},{"location":"k8s/3-k8s/#persistent-volumes-and-claims","text":"- Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this: apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : myfrontend image : nginx volumeMounts : - mountPath : \"/var/www/html\" name : mypd volumes : - name : mypd persistentVolumeClaim : claimName : myclaim kubectl get persistentvolume kubectl get persistentvolumeclaim","title":"Persistent Volumes and Claims"},{"location":"k8s/3-k8s/#storage-class","text":"When we create PV, then it is called Static Provisioning of Storage. In cloud, there is need to create storage dynamically when PVC is used. In this case, only Storage Class Objects are created. When a claim is made wihich references the Storage class, a PV is dynamically created. This is Dynamic Provisioning of Storage. The Storage Class makes use of VolumeBindingMode set to WaitForFirstConsumer . This will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created. Till then the PVC will be in Pending State . kubectl get storageclass","title":"Storage Class"},{"location":"k8s/3-k8s/#networking-basics","text":"Switching allows to configure machines to talk to each other in the same network Routing allows machines to talk across 2 or more networks Gateway allows machines to reach other network using a single entry point made in the routing table of each machine When a machine wants to reach to the internet, manual routes can be made to reach the destination on each machine To avoid the hassle of making entry for each IP address available on the Internet on each machine, a default route entry is made. When a route does not match, it goes through the default route. To separate Internal and External routing, you can make separate entries in the routing table of the machine.","title":"Networking Basics"},{"location":"k8s/3-k8s/#networking-native-commands","text":"What is the network interface configured for cluster connectivity on the controlplane node? Run the ip a / ip link command and identify the interface. What is the MAC address of the interface on the controlplane node? Run the command: ip link show eth0 What is the MAC address assigned to node01? Run the command: arp node01 on the controlplane node. What is the state of the interface docker0? Run the command: ip link show docker0 and look for the state. If you were to ping google from the controlplane node, which route does it take? What is the IP address of the Default Gateway? Run the command: ip route show default and look at for default gateway. What is the port the kube-scheduler is listening on in the controlplane node ? Use the command: netstat -tunlp Notice that ETCD is listening on two ports. Which of these have more client connections established? Run the command: netstat -anp | grep etcd . That's because 2379 is the port of ETCD to which all control plane components connect to. 2380 is only for etcd peer-to-peer connectivity. When you have multiple controlplane nodes.","title":"Networking Native Commands"},{"location":"k8s/3-k8s/#cni","text":"ps aux | grep kubelet # Show kubelet configuration for CNI ls /opt/cni/bin # Shows all CNI executables ls /etc/cni/net.d # Shows the configuration files kubectl exec busybox -- ip route # To show the routes a pod can take. Deploy a busybox pod first - What binary executable file will be run by kubelet after a container and its associated namespace are created. Look at the type field in file / etc/cni/net.d/10-flannel.conflist .","title":"CNI"},{"location":"k8s/3-k8s/#deploy-weave-for-pod-networking","text":"# Deploy Weave CNI, use the bookmark to find the command kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \" # To Trouble shoot the deployment kubectl logs -n kube-system <weave pod name> -c weave # If CNI is not deployed you get the following error in App Pod # failed to set up sandbox container network for pod \"app\": networkPlugin cni failed to set up pod \"app_default\" network: unable to allocate IP address ip a | grep eth0 # Identify the Host Network kubectl logs -n kube-system weave-net # Show the current failure logs # If error says \"Network 10.232.0.0/12 overlaps with the existing route\" of the host network, you need to allocate Weave Network to a differentIP range. # In this case, add &env.IPALLOC_RANGE=10.50.0.0/16 in the kubectl apply command. #NOTE: this env should be inside the quotes. # Once deployed, you can view the interface that weave creates, by ip link show weave # The network is configured with weave. Check the weave pods logs using command kubectl logs <weave-pod-name> weave -n kube-system and look for ipalloc-range # The above command will show POD IP address range configured by weave # What is the default gateway configured on the PODs scheduled on node01 by weave? kubectl run busybox --image = busybox --restart = Never --dry-run = client -o yaml -- sleep 1000 > po.yaml # Add nodeName: node01 to install the pod in Node01 and then exec to find the default gateway used by the Pod kubectl exec busybox -- route # OR kubectl exec busybox -- ip route","title":"Deploy Weave for Pod Networking"},{"location":"k8s/3-k8s/#service-networking","text":"Kube-Proxy assigns service ip and port combination to the iptables to enable networking. ps aux | grep kube-api-server # API server has the service ip range configured # search for --service-cluster-ip-range value to get the service ip range. Pod ip will come from the CNI plugin # OR cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range kubectl get service # Shows the service ip iptables -L -t nat | grep <service-name> # Shows the IP table rules for the service # Check the kube-proxy logs on the node cat /var/log/kube-proxy.log # Shows the logs of what iptable proxy it uses and also when a service entry is made to iptables","title":"Service Networking"},{"location":"k8s/3-k8s/#dns","text":"You need a DNS server and it can help manage name resolution in large environments with many hostnames and Ips and then configure your hosts to point to a DNS server. Domain names and the IP address of all the host in the network are added to the /etc/hosts file of the DNS server. host file of the individual server is made to point to the DNS server for getting server names resolved to IP addresses. When the servers need to resolve a name which is not part of the internal network, the internal DNS server can point to an external DNS server to handle the name resolution. In this case, ping to facebook.com will get correctly resolved using Google DNS. High level domain names are categorized based on their functions Domains are further sub-divided into sub domains. Root (.) being the top most level. DNS resolution request originating from an internal company DNS traverses through the Root domain servers till it gets a match and then the IP address is sent back to the calling DNS, where the response is cached based on Time To Live parameter. A records are stored which maps the DNS to the IP address. When you want yone DNS to map to multiple alias, you use the CNAME record. Using search inside an organization you can alias your domains. In this way, you can address your servers with only the sub-domain. search will append the domain name and then resolve the IP address","title":"DNS"},{"location":"k8s/3-k8s/#cluster-dns","text":"- Pod IP is replaced by - to derive its POD name in the DNS","title":"Cluster DNS"},{"location":"k8s/3-k8s/#coredns","text":"# Where is the configuration file located for configuring the CoreDNS service? Inspect the Args field of the coredns deployment and check the file used. cat /etc/coredns/Corefile # Shows the plugins configured and the error handling kubectl get svc -n kube-system # Shows the core dns service which is made available to all the pods # The IP address shown as the Cluster-IP of this service is the Nameservice that is configured inside all the pods `/etc/resolv.conf` file which then knows which DNS they need to point to. # Kubelet makes sure this entry is made inside all pods. # View the kubelet service config yaml to see the DNS config cat /var/lib/kubelet/config.yaml # clusterDNS section points to the coreDNS service cluster IP # How is the Corefile passed in to the CoreDNS POD? Use the kubectl get configmap command for kube-system namespace and inspect the correct ConfigMap. It will be passed as a Config Volume # What is the root domain/zone configured for this kubernetes cluster? Run the command: kubectl describe configmap coredns -n kube-system and look for the entry after kubernetes. This is where cluster.local as root domain is configured # Check the FQDN of any service host web-service # DNS will return the FQDN # Checking the FQDN of pod is not possible, you need to specify the FQDN host 10 -244-4-5.default.pod.cluster.local # hr app POD is hr namespace and mysql Service is in payroll namespace. From the hr pod nslookup the mysql service and redirect the output to a file /root/CKA/nslookup.out kubectl exec -it hr -- nslookup mysql.payroll > /root/CKA/nslookup.out # NOTE: the DNS name of the service is used along with namespace","title":"CoreDNS"},{"location":"k8s/3-k8s/#ingress-networking","text":"- Create a default-backend deployment to handle routes that are not managed. - Create a service default-backend-service to manage 404 error handling and link to the ingress resource. - NOTE : Ingress needs to be deployed in the same namespace as the deployment & service object. # Imperative command from K8s 1.20 kubectl create ingress <ingress-name> --rule = \"host/path=service:port\" # Example of Imperative kubectl create ingress ingress-test --rule = \"wear.my-online-store.com/wear*=wear-service:80\" kubectl get ingress # list the ingress","title":"Ingress Networking"},{"location":"k8s/3-k8s/#rewrite-target-option","text":"Our watch app displays the video streaming webpage at http://<watch-service>:<port>/ Our wear app displays the apparel webpage at http://<wear-service>:<port>/ We must configure Ingress to achieve the below. When user visits the URL on the left, his request should be forwarded internally to the URL on the right. Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forwarded users to the appropriate application in the backend. http:// < ingress-service > : < ingress-port > /watch` --> http:// < watch-service > : < port > / http:// < ingress-service > : < ingress-port > /wear --> http:// < wear-service > : < port > / Without the rewrite-target option, this is what would happen: http:// < ingress-service > : < ingress-port > /watch --> http:// < watch-service > : < port > /watch http:// < ingress-service > : < ingress-port > /wear --> http:// < wear-service > : < port > /wear Notice watch and wear at the end of the target URLs. The target applications are not configured with /watch or /wear paths. They are different applications built specifically for their purpose, so they don't expect /watch or /wear in the URLs. And as such the requests would fail and throw a 404 not found error. To fix that we want to \"ReWrite\" the URL when the request is passed on to the watch or wear applications. We don't want to pass in the same path that user typed in. So we specify the rewrite-target option. This rewrites the URL by replacing whatever is under rules->http->paths->path which happens to be /pay in this case with the value in rewrite-target. This works just like a search and replace function . For example: replace ( path, rewrite-target ) In our case : replace ( \"/path\" , \"/\" ) apiVersion : extensions/v1beta1 kind : Ingress metadata : name : test-ingress namespace : critical-space annotations : nginx.ingress.kubernetes.io/rewrite-target : / spec : rules : - http : paths : - path : /pay backend : serviceName : pay-service servicePort : 8282 replace ( \"/something(/| $ )(.*)\" , \"/ $2 \" ) In this ingress definition, any characters captured by (.*) will be assigned to the placeholder $2, which is then used as a parameter in the rewrite-target annotation. rewrite.bar.com/something rewrites to rewrite.bar.com/ rewrite.bar.com/something/new rewrites to rewrite.bar.com/new apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : nginx.ingress.kubernetes.io/rewrite-target : /$2 name : rewrite namespace : default spec : rules : - host : rewrite.bar.com http : paths : - backend : serviceName : http-svc servicePort : 80 path : /something(/|$)(.*)","title":"Rewrite Target Option"},{"location":"k8s/3-k8s/#network-policies","text":"- Important : Always look at Network policy from the perspective of the Pod. For Rules - Always pay attention to the Request (Ingress) and not the Response (Egress) as that may be already blocked due to cluster wide network policy. - Example: If DB pod needs to be accessed by API pod, then in DB pod the traffic is Ingress. - Flannel does not support Network Policy. If Network policy is still applied to this network, it will not have any effect. - Usecase 1: Rule - Apply Ingress policy using Pod Labels - Usecase 2: Rule - Apply Ingress policy using Pod Labels and Namespaces - This is a Logical AND operation where pods have same labels in other namespaces are ignored. - Usecase 3: Rule - Apply Ingress policy using Namespace only - Usecase 4: Rule - Apply Ingress policy using External IP - This is required, where the service resides outside the cluster and the service needs to connect to it. - Along with the Pod label AND Namesace, External service is an OR. So the Pod should have the correct label AND in the namespace OR extrenal service should have the IP. Either of these 2 rule matches traffic will be allowed. - Usecase 5: Rule - Apply Ingress policy using 3 Rules and OR operation - NOTE : There is a hypen to `namespaceSelector. This makes it as a separate rule. - Usecase 6: Rule - Apply Egress policy using External Service - The traffic is allowed from pod to the external service kubectl get networkpolicy","title":"Network Policies"},{"location":"k8s/3-k8s/#kubeconfig","text":"kubectl view config # $HOME/.kube/config file which is default is read kubectl view config --kubeconfig = my-custom-config # Pass a config file not in .kube dir kubectl config use-context prod-user@production # Sets the current context","title":"Kubeconfig"},{"location":"k8s/3-k8s/#api-versions","text":"- Important : kube proxy != kubectl proxy. # To access the K8s API from a local server, start the kubectl proxy. # This proxy will use the kubeconfig data in the default kubeconfig kubectl proxy # start the proxy service on port 8001. Use 8001 instead of 6443 kubectl http://localhost:8001 -k # Shows all the API groups kubectl http://localhost:8001 -k | grep \"name\" # Shows the named API groups","title":"API Versions"},{"location":"k8s/3-k8s/#deprecated-ap-versions","text":"# Convert an old API formatted file to a new stable version kubectl convert -f <old-file> --output-version <new api version> kubectl convert -f nginx.yaml --output-version apps/v1","title":"Deprecated AP Versions"},{"location":"k8s/3-k8s/#authentication-and-authorization","text":"Authentication Authorization","title":"Authentication and Authorization"},{"location":"k8s/3-k8s/#roles-and-rolebindings","text":"- IMPORTANT : Roles and RoleBindings are namespaced kubectl get roles kubectl get rolebindngs # Check access kubect auth can-i create deployments # As current user kubectl auth can-i delete nodes --as dev-user # As an Admin user, you can impersonate and test for another user kubectl auth can-i delete pods --as system:serviceaccount:<namespace>:<sa name>> # As an service account in a namespace","title":"Roles and Rolebindings"},{"location":"k8s/3-k8s/#cluster-roles-and-cluster-rolebindings","text":"kubectl api-resources --namespaced = true # Get resources which can be added to roles kubectl api-resources --namespaced = false # Get resources which can be added to clusterroles kubectl get clusterroles kubectl get clusterrolebindngs","title":"Cluster Roles and Cluster RoleBindings"},{"location":"k8s/3-k8s/#admission-controllers","text":"- Helps implement better security measures. - Validates configuration. - Performs additional operations before a pod is created. - Note : The NamespaceExists and NamespaceAutoProvision admission controllers are deprecated and now replaced by NamespaceLifecycle admission controller. - The NamespaceLifecycle admission controller will make sure that requests to a non-existent namespace is rejected and that the default namespaces such as default , kube-system and kube-public cannot be deleted. ##NOTE: # Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins. ps -ef | grep kube-apiserver | grep admission-plugins # To check all the values that are valid for kube-apiserver kube-apiserver -h | grep enable-admission-plugins # Shows enabled admin controllers # Incase kube-apiserver is running as a pod managed by kubeadm kubectl exec kube-apiserver-controlplane -n kube-system \\ # Exec into pod -- kube-apiserver -h | grep enable-admission-plugins # NOTE: -- for command execution","title":"Admission Controllers"},{"location":"k8s/3-k8s/#validating-and-mutating-addmission-controllers","text":"Validating AC : Are the controllers which validate the request that is submitted to the API server Mutating AC : Are the controllers which change or mutate the request that is submitted to the API server if it does not meet the standards defined. Mutating AC are always invoked before Validating AC otherwise some requests may be rejected otherwise. # A pod with a conflicting securityContext setting: it has to run as a non-root # user, but we explicitly request a user id of 0 (root). # Without the webhook, the pod could be created, but would be unable to launch # due to an unenforceable security context leading to it being stuck in a # 'CreateContainerConfigError' status. With the webhook, the creation of # the pod is outright rejected. apiVersion : v1 kind : Pod metadata : name : pod-with-conflict labels : app : pod-with-conflict spec : restartPolicy : OnFailure securityContext : runAsNonRoot : true runAsUser : 0 containers : - name : busybox image : busybox command : [ \"sh\" , \"-c\" , \"echo I am running as user $(id -u)\" ]","title":"Validating and Mutating Addmission Controllers"},{"location":"k8s/3-k8s/#helm","text":"# Install helm sudo snap install helm # OR sudo snap install helm --classic # This is the default helm repo helm install wordpress helm upgrade wordpress helm rollback wordpress helm uninstall wordpress # To work with a custom helm repo helm repo add bitnami https://charts.bitnami.com/bitnami # Add a custom helm repository helm search repo wordpress # Search for a chart in a named repository helm install release-1 bitnami/wordpress helm list helm uninstall release-1 helm pull --untar bitnami/wordpress # Just downloads the chart, does not install it ls wordpress # to check the chart contents helm install release-2 ./wordpress","title":"Helm"},{"location":"k8s/3-k8s/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"k8s/3-k8s/#application-troubleshooting","text":"# Check the Front End app service if that is reachable kubectl describe web-service # Get the Endpoint and Nodeport curl http://web-service:30080 # OR If you are in the same machine curl https://localhost:30080 # NOTE: When you describe the service, there should be an ENDPOINTS which is detected. If its None, then selector is not correct # If there is no response, check the labels on the service and the pod that is its serving traffic to # Check the Pod restarts and why it is failing kubectl logs web -f # -f to tail the live logs and wait for it to fail # OR kubectl logs web -f --previous # --previous to get the last pod's logss","title":"Application Troubleshooting"},{"location":"k8s/3-k8s/#cluster-troubleshooting","text":"# Check Node Status kubectl get nodes # Check Control plane pods kubectl get pods -n kube-system # Check Service Health of Control plane service kube-apiserver status service kube-controller-manager status service kube-scheduler status # Check Service Health of Workers service kubelet status service kube-proxy status # Check the logs kubectl logs -n kube-system kube-apiserver-master # Check the service logs sudo journalctl -u kube-apiserver -l # Remember this as its native # NOTE: The certificates inside the cluster components are mapped as Volumes from the host. The path needs to be correct","title":"Cluster Troubleshooting"},{"location":"k8s/3-k8s/#worker-troubleshooting","text":"# Check Node Status kubectl get nodes # If the Nodes are not Ready, describe the node to get the error reason kubectl describe nodes node01 # Check for memory and disk space on the node top df -h # Check the service logs for kubelet sudo journalctl -u kubelet -f # -f to follow live logs # Start the service without change node ssh node01 \"service kubelet start\" # OR SSH to node01 and then run systemctl start kubelet # Check the kubelet certificates, make sure they are not expired and are of the right group and issued by the right CA openssl x509 -text -in /var/lib/kubelet/worker-1.crt # Kubelet Configuration Mismatch # Another issue from the worker node, watch the logs for kubelet # Check the kubelet.conf file at /etc/kubernetes/kubelet.conf on the worker node. # kubelet is trying to connect to the API server on the controlplane node on port 6553. This is incorrect. # Update the conf to port 6443. Restart kubelet # Kubelet Service Configuration mismatch # There appears to be a mistake path used for the CA certificate in the kubelet configuration. This can be corrected by updating the file /var/lib/kubelet/config.yaml. # Once this is fixed, restart the kubelet service.","title":"Worker Troubleshooting"},{"location":"k8s/3-k8s/#network-troubleshooting","text":"Kubernetes uses CNI plugins to setup network. The kubelet is responsible for executing plugins as we mention the following parameters in kubelet configuration. - cni-bin-dir: Kubelet probes this directory for plugins on startup - network-plugin: The network plugin to use from cni-bin-dir. It must match the name reported by a plugin probed from the plugin directory. Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order. # DNS in Kubernetes # Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. # In large scale Kubernetes clusters, CoreDNS's memory usage is predominantly affected by the number of Pods and Services in the cluster. Other factors include the size of the filled DNS answer cache, and the rate of queries received (QPS) per CoreDNS instance. Kubernetes resources for coreDNS are: 1 . a service account named coredns, 2 . cluster-roles named coredns and kube-dns 3 . clusterrolebindings named coredns and kube-dns, 4 . a deployment named coredns, 5 . a configmap named coredns and a 6 . service named kube-dns. While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap. This is the backend to k8s for cluster.local and reverse domains. proxy . /etc/resolv.conf Forward out of cluster domains directly to right authoritative DNS server. 1 . If you find CoreDNS pods in pending state first check network plugin is installed. 2 . coredns pods have CrashLoopBackOff or Error state If you have nodes that are running SELinux with an older version of Docker you might experience a scenario where the coredns pods are not starting. To solve that you can try one of the following options: a ) Upgrade to a newer version of Docker. b ) Disable SELinux. c ) Modify the coredns deployment to set allowPrivilegeEscalation to true: kubectl -n kube-system get deployment coredns -o yaml | \\ sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \\ kubectl apply -f - d ) Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop. There are many ways to work around this issue, some are listed here: - Add the following to your kubelet config yaml: resolvConf: <path-to-your-real-resolv-conf-file> This flag tells kubelet to pass an alternate resolv.conf to Pods. For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the \"real\" resolv.conf, although this can be different depending on your distribution. - Disable the local DNS cache on host nodes, and restore /etc/resolv.conf to the original. 3 . If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints. kubectl -n kube-system get ep kube-dns If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports. kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. These network rules allow network communication to the Pods from network sessions inside or outside of the cluster. In a cluster configured with kubeadm, you can find kube-proxy as a daemonset. kubeproxy is responsible for watching services and endpoint associated with each service. When the client is going to connect to the service using the virtual IP the kubeproxy is responsible for sending traffic to actual pods. If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following command inside the kube-proxy container. Command: /usr/local/bin/kube-proxy --config = /var/lib/kube-proxy/config.conf --hostname-override = $( NODE_NAME ) So it fetches the configuration from a configuration file ie, /var/lib/kube-proxy/config.conf and we can override the hostname with the node name of at which the pod is running. 1 . Check kube-proxy pod in the kube-system namespace is running. 2 . Check kube-proxy logs. 3 . Check configmap is correctly defined and the config file for running kube-proxy binary is correct. 4 . kube-config is defined in the config map. 5 . check kube-proxy is running inside the container netstat -plan | grep kube-proxy","title":"Network Troubleshooting"},{"location":"k8s/3-k8s/#jsonpath","text":"NOTE: use jq to see the data in proper format kubectl get nodes -o json | jq -c 'paths' | grep Output of this can be used in jsonpath search query jsonpath data can be filtered using jq # Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json. # Retrieve just the first 2 columns of pv output and store it in /opt/outputs/pv-and-capacity-sorted.txt. # The columns should be named NAME and CAPACITY. Use the custom-columns option and remember, it should still be sorted based on storage capacity. # Use the command kubectl get pv --sort-by = .spec.capacity.storage -o = custom-columns = NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt # Use a JSON PATH query to identify the context configured for the aws-user in the my-kube-config context file and store the result in /opt/outputs/aws-context-name. kubectl config view --kubeconfig = my-kube-config -o jsonpath = \"{.contexts[?(@.context.user=='aws-user')].name}\" > /opt/outputs/aws-context-name","title":"JSONPath"},{"location":"k8s/4-api/","text":"Example Apps to run on K8s \u00b6 Kuard Kubectl Book Kubernetes API \u00b6 The Kubernetes API is a RESTful API based on HTTP and JSON and provided by an API server. All of the components in Kubernetes communicate through the API. Basic Objects : Pods, ReplicaSets, and Services STORAGE : PERSISTENT VOLUMES, CONFIGMAPS, AND SECRETS Organizing Your Cluster with Namespaces, Labels, and Annotations Advanced Concepts : Deployments, Ingress, and StatefulSets API via Command Line \u00b6 # reveal all the API resources kubectl get --raw / # At the top of this list is v1 and under that is namespaces, so request the namespaces kubectl get --raw /api/v1/namespaces # One of the namespaces is called default, so request details on the default namespace kubectl get --raw /api/v1/namespaces/default # jq is like sed for JSON data. Using jq can make the JSON output from kubectl much easier to read with syntax highlighting. kubectl get --raw /api/v1/namespaces/default | jq . # There is also a Python json.tool kubectl get -v = 9 --raw /api/v1/namespaces/default | python -m json.tool - These are all the versions behind the API root path /apis/. In the version list, most of the lines are composed of two parts separated with a slash (/). The left token is the API Group and the right side is the version in that group. - Such as: batch/v1 and batch/v1beta Proxy \u00b6 There is a proxy command that will allow you to access the cluster via localhost. This proxy will run in the background. kubectl proxy 8001 > /dev/null & # Hit Enter to ensure you get the shell prompt back. # With this proxy you can access the Kubernetes API locally at the specified port. curl localhost:8001 curl localhost:8001/api/v1/namespaces/default | jq . # if you want to stop the proxy, use the command fg to move the proxy to the foregound and then exit the proxy - The easiest way to \u201caccess a terminal\u201d within a namespace is to launch a pod with an interactive terminal inside the desired namespace. kubectl run curl --namespace $SESSION_NAMESPACE --image = radial/busyboxplus:curl -i --tty --rm --overrides = '{\"spec\": { \"securityContext\": { \"runAsUser\": 1000 }}}' API-Resources \u00b6 # Get a list of api-resources kubectl api-resources # Most resources are associated with Namespaces, however, some cluster scope resources do not make sense to be associated with a Namespace. # List the cluster scoped resources kubectl api-resources --namespaced = false # As you can see, resources like PersistentVolumes are scoped at the cluster level and not associated with Namespaces. # Most of the api-resources are grouped. # For instance, the two job types are grouped in the batch group. kubectl api-resources --api-group = batch # Check Permissions on the User kubectl auth can-i --list Explaining Resources \u00b6 The Explain command is a great way to understand the defined structure of a resource or kind. kubectl explain ns # To get the full structure of this kind, use the --recursive flag kubectl explain ns --recursive | less # Notice the status field phase. Let's display that as an output kubectl get ns -o custom-columns = NAME:.metadata.name,PHASE:.status.phase Describe \u00b6 Don't confuse the Explain command with the Describe command. While Explain reports on the type of the resource, Describe reports the details of the instance of a resource. kubectl describe namespace kube-system Cluster Components \u00b6 # Kubernetes proxy is responsible for routing network traffic to load-balanced services in the Kubernetes cluster. To do its job, the proxy must be present on every node in the cluster. kubectl get daemonSets --namespace = kube-system kube-proxy # Kubernetes also runs a DNS server, which provides naming and discovery for the services that are defined in the cluster. Can be replaced by coredns kubectl get deployments --namespace = kube-system kube-dns [ coredns ] # Kubernetes service that performs load-balancing for the DNS server. kubectl get services --namespace = kube-system kube-dns Contexts \u00b6 # creates a new context, but it doesn\u2019t actually start using it yet. kubectl config set-context my-context --namespace = mystuff # use this newly created context kubectl config use-context my-context # Contexts can also be used to manage different clusters or different users for authenticating to those clusters using the --users or --clusters flags with the set-context command. Labeling and Annotating Objects \u00b6 kubectl label pods bar color = red,env = dev # Add Label # By default, label and annotate will not let you overwrite an existing label. To do this, you need to add the `--overwrite` flag # Remove a label, you can use the -<label-name> syntax kubectl label pods bar color- kubectl get pods --show-labels kubectl get pods -L labels # L adds a custom column called labels in tablular output kubectl get pods --selector = \"env=dev\" # If we specify two selectors separated by a comma, only the objects that satisfy both will be returned. This is a logical AND operation: kubectl get pods --selector = \"color=red,env=dev\" kubectl label deployments alpaca-test \"canary=true\" kubectl get deployments --show-labels \u00b6 Operator Description \u00b6 key=value key is set to value key!=value key is not set to value key in (value1, value2) key is one of value1 or value2 key notin (value1, value2) key is not one of value1 or value2 key key is set !key key is not set Filter output using jq kubectl get pods -n kube-system calico -o json | jq .metadata.labels Debugging Commands \u00b6 kubectl logs <pod-name> # If you have multiple containers in your pod you can choose the container to view using the -c flag. # If you instead want to continuously stream the logs back to the terminal without exiting, you can add the -f (follow) command-line flag. # Adding the --previous flag will get logs from a previous instance of the container. This is useful, for example, if your containers are continuously restarting due to a problem at container startup. # Use the exec command to execute a command in a running container kubectl exec -it <pod-name> -- bash # copy files to and from a container using the cp command kubectl cp <pod-name>:/path/to/remote/file /path/to/local/file # You can also specify directories, or reverse the syntax to copy a file from your local machine back out into the container. # A secure tunnel is created from your local machine, through the Kubernetes master, to the instance of the Pod running on one of the worker nodes. kubectl port-forward nginx 80 :8080 Replacing Objects \u00b6 Download Deployment or any object into a YAML file and then use the replace command Adding --save-config adds an annotation so that, when applying changes in the future, kubectl will know what the last applied configuration was for smarter merging of configs. kubectl get deployments nginx --export -o yaml > nginx-deployment.yaml kubectl replace -f nginx-deployment.yaml --save-config Pods \u00b6 In general, the right question to ask yourself when designing Pods is, \u201cWill these containers work correctly if they land on different machines?\u201d If the answer is \u201cno,\u201d a Pod is the correct grouping for the containers. If the answer is \u201cyes,\u201d multiple Pods is probably the correct solution. Declarative configuration means that you write down the desired state of the world in a configuration and then submit that configuration to a service that takes actions to ensure the desired state becomes the actual state. Imperative configuration , where you simply take a series of actions (e.g., apt-get install foo) to modify the world. Years of production experience have taught us that maintaining a written record of the system\u2019s desired state leads to a more manageable, reliable system. Pod manifests can be written using YAML or JSON, but YAML is generally preferred because it is slightly more human-editable and has the ability to add comments. All Pods have a termination grace period. By default, this is 30 seconds. Liveness health checks run application-specific logic (e.g., loading a web page) to verify that the application is not just still running, but is functioning properly. Readiness describes when a container is ready to serve user requests. Containers that fail readiness checks are removed from service load balancers. What probe type (liveness/readiness) should be used for each and what handler should be used (TCP, HTTP, EXEC)? Figure out which ones to use? Port Check - Liveness using TCP handler DB Query - Readiness using an EXEC handler executing a SQL query \u201crequest\u201d specifies a minimum. It does not specify a maximum cap on the resources a Pod may use. If a container is over its memory request, the OS can\u2019t just remove memory from the process, because it\u2019s been allocated. Consequently, when the system runs out of memory, the kubelet terminates containers whose memory usage is greater than their requested memory. These containers are automatically restarted, but with less available memory on the machine for the container to consume. \"limits\" specifies a maximum cap on the resources a Pod may use. When you establish limits on a container, the kernel is configured to ensure that consumption cannot exceed these limits. A container with a CPU limit of 0.5 cores will only ever get 0.5 cores, even if the CPU is otherwise idle. A container with a memory limit of 256 MB will not be allowed additional memory. Labels selectors are used to filter Kubernetes objects based on a set of labels. Annotations provide a place to store additional metadata for Kubernetes objects with the sole purpose of assisting tools and libraries. Annotations can be used for the tool itself or to pass configuration information between external systems. There is overlap with labels, and it is a matter of taste as to when to use an annotation or a label. When in doubt, add information to an object as an annotation and promote it to a label if you find yourself wanting to use it in a selector. During rolling deployments, annotations are used to track rollout status and provide the necessary information required to roll back a deployment to a previous state. Service \u00b6 Real service discovery in Kubernetes starts with a Service object. kubectl run alpaca-prod \\ --image = gcr.io/kuar-demo/kuard-amd64:1 \\ --replicas = 3 \\ --port = 8080 \\ --labels = \"ver=1,app=alpaca,env=prod\" kubectl expose deployment alpaca-prod kubectl run bandicoot-prod \\ --image = gcr.io/kuar-demo/kuard-amd64:2 \\ --replicas = 2 \\ --port = 8080 \\ --labels = \"ver=2,app=bandicoot,env=prod\" kubectl expose deployment bandicoot-prod kubectl get services -o wide After running these commands, we have three services. The ones we just created are alpaca-prod and bandicoot-prod. The kubernetes service is automatically created for you so that you can find and talk to the Kubernetes API from within the app. Endpoints are a lower-level way of finding what a service is sending traffic to. kubectl get endpoints alpaca-prod --watch At some point, we have to allow new traffic in! The most portable way to do this is to use a feature called NodePorts . You use the NodePort without knowing where any of the Pods for that service are running. kubectl expose deployment alpaca-prod --type = NodePort kubectl describe svc alpaca-prod # Assume Port 32711 is assigned # If your cluster is in the cloud someplace, you can use SSH tunneling with something like this: ssh <node> -L 8080 :localhost:32711 # Now if you open your browser to http://localhost:8080 you will be connected to that service. ReplicaSets \u00b6 A ReplicaSet acts as a cluster-wide Pod manager, ensuring that the right types and number of Pods are running at all times. When we define a ReplicaSet, we define a specification for the Pods we want to create (the \u201ccookie cutter\u201d), and a desired number of replicas. Additionally, we need to define a way of finding Pods that the ReplicaSet should control. The actual act of managing the replicated Pods is an example of a reconciliation loop . -Though ReplicaSets create and manage Pods, they do not own the Pods they create. ReplicaSets use label queries to identify the set of Pods they should be managing. Because ReplicaSets are decoupled from the Pods they manage, you can simply create a ReplicaSet that will \u201cadopt\u201d the existing Pod, and scale out additional copies of those containers. In this way you can seamlessly move from a single imperative Pod to a replicated set of Pods managed by a ReplicaSet. A Pod can be misbehaving but still be part of the replicated set. You can modify the set of labels on the sick Pod. Doing so will disassociate it from the ReplicaSet (and service) so that you can debug the Pod. The ReplicaSet controller will notice that a Pod is missing and create a new copy, but because the Pod is still running, it is available to developers for interactive debugging, which is significantly more valuable than debugging from logs. ReplicaSets are designed to represent a single, scalable microservice inside your architecture. ReplicaSets are designed for stateless (or nearly stateless) services. Finding a ReplicaSet from a Pod \u00b6 Sometimes you may wonder if a Pod is being managed by a ReplicaSet, and, if it is, which ReplicaSet. To enable this kind of discovery, the ReplicaSet controller adds an annotation to every Pod that it creates. - The key for the annotation is kubernetes.io/created-by . If you run the following, look for the kubernetes.io/created-by entry in the annotations section: kubectl get pods <pod-name> -o yaml Note that such annotations are best-effort; they are only created when the Pod is created by the ReplicaSet, and can be removed by a Kubernetes user at any time. Finding a Set of Pods for a ReplicaSet \u00b6 First, you can get the set of labels using the kubectl describe command. To find the Pods that match this selector, use the --selector flag or the shorthand -l : kubectl get pods -l app = kuard,version = 2 This is exactly the same query that the ReplicaSet executes to determine the current number of Pods. Scaling ReplicaSets \u00b6 kubectl scale replicasets kuard --replicas = 4 Horizontal Pod Autoscaling (HPA) \u00b6 HPA requires the presence of the heapster Pod on your cluster. heapster keeps track of metrics and provides an API for consuming metrics HPA uses when making scaling decisions. # Creates an autoscaler that scales between two and five replicas with a CPU threshold of 80%. kubectl autoscale rs kuard --min = 2 --max = 5 --cpu-percent = 80 kubectl get hpa Deleting ReplicaSets \u00b6 # This also deletes the Pods that are managed by the ReplicaSet kubectl delete rs kuard # If you don\u2019t want to delete the Pods that are being managed by the ReplicaSet you can set the --cascade flag to false kubectl delete rs kuard --cascade = false DaemonSets \u00b6 A DaemonSet ensures a copy of a Pod is running across a set of nodes in a Kubernetes cluster. DaemonSets are used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node. However, there are some cases where you want to deploy a Pod to only a subset of nodes. In cases like these node labels can be used to tag specific nodes that meet workload requirements. # Using a label selector we can filter nodes based on labels. kubectl get nodes --selector ssd = true Node selectors can be used to limit what nodes a Pod can run on in a given Kubernetes cluster. Node selectors are defined as part of the Pod spec when creating a DaemonSet. The inverse is also true: if a required label is removed from a node, the Pod will be removed by the DaemonSet controller. Deleting a DaemonSet will also delete all the Pods being managed by that DaemonSet. Set the --cascade flag to false to ensure only the DaemonSet is deleted and not the Pods. Jobs \u00b6 Jobs are designed to manage batch-like workloads where work items are processed by one or more Pods. By default each Job runs a single Pod once until successful termination. --restart=OnFailure is the option that tells kubectl to create a Job object. Because Jobs have a finite beginning and ending, it is common for users to create many of them. This makes picking unique labels more difficult and more critical. For this reason, the Job object will automatically pick a unique label and use it to identify the pods it creates. kubectl run -i oneshot \\ --image = gcr.io/kuar-demo/kuard-amd64:1 \\ --restart = OnFailure \\ -- --keygen-enable \\ --keygen-exit-on-complete \\ --keygen-num-to-gen 10 # The -i option to kubectl indicates that this is an interactive command. kubectl will wait until the Job is running and then show the log output from the first (and in this case only) pod in the Job. # All of the options after -- are command-line arguments to the container image. These instruct our test server (kuard) to generate 10 4,096-bit SSH keys and then exit. kubectl get pod -l job-name = oneshot -a # Without -a flag kubectl hides completed Jobs. kubectl delete jobs oneshot ConfigMaps \u00b6 Configmaps can be used as a set of variables that can be used when defining the environment or command line for your containers. The key thing is that the ConfigMap is combined with the Pod right before it is run. This means that the container image and the pod definition itself can be reused across many apps by just changing the ConfigMap that is used. There are three main ways to use a ConfigMap: Filesystem : You can mount a ConfigMap into a Pod. A file is created for each entry based on the key name. The contents of that file are set to the value. Environment variable : A ConfigMap can be used to dynamically set the value of an environment variable. Command-line argument : Kubernetes supports dynamically creating the command line for a container based on ConfigMap values. cat my-config.txt parameter1 = value1 parameter2 = value2 kubectl create configmap my-config \\ --from-file = my-config.txt \\ --from-literal = extra-param = extra-value \\ --from-literal = another-param = another-value Secrets \u00b6 There is certain data that is extra-sensitive. This can include passwords, security tokens, or other types of private keys. Collectively, we call this type of data \u201csecrets.\u201d Secret data can be exposed to pods using the secrets volume type. Secrets volumes are managed by the kubelet and are created at pod creation time. Secrets are stored on tmpfs volumes (aka RAM disks) and, as such, are not written to disk on nodes. Each data element of a secret is stored in a separate file under the target mount point specified in the volume mount. # The TLS key and certificate for the kuard application can be downloaded by running the following commands. curl -o kuard.crt https://storage.googleapis.com/kuar-demo/kuard.crt curl -o kuard.key https://storage.googleapis.com/kuar-demo/kuard.key # With the kuard.crt and kuard.key files stored locally, we are ready to create a secret. kubectl create secret generic kuard-tls \\ --from-file = kuard.crt \\ --from-file = kuard.key # Replacing secrets from file kubectl create secret generic kuard-tls \\ --from-file = kuard.crt --from-file = kuard.key \\ --dry-run -o yaml | kubectl replace -f - # This command line first creates a new secret with the same name as our existing secret. If we just stopped there, the Kubernetes API server would return an error complaining that we are trying to create a secret that already exists. Instead, we tell kubectl not to actually send the data to the server but instead to dump the YAML that it would have sent to the API server to stdout. We then pipe that to kubectl replace and use -f - to tell it to read from stdin. In this way we can update a secret from files on disk without having to manually base64-encode data. Extracting secrets into a file kubectl get secret demo-secret -o json | jq -r .data.value | base64 --decode > ./demo-secret # Output the value of secret as JSON, run JQ to parse the output # As the secret data is base64 encoded, decode it before writing the data to the client machine Deployments \u00b6 The Deployment object exists to manage the release of new versions. -In the output of describe there is a great deal of important information. Two of the most important pieces of information in the output are OldReplicaSets and NewReplicaSet . These fields point to the ReplicaSet objects this Deployment is currently managing. If a Deployment is in the middle of a rollout, both fields will be set to a value. If a rollout is complete, OldReplicaSets will be set to . You can use kubectl rollout history to obtain the history of rollouts associated with a particular Deployment. If you have a current Deployment in progress, then you can use kubectl rollout status to obtain the current status of a rollout. You can undo both partially completed and fully completed rollouts. An undo of a rollout is actually simply a rollout in reverse (e.g., from v2 to v1, instead of from v1 to v2), and all of the same policies that control the rollout strategy apply to the undo strategy as well. Specifying a revision of 0 is a shorthand way of specifying the previous revision. Or kubectl rollout undo If you ever want to manage that ReplicaSet directly, you need to delete the Deployment (remember to set --cascade to false, or else it will delete the ReplicaSet and Pods as well!). # You can see the label selector kubectl get deployments nginx \\ -o jsonpath --template { .spec.selector.matchLabels } # From this you can see that the Deployment is managing a ReplicaSet with the labels run=nginx. kubectl get replicasets --selector = run = nginx # If you are in the middle of a rollout and you want to temporarily pause it for some reason (e.g., if you start seeing weird behavior in your system and you want to investigate), you can use the pause command: kubectl rollout pause deployments nginx # If, after investigation, you believe the rollout can safely proceed, you can use the resume command to start up where you left off: kubectl rollout resume deployments nginx # You can see the deployment history by running: kubectl rollout history deployment nginx # If you are interested in more details about a particular revision, you can add the --revision flag to view details about that specific revision: kubectl rollout history deployment nginx --revision = 2 # You can roll back to a specific revision in the history using the --to-revision flag: kubectl rollout undo deployments nginx --to-revision = 3 NOTE: When you do a kubectl rollout undo you are updating the production state in a way that isn\u2019t reflected in your source control. An alternate (and perhaps preferred) way to undo a rollout is to revert your YAML file and kubectl apply the previous version. In this way your \u201cchange tracked configuration\u201d more closely tracks what is really running in your cluster. Deployment strategies \u00b6 Deployment strategies for rollingUpdate using the maxUnavailable parameter or the maxSurge parameter. The maxUnavailable parameter sets the maximum number of Pods that can be unavailable during a rolling update. It can either be set to an absolute number (e.g., 3 meaning a maximum of three Pods can be unavailable) or to a percentage (e.g., 20% meaning a maximum of 20% of the desired number of replicas can be unavailable). Generally speaking, using a percentage is a good approach for most services, since the value is correctly applicable regardless of the desired number of replicas in the Deployment. If you have four replicas and have set maxUnavailable to 50%, it will scale it down to two replicas. The rolling update will then replace the removed pods by scaling the new ReplicaSet up to two replicas, for a total of four replicas (two old, two new). It will then scale the old ReplicaSet down to zero replicas, for a total size of two new replicas. Finally, it will scale the new ReplicaSet up to four replicas, completing the rollout. Thus, with maxUnavailableset to 50%, our rollout completes in four steps, but with only 50% of our service capacity at times. However, there are situations where you don\u2019t want to fall below 100% capacity, but you are willing to temporarily use additional resources in order to perform a rollout. In these situations, you can set the maxUnavailable parameter to 0% , and instead control the rollout using the maxSurge parameter. The maxSurge parameter controls how many extra resources can be created to achieve a rollout. To illustrate how this works, imagine we have a service with 10 replicas. We set maxUnavailable to 0 and maxSurge to 20%. The first thing the rollout will do is scale the new ReplicaSet up to 2 replicas, for a total of 12 (120%) in the service. It will then scale the old ReplicaSet down to 8 replicas, for a total of 10 (8 old, 2 new) in the service. This process proceeds until the rollout is complete. At any time, the capacity of the service is guaranteed to be at least 100% and the maximum extra resources used for the rollout are limited to an additional 20% of all resources. Setting maxSurge to 100% is equivalent to a blue/green deployment . The Deployment controller first scales the new version up to 100% of the old version. Once the new version is healthy, it immediately scales the old version down to 0%. Setting minReadySeconds to 60 indicates that the Deployment must wait for 60 seconds after seeing a Pod become healthy before moving on to updating the next Pod. To set the timeout period, the Deployment parameter progressDeadlineSeconds is set to 600 . This sets the progress deadline to 10 minutes. If any particular stage in the rollout fails to progress in 10 minutes, then the Deployment is marked as failed, and all attempts to move the Deployment forward are halted. Ingress \u00b6 Service objects provide a great way to do simple TCP-level load balancing, they don\u2019t provide an application-level way to do load balancing and routing. The truth is that most of the applications that users deploy using containers and Kubernetes are HTTP web-based applications. These are better served by a load balancer that understands HTTP. To address these needs, the Ingress API was added to Kubernetes. Ingress represents a path and host-based HTTP load balancer and router. When you create an Ingress object, it receives a virtual IP address just like a Service, but instead of the one-to-one relationship between a Service IP address and a set of Pods, an Ingress can use the content of an HTTP request to route requests to different Services. StatefulSets \u00b6 Some applications, especially stateful storage workloads or sharded applications, require more differentiation between the replicas in the application. To resolve this, Kubernetes has recently introduced StatefulSets as a complement to ReplicaSets, but for more stateful workloads. Like ReplicaSets, StatefulSets create multiple instances of the same container image running in a Kubernetes cluster, but the manner in which containers are created and destroyed is more deterministic , as are the names of each container. With StatefulSets, each replica receives a monotonically increasing index (e.g., backed-0, backend-1, and so on). StatefulSets guarantee that replica zero will be created and become healthy before replica one is created and so forth. StatefulSets receive DNS names so that each replica can be accessed directly. This allows clients to easily target specific shards in a sharded service.","title":"Example Apps to run on K8s"},{"location":"k8s/4-api/#example-apps-to-run-on-k8s","text":"Kuard Kubectl Book","title":"Example Apps to run on K8s"},{"location":"k8s/4-api/#kubernetes-api","text":"The Kubernetes API is a RESTful API based on HTTP and JSON and provided by an API server. All of the components in Kubernetes communicate through the API. Basic Objects : Pods, ReplicaSets, and Services STORAGE : PERSISTENT VOLUMES, CONFIGMAPS, AND SECRETS Organizing Your Cluster with Namespaces, Labels, and Annotations Advanced Concepts : Deployments, Ingress, and StatefulSets","title":"Kubernetes API"},{"location":"k8s/4-api/#api-via-command-line","text":"# reveal all the API resources kubectl get --raw / # At the top of this list is v1 and under that is namespaces, so request the namespaces kubectl get --raw /api/v1/namespaces # One of the namespaces is called default, so request details on the default namespace kubectl get --raw /api/v1/namespaces/default # jq is like sed for JSON data. Using jq can make the JSON output from kubectl much easier to read with syntax highlighting. kubectl get --raw /api/v1/namespaces/default | jq . # There is also a Python json.tool kubectl get -v = 9 --raw /api/v1/namespaces/default | python -m json.tool - These are all the versions behind the API root path /apis/. In the version list, most of the lines are composed of two parts separated with a slash (/). The left token is the API Group and the right side is the version in that group. - Such as: batch/v1 and batch/v1beta","title":"API via Command Line"},{"location":"k8s/4-api/#proxy","text":"There is a proxy command that will allow you to access the cluster via localhost. This proxy will run in the background. kubectl proxy 8001 > /dev/null & # Hit Enter to ensure you get the shell prompt back. # With this proxy you can access the Kubernetes API locally at the specified port. curl localhost:8001 curl localhost:8001/api/v1/namespaces/default | jq . # if you want to stop the proxy, use the command fg to move the proxy to the foregound and then exit the proxy - The easiest way to \u201caccess a terminal\u201d within a namespace is to launch a pod with an interactive terminal inside the desired namespace. kubectl run curl --namespace $SESSION_NAMESPACE --image = radial/busyboxplus:curl -i --tty --rm --overrides = '{\"spec\": { \"securityContext\": { \"runAsUser\": 1000 }}}'","title":"Proxy"},{"location":"k8s/4-api/#api-resources","text":"# Get a list of api-resources kubectl api-resources # Most resources are associated with Namespaces, however, some cluster scope resources do not make sense to be associated with a Namespace. # List the cluster scoped resources kubectl api-resources --namespaced = false # As you can see, resources like PersistentVolumes are scoped at the cluster level and not associated with Namespaces. # Most of the api-resources are grouped. # For instance, the two job types are grouped in the batch group. kubectl api-resources --api-group = batch # Check Permissions on the User kubectl auth can-i --list","title":"API-Resources"},{"location":"k8s/4-api/#explaining-resources","text":"The Explain command is a great way to understand the defined structure of a resource or kind. kubectl explain ns # To get the full structure of this kind, use the --recursive flag kubectl explain ns --recursive | less # Notice the status field phase. Let's display that as an output kubectl get ns -o custom-columns = NAME:.metadata.name,PHASE:.status.phase","title":"Explaining Resources"},{"location":"k8s/4-api/#describe","text":"Don't confuse the Explain command with the Describe command. While Explain reports on the type of the resource, Describe reports the details of the instance of a resource. kubectl describe namespace kube-system","title":"Describe"},{"location":"k8s/4-api/#cluster-components","text":"# Kubernetes proxy is responsible for routing network traffic to load-balanced services in the Kubernetes cluster. To do its job, the proxy must be present on every node in the cluster. kubectl get daemonSets --namespace = kube-system kube-proxy # Kubernetes also runs a DNS server, which provides naming and discovery for the services that are defined in the cluster. Can be replaced by coredns kubectl get deployments --namespace = kube-system kube-dns [ coredns ] # Kubernetes service that performs load-balancing for the DNS server. kubectl get services --namespace = kube-system kube-dns","title":"Cluster Components"},{"location":"k8s/4-api/#contexts","text":"# creates a new context, but it doesn\u2019t actually start using it yet. kubectl config set-context my-context --namespace = mystuff # use this newly created context kubectl config use-context my-context # Contexts can also be used to manage different clusters or different users for authenticating to those clusters using the --users or --clusters flags with the set-context command.","title":"Contexts"},{"location":"k8s/4-api/#labeling-and-annotating-objects","text":"","title":"Labeling and Annotating Objects"},{"location":"k8s/4-api/#kubectl-label-pods-bar-colorredenvdev---------add-label-by-default-label-and-annotate-will-not-let-you-overwrite-an-existing-label-to-do-this-you-need-to-add-the---overwrite-flag-remove-a-label-you-can-use-the--label-name-syntaxkubectl-label-pods-bar-color-kubectl-get-pods---show-labelskubectl-get-pods--l-labels-------------l-adds-a-custom-column-called-labels-in-tablular-outputkubectl-get-pods---selectorenvdev-if-we-specify-two-selectors-separated-by-a-comma-only-the-objects-that-satisfy-both-will-be-returned-this-is-a-logical-and-operationkubectl-get-pods---selectorcolorredenvdevkubectl-label-deployments-alpaca-test-canarytruekubectl-get-deployments---show-labels","text":"","title":"kubectl label pods bar color=red,env=dev        # Add Label\n# By default, label and annotate will not let you overwrite an existing label. To do this, you need to add the `--overwrite` flag\n# Remove a label, you can use the -&lt;label-name&gt; syntax\nkubectl label pods bar color-\nkubectl get pods --show-labels\nkubectl get pods -L labels            # L adds a custom column called labels in tablular output\nkubectl get pods --selector=&quot;env=dev&quot;\n# If we specify two selectors separated by a comma, only the objects that satisfy both will be returned. This is a logical AND operation:\nkubectl get pods --selector=&quot;color=red,env=dev&quot;\nkubectl label deployments alpaca-test &quot;canary=true&quot;\nkubectl get deployments --show-labels\n"},{"location":"k8s/4-api/#operator--------------------description","text":"key=value key is set to value key!=value key is not set to value key in (value1, value2) key is one of value1 or value2 key notin (value1, value2) key is not one of value1 or value2 key key is set !key key is not set Filter output using jq kubectl get pods -n kube-system calico -o json | jq .metadata.labels","title":"Operator                    Description"},{"location":"k8s/4-api/#debugging-commands","text":"kubectl logs <pod-name> # If you have multiple containers in your pod you can choose the container to view using the -c flag. # If you instead want to continuously stream the logs back to the terminal without exiting, you can add the -f (follow) command-line flag. # Adding the --previous flag will get logs from a previous instance of the container. This is useful, for example, if your containers are continuously restarting due to a problem at container startup. # Use the exec command to execute a command in a running container kubectl exec -it <pod-name> -- bash # copy files to and from a container using the cp command kubectl cp <pod-name>:/path/to/remote/file /path/to/local/file # You can also specify directories, or reverse the syntax to copy a file from your local machine back out into the container. # A secure tunnel is created from your local machine, through the Kubernetes master, to the instance of the Pod running on one of the worker nodes. kubectl port-forward nginx 80 :8080","title":"Debugging Commands"},{"location":"k8s/4-api/#replacing-objects","text":"Download Deployment or any object into a YAML file and then use the replace command Adding --save-config adds an annotation so that, when applying changes in the future, kubectl will know what the last applied configuration was for smarter merging of configs. kubectl get deployments nginx --export -o yaml > nginx-deployment.yaml kubectl replace -f nginx-deployment.yaml --save-config","title":"Replacing Objects"},{"location":"k8s/4-api/#pods","text":"In general, the right question to ask yourself when designing Pods is, \u201cWill these containers work correctly if they land on different machines?\u201d If the answer is \u201cno,\u201d a Pod is the correct grouping for the containers. If the answer is \u201cyes,\u201d multiple Pods is probably the correct solution. Declarative configuration means that you write down the desired state of the world in a configuration and then submit that configuration to a service that takes actions to ensure the desired state becomes the actual state. Imperative configuration , where you simply take a series of actions (e.g., apt-get install foo) to modify the world. Years of production experience have taught us that maintaining a written record of the system\u2019s desired state leads to a more manageable, reliable system. Pod manifests can be written using YAML or JSON, but YAML is generally preferred because it is slightly more human-editable and has the ability to add comments. All Pods have a termination grace period. By default, this is 30 seconds. Liveness health checks run application-specific logic (e.g., loading a web page) to verify that the application is not just still running, but is functioning properly. Readiness describes when a container is ready to serve user requests. Containers that fail readiness checks are removed from service load balancers. What probe type (liveness/readiness) should be used for each and what handler should be used (TCP, HTTP, EXEC)? Figure out which ones to use? Port Check - Liveness using TCP handler DB Query - Readiness using an EXEC handler executing a SQL query \u201crequest\u201d specifies a minimum. It does not specify a maximum cap on the resources a Pod may use. If a container is over its memory request, the OS can\u2019t just remove memory from the process, because it\u2019s been allocated. Consequently, when the system runs out of memory, the kubelet terminates containers whose memory usage is greater than their requested memory. These containers are automatically restarted, but with less available memory on the machine for the container to consume. \"limits\" specifies a maximum cap on the resources a Pod may use. When you establish limits on a container, the kernel is configured to ensure that consumption cannot exceed these limits. A container with a CPU limit of 0.5 cores will only ever get 0.5 cores, even if the CPU is otherwise idle. A container with a memory limit of 256 MB will not be allowed additional memory. Labels selectors are used to filter Kubernetes objects based on a set of labels. Annotations provide a place to store additional metadata for Kubernetes objects with the sole purpose of assisting tools and libraries. Annotations can be used for the tool itself or to pass configuration information between external systems. There is overlap with labels, and it is a matter of taste as to when to use an annotation or a label. When in doubt, add information to an object as an annotation and promote it to a label if you find yourself wanting to use it in a selector. During rolling deployments, annotations are used to track rollout status and provide the necessary information required to roll back a deployment to a previous state.","title":"Pods"},{"location":"k8s/4-api/#service","text":"Real service discovery in Kubernetes starts with a Service object. kubectl run alpaca-prod \\ --image = gcr.io/kuar-demo/kuard-amd64:1 \\ --replicas = 3 \\ --port = 8080 \\ --labels = \"ver=1,app=alpaca,env=prod\" kubectl expose deployment alpaca-prod kubectl run bandicoot-prod \\ --image = gcr.io/kuar-demo/kuard-amd64:2 \\ --replicas = 2 \\ --port = 8080 \\ --labels = \"ver=2,app=bandicoot,env=prod\" kubectl expose deployment bandicoot-prod kubectl get services -o wide After running these commands, we have three services. The ones we just created are alpaca-prod and bandicoot-prod. The kubernetes service is automatically created for you so that you can find and talk to the Kubernetes API from within the app. Endpoints are a lower-level way of finding what a service is sending traffic to. kubectl get endpoints alpaca-prod --watch At some point, we have to allow new traffic in! The most portable way to do this is to use a feature called NodePorts . You use the NodePort without knowing where any of the Pods for that service are running. kubectl expose deployment alpaca-prod --type = NodePort kubectl describe svc alpaca-prod # Assume Port 32711 is assigned # If your cluster is in the cloud someplace, you can use SSH tunneling with something like this: ssh <node> -L 8080 :localhost:32711 # Now if you open your browser to http://localhost:8080 you will be connected to that service.","title":"Service"},{"location":"k8s/4-api/#replicasets","text":"A ReplicaSet acts as a cluster-wide Pod manager, ensuring that the right types and number of Pods are running at all times. When we define a ReplicaSet, we define a specification for the Pods we want to create (the \u201ccookie cutter\u201d), and a desired number of replicas. Additionally, we need to define a way of finding Pods that the ReplicaSet should control. The actual act of managing the replicated Pods is an example of a reconciliation loop . -Though ReplicaSets create and manage Pods, they do not own the Pods they create. ReplicaSets use label queries to identify the set of Pods they should be managing. Because ReplicaSets are decoupled from the Pods they manage, you can simply create a ReplicaSet that will \u201cadopt\u201d the existing Pod, and scale out additional copies of those containers. In this way you can seamlessly move from a single imperative Pod to a replicated set of Pods managed by a ReplicaSet. A Pod can be misbehaving but still be part of the replicated set. You can modify the set of labels on the sick Pod. Doing so will disassociate it from the ReplicaSet (and service) so that you can debug the Pod. The ReplicaSet controller will notice that a Pod is missing and create a new copy, but because the Pod is still running, it is available to developers for interactive debugging, which is significantly more valuable than debugging from logs. ReplicaSets are designed to represent a single, scalable microservice inside your architecture. ReplicaSets are designed for stateless (or nearly stateless) services.","title":"ReplicaSets"},{"location":"k8s/4-api/#finding-a-replicaset-from-a-pod","text":"Sometimes you may wonder if a Pod is being managed by a ReplicaSet, and, if it is, which ReplicaSet. To enable this kind of discovery, the ReplicaSet controller adds an annotation to every Pod that it creates. - The key for the annotation is kubernetes.io/created-by . If you run the following, look for the kubernetes.io/created-by entry in the annotations section: kubectl get pods <pod-name> -o yaml Note that such annotations are best-effort; they are only created when the Pod is created by the ReplicaSet, and can be removed by a Kubernetes user at any time.","title":"Finding a ReplicaSet from a Pod"},{"location":"k8s/4-api/#finding-a-set-of-pods-for-a-replicaset","text":"First, you can get the set of labels using the kubectl describe command. To find the Pods that match this selector, use the --selector flag or the shorthand -l : kubectl get pods -l app = kuard,version = 2 This is exactly the same query that the ReplicaSet executes to determine the current number of Pods.","title":"Finding a Set of Pods for a ReplicaSet"},{"location":"k8s/4-api/#scaling-replicasets","text":"kubectl scale replicasets kuard --replicas = 4","title":"Scaling ReplicaSets"},{"location":"k8s/4-api/#horizontal-pod-autoscaling-hpa","text":"HPA requires the presence of the heapster Pod on your cluster. heapster keeps track of metrics and provides an API for consuming metrics HPA uses when making scaling decisions. # Creates an autoscaler that scales between two and five replicas with a CPU threshold of 80%. kubectl autoscale rs kuard --min = 2 --max = 5 --cpu-percent = 80 kubectl get hpa","title":"Horizontal Pod Autoscaling (HPA)"},{"location":"k8s/4-api/#deleting-replicasets","text":"# This also deletes the Pods that are managed by the ReplicaSet kubectl delete rs kuard # If you don\u2019t want to delete the Pods that are being managed by the ReplicaSet you can set the --cascade flag to false kubectl delete rs kuard --cascade = false","title":"Deleting ReplicaSets"},{"location":"k8s/4-api/#daemonsets","text":"A DaemonSet ensures a copy of a Pod is running across a set of nodes in a Kubernetes cluster. DaemonSets are used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node. However, there are some cases where you want to deploy a Pod to only a subset of nodes. In cases like these node labels can be used to tag specific nodes that meet workload requirements. # Using a label selector we can filter nodes based on labels. kubectl get nodes --selector ssd = true Node selectors can be used to limit what nodes a Pod can run on in a given Kubernetes cluster. Node selectors are defined as part of the Pod spec when creating a DaemonSet. The inverse is also true: if a required label is removed from a node, the Pod will be removed by the DaemonSet controller. Deleting a DaemonSet will also delete all the Pods being managed by that DaemonSet. Set the --cascade flag to false to ensure only the DaemonSet is deleted and not the Pods.","title":"DaemonSets"},{"location":"k8s/4-api/#jobs","text":"Jobs are designed to manage batch-like workloads where work items are processed by one or more Pods. By default each Job runs a single Pod once until successful termination. --restart=OnFailure is the option that tells kubectl to create a Job object. Because Jobs have a finite beginning and ending, it is common for users to create many of them. This makes picking unique labels more difficult and more critical. For this reason, the Job object will automatically pick a unique label and use it to identify the pods it creates. kubectl run -i oneshot \\ --image = gcr.io/kuar-demo/kuard-amd64:1 \\ --restart = OnFailure \\ -- --keygen-enable \\ --keygen-exit-on-complete \\ --keygen-num-to-gen 10 # The -i option to kubectl indicates that this is an interactive command. kubectl will wait until the Job is running and then show the log output from the first (and in this case only) pod in the Job. # All of the options after -- are command-line arguments to the container image. These instruct our test server (kuard) to generate 10 4,096-bit SSH keys and then exit. kubectl get pod -l job-name = oneshot -a # Without -a flag kubectl hides completed Jobs. kubectl delete jobs oneshot","title":"Jobs"},{"location":"k8s/4-api/#configmaps","text":"Configmaps can be used as a set of variables that can be used when defining the environment or command line for your containers. The key thing is that the ConfigMap is combined with the Pod right before it is run. This means that the container image and the pod definition itself can be reused across many apps by just changing the ConfigMap that is used. There are three main ways to use a ConfigMap: Filesystem : You can mount a ConfigMap into a Pod. A file is created for each entry based on the key name. The contents of that file are set to the value. Environment variable : A ConfigMap can be used to dynamically set the value of an environment variable. Command-line argument : Kubernetes supports dynamically creating the command line for a container based on ConfigMap values. cat my-config.txt parameter1 = value1 parameter2 = value2 kubectl create configmap my-config \\ --from-file = my-config.txt \\ --from-literal = extra-param = extra-value \\ --from-literal = another-param = another-value","title":"ConfigMaps"},{"location":"k8s/4-api/#secrets","text":"There is certain data that is extra-sensitive. This can include passwords, security tokens, or other types of private keys. Collectively, we call this type of data \u201csecrets.\u201d Secret data can be exposed to pods using the secrets volume type. Secrets volumes are managed by the kubelet and are created at pod creation time. Secrets are stored on tmpfs volumes (aka RAM disks) and, as such, are not written to disk on nodes. Each data element of a secret is stored in a separate file under the target mount point specified in the volume mount. # The TLS key and certificate for the kuard application can be downloaded by running the following commands. curl -o kuard.crt https://storage.googleapis.com/kuar-demo/kuard.crt curl -o kuard.key https://storage.googleapis.com/kuar-demo/kuard.key # With the kuard.crt and kuard.key files stored locally, we are ready to create a secret. kubectl create secret generic kuard-tls \\ --from-file = kuard.crt \\ --from-file = kuard.key # Replacing secrets from file kubectl create secret generic kuard-tls \\ --from-file = kuard.crt --from-file = kuard.key \\ --dry-run -o yaml | kubectl replace -f - # This command line first creates a new secret with the same name as our existing secret. If we just stopped there, the Kubernetes API server would return an error complaining that we are trying to create a secret that already exists. Instead, we tell kubectl not to actually send the data to the server but instead to dump the YAML that it would have sent to the API server to stdout. We then pipe that to kubectl replace and use -f - to tell it to read from stdin. In this way we can update a secret from files on disk without having to manually base64-encode data. Extracting secrets into a file kubectl get secret demo-secret -o json | jq -r .data.value | base64 --decode > ./demo-secret # Output the value of secret as JSON, run JQ to parse the output # As the secret data is base64 encoded, decode it before writing the data to the client machine","title":"Secrets"},{"location":"k8s/4-api/#deployments","text":"The Deployment object exists to manage the release of new versions. -In the output of describe there is a great deal of important information. Two of the most important pieces of information in the output are OldReplicaSets and NewReplicaSet . These fields point to the ReplicaSet objects this Deployment is currently managing. If a Deployment is in the middle of a rollout, both fields will be set to a value. If a rollout is complete, OldReplicaSets will be set to . You can use kubectl rollout history to obtain the history of rollouts associated with a particular Deployment. If you have a current Deployment in progress, then you can use kubectl rollout status to obtain the current status of a rollout. You can undo both partially completed and fully completed rollouts. An undo of a rollout is actually simply a rollout in reverse (e.g., from v2 to v1, instead of from v1 to v2), and all of the same policies that control the rollout strategy apply to the undo strategy as well. Specifying a revision of 0 is a shorthand way of specifying the previous revision. Or kubectl rollout undo If you ever want to manage that ReplicaSet directly, you need to delete the Deployment (remember to set --cascade to false, or else it will delete the ReplicaSet and Pods as well!). # You can see the label selector kubectl get deployments nginx \\ -o jsonpath --template { .spec.selector.matchLabels } # From this you can see that the Deployment is managing a ReplicaSet with the labels run=nginx. kubectl get replicasets --selector = run = nginx # If you are in the middle of a rollout and you want to temporarily pause it for some reason (e.g., if you start seeing weird behavior in your system and you want to investigate), you can use the pause command: kubectl rollout pause deployments nginx # If, after investigation, you believe the rollout can safely proceed, you can use the resume command to start up where you left off: kubectl rollout resume deployments nginx # You can see the deployment history by running: kubectl rollout history deployment nginx # If you are interested in more details about a particular revision, you can add the --revision flag to view details about that specific revision: kubectl rollout history deployment nginx --revision = 2 # You can roll back to a specific revision in the history using the --to-revision flag: kubectl rollout undo deployments nginx --to-revision = 3 NOTE: When you do a kubectl rollout undo you are updating the production state in a way that isn\u2019t reflected in your source control. An alternate (and perhaps preferred) way to undo a rollout is to revert your YAML file and kubectl apply the previous version. In this way your \u201cchange tracked configuration\u201d more closely tracks what is really running in your cluster.","title":"Deployments"},{"location":"k8s/4-api/#deployment-strategies","text":"Deployment strategies for rollingUpdate using the maxUnavailable parameter or the maxSurge parameter. The maxUnavailable parameter sets the maximum number of Pods that can be unavailable during a rolling update. It can either be set to an absolute number (e.g., 3 meaning a maximum of three Pods can be unavailable) or to a percentage (e.g., 20% meaning a maximum of 20% of the desired number of replicas can be unavailable). Generally speaking, using a percentage is a good approach for most services, since the value is correctly applicable regardless of the desired number of replicas in the Deployment. If you have four replicas and have set maxUnavailable to 50%, it will scale it down to two replicas. The rolling update will then replace the removed pods by scaling the new ReplicaSet up to two replicas, for a total of four replicas (two old, two new). It will then scale the old ReplicaSet down to zero replicas, for a total size of two new replicas. Finally, it will scale the new ReplicaSet up to four replicas, completing the rollout. Thus, with maxUnavailableset to 50%, our rollout completes in four steps, but with only 50% of our service capacity at times. However, there are situations where you don\u2019t want to fall below 100% capacity, but you are willing to temporarily use additional resources in order to perform a rollout. In these situations, you can set the maxUnavailable parameter to 0% , and instead control the rollout using the maxSurge parameter. The maxSurge parameter controls how many extra resources can be created to achieve a rollout. To illustrate how this works, imagine we have a service with 10 replicas. We set maxUnavailable to 0 and maxSurge to 20%. The first thing the rollout will do is scale the new ReplicaSet up to 2 replicas, for a total of 12 (120%) in the service. It will then scale the old ReplicaSet down to 8 replicas, for a total of 10 (8 old, 2 new) in the service. This process proceeds until the rollout is complete. At any time, the capacity of the service is guaranteed to be at least 100% and the maximum extra resources used for the rollout are limited to an additional 20% of all resources. Setting maxSurge to 100% is equivalent to a blue/green deployment . The Deployment controller first scales the new version up to 100% of the old version. Once the new version is healthy, it immediately scales the old version down to 0%. Setting minReadySeconds to 60 indicates that the Deployment must wait for 60 seconds after seeing a Pod become healthy before moving on to updating the next Pod. To set the timeout period, the Deployment parameter progressDeadlineSeconds is set to 600 . This sets the progress deadline to 10 minutes. If any particular stage in the rollout fails to progress in 10 minutes, then the Deployment is marked as failed, and all attempts to move the Deployment forward are halted.","title":"Deployment strategies"},{"location":"k8s/4-api/#ingress","text":"Service objects provide a great way to do simple TCP-level load balancing, they don\u2019t provide an application-level way to do load balancing and routing. The truth is that most of the applications that users deploy using containers and Kubernetes are HTTP web-based applications. These are better served by a load balancer that understands HTTP. To address these needs, the Ingress API was added to Kubernetes. Ingress represents a path and host-based HTTP load balancer and router. When you create an Ingress object, it receives a virtual IP address just like a Service, but instead of the one-to-one relationship between a Service IP address and a set of Pods, an Ingress can use the content of an HTTP request to route requests to different Services.","title":"Ingress"},{"location":"k8s/4-api/#statefulsets","text":"Some applications, especially stateful storage workloads or sharded applications, require more differentiation between the replicas in the application. To resolve this, Kubernetes has recently introduced StatefulSets as a complement to ReplicaSets, but for more stateful workloads. Like ReplicaSets, StatefulSets create multiple instances of the same container image running in a Kubernetes cluster, but the manner in which containers are created and destroyed is more deterministic , as are the names of each container. With StatefulSets, each replica receives a monotonically increasing index (e.g., backed-0, backend-1, and so on). StatefulSets guarantee that replica zero will be created and become healthy before replica one is created and so forth. StatefulSets receive DNS names so that each replica can be accessed directly. This allows clients to easily target specific shards in a sharded service.","title":"StatefulSets"},{"location":"k8s/chaos/","text":"Chaos Enginnering \u00b6 Generating Random number d = $(( ( RANDOM % 10 ) + 1 )) Example Cluster with examples Pure Chaos \u00b6 Install Registry # It's helpful to have a container registry during the build, push, and deploy phases. There is no need to shuttle private images over the internet. helm repo add twuni https://helm.twun.io helm install registry twuni/docker-registry \\ --version 1 .10.0 \\ --namespace kube-system \\ --set service.type = NodePort \\ --set service.nodePort = 31500 kubectl get service --namespace kube-system # Assign an environment variable to the common registry location export REGISTRY = 2886795330 -31500-kira01.environments.katacoda.com # It will be a few moments before the registry deployment reports it's Available kubectl get deployments registry-docker-registry --namespace kube-system # Once the registry is serving, inspect the contents of the empty registry curl $REGISTRY /v2/_catalog | jq -c Install Sample Application # Let's create a small collection of applications. # you will create a deployment of applications that log random messages. kubectl create namespace learning-place # Run the random-logger container in a Pod to start generating continuously random logging events kubectl create deployment random-logger --image = chentex/random-logger -n learning-place kubectl scale deployment/random-logger --replicas = 10 -n learning-place kubectl get pods -n learning-place Random Logger Snowflake Melter # The most common chaos for Kubernetes is to periodically and randomly terminate Pods. # To define the terminator, all we need is a container that has some logic in it to find the application Pods you just started and terminate them. The Kubernetes API offers all the control we need to find and remove Pods. # We'll choose Python as we can import a helpful Kubernetes API and the script can be loaded into a Python container. # cat snowflake_melter.py from kubernetes import client , config import random # Access Kubernetes config . load_incluster_config () v1 = client . CoreV1Api () # List Namespaces all_namespaces = v1 . list_namespace () # Get Pods from namespaces annotated with chaos marker pod_candidates = [] for namespace in all_namespaces . items : if ( namespace . metadata . annotations is not None and namespace . metadata . annotations . get ( \"chaos\" , None ) == 'yes' ): pods = v1 . list_namespaced_pod ( namespace . metadata . name ) pod_candidates . extend ( pods . items ) # Determine how many Pods to remove removal_count = random . randint ( 0 , len ( pod_candidates )) if len ( pod_candidates ) > 0 : print ( \"Found\" , len ( pod_candidates ), \"pods and melting\" , removal_count , \"of them.\" ) else : print ( \"No eligible Pods found with annotation chaos=yes.\" ) # Remove a few Pods for _ in range ( removal_count ): pod = random . choice ( pod_candidates ) pod_candidates . remove ( pod ) print ( \"Removing pod\" , pod . metadata . name , \"from namespace\" , pod . metadata . namespace , \".\" ) body = client . V1DeleteOptions () v1 . delete_namespaced_pod ( pod . metadata . name , pod . metadata . namespace , body = body ) Dockerfile # cat Dockerfile # ARGS at this level referenced only by FROMs ARG BASE_IMAGE = python:3.8.5-alpine3.12 # -------------------------------------- # Build dependencies in build stage # -------------------------------------- FROM ${ BASE_IMAGE } as builder WORKDIR /app # Cache installed dependencies between builds COPY ./requirements.txt ./requirements.txt RUN pip install -r ./requirements.txt --user # -------------------------------------- # Create final container loaded with app # -------------------------------------- FROM ${ BASE_IMAGE } LABEL scenario = pure-chaos ENV USER = docker GROUP = docker \\ UID = 12345 GID = 23456 \\ HOME = /app PYTHONUNBUFFERED = 1 # Create user/group RUN addgroup --gid \" ${ GID } \" \" ${ GROUP } \" \\ && adduser \\ --disabled-password \\ --gecos \"\" \\ --home \" $( pwd ) \" \\ --ingroup \" ${ GROUP } \" \\ --no-create-home \\ --uid \" ${ UID } \" \\ \" ${ USER } \" WORKDIR ${ HOME } # TODO, will switching user work? # USER ${USER} COPY --from = builder /root/.local /usr/local COPY --chown = ${ USER } : ${ GROUP } . . CMD [ \"python\" , \"snowflake_melter.py\" ] # cat requirements.txt kubernetes == 11 .0.0 Build and Push Image export IMAGE = $REGISTRY /snowflake_melter:0.1.0 docker build -t $IMAGE . docker push $IMAGE curl $REGISTRY /v2/_catalog | jq Invoke Chaos # Run your newly created application as a Kubernetes CronJob kubectl create cronjob snowflake-melter --image = $IMAGE --schedule = '*/1 * * * *' # The chaos CronJob is will now be running once a minute. More flexible chaos systems would randomize this period. kubectl get cronjobs # At the beginning of the next minute on the clock, the CronJob will create a new Pod. kubectl get pods # Every minute a new Pod will create and run the chaos logic. Kubernetes automatically purges the older Job Pods. Getting the logs from all the Jobs is a bit tricky, but there is a common client tool called Stern that collates and displays logs from related Pods. stern snowflake-melter --container-state terminated --since 2m --timestamps # You will discover in the logs that the code is reporting that it's not finding Pods that are eligible for deleting. Target the Chaos # The current logic for the chaotic Pod deletion requires a namespace to be annotated with chaos=yes. Assign the random-logger Pods as chaos targets by annotating the learning-place namespace. kubectl annotate namespace learning-place chaos = yes kubectl describe namespace learning-place # The next time chaos Job runs it will see this annotation and the interesting work will be reported. watch kubectl get pods -n learning-place For real applications, if scaled correctly, all this chaos and resilience will be happening behind the scenes in the cluster while your users experience no downtime or delays. You could modify the Python code a bit more and go crazy with other Kubernetes API calls to create clever forms of havoc. Chaos Mesh \u00b6 Chaos Mesh is a cloud native Chaos Engineering platform that orchestrates chaos on Kubernetes environments. At the current stage, it has the following components: - Chaos Operator : the core component for chaos orchestration; fully open source. - Chaos Dashboard : a Web UI for managing, designing, and monitoring Chaos Experiments; under development. Choas Mesh is one of the better chaos engines for Kubernetes because: 1. In a short amount of time there has been heavy community support and it's a CNCF sandbox project. 2. It's a native experience to Kubernetes leveraging the Operator Pattern and CRDs permitting IaC with your pipelines. 3. If you have followed the best practices by applying plenty of labels and annotations to your Deployments, then there is no need to make modifications to your apps for your chaos experiments. 4. There are a wide variety of experiment types, not just Pod killing. 5. Installs with a Helm chart and you have complete control over the engine with CRDs. - Install Chaos Mesh kubectl create namespace chaos-mesh # Add the chart repository for the Helm chart to be installed helm search repo chaos-mesh -l helm repo add chaos-mesh https://charts.chaos-mesh.org helm install chaos-mesh chaos-mesh/chaos-mesh \\ --version v2.0.0 \\ --namespace chaos-mesh \\ --set chaosDaemon.runtime = containerd \\ --set chaosDaemon.socketPath = /run/containerd/containerd.sock # Verify kubectl get deployments,pods,services --namespace chaos-mesh The control plane components for the Chaos Mesh are: 1. chaos=controller-manager : This is used to schedule and manage the lifecycle of chaos experiments. (This is a misnomer. This should be just named controller, not controller-manager, as it's the controller based on the Operator Pattern. The controller-manager is the Kubernetes control plane component that manages all the controllers like this one). 2. chaos-daemon : These are the Pods that control the chaos mesh. The Pods run on every cluster Node and are wrapped in a DaemonSet. These DaemonSets have privileged system permissions to access each Node's network, cgroups, chroot, and other resources that are accessed based on your experiments. 3. chaos-dashboard : An optional web interface providing you an alternate means to administer the engine and experiments. Its use is for convenience and any production use of the engine should be through the YAML resources for the Chaos Mesh CRDs. - Chaos Mesh Dashboard The chaos dashboard is accessible via a NodePort. For this scenario we need the nodePort at a specific value, rather than its current random port number. Set the nodePort to a specific port: kubectl patch service chaos-dashboard -n chaos-mesh --type = 'json' --patch = '[{\"op\": \"replace\", \"path\": \"/spec/ports/0/nodePort\", \"value\":31111}]' # With the correct port value set, the web interface for Chaos Mesh dashboard can be seen from the tab Chaos Mesh above the command-line area or this link: https://2886795275-31111-kira01.environments.katacoda.com/. - There are no experiments yet, but take a few moments to explore the general layout of the dashboard. There is a way through the user interface to create, update, and delete experiments. - Chaos Mesh Experiment Types ============================================================ Category Type Experiment Description Pod Lifecycle Pod Failure Killing pods. Pod Lifecycle Pod Kill Pods becoming unavailable. Pod Lifecycle Container Kill Killing containers in pods. Network Partition Separate Pods into independent subnets by blocking communication between them. Network Loss Inject network communication loss. Network Delay Inject network communication latency. Network Duplication Inject packet duplications. Network Corrupt Inject network communication corruption. Network Bandwidth Limit the network bandwidth. I/O Delay Inject delay during I/O. I/O Errno Inject error during I/O. I/O Delay and Errno Inject both delays and errors with I/O. Linux Kernel Inject kernel errors into pods. Clock Offset Inject clock skew into pods. Stress CPU Simulate pod CPU stress. Stress Memory Simulate pod memory stress. Stress CPU & Memory Simulate both CPU and memory stress on Pods. # cat web-show-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : web-show labels : app : web-show spec : replicas : 1 selector : matchLabels : app : web-show template : metadata : labels : app : web-show spec : containers : - name : web-show image : pingcap/web-show imagePullPolicy : Always command : - /usr/local/bin/web-show - --target-ip=$(TARGET_IP) env : - name : TARGET_IP valueFrom : configMapKeyRef : name : web-show-context key : target.ip ports : - name : web-port containerPort : 8081 hostPort : 8081 # cat web-show-service.yaml apiVersion : v1 kind : Service metadata : name : web-show labels : app : web-show spec : selector : app : web-show type : NodePort ports : - port : 8081 protocol : TCP targetPort : 8081 nodePort : 30081 # cat nginx.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx annotations : deployment.kubernetes.io/revision : \"1\" labels : app : nginx spec : replicas : 8 selector : matchLabels : app : nginx template : metadata : labels : app : nginx chaos : blast-here spec : containers : - image : nginx name : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : shine-on-you-crazy-diamond spec : replicas : 2 selector : matchLabels : app : cant-touch-dis template : metadata : labels : app : cant-touch-dis spec : containers : - image : nginx name : nginx # cat network-delay-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : NetworkChaos metadata : name : web-show-network-delay spec : action : delay mode : one selector : namespaces : - default labelSelectors : app : web-show delay : latency : 10ms # cat scheduled-network-delay-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : Schedule metadata : name : web-show-scheduled-network-delay spec : schedule : '@every 60s' type : NetworkChaos historyLimit : 5 concurrencyPolicy : Forbid networkChaos : action : delay mode : one selector : namespaces : - default labelSelectors : app : web-show delay : latency : 10ms duration : 30s # cat pod-removal-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : Schedule metadata : name : pod-kill-example namespace : chaos-mesh spec : schedule : '@every 15s' type : PodChaos historyLimit : 5 concurrencyPolicy : Forbid podChaos : action : pod-kill mode : one selector : namespaces : - chaos-sandbox labelSelectors : chaos : blast-here - Network Delay Experiment # Install an example application as a target for the experiment. This application is designed by the Chaos Mesh project as a hello world example for your first experiment. # The application needs an environment variable for the TARGET_IP, which is the cluster IP, so this context you provide as a ConfigMap. That ConfigMap variable is referenced in the Deployment YAML. TARGET_IP = $( kubectl get pod -n kube-system -o wide | grep kube-controller | head -n 1 | awk '{print $6}' ) kubectl create configmap web-show-context --from-literal = target.ip = ${ TARGET_IP } kubectl apply -f web-show-deployment.yaml kubectl apply -f web-show-service.yaml # With the web-show application running, its web interface can be accessed from the \"Web Show\" above the command-line area or this link: https://2886795275-30081-kira01.environments.katacoda.com/ # Define Experiment kubectl get crds # The Chaos Mesh has installed several custom resources # You can reference these resources to create declarative YAML manifests that define your experiment. # For your first experiment, you will impose a network delay. The delay is defined in the NetworkChaos manifest # The experiment declares that a 10ms network delay should be injected. The delay will only be applied to the target service labeled \"app\": \"web-show\". # This is the **blast radius**. kubectl get deployments,pods -l app = 'web-show' # Apply Experiment kubectl apply -f network-delay-experiment.yaml # The experiment is now running. kubectl get NetworkChaos # The application has a built-in graph that will show the latency it's experiencing. With the experiment applied you will see the 10ms delay. # Update Experiment # At any time you can change the YAML declaration and apply further experiment updates. kubectl apply -f network-delay-experiment.yaml # The experiment can be paused kubectl annotate networkchaos web-show-network-delay experiment.chaos-mesh.org/pause = true # and resumed kubectl annotate networkchaos web-show-network-delay experiment.chaos-mesh.org/pause- # Since the NetworkChaos is like any other Kubernetes resource, the experiment can be easily removed. kubectl delete -f network-delay-experiment.yaml - Scheduled Experiment # This experiment will inject network chaos periodically: 10ms network delay should be injected every minute that lasts for 30 seconds # Apply Scheduled Experiment kubectl apply -f scheduled-network-delay-experiment.yaml # The schedule experiment is now running. Scheduled experiment will not create NetworkChaos object immediately, intead it creates an Schedule object called web-show-scheduled-network-delay kubectl get Schedule # NetworkChaos is very similar with what between CronJob and Job: Schedule will spawn NetworkChaos when trigger by @every 60s. kubectl get NetworkChaos -w # The experiment can be paused kubectl annotate schedule web-show-scheduled-network-delay experiment.chaos-mesh.org/pause = true # and resumed: kubectl annotate schedule web-show-scheduled-network-delay experiment.chaos-mesh.org/pause- - Pod Removal Experiment # Install an example application as a target for the experiment. It's just a deployment of the common Nginx web server with Pod replications. Apply the Deployment to the chaos-sandbox namespace. kubectl create namespace chaos-sandbox kubectl apply -f nginx.yaml -n chaos-sandbox # The experiment declares that the specific pod should be killed every 15s. The removal will only be applied to the target pod labeled \"chaos\": \"blast here\", which is the blast radius. # Apply Experiment kubectl apply -f pod-removal-experiment.yaml kubectl get Schedule -n chaos-mesh # Based on the cron time in the experiment, watch the Pods randomly terminate and new ones start. watch kubectl get -n chaos-sandbox deployments,pods,services # Notice the blast radius is targeting only the nginx Pods, while the shine-on-you-crazy-diamond Pods remain undisturbed. Litmus \u00b6 Litmus is a toolset to do cloud native chaos engineering. Litmus provides tools to orchestrate chaos on Kubernetes to help SREs find weaknesses in their deployments. SREs use Litmus to run chaos experiments initially in the staging environment and eventually in production to find bugs and vulnerabilities. Fixing the weaknesses leads to increased resilience of the system. - Litmus offers you these compelling features: 1. Kubernetes native CRDs to manage chaos. Using chaos API, orchestration, scheduling, and complex workflow management can be orchestrated declaratively. 2. Most of the generic chaos experiments are readily available for you to get started with your initial chaos engineering needs. 3. An SDK is available in GO, Python, and Ansible. A basic experiment structure is created quickly using SDK and developers and SREs just need to add the chaos logic to make a new experiment. 4. It's simple to complex chaos workflows are easy to construct. Use GitOps and the chaos workflows to scale your chaos engineering efforts and increase the resilience of your Kubernetes platform. # For this scenario, we'll install the standard NGINX application and make it a target. Install NGINX into the default namespace. kubectl create deploy nginx --image = nginx kubectl get deployments,pods --show-labels - Install Litmus Operator # The recommended way to start Litmus is by installing the Litmus Operator. kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v1.8.0.yaml kubectl get namespaces # In the list, you see litmus as a new namespace. # An operator is a custom Kubernetes controller that uses custom resources (CR) to manage applications and their components. The Litmus Operator is comprised of a few controllers maintaining the CRs. kubectl get crds | grep litmus # Check the Litmus API resources are available kubectl api-resources | grep litmus kubectl get all -n litmus - The key components and object associated with Litmus are: 1. RBAC for chaotic administration access targeted objects on your cluster. 2. The Litmus controller that manages the custom resources and the following apps: - ChaosEngine : A resource to link a Kubernetes application or Kubernetes node to a ChaosExperiment. ChaosEngine is watched by Litmus' Chaos-Operator which then invokes Chaos-Experiments. - ChaosExperiment : A resource to group the configuration parameters of a chaos experiment. ChaosExperiment CRs are created by the operator when experiments are invoked by ChaosEngine. - ChaosResult : A resource to hold the results of a chaos-experiment. The Chaos-exporter reads the results and exports the metrics into a configured Prometheus server. - Install Chaos Experiments - These experiments are installed on your cluster as Litmus resources declarations in the form of the Kubernetes CRDs. Because the chaos experiments are just Kubernetes YAML manifests, these experiments are published on Chaos Hub . - generic/pod-delete kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/experiment.yaml # Verify the pod-delete experiment has been installed kubectl get chaosexperiments # Setup RBAC with Service Account # A service account should be created to allow ChaosEngine to run experiments in your application namespace. kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/rbac.yaml # Verify the ServiceAccount RBAC rules have been applied for pod-delete-sa kubectl get serviceaccount,role,rolebinding # Annotate Application # In this case, we'll annotate the NGINX deployment with litmuschaos.io/chaos=\"true\" kubectl annotate deploy/nginx litmuschaos.io/chaos = \"true\" # Verify the annotation has been applied kubectl get deployment nginx -o = custom-columns = 'ANNOTATIONS:metadata.annotations' # Run the Experiment kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/engine.yaml # Start watching the Pods in the default namespace watch -n 1 kubectl get pods # In a moment an nginx-chaos-runner Pod will start. This Pod is created by the Litmus engine based on the experiment criteria. # In a moment, the chaos-runner will create a new Pod called pod-delete-<hash>. This Pod is responsible for the actual Pod deletion. # Shortly after the pod-delete-<hash> Pod starts, you'll notice the NGINX Pod is killed. # Observe and Verify Experiments kubectl describe chaosresult nginx-chaos-pod-delete # The status.verdict is set to Awaited when the experiment is in progress, eventually changing to either Pass or Fail. Chaoskube \u00b6 Chaoskube periodically kills random Pods in your Kubernetes cluster, which allows you to test how your system behaves under arbitrary Pod failures. Helm Input kubectl version --short && \\ kubectl get componentstatus && \\ kubectl get nodes && \\ kubectl cluster-info helm version --short kubectl create namespace chaoskube helm repo add chaoskube https://linki.github.io/chaoskube # Install the chart # The interval parameter instructs Chaoskube to kill Pods every 20 seconds. # The targeted Pods are any with the label app-purpose=chaos, and the kube-system namespace has to be explicitly excluded (!) from the list of namespaces to look for Pods to kill. helm install chaoskube chaoskube/chaoskube \\ --version = 0 .1.0 \\ --namespace chaoskube \\ --set image.tag = v0.21.0 \\ --set dryRun = false \\ --set 'namespaces=!kube-system' \\ --set labels = app-purpose = chaos \\ --set interval = 20s kubectl get -n chaoskube deployments kubectl rollout -n chaoskube status deployment chaoskube # You can periodically check the Chaoskube log to see its Pod killing activity. POD = $( kubectl -n chaoskube get pods -l = 'app.kubernetes.io/instance=chaoskube' --output = jsonpath = '{.items[0].metadata.name}' ) kubectl -n chaoskube logs -f $POD # nginx.yaml apiVersion : apps/v1 kind : Deployment metadata : annotations : deployment.kubernetes.io/revision : \"1\" labels : app : nginx app-purpose : chaos name : nginx spec : replicas : 8 selector : matchLabels : app : nginx template : metadata : labels : app : nginx app-purpose : chaos spec : containers : - image : nginx name : nginx # cat ghost.yaml apiVersion : apps/v1 kind : Deployment metadata : annotations : deployment.kubernetes.io/revision : \"1\" labels : app : ghost app-purpose : chaos name : ghost spec : replicas : 3 selector : matchLabels : app : ghost template : metadata : labels : app : ghost app-purpose : chaos spec : containers : - image : ghost:3.11.0-alpine name : ghost The Deployments and Pods are labeled to mark these Pods as potential victim targets of the Chaoskube Pod killer. The Deployment and Pod template have the label app-purpose: chaos that makes the Pod an eligible target for Chaoskube. The label is provided as a configuration value during the Helm chart installation. kubectl apply -f nginx.yaml kubectl create namespace more-apps kubectl create --namespace more-apps kubectl apply -f ghost.yaml Observe the Chaos Notice as Pods are deleted every 20 secs, the Kubernetes resilience feature is making sure they are restored. watch kubectl get deployments,pods --all-namespaces -l app-purpose = chaos In a real chaos testing platform, you should complement this Pod killing activity with automated tests to ensure these disruptions are either unnoticed or acceptable for your business processes.","title":"Chaos Enginnering"},{"location":"k8s/chaos/#chaos-enginnering","text":"Generating Random number d = $(( ( RANDOM % 10 ) + 1 )) Example Cluster with examples","title":"Chaos Enginnering"},{"location":"k8s/chaos/#pure-chaos","text":"Install Registry # It's helpful to have a container registry during the build, push, and deploy phases. There is no need to shuttle private images over the internet. helm repo add twuni https://helm.twun.io helm install registry twuni/docker-registry \\ --version 1 .10.0 \\ --namespace kube-system \\ --set service.type = NodePort \\ --set service.nodePort = 31500 kubectl get service --namespace kube-system # Assign an environment variable to the common registry location export REGISTRY = 2886795330 -31500-kira01.environments.katacoda.com # It will be a few moments before the registry deployment reports it's Available kubectl get deployments registry-docker-registry --namespace kube-system # Once the registry is serving, inspect the contents of the empty registry curl $REGISTRY /v2/_catalog | jq -c Install Sample Application # Let's create a small collection of applications. # you will create a deployment of applications that log random messages. kubectl create namespace learning-place # Run the random-logger container in a Pod to start generating continuously random logging events kubectl create deployment random-logger --image = chentex/random-logger -n learning-place kubectl scale deployment/random-logger --replicas = 10 -n learning-place kubectl get pods -n learning-place Random Logger Snowflake Melter # The most common chaos for Kubernetes is to periodically and randomly terminate Pods. # To define the terminator, all we need is a container that has some logic in it to find the application Pods you just started and terminate them. The Kubernetes API offers all the control we need to find and remove Pods. # We'll choose Python as we can import a helpful Kubernetes API and the script can be loaded into a Python container. # cat snowflake_melter.py from kubernetes import client , config import random # Access Kubernetes config . load_incluster_config () v1 = client . CoreV1Api () # List Namespaces all_namespaces = v1 . list_namespace () # Get Pods from namespaces annotated with chaos marker pod_candidates = [] for namespace in all_namespaces . items : if ( namespace . metadata . annotations is not None and namespace . metadata . annotations . get ( \"chaos\" , None ) == 'yes' ): pods = v1 . list_namespaced_pod ( namespace . metadata . name ) pod_candidates . extend ( pods . items ) # Determine how many Pods to remove removal_count = random . randint ( 0 , len ( pod_candidates )) if len ( pod_candidates ) > 0 : print ( \"Found\" , len ( pod_candidates ), \"pods and melting\" , removal_count , \"of them.\" ) else : print ( \"No eligible Pods found with annotation chaos=yes.\" ) # Remove a few Pods for _ in range ( removal_count ): pod = random . choice ( pod_candidates ) pod_candidates . remove ( pod ) print ( \"Removing pod\" , pod . metadata . name , \"from namespace\" , pod . metadata . namespace , \".\" ) body = client . V1DeleteOptions () v1 . delete_namespaced_pod ( pod . metadata . name , pod . metadata . namespace , body = body ) Dockerfile # cat Dockerfile # ARGS at this level referenced only by FROMs ARG BASE_IMAGE = python:3.8.5-alpine3.12 # -------------------------------------- # Build dependencies in build stage # -------------------------------------- FROM ${ BASE_IMAGE } as builder WORKDIR /app # Cache installed dependencies between builds COPY ./requirements.txt ./requirements.txt RUN pip install -r ./requirements.txt --user # -------------------------------------- # Create final container loaded with app # -------------------------------------- FROM ${ BASE_IMAGE } LABEL scenario = pure-chaos ENV USER = docker GROUP = docker \\ UID = 12345 GID = 23456 \\ HOME = /app PYTHONUNBUFFERED = 1 # Create user/group RUN addgroup --gid \" ${ GID } \" \" ${ GROUP } \" \\ && adduser \\ --disabled-password \\ --gecos \"\" \\ --home \" $( pwd ) \" \\ --ingroup \" ${ GROUP } \" \\ --no-create-home \\ --uid \" ${ UID } \" \\ \" ${ USER } \" WORKDIR ${ HOME } # TODO, will switching user work? # USER ${USER} COPY --from = builder /root/.local /usr/local COPY --chown = ${ USER } : ${ GROUP } . . CMD [ \"python\" , \"snowflake_melter.py\" ] # cat requirements.txt kubernetes == 11 .0.0 Build and Push Image export IMAGE = $REGISTRY /snowflake_melter:0.1.0 docker build -t $IMAGE . docker push $IMAGE curl $REGISTRY /v2/_catalog | jq Invoke Chaos # Run your newly created application as a Kubernetes CronJob kubectl create cronjob snowflake-melter --image = $IMAGE --schedule = '*/1 * * * *' # The chaos CronJob is will now be running once a minute. More flexible chaos systems would randomize this period. kubectl get cronjobs # At the beginning of the next minute on the clock, the CronJob will create a new Pod. kubectl get pods # Every minute a new Pod will create and run the chaos logic. Kubernetes automatically purges the older Job Pods. Getting the logs from all the Jobs is a bit tricky, but there is a common client tool called Stern that collates and displays logs from related Pods. stern snowflake-melter --container-state terminated --since 2m --timestamps # You will discover in the logs that the code is reporting that it's not finding Pods that are eligible for deleting. Target the Chaos # The current logic for the chaotic Pod deletion requires a namespace to be annotated with chaos=yes. Assign the random-logger Pods as chaos targets by annotating the learning-place namespace. kubectl annotate namespace learning-place chaos = yes kubectl describe namespace learning-place # The next time chaos Job runs it will see this annotation and the interesting work will be reported. watch kubectl get pods -n learning-place For real applications, if scaled correctly, all this chaos and resilience will be happening behind the scenes in the cluster while your users experience no downtime or delays. You could modify the Python code a bit more and go crazy with other Kubernetes API calls to create clever forms of havoc.","title":"Pure Chaos"},{"location":"k8s/chaos/#chaos-mesh","text":"Chaos Mesh is a cloud native Chaos Engineering platform that orchestrates chaos on Kubernetes environments. At the current stage, it has the following components: - Chaos Operator : the core component for chaos orchestration; fully open source. - Chaos Dashboard : a Web UI for managing, designing, and monitoring Chaos Experiments; under development. Choas Mesh is one of the better chaos engines for Kubernetes because: 1. In a short amount of time there has been heavy community support and it's a CNCF sandbox project. 2. It's a native experience to Kubernetes leveraging the Operator Pattern and CRDs permitting IaC with your pipelines. 3. If you have followed the best practices by applying plenty of labels and annotations to your Deployments, then there is no need to make modifications to your apps for your chaos experiments. 4. There are a wide variety of experiment types, not just Pod killing. 5. Installs with a Helm chart and you have complete control over the engine with CRDs. - Install Chaos Mesh kubectl create namespace chaos-mesh # Add the chart repository for the Helm chart to be installed helm search repo chaos-mesh -l helm repo add chaos-mesh https://charts.chaos-mesh.org helm install chaos-mesh chaos-mesh/chaos-mesh \\ --version v2.0.0 \\ --namespace chaos-mesh \\ --set chaosDaemon.runtime = containerd \\ --set chaosDaemon.socketPath = /run/containerd/containerd.sock # Verify kubectl get deployments,pods,services --namespace chaos-mesh The control plane components for the Chaos Mesh are: 1. chaos=controller-manager : This is used to schedule and manage the lifecycle of chaos experiments. (This is a misnomer. This should be just named controller, not controller-manager, as it's the controller based on the Operator Pattern. The controller-manager is the Kubernetes control plane component that manages all the controllers like this one). 2. chaos-daemon : These are the Pods that control the chaos mesh. The Pods run on every cluster Node and are wrapped in a DaemonSet. These DaemonSets have privileged system permissions to access each Node's network, cgroups, chroot, and other resources that are accessed based on your experiments. 3. chaos-dashboard : An optional web interface providing you an alternate means to administer the engine and experiments. Its use is for convenience and any production use of the engine should be through the YAML resources for the Chaos Mesh CRDs. - Chaos Mesh Dashboard The chaos dashboard is accessible via a NodePort. For this scenario we need the nodePort at a specific value, rather than its current random port number. Set the nodePort to a specific port: kubectl patch service chaos-dashboard -n chaos-mesh --type = 'json' --patch = '[{\"op\": \"replace\", \"path\": \"/spec/ports/0/nodePort\", \"value\":31111}]' # With the correct port value set, the web interface for Chaos Mesh dashboard can be seen from the tab Chaos Mesh above the command-line area or this link: https://2886795275-31111-kira01.environments.katacoda.com/. - There are no experiments yet, but take a few moments to explore the general layout of the dashboard. There is a way through the user interface to create, update, and delete experiments. - Chaos Mesh Experiment Types ============================================================ Category Type Experiment Description Pod Lifecycle Pod Failure Killing pods. Pod Lifecycle Pod Kill Pods becoming unavailable. Pod Lifecycle Container Kill Killing containers in pods. Network Partition Separate Pods into independent subnets by blocking communication between them. Network Loss Inject network communication loss. Network Delay Inject network communication latency. Network Duplication Inject packet duplications. Network Corrupt Inject network communication corruption. Network Bandwidth Limit the network bandwidth. I/O Delay Inject delay during I/O. I/O Errno Inject error during I/O. I/O Delay and Errno Inject both delays and errors with I/O. Linux Kernel Inject kernel errors into pods. Clock Offset Inject clock skew into pods. Stress CPU Simulate pod CPU stress. Stress Memory Simulate pod memory stress. Stress CPU & Memory Simulate both CPU and memory stress on Pods. # cat web-show-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : web-show labels : app : web-show spec : replicas : 1 selector : matchLabels : app : web-show template : metadata : labels : app : web-show spec : containers : - name : web-show image : pingcap/web-show imagePullPolicy : Always command : - /usr/local/bin/web-show - --target-ip=$(TARGET_IP) env : - name : TARGET_IP valueFrom : configMapKeyRef : name : web-show-context key : target.ip ports : - name : web-port containerPort : 8081 hostPort : 8081 # cat web-show-service.yaml apiVersion : v1 kind : Service metadata : name : web-show labels : app : web-show spec : selector : app : web-show type : NodePort ports : - port : 8081 protocol : TCP targetPort : 8081 nodePort : 30081 # cat nginx.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx annotations : deployment.kubernetes.io/revision : \"1\" labels : app : nginx spec : replicas : 8 selector : matchLabels : app : nginx template : metadata : labels : app : nginx chaos : blast-here spec : containers : - image : nginx name : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : shine-on-you-crazy-diamond spec : replicas : 2 selector : matchLabels : app : cant-touch-dis template : metadata : labels : app : cant-touch-dis spec : containers : - image : nginx name : nginx # cat network-delay-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : NetworkChaos metadata : name : web-show-network-delay spec : action : delay mode : one selector : namespaces : - default labelSelectors : app : web-show delay : latency : 10ms # cat scheduled-network-delay-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : Schedule metadata : name : web-show-scheduled-network-delay spec : schedule : '@every 60s' type : NetworkChaos historyLimit : 5 concurrencyPolicy : Forbid networkChaos : action : delay mode : one selector : namespaces : - default labelSelectors : app : web-show delay : latency : 10ms duration : 30s # cat pod-removal-experiment.yaml apiVersion : chaos-mesh.org/v1alpha1 kind : Schedule metadata : name : pod-kill-example namespace : chaos-mesh spec : schedule : '@every 15s' type : PodChaos historyLimit : 5 concurrencyPolicy : Forbid podChaos : action : pod-kill mode : one selector : namespaces : - chaos-sandbox labelSelectors : chaos : blast-here - Network Delay Experiment # Install an example application as a target for the experiment. This application is designed by the Chaos Mesh project as a hello world example for your first experiment. # The application needs an environment variable for the TARGET_IP, which is the cluster IP, so this context you provide as a ConfigMap. That ConfigMap variable is referenced in the Deployment YAML. TARGET_IP = $( kubectl get pod -n kube-system -o wide | grep kube-controller | head -n 1 | awk '{print $6}' ) kubectl create configmap web-show-context --from-literal = target.ip = ${ TARGET_IP } kubectl apply -f web-show-deployment.yaml kubectl apply -f web-show-service.yaml # With the web-show application running, its web interface can be accessed from the \"Web Show\" above the command-line area or this link: https://2886795275-30081-kira01.environments.katacoda.com/ # Define Experiment kubectl get crds # The Chaos Mesh has installed several custom resources # You can reference these resources to create declarative YAML manifests that define your experiment. # For your first experiment, you will impose a network delay. The delay is defined in the NetworkChaos manifest # The experiment declares that a 10ms network delay should be injected. The delay will only be applied to the target service labeled \"app\": \"web-show\". # This is the **blast radius**. kubectl get deployments,pods -l app = 'web-show' # Apply Experiment kubectl apply -f network-delay-experiment.yaml # The experiment is now running. kubectl get NetworkChaos # The application has a built-in graph that will show the latency it's experiencing. With the experiment applied you will see the 10ms delay. # Update Experiment # At any time you can change the YAML declaration and apply further experiment updates. kubectl apply -f network-delay-experiment.yaml # The experiment can be paused kubectl annotate networkchaos web-show-network-delay experiment.chaos-mesh.org/pause = true # and resumed kubectl annotate networkchaos web-show-network-delay experiment.chaos-mesh.org/pause- # Since the NetworkChaos is like any other Kubernetes resource, the experiment can be easily removed. kubectl delete -f network-delay-experiment.yaml - Scheduled Experiment # This experiment will inject network chaos periodically: 10ms network delay should be injected every minute that lasts for 30 seconds # Apply Scheduled Experiment kubectl apply -f scheduled-network-delay-experiment.yaml # The schedule experiment is now running. Scheduled experiment will not create NetworkChaos object immediately, intead it creates an Schedule object called web-show-scheduled-network-delay kubectl get Schedule # NetworkChaos is very similar with what between CronJob and Job: Schedule will spawn NetworkChaos when trigger by @every 60s. kubectl get NetworkChaos -w # The experiment can be paused kubectl annotate schedule web-show-scheduled-network-delay experiment.chaos-mesh.org/pause = true # and resumed: kubectl annotate schedule web-show-scheduled-network-delay experiment.chaos-mesh.org/pause- - Pod Removal Experiment # Install an example application as a target for the experiment. It's just a deployment of the common Nginx web server with Pod replications. Apply the Deployment to the chaos-sandbox namespace. kubectl create namespace chaos-sandbox kubectl apply -f nginx.yaml -n chaos-sandbox # The experiment declares that the specific pod should be killed every 15s. The removal will only be applied to the target pod labeled \"chaos\": \"blast here\", which is the blast radius. # Apply Experiment kubectl apply -f pod-removal-experiment.yaml kubectl get Schedule -n chaos-mesh # Based on the cron time in the experiment, watch the Pods randomly terminate and new ones start. watch kubectl get -n chaos-sandbox deployments,pods,services # Notice the blast radius is targeting only the nginx Pods, while the shine-on-you-crazy-diamond Pods remain undisturbed.","title":"Chaos Mesh"},{"location":"k8s/chaos/#litmus","text":"Litmus is a toolset to do cloud native chaos engineering. Litmus provides tools to orchestrate chaos on Kubernetes to help SREs find weaknesses in their deployments. SREs use Litmus to run chaos experiments initially in the staging environment and eventually in production to find bugs and vulnerabilities. Fixing the weaknesses leads to increased resilience of the system. - Litmus offers you these compelling features: 1. Kubernetes native CRDs to manage chaos. Using chaos API, orchestration, scheduling, and complex workflow management can be orchestrated declaratively. 2. Most of the generic chaos experiments are readily available for you to get started with your initial chaos engineering needs. 3. An SDK is available in GO, Python, and Ansible. A basic experiment structure is created quickly using SDK and developers and SREs just need to add the chaos logic to make a new experiment. 4. It's simple to complex chaos workflows are easy to construct. Use GitOps and the chaos workflows to scale your chaos engineering efforts and increase the resilience of your Kubernetes platform. # For this scenario, we'll install the standard NGINX application and make it a target. Install NGINX into the default namespace. kubectl create deploy nginx --image = nginx kubectl get deployments,pods --show-labels - Install Litmus Operator # The recommended way to start Litmus is by installing the Litmus Operator. kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v1.8.0.yaml kubectl get namespaces # In the list, you see litmus as a new namespace. # An operator is a custom Kubernetes controller that uses custom resources (CR) to manage applications and their components. The Litmus Operator is comprised of a few controllers maintaining the CRs. kubectl get crds | grep litmus # Check the Litmus API resources are available kubectl api-resources | grep litmus kubectl get all -n litmus - The key components and object associated with Litmus are: 1. RBAC for chaotic administration access targeted objects on your cluster. 2. The Litmus controller that manages the custom resources and the following apps: - ChaosEngine : A resource to link a Kubernetes application or Kubernetes node to a ChaosExperiment. ChaosEngine is watched by Litmus' Chaos-Operator which then invokes Chaos-Experiments. - ChaosExperiment : A resource to group the configuration parameters of a chaos experiment. ChaosExperiment CRs are created by the operator when experiments are invoked by ChaosEngine. - ChaosResult : A resource to hold the results of a chaos-experiment. The Chaos-exporter reads the results and exports the metrics into a configured Prometheus server. - Install Chaos Experiments - These experiments are installed on your cluster as Litmus resources declarations in the form of the Kubernetes CRDs. Because the chaos experiments are just Kubernetes YAML manifests, these experiments are published on Chaos Hub . - generic/pod-delete kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/experiment.yaml # Verify the pod-delete experiment has been installed kubectl get chaosexperiments # Setup RBAC with Service Account # A service account should be created to allow ChaosEngine to run experiments in your application namespace. kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/rbac.yaml # Verify the ServiceAccount RBAC rules have been applied for pod-delete-sa kubectl get serviceaccount,role,rolebinding # Annotate Application # In this case, we'll annotate the NGINX deployment with litmuschaos.io/chaos=\"true\" kubectl annotate deploy/nginx litmuschaos.io/chaos = \"true\" # Verify the annotation has been applied kubectl get deployment nginx -o = custom-columns = 'ANNOTATIONS:metadata.annotations' # Run the Experiment kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.8.0?file = charts/generic/pod-delete/engine.yaml # Start watching the Pods in the default namespace watch -n 1 kubectl get pods # In a moment an nginx-chaos-runner Pod will start. This Pod is created by the Litmus engine based on the experiment criteria. # In a moment, the chaos-runner will create a new Pod called pod-delete-<hash>. This Pod is responsible for the actual Pod deletion. # Shortly after the pod-delete-<hash> Pod starts, you'll notice the NGINX Pod is killed. # Observe and Verify Experiments kubectl describe chaosresult nginx-chaos-pod-delete # The status.verdict is set to Awaited when the experiment is in progress, eventually changing to either Pass or Fail.","title":"Litmus"},{"location":"k8s/chaos/#chaoskube","text":"Chaoskube periodically kills random Pods in your Kubernetes cluster, which allows you to test how your system behaves under arbitrary Pod failures. Helm Input kubectl version --short && \\ kubectl get componentstatus && \\ kubectl get nodes && \\ kubectl cluster-info helm version --short kubectl create namespace chaoskube helm repo add chaoskube https://linki.github.io/chaoskube # Install the chart # The interval parameter instructs Chaoskube to kill Pods every 20 seconds. # The targeted Pods are any with the label app-purpose=chaos, and the kube-system namespace has to be explicitly excluded (!) from the list of namespaces to look for Pods to kill. helm install chaoskube chaoskube/chaoskube \\ --version = 0 .1.0 \\ --namespace chaoskube \\ --set image.tag = v0.21.0 \\ --set dryRun = false \\ --set 'namespaces=!kube-system' \\ --set labels = app-purpose = chaos \\ --set interval = 20s kubectl get -n chaoskube deployments kubectl rollout -n chaoskube status deployment chaoskube # You can periodically check the Chaoskube log to see its Pod killing activity. POD = $( kubectl -n chaoskube get pods -l = 'app.kubernetes.io/instance=chaoskube' --output = jsonpath = '{.items[0].metadata.name}' ) kubectl -n chaoskube logs -f $POD # nginx.yaml apiVersion : apps/v1 kind : Deployment metadata : annotations : deployment.kubernetes.io/revision : \"1\" labels : app : nginx app-purpose : chaos name : nginx spec : replicas : 8 selector : matchLabels : app : nginx template : metadata : labels : app : nginx app-purpose : chaos spec : containers : - image : nginx name : nginx # cat ghost.yaml apiVersion : apps/v1 kind : Deployment metadata : annotations : deployment.kubernetes.io/revision : \"1\" labels : app : ghost app-purpose : chaos name : ghost spec : replicas : 3 selector : matchLabels : app : ghost template : metadata : labels : app : ghost app-purpose : chaos spec : containers : - image : ghost:3.11.0-alpine name : ghost The Deployments and Pods are labeled to mark these Pods as potential victim targets of the Chaoskube Pod killer. The Deployment and Pod template have the label app-purpose: chaos that makes the Pod an eligible target for Chaoskube. The label is provided as a configuration value during the Helm chart installation. kubectl apply -f nginx.yaml kubectl create namespace more-apps kubectl create --namespace more-apps kubectl apply -f ghost.yaml Observe the Chaos Notice as Pods are deleted every 20 secs, the Kubernetes resilience feature is making sure they are restored. watch kubectl get deployments,pods --all-namespaces -l app-purpose = chaos In a real chaos testing platform, you should complement this Pod killing activity with automated tests to ensure these disruptions are either unnoticed or acceptable for your business processes.","title":"Chaoskube"},{"location":"k8s/install/","text":"Installing K8s \u00b6 Vagrant Setup \u00b6 Install Dev Setup Publish a web app Deployment Pattern Terraform App Deployment Kubeadm \u00b6 Kops \u00b6 Verification \u00b6 Networking vagrant ssh k8s-m-1 # Check Routing within the master node sudo apt-get install net-tools route # Displays the routing network # Check syslog errors tail -f /var/log/syslog # Ctrl + Z to exit # Copy file from Master Node to Host machine mkdir -p ~/.kube vagrant port k8s-m-1 # Find the SSH port of the k8s-m-1 server # Copy the file using scp (ssh password is vagrant) scp -P 2222 vagrant@127.0.0.1:/home/vagrant/.kube/config ~/.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config # Get Cluster Information kubectl cluster-info # Get Master Node Component health kubectl get componentstatus # In case Scheduler or Controller Manager is showing as Unhealthy or Connection refused. # Modify the following files on all master nodes: sudo vi /etc/kubernetes/manifests/kube-scheduler.yaml # Clear the line (spec->containers->command) containing this phrase: - --port=0 sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml # Clear the line (spec->containers->command) containing this phrase: - --port=0 sudo systemctl restart kubelet.service","title":"Installation"},{"location":"k8s/install/#installing-k8s","text":"","title":"Installing K8s"},{"location":"k8s/install/#vagrant-setup","text":"Install Dev Setup Publish a web app Deployment Pattern Terraform App Deployment","title":"Vagrant Setup"},{"location":"k8s/install/#kubeadm","text":"","title":"Kubeadm"},{"location":"k8s/install/#kops","text":"","title":"Kops"},{"location":"k8s/install/#verification","text":"Networking vagrant ssh k8s-m-1 # Check Routing within the master node sudo apt-get install net-tools route # Displays the routing network # Check syslog errors tail -f /var/log/syslog # Ctrl + Z to exit # Copy file from Master Node to Host machine mkdir -p ~/.kube vagrant port k8s-m-1 # Find the SSH port of the k8s-m-1 server # Copy the file using scp (ssh password is vagrant) scp -P 2222 vagrant@127.0.0.1:/home/vagrant/.kube/config ~/.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config # Get Cluster Information kubectl cluster-info # Get Master Node Component health kubectl get componentstatus # In case Scheduler or Controller Manager is showing as Unhealthy or Connection refused. # Modify the following files on all master nodes: sudo vi /etc/kubernetes/manifests/kube-scheduler.yaml # Clear the line (spec->containers->command) containing this phrase: - --port=0 sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml # Clear the line (spec->containers->command) containing this phrase: - --port=0 sudo systemctl restart kubelet.service","title":"Verification"},{"location":"k8s/task/","text":"Cluster - Use kubeadm to install a basic cluster - Kubernetes the Hard way - Perform a version upgrade (Minor version upgrade) - Implement etcd backup and restore (Always check where the data directory is on the host path mentioned in Volume section) - Provision underlying infrastructure to deploy a cluster - Manage a HA cluster RBAC - Manage RBAC Troubleshoot Worker Node (30%) - Kubelet Service status and Journald Log parsing - Docker Service status - Consult container logs directly from Docker - Consult resources pod, ds, cm in kube-system namespaces Challenges \u00b6 Use etcdctl to get data of a pod Use etcdctl for changes by watching output docker commands to check logs, exec into running containers openssl to check expiry of certs # On Master cd /etc/kubernetes/pki openssl x509 -in ./apiserver.crt -text -noout # Commit to memory openssl x509 -in ./apiserver.crt -text -noout | grep Validity -A 2 apt-get to check package information, cache, available versions ETCD has its own CA. The right CA must be used for the ETCD-CA file in /etc/kubernetes/manifests/kube-apiserver.yaml. You can change the CA for ETCD and watch what happens to Apiserver Kubeconfig Error W0804 06:44:49.196716 17546 loader.go:221] Config not found: . This could be mismatch in the current context information. Either User certificate path is incorrect Components \u00b6 Understanding Kubernetes components and being able to fix and investigate clusters: https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster Know advanced scheduling: https://kubernetes.io/docs/concepts/scheduling/kube-scheduler When you have to fix a component (like kubelet) in one cluster, just check how its setup on another node in the same or even another cluster. You can copy config files over etc If you like you can look at Kubernetes The Hard Way once. But it's NOT necessary to do, the CKA is not that complex. But KTHW helps understanding the concepts You should install your own cluster using kubeadm (one master, one worker) in a VM or using a cloud provider and investigate the components Know how to use Kubeadm to for example add nodes to a cluster Know how to create an Ingress resources Know how to snapshot/restore ETCD from another machine","title":"Task"},{"location":"k8s/task/#challenges","text":"Use etcdctl to get data of a pod Use etcdctl for changes by watching output docker commands to check logs, exec into running containers openssl to check expiry of certs # On Master cd /etc/kubernetes/pki openssl x509 -in ./apiserver.crt -text -noout # Commit to memory openssl x509 -in ./apiserver.crt -text -noout | grep Validity -A 2 apt-get to check package information, cache, available versions ETCD has its own CA. The right CA must be used for the ETCD-CA file in /etc/kubernetes/manifests/kube-apiserver.yaml. You can change the CA for ETCD and watch what happens to Apiserver Kubeconfig Error W0804 06:44:49.196716 17546 loader.go:221] Config not found: . This could be mismatch in the current context information. Either User certificate path is incorrect","title":"Challenges"},{"location":"k8s/task/#components","text":"Understanding Kubernetes components and being able to fix and investigate clusters: https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster Know advanced scheduling: https://kubernetes.io/docs/concepts/scheduling/kube-scheduler When you have to fix a component (like kubelet) in one cluster, just check how its setup on another node in the same or even another cluster. You can copy config files over etc If you like you can look at Kubernetes The Hard Way once. But it's NOT necessary to do, the CKA is not that complex. But KTHW helps understanding the concepts You should install your own cluster using kubeadm (one master, one worker) in a VM or using a cloud provider and investigate the components Know how to use Kubeadm to for example add nodes to a cluster Know how to create an Ingress resources Know how to snapshot/restore ETCD from another machine","title":"Components"},{"location":"k8s/testing/","text":"Consumer-driven contracts (CDC) \u00b6 CDC is a concept and testing approach that embraces the perspective of multiple consumers that communicate with providers. Typically, testing tends to define API contracts from the provider's perspective. Through a registry of contracts, multiple consumers now have a voice to provide producers their expectations on how data should be exchanged between the consumers and producers. 1. Set up a Pact Broker on Kubernetes 1. Write a consumer that defines and publishes Pact contracts 1. Deploy and run a few Spring Boot microservices on Kubernetes 1. Connect microservices to a database and public data source 1. Verify the consumer pacts against a producer 1. Find API defects and fix them What is consumer-driven contract testing? \u00b6 The \"consumer-driven\" prefix simply states an additional philosophical position that advocates for better internal microservices design by putting the consumers of such APIs at the heart of the design process. Provider-driven APIs tend to be biased towards the data that is being exposed and the system that is exposing it. - PACT # Adding the Private Repo helmchart (Refer Chaos) helm repo add twuni https://helm.twun.io helm install registry twuni/docker-registry \\ --version 1 .10.0 \\ --namespace kube-system \\ --set service.type = NodePort \\ --set service.nodePort = 31500 export REGISTRY = 2886795362 -31500-kira01.environments.katacoda.com curl $REGISTRY /v2/_catalog | jq # Registry Proxy for localhost:5000 # Docker tags require the address of the registry to be in the tag. # If we push and pull from the registry, the registry name tag must be the same when we are on the client or within Kubernetes. Within your cluster, the registry is available at localhost:5000. Use a port-forwarding command to make this client's localhost:5000 to be the same registry. kubectl port-forward -n kube-system service/registry-docker-registry 5000 :5000 > /dev/null & # This port forwarding will run in the background for the remainder of this scenario. # Now you can access the registry via localhost. curl http://localhost:5000/v2/_catalog | jq . # This means when you push to localhost:5000, your container images will be routed to the private registry running as a service on Kubernetes. # But what happens in the Pod specification when you want to pull the image using the tag localhost:5000? # We can add a proxy that runs as a DaemonSet that will resolve localhost:5000 to the registry whenever a Pod requests a container from localhost:5000. Install the proxy. helm repo add incubator https://charts.helm.sh/incubator # With the added repo, install the proxy daemons. helm install registry-proxy incubator/kube-registry-proxy \\ --version 0 .3.2 \\ --namespace kube-system \\ --set registry.host = registry-docker-registry.kube-system \\ --set registry.port = 5000 \\ --set hostPort = 5000 # For mature environments, you would have an official host name with a load balancer and an ingress that would resolve to a hardened registry service, albeit still running on Kubernetes. # You will use docker push commands and YAML container references both using localhost:5000. About the Application \u00b6 # Clone Source Code git clone https://github.com/javajon/cdc-with-k8s cd ~/cdc-with-k8s tree -d -L 2 # In summary, the aggregator serves data combining the daily COVID-19 metrics with the world population. Add Pact Broker \u00b6 The Pact broker is the key that enables separation between the consumers and producers. The pact broker is a registry, or a library, that serves the collections of contacts generated by the consumers during mock testing and referenced by the producers during verification. cd ~/cdc-with-k8s/cluster # There are two Kubernetes manifest files that declare the Deployments for the Pact Broker and its associated Postgres database. kubectl apply -f pact-broker-postgres.yaml kubectl apply -f pact-broker.yaml # The database is used for storing the Pacts and is attached to a Persistent Volume (PV) reserved in this cluster. # Open the Pact Broker web interface and observe its contents. # The broker is essentially empty. It does have an Example App, but this is just a sample. Generate Pact from Consumer \u00b6 With the Pact framework, it's the consumers that create the Pacts. Independent of the producers or any service, the consumers write testing code that creates conversations with local mocked services. The Mocks are the consumer's perspectives of how the producers should react to the consumer's requests. This particular consumer is written in Node.js. It also uses Jest - a delightful JavaScript Testing Framework with a focus on simplicity. CDC with Jest and Pact cd ~/cdc-with-k8s/pact # Generate Pact Contracts npm install # Produce the two Pact files: npm run test:consumer-a npm run test:consumer-b # Once complete, new Pact JSON files are in the pacts directory. Inspect one of the contract files: cat pacts/consumer_a-aggregator.json | jq -C . # Publish Pacts to Pact Broker # Define access to the Pact Broker: export PACT_BROKER_URL = https://2886795362-30111-kira01.environments.katacoda.com/ export BROKER_USERNAME = pactbroker export BROKER_PASSWORD = pactbroker # Publish the pact to the broker: npm run publish:pact # Verify the pact has been published to the Pact Broker. Run H2 Database A low-level container in this application is a small database that contains world population data. You will use it as a read-only datastore providing the populations for the countries of the world, as well as the populations and locations of major cities. Provided in this example is a SQL script that will seed a relational database, so we need to create an \"initContainer\" that will run next to the H2 container and seed it with the country and city population data when it starts. The InitContainer pattern is very common for ensuring Pods are in the correct state when started. cd ~/cdc-with-k8s/h2-seeder # Take a look at the Dockerfile to see how when it runs is uses an H2 RunScript utility to inject the world.sql into the H2 database that is assumed to be local, in the same Pod. docker build -t localhost:5000/ $( basename $PWD ) :0.0.1 . # During the image build you can safely ignore the TLS certificate validation not implemented docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Apply this manifest declaration to set up a Pod and Service for H2. kubectl apply -f ../cluster/h2-world.yaml # The H2 database serves a convenient web interface for you to interact with the database. When you are presented with the connection information just put in jdbc:h2:/h2-data/world for the JDBC driver URL and leave the username and password blank. # Use the Connect button to enter the SQL explorer. Enter select * from country Run World Population Microservice This is a Spring Boot based Microservice that simply reads world population data from the H2 database using SQL select calls. It offers REST endpoints to get the populations from /countries and /cities . Data is provided in the JSON format. cd ~/cdc-with-k8s/world-pop # Build Microservice Container Image # Spring Boot with Gradle (or Maven) has a convenient task called bootBuildImage. Without having to write a Dockerfile this task will bundle the Java application into an optimized container image. Build and tag the microservice container image ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml # The microservice will be running in a moment. # Verify Microservice curl https://2886795296-30101-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30101-kira01.environments.katacoda.com/countries | jq . curl https://2886795296-30101-kira01.environments.katacoda.com/cities | jq . Run COVID-19 Microservice This is a Spring Boot based Microservice that reads COVID-19 metrics from a public CSV file. This microservice offers REST endpoints to get the data from the /metrics. Data is provided in the JSON format. cd ~/cdc-with-k8s/covid-19 ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml kubectl get pods,deployments,services -l app = $( basename $PWD ) # Verify Microservice curl https://2886795296-30102-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30102-kira01.environments.katacoda.com/metrics | jq . Run Aggregator Microservice This microservice is called the Aggregator as it follows the common architecture pattern of an aggregator. It provides a single API gateway to access the other two microservices: world-pop and covid-19. The data from the two other microservices are merged into responses where COVID-19 data is merged with population data. With population data, you can get visibility in infection rates based on per capita. cd ~/cdc-with-k8s/aggregator ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml # Verify Microservice curl https://2886795296-30103-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30103-kira01.environments.katacoda.com/countries | jq . # Get a single country. curl https://2886795296-30103-kira01.environments.katacoda.com/countries/ind | jq . # Get the top countries with the highest infections per capita: curl https://2886795296-30103-kira01.environments.katacoda.com/countries/percapita | jq . Verify Application (Failed?) Now that everything is started let's verify the contracts against the actual service. cd ~/cdc-with-k8s/aggregator # Verify the pacts on the producer side: ./gradlew pactVerify # You will see that the verification failed. This shows us that a consumer disagrees with the producer. This disagreement is fantastic because it's unveiling a defect before it rolls further to production. # If you inspect all the percentCases fields from the producer, they are all zero: curl https://2886795296-30103-kira01.environments.katacoda.com/countries/percapita | jq . | grep percentCases # The consumer contracts all expect the percentage of infection values to be greater than zero. The consumer code is written with these rules. ` expect ( Number ( response [ 0 ] .percentCases )) .toBeGreaterThan ( 0 .0 ) ; percentCases: term ({ generate: \"0.3333\" , matcher: \"^([0-9]*[1-9][0-9]*(\\.[0-9]+)?|[0]+\\.[0-9]*[1-9][0-9]*) $ \" }) , ` # So for some reason, the producer is producing only zeros. A typical defect on any normal day. Make Correction The problem is in the producer (aggregator) code cd ~/cdc-with-k8s/aggregator # Look at the code in models/Country.java: sed -n '{;=;p}' src/main/java/com/dijure/aggregator/models/Country.java | sed \"N;s/\\n/ /g\" | sed -n '65,70p;!d' # There's the bugger... Someone left in some testing code with an experimentation comment! # Find and remove the offending line: OFFENDING_LINE = $( sed -n '\\|population = 0;|=' src/main/java/com/dijure/aggregator/models/Country.java ) sed -i \" ${ OFFENDING_LINE } d\" src/main/java/com/dijure/aggregator/models/Country.java # Verify the moth has been removed from the relay Verify Application Rebuild the new container with an increased SemVer number. We bump the patch value, since it's a bug fix ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.2 # Push the corrected container image to the private registry on your Kubernetes cluster docker push localhost:5000/ $( basename $PWD ) :0.0.2 # Patch the current deployment to use the new container version kubectl patch deployment aggregator -p \\ '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"aggregator\",\"image\":\"localhost:5000/aggregator:0.0.2\"}]}}}}' # Verify kubectl get pods,deployments,services -l app = $( basename $PWD ) # Finally, run the pactVerify task again, and let's see if that bug has been squashed: ./gradlew pactVerify This means that your producer agrees with the contracts independently generated by all its consumers.","title":"Consumer-driven contracts (CDC)"},{"location":"k8s/testing/#consumer-driven-contracts-cdc","text":"CDC is a concept and testing approach that embraces the perspective of multiple consumers that communicate with providers. Typically, testing tends to define API contracts from the provider's perspective. Through a registry of contracts, multiple consumers now have a voice to provide producers their expectations on how data should be exchanged between the consumers and producers. 1. Set up a Pact Broker on Kubernetes 1. Write a consumer that defines and publishes Pact contracts 1. Deploy and run a few Spring Boot microservices on Kubernetes 1. Connect microservices to a database and public data source 1. Verify the consumer pacts against a producer 1. Find API defects and fix them","title":"Consumer-driven contracts (CDC)"},{"location":"k8s/testing/#what-is-consumer-driven-contract-testing","text":"The \"consumer-driven\" prefix simply states an additional philosophical position that advocates for better internal microservices design by putting the consumers of such APIs at the heart of the design process. Provider-driven APIs tend to be biased towards the data that is being exposed and the system that is exposing it. - PACT # Adding the Private Repo helmchart (Refer Chaos) helm repo add twuni https://helm.twun.io helm install registry twuni/docker-registry \\ --version 1 .10.0 \\ --namespace kube-system \\ --set service.type = NodePort \\ --set service.nodePort = 31500 export REGISTRY = 2886795362 -31500-kira01.environments.katacoda.com curl $REGISTRY /v2/_catalog | jq # Registry Proxy for localhost:5000 # Docker tags require the address of the registry to be in the tag. # If we push and pull from the registry, the registry name tag must be the same when we are on the client or within Kubernetes. Within your cluster, the registry is available at localhost:5000. Use a port-forwarding command to make this client's localhost:5000 to be the same registry. kubectl port-forward -n kube-system service/registry-docker-registry 5000 :5000 > /dev/null & # This port forwarding will run in the background for the remainder of this scenario. # Now you can access the registry via localhost. curl http://localhost:5000/v2/_catalog | jq . # This means when you push to localhost:5000, your container images will be routed to the private registry running as a service on Kubernetes. # But what happens in the Pod specification when you want to pull the image using the tag localhost:5000? # We can add a proxy that runs as a DaemonSet that will resolve localhost:5000 to the registry whenever a Pod requests a container from localhost:5000. Install the proxy. helm repo add incubator https://charts.helm.sh/incubator # With the added repo, install the proxy daemons. helm install registry-proxy incubator/kube-registry-proxy \\ --version 0 .3.2 \\ --namespace kube-system \\ --set registry.host = registry-docker-registry.kube-system \\ --set registry.port = 5000 \\ --set hostPort = 5000 # For mature environments, you would have an official host name with a load balancer and an ingress that would resolve to a hardened registry service, albeit still running on Kubernetes. # You will use docker push commands and YAML container references both using localhost:5000.","title":"What is consumer-driven contract testing?"},{"location":"k8s/testing/#about-the-application","text":"# Clone Source Code git clone https://github.com/javajon/cdc-with-k8s cd ~/cdc-with-k8s tree -d -L 2 # In summary, the aggregator serves data combining the daily COVID-19 metrics with the world population.","title":"About the Application"},{"location":"k8s/testing/#add-pact-broker","text":"The Pact broker is the key that enables separation between the consumers and producers. The pact broker is a registry, or a library, that serves the collections of contacts generated by the consumers during mock testing and referenced by the producers during verification. cd ~/cdc-with-k8s/cluster # There are two Kubernetes manifest files that declare the Deployments for the Pact Broker and its associated Postgres database. kubectl apply -f pact-broker-postgres.yaml kubectl apply -f pact-broker.yaml # The database is used for storing the Pacts and is attached to a Persistent Volume (PV) reserved in this cluster. # Open the Pact Broker web interface and observe its contents. # The broker is essentially empty. It does have an Example App, but this is just a sample.","title":"Add Pact Broker"},{"location":"k8s/testing/#generate-pact-from-consumer","text":"With the Pact framework, it's the consumers that create the Pacts. Independent of the producers or any service, the consumers write testing code that creates conversations with local mocked services. The Mocks are the consumer's perspectives of how the producers should react to the consumer's requests. This particular consumer is written in Node.js. It also uses Jest - a delightful JavaScript Testing Framework with a focus on simplicity. CDC with Jest and Pact cd ~/cdc-with-k8s/pact # Generate Pact Contracts npm install # Produce the two Pact files: npm run test:consumer-a npm run test:consumer-b # Once complete, new Pact JSON files are in the pacts directory. Inspect one of the contract files: cat pacts/consumer_a-aggregator.json | jq -C . # Publish Pacts to Pact Broker # Define access to the Pact Broker: export PACT_BROKER_URL = https://2886795362-30111-kira01.environments.katacoda.com/ export BROKER_USERNAME = pactbroker export BROKER_PASSWORD = pactbroker # Publish the pact to the broker: npm run publish:pact # Verify the pact has been published to the Pact Broker. Run H2 Database A low-level container in this application is a small database that contains world population data. You will use it as a read-only datastore providing the populations for the countries of the world, as well as the populations and locations of major cities. Provided in this example is a SQL script that will seed a relational database, so we need to create an \"initContainer\" that will run next to the H2 container and seed it with the country and city population data when it starts. The InitContainer pattern is very common for ensuring Pods are in the correct state when started. cd ~/cdc-with-k8s/h2-seeder # Take a look at the Dockerfile to see how when it runs is uses an H2 RunScript utility to inject the world.sql into the H2 database that is assumed to be local, in the same Pod. docker build -t localhost:5000/ $( basename $PWD ) :0.0.1 . # During the image build you can safely ignore the TLS certificate validation not implemented docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Apply this manifest declaration to set up a Pod and Service for H2. kubectl apply -f ../cluster/h2-world.yaml # The H2 database serves a convenient web interface for you to interact with the database. When you are presented with the connection information just put in jdbc:h2:/h2-data/world for the JDBC driver URL and leave the username and password blank. # Use the Connect button to enter the SQL explorer. Enter select * from country Run World Population Microservice This is a Spring Boot based Microservice that simply reads world population data from the H2 database using SQL select calls. It offers REST endpoints to get the populations from /countries and /cities . Data is provided in the JSON format. cd ~/cdc-with-k8s/world-pop # Build Microservice Container Image # Spring Boot with Gradle (or Maven) has a convenient task called bootBuildImage. Without having to write a Dockerfile this task will bundle the Java application into an optimized container image. Build and tag the microservice container image ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml # The microservice will be running in a moment. # Verify Microservice curl https://2886795296-30101-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30101-kira01.environments.katacoda.com/countries | jq . curl https://2886795296-30101-kira01.environments.katacoda.com/cities | jq . Run COVID-19 Microservice This is a Spring Boot based Microservice that reads COVID-19 metrics from a public CSV file. This microservice offers REST endpoints to get the data from the /metrics. Data is provided in the JSON format. cd ~/cdc-with-k8s/covid-19 ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml kubectl get pods,deployments,services -l app = $( basename $PWD ) # Verify Microservice curl https://2886795296-30102-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30102-kira01.environments.katacoda.com/metrics | jq . Run Aggregator Microservice This microservice is called the Aggregator as it follows the common architecture pattern of an aggregator. It provides a single API gateway to access the other two microservices: world-pop and covid-19. The data from the two other microservices are merged into responses where COVID-19 data is merged with population data. With population data, you can get visibility in infection rates based on per capita. cd ~/cdc-with-k8s/aggregator ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.1 docker push localhost:5000/ $( basename $PWD ) :0.0.1 curl $REGISTRY /v2/_catalog | jq # Start Microservice kubectl apply -f ../cluster/ $( basename $PWD ) .yaml # Verify Microservice curl https://2886795296-30103-kira01.environments.katacoda.com/ping ; echo curl https://2886795296-30103-kira01.environments.katacoda.com/countries | jq . # Get a single country. curl https://2886795296-30103-kira01.environments.katacoda.com/countries/ind | jq . # Get the top countries with the highest infections per capita: curl https://2886795296-30103-kira01.environments.katacoda.com/countries/percapita | jq . Verify Application (Failed?) Now that everything is started let's verify the contracts against the actual service. cd ~/cdc-with-k8s/aggregator # Verify the pacts on the producer side: ./gradlew pactVerify # You will see that the verification failed. This shows us that a consumer disagrees with the producer. This disagreement is fantastic because it's unveiling a defect before it rolls further to production. # If you inspect all the percentCases fields from the producer, they are all zero: curl https://2886795296-30103-kira01.environments.katacoda.com/countries/percapita | jq . | grep percentCases # The consumer contracts all expect the percentage of infection values to be greater than zero. The consumer code is written with these rules. ` expect ( Number ( response [ 0 ] .percentCases )) .toBeGreaterThan ( 0 .0 ) ; percentCases: term ({ generate: \"0.3333\" , matcher: \"^([0-9]*[1-9][0-9]*(\\.[0-9]+)?|[0]+\\.[0-9]*[1-9][0-9]*) $ \" }) , ` # So for some reason, the producer is producing only zeros. A typical defect on any normal day. Make Correction The problem is in the producer (aggregator) code cd ~/cdc-with-k8s/aggregator # Look at the code in models/Country.java: sed -n '{;=;p}' src/main/java/com/dijure/aggregator/models/Country.java | sed \"N;s/\\n/ /g\" | sed -n '65,70p;!d' # There's the bugger... Someone left in some testing code with an experimentation comment! # Find and remove the offending line: OFFENDING_LINE = $( sed -n '\\|population = 0;|=' src/main/java/com/dijure/aggregator/models/Country.java ) sed -i \" ${ OFFENDING_LINE } d\" src/main/java/com/dijure/aggregator/models/Country.java # Verify the moth has been removed from the relay Verify Application Rebuild the new container with an increased SemVer number. We bump the patch value, since it's a bug fix ./gradlew bootBuildImage --imageName = localhost:5000/ $( basename $PWD ) :0.0.2 # Push the corrected container image to the private registry on your Kubernetes cluster docker push localhost:5000/ $( basename $PWD ) :0.0.2 # Patch the current deployment to use the new container version kubectl patch deployment aggregator -p \\ '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"aggregator\",\"image\":\"localhost:5000/aggregator:0.0.2\"}]}}}}' # Verify kubectl get pods,deployments,services -l app = $( basename $PWD ) # Finally, run the pactVerify task again, and let's see if that bug has been squashed: ./gradlew pactVerify This means that your producer agrees with the contracts independently generated by all its consumers.","title":"Generate Pact from Consumer"},{"location":"learning/docker/","text":"Running Docker and passing shell commands \u00b6 docker run -e TERM -e COLORTERM -it \u2013rm alpine sh -uec ' apk update apk add git zsh nano vim git clone \u2013depth=1 romkatv/powerlevel10k.git ~/powerlevel10k echo \"source ~/powerlevel10k/powerlevel10k.zsh-theme\" >>~/.zshrc cd ~/powerlevel10k exec zsh'","title":"Running Docker and passing shell commands"},{"location":"learning/docker/#running-docker-and-passing-shell-commands","text":"docker run -e TERM -e COLORTERM -it \u2013rm alpine sh -uec ' apk update apk add git zsh nano vim git clone \u2013depth=1 romkatv/powerlevel10k.git ~/powerlevel10k echo \"source ~/powerlevel10k/powerlevel10k.zsh-theme\" >>~/.zshrc cd ~/powerlevel10k exec zsh'","title":"Running Docker and passing shell commands"},{"location":"learning/git/","text":"Git Book Setting up multiple Github users have different ssh keys \u00b6 https://gist.github.com/oanhnn/80a89405ab9023894df7 It has a solution to test ssh configuration Create a new repository on the command line \u00b6 git init git add README.md git commit -m \"first commit\" git branch -M master git remote add origin leslieclif/dotfiles.git git push -u origin master Git Commands \u00b6 cd into git folder \u2192 ls -la \u2192 cd .git \u2192 ls -la Git help \u00b6 git help To quit help \u2192 q \u00b6 Best practise: Always do a pull before a push to merge changes from remote \u00b6 git pull origin master To git add and git commit for tracked files in a single comand use -a \u00b6 git commit -am \"Commit message\" Amend Commit message \u00b6 git commit \u2013amend \"New commit message\" Check for tracked files in git \u00b6 git ls-files Back out changes that have been commited, but not pushed to remote. Once unstaged, you can remove the changes using checkout \u00b6 git reset HEAD git checkout \u2013 Rename file-name. It also automatically stages the changes, so need to do git add \u00b6 git mv level3\u2013file.txt level3.txt If file is renamed outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A \u00b6 git add -A Moving files and staging the changes \u00b6 git mv level2.txt new-folder If file is moved outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A \u00b6 mv level2.txt .. git add -A file renamed in OS. But say, git has identifed unwanted files during git status and you dont want to add those files, then don't use -A, use -u \u00b6 Individually add the new renamed file first then update git \u00b6 git add level1.txt git add -u Delete files tracked by git \u00b6 git rm doomed.txt If file is delete outside git, it will delete and is not staged. To add and stage the deleted file use -A \u00b6 git add -A Git History \u00b6 git log To quit help \u2192 q \u00b6 Git history in one line \u00b6 git log \u2013oneline \u2013graph \u2013decorate Git history using duration \u00b6 git log \u2013since=\"3 days ago\" Show all user actions \u00b6 git reflog Show commit history \u2192 do git log get commit id \u00b6 git show #TODO: Get a git diff tool Show git config \u00b6 git config \u2013global \u2013list Compare with staging and current changes \u00b6 git diff Compare between current changes and remote last commit \u00b6 git diff HEAD Compare between staging and remote last commit \u00b6 git diff \u2013staged HEAD Compare file changes with staging and current changes \u00b6 git diff \u2013 Compare between commits (do git log to get commits) \u00b6 git diff Compare local and remote branches \u00b6 git diff master origin/master Compare local branches \u00b6 git diff master test-branch Branching \u00b6 List local and remote branches \u00b6 git branch -a Create new branch \u00b6 git branch Rename local branch \u00b6 git branch -m Delete a branch. Note: You have to be on another bracnh before you can delete the target branch \u00b6 git branch -d Create new branch and switch to it in single command \u00b6 git checkout -b Fash forward Merges \u2192 First switch to the target branches, do a git diff to review the changes. \u00b6 git merge Disable fast forward merge \u2192 Give tracing of merge by giving a custom merge message and also the commit history of the branch \u00b6 git merge \u2013no-ff Automatic merge \u00b6 git merge -m \" \" Merge Conflict and Resolution \u00b6 Inside the merging workspace incase of conflict, open the conflicting file in editor or the merge-diff tool. Resolve conflict and close the file. \u00b6 Rebase feature branch from master \u00b6 git checkout feature-branch git rebase master Abort rebase \u00b6 git rebase \u2013abort Rebase conflict resolution \u2192 Use merging tool to fix conflict, save and quit. Add file to git staging, then continue rebase \u00b6 git rebase \u2013continue Pull with Rebase (Rebase local master with remote master) \u00b6 git fetch origin master (non destructive merge which only updates references) git pull \u2013rebase origin master Stash \u00b6 git stash Stash + saving untracked files of git as well \u00b6 git stash -u Get the stash back to local \u00b6 git stash apply List the stash \u00b6 git stash list Drop the stash \u00b6 git stash drop Combination of apply and drop in one command. Brings the last saved state \u00b6 git stash pop Multiple Stashes \u00b6 git stash save \" \" Show any arbitary stash changes whithout popping. Do stash list first to get the stash ID \u00b6 git stash show stash@{1} Apply any arbitary stash changes. Do stash list first to get the stash ID \u00b6 git stash apply stash@{1} Drop any arbitary stash changes that was applied or not needed. \u00b6 git stash drop stash@{1} Stashing changes into a new branch. First see if you have any untracked files that also needs to be saved \u00b6 git stash -u git stash branch newbranchName Tagging \u00b6 Create Lightweight tag \u00b6 git tag mytag List existing tags \u00b6 git tag \u2013list Delete tag \u00b6 git tag \u2013delete mytag Create Annotated tags (It has additional information like release notes) \u00b6 git tag -a v1.0.0 -m \"Release 1.0.0\" Comparing tags \u00b6 git diff v1.0.0 v1.0.1 Tagging a specific commit ID \u00b6 git tag -a v0.0.9 -m \"Release 0.0.9\" Updating an existing tag with new commit id \u00b6 git tag -a v0.0.9 -f -m \"Correct Release 0.0.9\" Pushing tags to remote \u00b6 git push origin v1.0.0 Pushing all local tags to remote \u00b6 git push origin master \u2013tags Deleting tags in remote (puting :before tag name will delete it from remote). Does not delete tag from local \u00b6 git push origin :v0.0.9 Reset HEAD position \u00b6 git reset Using Stash and Branch combination \u00b6 First stash the changes (WIP) in one brabch, then checkout a new test branch and then pop the changes into this test branch \u00b6 git stash git checkout -b test git stash pop Cherry Pick (Hot Fix scenario) \u00b6 git cherry-pick","title":"Git"},{"location":"learning/git/#setting-up-multiple-github-users-have-different-ssh-keys","text":"https://gist.github.com/oanhnn/80a89405ab9023894df7 It has a solution to test ssh configuration","title":"Setting up multiple Github users have different ssh keys"},{"location":"learning/git/#create-a-new-repository-on-the-command-line","text":"git init git add README.md git commit -m \"first commit\" git branch -M master git remote add origin leslieclif/dotfiles.git git push -u origin master","title":"Create a new repository on the command line"},{"location":"learning/git/#git-commands","text":"cd into git folder \u2192 ls -la \u2192 cd .git \u2192 ls -la","title":"Git Commands"},{"location":"learning/git/#git-help","text":"git help","title":"Git help"},{"location":"learning/git/#to-quit-help----q","text":"","title":"To quit help --&gt; q"},{"location":"learning/git/#best-practise-always-do-a-pull-before-a-push-to-merge-changes-from-remote","text":"git pull origin master","title":"Best practise: Always do a pull before a push to merge changes from remote"},{"location":"learning/git/#to-git-add-and-git-commit-for-tracked-files-in-a-single-comand-use--a","text":"git commit -am \"Commit message\"","title":"To git add and git commit for tracked files in a single comand use -a"},{"location":"learning/git/#amend-commit-message","text":"git commit \u2013amend \"New commit message\"","title":"Amend Commit message"},{"location":"learning/git/#check-for-tracked-files-in-git","text":"git ls-files","title":"Check for tracked files in git"},{"location":"learning/git/#back-out-changes-that-have-been-commited-but-not-pushed-to-remote-once-unstaged-you-can-remove-the-changes-using-checkout","text":"git reset HEAD git checkout \u2013","title":"Back out changes that have been commited, but not pushed to remote. Once unstaged, you can remove the changes using checkout"},{"location":"learning/git/#rename-file-name-it-also-automatically-stages-the-changes-so-need-to-do-git-add","text":"git mv level3\u2013file.txt level3.txt","title":"Rename file-name. It also automatically stages the changes, so need to do git add"},{"location":"learning/git/#if-file-is-renamed-outside-git-it-will-delete-and-add-the-files-in-git-history-and-is-not-staged-to-add-new-file-and-stage-the-renamed-files-use--a","text":"git add -A","title":"If file is renamed outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A"},{"location":"learning/git/#moving-files-and-staging-the-changes","text":"git mv level2.txt new-folder","title":"Moving files and staging the changes"},{"location":"learning/git/#if-file-is-moved-outside-git-it-will-delete-and-add-the-files-in-git-history-and-is-not-staged-to-add-new-file-and-stage-the-renamed-files-use--a","text":"mv level2.txt .. git add -A","title":"If file is moved outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A"},{"location":"learning/git/#file-renamed-in-os-but-say-git-has-identifed-unwanted-files-during-git-status-and-you-dont-want-to-add-those-files-then-dont-use--a-use--u","text":"","title":"file renamed in OS. But say, git has identifed unwanted files during git status and you dont want to add those files, then don't use -A, use -u"},{"location":"learning/git/#individually-add-the-new-renamed-file-first-then-update-git","text":"git add level1.txt git add -u","title":"Individually add the new renamed file first then update git"},{"location":"learning/git/#delete-files-tracked-by-git","text":"git rm doomed.txt","title":"Delete files tracked by git"},{"location":"learning/git/#if-file-is-delete-outside-git-it-will-delete-and-is-not-staged-to-add-and-stage-the-deleted-file--use--a","text":"git add -A","title":"If file is delete outside git, it will delete and is not staged. To add and stage the deleted file  use -A"},{"location":"learning/git/#git-history","text":"git log","title":"Git History"},{"location":"learning/git/#to-quit-help----q_1","text":"","title":"To quit help --&gt; q"},{"location":"learning/git/#git-history-in-one-line","text":"git log \u2013oneline \u2013graph \u2013decorate","title":"Git history in one line"},{"location":"learning/git/#git-history-using-duration","text":"git log \u2013since=\"3 days ago\"","title":"Git history using duration"},{"location":"learning/git/#show-all-user-actions","text":"git reflog","title":"Show all user actions"},{"location":"learning/git/#show-commit-history----do-git-log-get-commit-id","text":"git show #TODO: Get a git diff tool","title":"Show commit history --&gt; do git log get commit id"},{"location":"learning/git/#show-git-config","text":"git config \u2013global \u2013list","title":"Show git config"},{"location":"learning/git/#compare-with-staging-and-current-changes","text":"git diff","title":"Compare with staging and current changes"},{"location":"learning/git/#compare-between-current-changes-and-remote-last-commit","text":"git diff HEAD","title":"Compare between current changes and remote last commit"},{"location":"learning/git/#compare-between-staging-and-remote-last-commit","text":"git diff \u2013staged HEAD","title":"Compare between staging and remote last commit"},{"location":"learning/git/#compare-file-changes-with-staging-and-current-changes","text":"git diff \u2013","title":"Compare file changes with staging and current changes"},{"location":"learning/git/#compare-between-commits-do-git-log-to-get-commits","text":"git diff","title":"Compare between commits (do git log to get commits)"},{"location":"learning/git/#compare-local-and-remote-branches","text":"git diff master origin/master","title":"Compare local and remote branches"},{"location":"learning/git/#compare-local-branches","text":"git diff master test-branch","title":"Compare local branches"},{"location":"learning/git/#branching","text":"","title":"Branching"},{"location":"learning/git/#list-local-and-remote-branches","text":"git branch -a","title":"List local and remote branches"},{"location":"learning/git/#create-new-branch","text":"git branch","title":"Create new branch"},{"location":"learning/git/#rename-local-branch","text":"git branch -m","title":"Rename local branch"},{"location":"learning/git/#delete-a-branch-note-you-have-to-be-on-another-bracnh-before-you-can-delete-the-target-branch","text":"git branch -d","title":"Delete a branch. Note: You have to be on another bracnh before you can delete the target branch"},{"location":"learning/git/#create-new-branch-and-switch-to-it-in-single-command","text":"git checkout -b","title":"Create new branch and switch to it in single command"},{"location":"learning/git/#fash-forward-merges-----first-switch-to-the-target-branches-do-a-git-diff-to-review-the-changes","text":"git merge","title":"Fash forward Merges  --&gt; First switch to the target branches, do a git diff to review the changes."},{"location":"learning/git/#disable-fast-forward-merge----give-tracing-of-merge-by-giving-a-custom-merge-message-and-also-the-commit-history-of-the-branch","text":"git merge \u2013no-ff","title":"Disable fast forward merge --&gt; Give tracing of merge by giving a custom merge message and also the commit history of the branch"},{"location":"learning/git/#automatic-merge","text":"git merge -m \" \"","title":"Automatic merge"},{"location":"learning/git/#merge-conflict-and-resolution","text":"","title":"Merge Conflict and Resolution"},{"location":"learning/git/#inside-the-merging-workspace-incase-of-conflict-open-the-conflicting-file-in-editor-or-the-merge-diff-tool-resolve-conflict-and-close-the-file","text":"","title":"Inside the merging workspace incase of conflict, open the conflicting file in editor or the merge-diff tool. Resolve conflict and close the file."},{"location":"learning/git/#rebase-feature-branch-from-master","text":"git checkout feature-branch git rebase master","title":"Rebase feature branch from master"},{"location":"learning/git/#abort-rebase","text":"git rebase \u2013abort","title":"Abort rebase"},{"location":"learning/git/#rebase-conflict-resolution----use-merging-tool-to-fix-conflict-save-and-quit-add-file-to-git-staging-then-continue-rebase","text":"git rebase \u2013continue","title":"Rebase conflict resolution --&gt; Use merging tool to fix conflict, save and quit. Add file to git staging, then continue rebase"},{"location":"learning/git/#pull-with-rebase-rebase-local-master-with-remote-master","text":"git fetch origin master (non destructive merge which only updates references) git pull \u2013rebase origin master","title":"Pull with Rebase (Rebase local master with remote master)"},{"location":"learning/git/#stash","text":"git stash","title":"Stash"},{"location":"learning/git/#stash--saving-untracked-files-of-git-as-well","text":"git stash -u","title":"Stash + saving untracked files of git as well"},{"location":"learning/git/#get-the-stash-back-to-local","text":"git stash apply","title":"Get the stash back to local"},{"location":"learning/git/#list-the-stash","text":"git stash list","title":"List the stash"},{"location":"learning/git/#drop-the-stash","text":"git stash drop","title":"Drop the stash"},{"location":"learning/git/#combination-of-apply-and-drop-in-one-command-brings-the-last-saved-state","text":"git stash pop","title":"Combination of apply and drop in one command. Brings the last saved state"},{"location":"learning/git/#multiple-stashes","text":"git stash save \" \"","title":"Multiple Stashes"},{"location":"learning/git/#show-any-arbitary-stash-changes-whithout-popping-do-stash-list-first-to-get-the-stash-id","text":"git stash show stash@{1}","title":"Show any arbitary stash changes whithout popping. Do stash list first to get the stash ID"},{"location":"learning/git/#apply-any-arbitary-stash-changes-do-stash-list-first-to-get-the-stash-id","text":"git stash apply stash@{1}","title":"Apply any arbitary stash changes. Do stash list first to get the stash ID"},{"location":"learning/git/#drop-any-arbitary-stash-changes-that-was-applied-or-not-needed","text":"git stash drop stash@{1}","title":"Drop any arbitary stash changes that was applied or not needed."},{"location":"learning/git/#stashing-changes-into-a-new-branch-first-see-if-you-have-any-untracked-files-that-also-needs-to-be-saved","text":"git stash -u git stash branch newbranchName","title":"Stashing changes into a new branch. First see if you have any untracked files that also needs to be saved"},{"location":"learning/git/#tagging","text":"","title":"Tagging"},{"location":"learning/git/#create-lightweight-tag","text":"git tag mytag","title":"Create Lightweight tag"},{"location":"learning/git/#list-existing-tags","text":"git tag \u2013list","title":"List existing tags"},{"location":"learning/git/#delete-tag","text":"git tag \u2013delete mytag","title":"Delete tag"},{"location":"learning/git/#create-annotated-tags-it-has-additional-information-like-release-notes","text":"git tag -a v1.0.0 -m \"Release 1.0.0\"","title":"Create Annotated tags (It has additional information like release notes)"},{"location":"learning/git/#comparing-tags","text":"git diff v1.0.0 v1.0.1","title":"Comparing tags"},{"location":"learning/git/#tagging-a-specific-commit-id","text":"git tag -a v0.0.9 -m \"Release 0.0.9\"","title":"Tagging a specific commit ID"},{"location":"learning/git/#updating-an-existing-tag-with-new-commit-id","text":"git tag -a v0.0.9 -f -m \"Correct Release 0.0.9\"","title":"Updating an existing tag with new commit id"},{"location":"learning/git/#pushing-tags-to-remote","text":"git push origin v1.0.0","title":"Pushing tags to remote"},{"location":"learning/git/#pushing-all-local-tags-to-remote","text":"git push origin master \u2013tags","title":"Pushing all local tags to remote"},{"location":"learning/git/#deleting-tags-in-remote-puting-before-tag-name-will-delete-it-from-remote-does-not-delete-tag-from-local","text":"git push origin :v0.0.9","title":"Deleting tags in remote (puting :before tag name will delete it from remote). Does not delete tag from local"},{"location":"learning/git/#reset-head-position","text":"git reset","title":"Reset HEAD position"},{"location":"learning/git/#using-stash-and-branch-combination","text":"","title":"Using Stash and Branch combination"},{"location":"learning/git/#first-stash-the-changes-wip-in-one-brabch-then-checkout-a-new-test-branch-and-then-pop-the-changes-into-this-test-branch","text":"git stash git checkout -b test git stash pop","title":"First stash the changes (WIP) in one brabch, then checkout a new test branch and then pop the changes into this test branch"},{"location":"learning/git/#cherry-pick-hot-fix-scenario","text":"git cherry-pick","title":"Cherry Pick (Hot Fix scenario)"},{"location":"learning/python/","text":"# Range and For for index in range ( 6 ): print ( index ) # Range function is used generate a sequence of integers index = range ( 10 , - 1 , - 1 ) # start, stop and step, stops at 0 not including -1 # set class provides a mapping of unique immutable elements # One use of set is to remove duplicate elements dup_list = ( 'c' , 'd' , 'c' , 'e' ) beta = set ( dup_list ) uniq_list = list ( beta ) # dict class is an associative array of keys and values. keys must be unique immutable objects dict_syn = { 'k1' : 'v1' , 'k2' : 'v2' } dict_syn = dict ( k1 = 'v1' , k2 = 'v2' ) dict_syn [ 'k3' ] = 'v3' # adding new key value del ( dict_syn [ 'k3' ]) # delete key value print ( dict_syn . keys ()) # prints all keys print ( dict_syn . values ()) # prints all values # User Input name = input ( 'Name :' ) # Functions * A function is a piece of code , capable of performing a similar task repeatedly . * It is defined using ** def ** keyword in python . def < function_name > ( < parameter1 > , < parameter2 > , ... ): 'Function documentation' function_body return < value > * Parameters , return expression and documentation string are optional . def square ( n ): \"Returns Square of a given number\" return n ** 2 print ( square . __doc__ ) // prints the function documentation string * 4 types of arguments * Required Arguments : non - keyword arguments def showname ( name , age ) showname ( \"Jack\" , 40 ) // name = \"Jack\" , age = 40 showname ( 40 , \"Jack\" ) // name = 40 , age = \"Jack\" * Keyword Arguments : identified by paramater names def showname ( name , age ) showname ( age = 40 , name = \"Jack\" ) * Default Arguments : Assumes a default argument , if an arg is not passsed . def showname ( name , age = 50 ) showname ( \"Jack\" ) // name = \"Jack\" , age = 50 showname ( age = 40 , \"Jack\" ) // name = \"Jack\" , age = 40 showname ( name = \"Jack\" , age = 40 ) // name = \"Jack\" , age = 40 showname ( name = \"Jack\" , 40 ) // Python does not allow passing non - keyword after keyword arg . This will fail . * Variable Length Arguments : Function preocessed with more arguments than specified while defining the function def showname ( name , * vartuple , ** vardict ) # *vartuple = Variable non keyword argument which will be a tuple. Denoted by * # **vardict = Variable keyword argument which will be a dictionary. Denoted by ** showname ( \"Jack\" ) // name = \"Jack\" showname ( \"Jack\" , 35 , 'M' , 'Kansas' ) // name = \"Jack\" , * vartuple = ( 35 , 'M' , 'Kansas' ) showname ( \"Jack\" , 35 , city = 'Kansas' , sex = 'M' ) // name = \"Jack\" , * vartuple = ( 35 ), ** vardict = { city = 'Kansas' , sex = 'M' } # An Iterator is an object, which allows a programmer to traverse through all the elements of a collection, regardless of its specific implementation. x = [ 6 , 3 , 1 ] s = iter ( x ) print ( next ( s )) # -> 6 # List Comprehensions -> Alternative to for loops. * More concise , readable , efficient and mimic functional programming style . * Used to : Apply a method to all or specific elements of a list , and Filter elements of a list satisfying specific criteria . x = [ 6 , 3 , 1 ] y = [ i ** 2 for i in x ] # List Comprehension expression print ( y ) # -> [36, 9, 1] * Filter positive numbers ( using for and if ) vec = [ - 4 , - 2 , 0 , 2 , 4 ] pos_elm = [ x for x in vec if x >= 0 ] # Can be read as for every elem x in vec, filter x if x is greater than or equal to 0 print ( pos_elm ) # -> [0, 2, 4] * Applying a method to a list def add10 ( x ): return x + 10 n = [ 34 , 56 , 75 , 3 ] mod_n = [ add10 ( num ) for num in n ] print ( mod_n ) # A Generator is a function that produces a sequence of results instead of a single value def arithmatic_series ( a , r ): while a < 50 : yield a # yield is used in place of return which suspends processing a += r s = arithmatic_series ( 3 , 10 ) # Execution of further 'arithmetic series' can be resumed only by calling nextfunction again on generator 's' print ( s ) // Generator #output=3 print ( next ( s )) // Generator starts execution # output=13 print ( next ( s )) // resumed # output=23 # A Generator expresions are generator versions of list comprehensions. They return a generator instead of a list. x = [ 6 , 3 , 1 ] g = ( i ** 2 for i in x ) # generator expression print ( next ( g )) # -> 36 # Dictionary Comprehensions -> takes the form {key: value for (key, value) in iterable} myDict = { x : x ** 2 for x in [ 1 , 2 , 3 , 4 , 5 ]} print ( myDict ) # Output {1: 1, 2: 4, 3: 9, 4: 16, 5: 25} # Calculate the frequency of each identified unique word in the list words = [ 'Hello' , 'Hi' , 'Hello' ] freq = { w : words . count ( w ) for w in words } print ( freq ) # Output {'Hello': 2, 'Hi': 1} Create the dictionary frequent_words , which filter words having frequency greater than one words = [ 'Hello' , 'Hi' , 'Hello' ] freq = { w : words . count ( w ) for w in words if words . count ( w ) > 1 } print ( freq ) # Output {'Hello': 2} # Defining Classes * Syntax class < ClassName > ( < parent1 > , ... ): class_body # Creating Objects * An object is created by calling the class name followed by a pair of parenthesis . class Person : pass p1 = Person () # Creating the object 'p1' print ( p1 ) # -> '<__main__.Person object at 0x0A...>' # tells you what class it belongs to and hints on memory address it is referenced to. # initializer method -> __init__ * defined inside the class and called by default , during an object creation . * It also takes self as the first argument , which refers to the current object . class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname p1 = Person ( 'George' , 'Smith' ) print ( p1 . fname , '-' , p1 . lname ) # -> 'George - Smith' # Documenting a Class * Each class or a method definition can have an optional first line , known as docstring . class Person : 'Represents a person.' # Inheritance * Inheritance describes is a kind of relationship between two or more classes , abstracting common details into super class and storing specific ones in the subclass . * To create a child class , specify the parent class name inside the pair of parenthesis , followed by it 's name. class Child ( Parent ): pass * Every child class inherits all the behaviours exhibited by their parent class . * In Python , every class uses inheritance and is inherited from ** object ** by default . class MySubClass ( object ): # object is known as parent or super class. pass # Inheritance in Action class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname class Employee ( Person ): all_employees = [] def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) # Employee class utilizes __init __ method of the parent class Person to create its object. self . empid = empid Employee . all_employees . append ( self ) e1 = Employee ( 'Jack' , 'simmons' , 456342 ) print ( e1 . fname , '-' , e1 . empid ) # Output -> Jack - 456342 # Polymorphism * Polymorphism allows a subclass to override or change a specific behavior , exhibited by the parent class class Employee ( Person ): all_employees = EmployeesList () def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) self . empid = empid Employee . all_employees . append ( self ) def getSalary ( self ): return 'You get Monthly salary.' def getBonus ( self ): return 'You are eligible for Bonus.' * Definition of ContractEmployee class derived from Employee. It overrides functionality of getSalary and getBonus methods found in it 's parent class Employee. class ContractEmployee ( Employee ): def getSalary ( self ): return 'You will not get Salary from Organization.' def getBonus ( self ): return 'You are not eligible for Bonus.' e1 = Employee ( 'Jack' , 'simmons' , 456342 ) e2 = ContractEmployee ( 'John' , 'williams' , 123656 ) print ( e1 . getBonus ()) # Output - You are eligible for Bonus. print ( e2 . getBonus ()) # Output - You are not eligible for Bonus. # Abstraction * Abstraction means working with something you know how to use without knowing how it works internally . * It is hiding the defaults and sharing only necessary information . # Encapsulation * Encapsulation allows binding data and associated methods together in a unit i . e class . * Bringing related data and methods inside a class to avoid misuse outside . * These principles together allows a programmer to define an interface for applications , i . e . to define all tasks the program is capable to execute and their respective input and output data . * A good example is a television set . We don \u2019 t need to know the inner workings of a TV , in order to use it . All we need to know is how to use the remote control ( i . e the interface for the user to interact with the TV ) . # Abstracting Data * Direct access to data can be restricted by making required attributes or methods private , ** just by prefixing it 's name with one or two underscores.** * An attribute or a method starting with : + ** no underscores ** is a ** public ** one . + ** a single underscore ** is ** private ** , however , still accessible from outside. + ** double underscores ** is ** strongly private ** and not accessible from outside. # Abstraction and Encapsulation Example * ** empid ** attribute of Employee class is made private and is accessible outside the class only using the method ** getEmpid **. class Employee ( Person ): all_employees = EmployeesList () def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) self . __empid = empid Employee . all_employees . append ( self ) def getEmpid ( self ): return self . __empid e1 = Employee ( 'Jack' , 'simmons' , 456342 ) print ( e1 . fname , e1 . lname ) # Output -> Jack simmons print ( e1 . getEmpid ()) # Output -> 456342 print ( e1 . __empid ) # Output -> AttributeError: Employee instance has no attribute '__empid' # Exceptions * Python allows a programmer to handle such exceptions using ** try ... except ** clauses , thus avoiding the program to crash . * Some of the python expressions , though written correctly in syntax , result in error during execution . ** Such scenarios have to be handled .** * In Python , every error message has two parts . The first part tells what type of exception it is and second part explains the details of error . # Handling Exception * A try block is followed by one or more except clauses . * The code to be handled is written inside try clause and the code to be executed when an exception occurs is written inside except clause . try : a = pow ( 2 , 4 ) print ( \"Value of 'a' :\" , a ) b = pow ( 2 , 'hello' ) # results in exception print ( \"Value of 'b' :\" , b ) except TypeError as e : print ( 'oops!!!' ) print ( 'Out of try ... except.' ) Output -> Value of 'a' : 16 --> oops !!! --> Out of try ... except . # Raising Exceptions * ** raise ** keyword is used when a programmer wants a specific exception to occur . try : a = 2 ; b = 'hello' if not ( isinstance ( a , int ) and isinstance ( b , int )): raise TypeError ( 'Two inputs must be integers.' ) c = a ** b except TypeError as e : print ( e ) # User Defined Exception Functions * Python also allows a programmer to create custom exceptions , derived from base Exception class . class CustomError ( Exception ): def __init__ ( self , value ): self . value = value def __str__ ( self ): return str ( self . value ) try : a = 2 ; b = 'hello' if not ( isinstance ( a , int ) and isinstance ( b , int )): raise CustomError ( 'Two inputs must be integers.' ) # CustomError is raised in above example, instead of TypeError. c = a ** b except CustomError as e : print ( e ) # Using 'finally' clause * ** finally ** clause is an optional one that can be used with try ... except clauses . * All the statements under finally clause are executed irrespective of exception occurrence . def divide ( a , b ): try : result = a / b return result except ZeroDivisionError : print ( \"Dividing by Zero.\" ) finally : print ( \"In finally clause.\" ) # Statements inside finally clause are ALWAYS executed before the return back # Using 'else' clause * ** else ** clause is also an optional clause with try ... except clauses . * Statements under else clause are executed ** only when no exception occurs in try clause **. try : a = 14 / 7 except ZeroDivisionError : print ( 'oops!!!' ) else : print ( 'First ELSE' ) try : a = 14 / 0 except ZeroDivisionError : print ( 'oops!!!' ) else : print ( 'Second ELSE' ) Output : First ELSE --> oops !!! # Module * Any file containing logically organized Python code can be used as a module . * A module generally contains ** any of the defined functions , classes and variables **. A module can also include executable code . * Any Python source file can be used as a module by using an import statement in some other Python source file . # Packages * A package is a collection of modules present in a folder . * The name of the package is the name of the folder itself . * A package generally contains an empty file named ** __init__ . py ** in the same folder , which is required to treat the folder as a package . # Import Modules import math # Recommended method of importing a module import math as m from math import pi , tan from math import pi as pie , tan as tangent # Working with Files * Data from an opened file can be read using any of the methods : ** read , readline and readlines **. * Data can be written to a file using either ** write ** or ** writelines ** method . * A file ** must be opened ** , before it is used for reading or writing . fp = open ( 'temp.txt' , 'r' ) # opening ( operations 'r' & 'w') content = fp . read () # reading fp . close () # closing # read() -> Reads the entire contents of a file as bytes. # readline() -> Reads a single line at a time. # readlines() -> Reads a all the line & each line is stored as an element of a list. # write() -> Writes a single string to output file. # writelines() -> Writes multiple lines to output file & each string is stored as an element of a list. * Reading contents of file and storing as a dictionary fp = open ( 'emp_data.txt' , 'r' ) emps = fp . readlines () # Preprocessing data emps = [ emp . strip ( ' \\n ' ) for emp in emps ] emps = [ emp . split ( ';' ) for emp in emps ] header = emps . pop # remove header record separately emps = [ dict ( zip ( header , emp ) for emp in emps ] # header record is used to combine with data to form a dictionary print ( emps [: 2 ]) # prints first 2 records * Filtering data based on criteria fil_emps = [ emp [ 'Emp_name' ] for emp in emps if emp [ 'Emp_work_location' ] == 'HYD' ] * Filtering data based on pattern import re pattern = re . compile ( r 'oracle' , re . IGNORECASE ) # Regular Expression oracle_emps = [ emp [ 'Emp_name' ] for emp in emps if pattern . search ( emp [ 'Emp_skillset' ])] * Filter and Sort data in ascending order fil_emps = [ emp for emp in emps if emp [ 'Emp_designation' ] == 'ASE' ] fil_emps = sorted ( fil_emps , key = lambda k : k [ 'Emp_name' ]) print ( emp [ 'Emp_name' ] for emp in fil_emps ) * Sorting all employees based on custom sorting criteria order = { 'ASE' : 1 , 'ITA' : 2 , 'AST' : 3 } sorted_emp = sorted ( emp , key = lambda k : order [ k [ 'designation' ]]) * Filter data and write into files fil_emps = [ emp for emp in emps if emp [ 'Emp_Designation' ] == 'ITA' ] ofp = open ( outputtext . txt , 'w' ) keys = fil_emps [ 0 ] . keys () # Remove header from key name for key in keys : ofp . write ( key + \" \\t \" ) ofp . write ( \" \\n \" ) for emp in fil_emps : for key in keys : ofp . write ( emp [ key ] + \" \\t \" ) ofp . write ( \" \\n \" ) ofp . close () # Regular Expressions * Regex are useful to construct patterns that helps in filtering the text possessing the pattern . * ** re module ** is used to deal with regex . * ** search ** method takes pattern and text to scan and returns a Match object . Return None if not found . * Match object holds info on the nature of the match like ** original input string , Regular expression used , location within the original string ** match = re . search ( pattern , text ) start_index = match . start () # start location of match end_index = match . end () regex = match . re . pattern () print ( 'Found \" {} \" pattern in \" {} \" from {} to {} ' . format ( st , text , start_index , end_index )) # Compiling Expressions * In Python , its more efficient t compile the patterns that are frequently used . * ** compile ** function of re module converts an expression string into a ** RegexObject **. patterns = [ 'this' , 'that' ] regexes = [ re . compile ( p ) for p in patterns ] for regex in regexes : if regex . search ( text ): # pattern is not required print ( 'Match found' ) * search method only returns the first matching occurrence . # Finding Multiple Matches * findall method returns all the substrings of the pattern without overlapping pattern = 'ab' for match in re . findall ( pattern , text ): print ( 'match found - {} ' . format ( match )) # Grouping Matches * Adding groups to a pattern enables us to isolate parts of the matching text , expanding those capabilities to create a parser . * Groups are defined by enclosing patterns within parenthesis text = 'This is some text -- with punctuations.' for pattern in [ r '^(\\w+)' , # word at the start of the string r '(\\w+)\\S*$' , # word at the end of the string with punctuation r '(\\bt\\w+)\\W+(\\w+)' , # word staring with 't' and the next word r '(\\w+t)\\b' ]: # word ending with t regex = re . compile ( pattern ) match = regex . search ( text ) print ( match . groups ()) # Output -> ('This',) ('punctuations',) ('text','with') ('text',) # Naming Grouped Matches * Accessing the groups with defined names text = 'This is some text -- with punctuations.' for pattern in [ r '^(?P<first_word>\\w+)' , # word at the start of the string r '(?P<last_word>\\w+)\\S*$' , # word at the end of the string with punctuation r '(?P<t_word>\\bt\\w+)\\W+(?P<other_word>\\w+)' , # word staring with 't' and the next word r '(?P<ends_with_t>\\w+t)\\b' ]: # word ending with t regex = re . compile ( pattern ) match = regex . search ( text ) print ( \"Groups: \" , match . groups ()) # Output -> ('This',) ('punctuations',) ('text','with') ('text',) print ( \"Group Dictionary: \" , match . groupdict ()) # Output -> {'first_word':'This'} {'last_word': 'punctuations'} {'t_word':'text', 'other_word':'with'} {'ends_with_t':'text'} # Data Handling # Handling XML files * ** lxml ** 3 rd party module is a highly feature rich with ElementTree API and supports querying wthe xml content using XPATH . * In the ElementTree API , an element acts like a list . The items of the list are the elements children . * XML search is faster in lxml . < ? xml > < employee > < skill name = \"Python\" /> </ employee > from lxml import etree tree = etree . parse ( 'sample.xml' ) root = tree . getroot () # gets doc root <?xml> skills = tree . findall ( '//skill' ) # gets all skill tags for skill in skills : print ( \"Skills: \" , skill . attrib [ 'name' ]) # Adding new skill in the xml skill = etree . SubElement ( root , 'skill' , attrib = { 'name' : 'PHP' }) # Handling HTML files * ** lxml ** 3 rd party module is used for parsing HTML files as well . import urllib.request from lxml import etree def readURL ( url ): urlfile = urllib . request . urlopen ( url ) if urlfile . getcode () == 200 : contents = urlfile . read () return contents if __name__ == '__main__' : url = 'http://xkcd.com' html = readURL ( url ) # Data Serialization * Process of converting ** data types / objects ** into ** Transmittable / Storable ** format is called Data Serialization . * In python , ** pickle and json ** modules are used for Data Serialization . * Serialized data can then be written to file / Socket / Pipe . From these it can be de - serialized and stored into a new Object . json . dump ( data , file , indent = 2 ) # serialized data is written to file with indentation using dump method data_new = json . load ( file ) # de-serialized data is written to new object using load method # Database Connectivity * ** Python Database API ( DB - API ) ** is a standard interface to interact with various databases . * Different DB API \u2019 s are used for accessing different databases . Hence a programmer has to install DB API corresponding to the database one is working with . * Working with a database includes the following steps : + Importing the corresponding DB - API module . + Acquiring a connection with the database . + Executing SQL statements and stored procedures . + Closing the connection import sqlite3 # establishing a database connection con = sqlite3 . connect ( 'D: \\\\ TEST.db' ) # preparing a cursor object cursor = con . cursor () # preparing sql statements sql1 = 'DROP TABLE IF EXISTS EMPLOYEE' # closing the database connection con . close () # Inserting Data * Single rows are inserted using ** execute ** and multiple rows using ** executeMany ** method of created cursor object . # preparing sql statement rec = ( 456789 , 'Frodo' , 45 , 'M' , 100000.00 ) sql = ''' INSERT INTO EMPLOYEE VALUES ( ?, ?, ?, ?, ?) ''' # executing sql statement using try ... except blocks try : cursor . execute ( sql , rec ) con . commit () except Exception as e : print ( \"Error Message :\" , str ( e )) con . rollback () # Fetching Data * ** fetchone ** : It retrieves one record at a time in the form of a tuple . * ** fetchall ** : It retrieves all fetched records at a point in the form of tuple of tuples . # fetching the records records = cursor . fetchall () # Displaying the records for record in records : print ( record ) # Object Relational Mappers * An object - relational mapper ( ORM ) is a library that automates the transfer of data stored in relational database tables into objects that are adopted in application code . * ORMs offer a high - level abstraction upon a relational database , which permits a developer to write Python code rather than SQL to create , read , update and delete data and schemas in their database . * Such an ability to write Python code instead of SQL speeds up web application development . # Higher Order Functions * A ** Higher Order function ** is a function , which is capable of doing any one of the following things : + It can be functioned as a ** data ** and be assigned to a variable . + It can accept any other ** function as an argument **. + It can return a ** function as its result **. * The ability to build Higher order functions , ** allows a programmer to create Closures , which in turn are used to create Decorators **. # Function as a Data def greet (): return 'Hello Everyone!' print ( greet ()) wish = greet # 'greet' function assigned to variable 'wish' print ( type ( wish )) # Output -> <type 'function'> print ( wish ()) # Output -> Hello Everyone! # Function as an Argument def add ( x , y ): return x + y def sub ( x , y ): return x - y def prod ( x , y ): return x * y def do ( func , x , y ): return func ( x , y ) print ( do ( add , 12 , 4 )) # 'add' as arg # Output -> 16 print ( do ( sub , 12 , 4 )) # 'sub' as arg # Output -> 8 print ( do ( prod , 12 , 4 )) # 'prod' as arg # Output -> 48 # Returning a Function def outer (): def inner (): s = 'Hello world!' return s return inner () print ( outer ()) # Output -> Hello world! * You can observe from the output that the ** return value of 'outer' function is the return value of 'inner' function ** i . e 'Hello world!' . def outer (): def inner (): s = 'Hello world!' return s return inner # Removed '()' to return 'inner' function itself print ( outer ()) #returns 'inner' function # Output -> <function inner at 0xxxxxx> func = outer () print ( type ( func )) # Output -> <type 'function'> print ( func ()) # calling 'inner' function # Output -> Hello world! * Parenthesis after the ** inner ** function are removed so that the ** outer ** function returns ** inner function **. # Closures * A Closure is a ** function returned by a higher order function ** , whose return value depends on the data associated with the higher order function . def multiple_of ( x ): def multiple ( y ): return x * y return multiple c1 = multiple_of ( 5 ) # 'c1' is a closure c2 = multiple_of ( 6 ) # 'c2' is a closure print ( c1 ( 4 )) # Output -> 5 * 4 = 20 print ( c2 ( 4 )) # Output -> 6 * 4 = 24 * The first closure function , c1 binds the value 5 to argument x and when called with an argument 4 , it executes the body of multiple function and returns the product of 5 and 4. * Similarly c2 binds the value 6 to argument x and when called with argument 4 returns 24. # Decorators * Decorators are evolved from the concept of closures . * A decorator function is a higher order function that takes a function as an argument and returns the inner function . * A decorator is capable of adding extra functionality to an existing function , without altering it . * The decorator function is prefixed with **@ symbol ** and written above the function definition . + Shows the creation of closure function wish using the higher order function outer . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner def greet (): print ( 'Hello!' ) wish = outer ( greet ) # Output -> Accessing : greet wish () # Output -> Hello! - wish is the closure function obtained by calling an outer function with the argument greet . When wish function is called , inner function gets executed . + The second one shows the creation of decorator function outer , which is used to decorate function greet . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner def greet (): return 'Hello!' greet = outer ( greet ) # decorating 'greet' # Output -> No Output as return is used instead of print greet () # calling new 'greet' # Output -> Accessing : greet - The function returned by outer is assigned to greet i . e the function name passed as argument to outer . This makes outer a decorator to greet . + Third one displays decorating the greet function with decorator function , outer , using @ symbol . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner @outer # This is same as **greet = outer(greet)** def greet (): return 'Hello!' greet () # Output -> Accessing : greet # Descriptors * Python descriptors allow a programmer to create managed attributes . * In other object - oriented languages , you will find ** getter and setter ** methods to manage attributes . * However , Python allows a programmer to manage the attributes simply with the attribute name , without losing their protection . * This is achieved by defining a ** descriptor class ** , that implements any of ** __get__ , __set__ , __delete__ ** methods . class EmpNameDescriptor : def __get__ ( self , obj , owner ): return self . __empname def __set__ ( self , obj , value ): if not isinstance ( value , str ): raise TypeError ( \"'empname' must be a string.\" ) self . __empname = value * The descriptor , EmpNameDescriptor is defined to manage empname attribute . It checks if the value of empname attribute is a string or not . class EmpIdDescriptor : def __get__ ( self , obj , owner ): return self . __empid def __set__ ( self , obj , value ): if hasattr ( obj , 'empid' ): raise ValueError ( \"'empid' is read only attribute\" ) if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value * The descriptor , EmpIdDescriptor is defined to manage empid attribute . class Employee : empid = EmpIdDescriptor () empname = EmpNameDescriptor () def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name * Employee class is defined such that , it creates empid and empname attributes from descriptors EmpIdDescriptor and EmpNameDescriptor . e1 = Employee ( 123456 , 'John' ) print ( e1 . empid , '-' , e1 . empname ) # Output -> '123456 - John' e1 . empid = 76347322 # Output -> ValueError: 'empid' is read only attribute # Properties * Descriptors can also be created using property () type . + Syntax : property ( fget = None , fset = None , fdel = None , doc = None ) - where , fget : attribute get method fset : attribute set method fdel \u2013 attribute delete method doc \u2013 docstring class Employee : def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name def getEmpID ( self ): return self . __empid def setEmpID ( self , value ): if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value empid = property ( getEmpID , setEmpID ) # Property Decorators * Descriptors can also be created with property decorators . * While using property decorators , an attribute 's get method will be same as its name and will be decorated with property. * In a case of defining any set or delete methods , they will be decorated with respective setter and deleter methods . class Employee : def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name @property def empid ( self ): return self . __empid @empid . setter def empid ( self , value ): if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value e1 = Employee ( 123456 , 'John' ) print ( e1 . empid , '-' , e1 . empname ) # Output -> '123456 - John' # Introduction to Class and Static Methods Based on the ** scope ** , functions / methods are of two types . They are : * Class methods * Static methods # Class Methods * A method defined inside a class is bound to its object , by default . * However , if the method is bound to a Class , then it is known as ** classmethod **. class Circle ( object ): no_of_circles = 0 def __init__ ( self , radius ): self . __radius = radius Circle . no_of_circles += 1 def getCirclesCount ( self ): return Circle . no_of_circles c1 = Circle ( 3.5 ) c2 = Circle ( 5.2 ) c3 = Circle ( 4.8 ) print ( c1 . getCirclesCount ()) # -> 3 print ( Circle . getCirclesCount ( c3 )) # -> 3 print ( Circle . getCirclesCount ()) # -> TypeError: getCirclesCount() missing 1 required positional argument: 'self' class Circle ( object ): no_of_circles = 0 def __init__ ( self , radius ): self . __radius = radius Circle . no_of_circles += 1 @classmethod def getCirclesCount ( self ): return Circle . no_of_circles c1 = Circle ( 3.5 ) c2 = Circle ( 5.2 ) c3 = Circle ( 4.8 ) print ( c1 . getCirclesCount ()) # -> 3 print ( Circle . getCirclesCount ()) # -> 3 # Static Method * A method defined inside a class and not bound to either a class or an object is known as ** Static ** Method . * Decorating a method using ** @staticmethod ** decorator makes it a static method . def square ( x ): return x ** 2 class Circle ( object ): def __init__ ( self , radius ): self . __radius = radius def area ( self ): return 3.14 * square ( self . __radius ) c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 print ( square ( 10 )) # -> 100 * square function is not packaged properly and does not appear as integral part of class Circle . class Circle ( object ): def __init__ ( self , radius ): self . __radius = radius @staticmethod def square ( x ): return x ** 2 def area ( self ): return 3.14 * self . square ( self . __radius ) c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 print ( square ( 10 )) # -> NameError: name 'square' is not defined * square method is no longer accessible from outside the class Circle . * However , it is possible to access the static method using Class or the Object as shown below . print ( Circle . square ( 10 )) # -> 100 print ( c1 . square ( 10 )) # -> 100 # Abstract Base Classes * An ** Abstract Base Class ** or ** ABC ** mandates the derived classes to implement specific methods from the base class . * It is not possible to create an object from a defined ABC class . * Creating objects of derived classes is possible only when derived classes override existing functionality of all abstract methods defined in an ABC class . * In Python , an Abstract Base Class can be created using module abc . from abc import ABC , abstractmethod class Shape ( ABC ): @abstractmethod def area ( self ): pass @abstractmethod def perimeter ( self ): pass * Abstract base class Shape is defined with two abstract methods area and perimeter . class Circle ( Shape ): def __init__ ( self , radius ): self . __radius = radius @staticmethod def square ( x ): return x ** 2 def area ( self ): return 3.14 * self . square ( self . __radius ) def perimeter ( self ): return 2 * 3.14 * self . __radius c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 # Context Manager * A Context Manager allows a programmer to perform required activities , automatically , while entering or exiting a Context . * For example , opening a file , doing few file operations , and closing the file is manged using Context Manager as shown below . with open ( 'sample.txt' , 'w' ) as fp : content = fp . read () * The keyword ** with ** is used in Python to enable a context manager . It automatically takes care of closing the file . import sqlite3 class DbConnect ( object ): def __init__ ( self , dbname ): self . dbname = dbname def __enter__ ( self ): self . dbConnection = sqlite3 . connect ( self . dbname ) return self . dbConnection def __exit__ ( self , exc_type , exc_val , exc_tb ): self . dbConnection . close () with DbConnect ( 'TEST.db' ) as db : cursor = db . cursor () ''' Few db operations ... ''' * Example from contextlib import contextmanager @contextmanager def context (): print ( 'Entering Context' ) yield print ( \"Exiting Context\" ) with context (): print ( 'In Context' ) # Output -> Entering Context -> In Context -> Exiting Context # Coroutines * A Coroutine is ** generator ** which is capable of constantly receiving input data , process input data and may or may not return any output . * Coroutines are majorly used to build better ** Data Processing Pipelines **. * Similar to a generator , execution of a coroutine stops when it reaches ** yield ** statement . * A Coroutine uses ** send ** method to send any input value , which is captured by yield expression . def TokenIssuer (): tokenId = 0 while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) t = TokenIssuer () next ( t ) t . send ( 'George' ) # -> Token number of George: 1 t . send ( 'Rosy' ) # -> Token number of Rosy: 2 * ** TokenIssuer ** is a coroutine function , which uses yield to accept name as input . * Execution of coroutine function begins only when next is called on coroutine t . * This results in the execution of all the statements till a yield statement is encountered . * Further execution of function resumes when an input is passed using send , and processes all statements till next yield statement . def TokenIssuer ( tokenId = 0 ): try : while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) except GeneratorExit : print ( 'Last issued Token is :' , tokenId ) t = TokenIssuer ( 100 ) next ( t ) t . send ( 'George' ) # Token number of George: 101 t . send ( 'Rosy' ) # Token number of Rosy: 102 t . send ( 'Smith' ) # Token number of Smith: 103 t . close () # Last issued Token is: 103 * The coroutine function TokenIssuer takes an argument , which is used to set a starting number for tokens . * When coroutine t is closed , statements under GeneratorExit block are executed . * Many programmers may forget that passing input to coroutine is possible only after the first next function call , which results in error . * Such a scenario can be avoided using a decorator . def coroutine_decorator ( func ): def wrapper ( * args , ** kwdargs ): c = func ( * args , ** kwdargs ) next ( c ) return c return wrapper @coroutine_decorator def TokenIssuer ( tokenId = 0 ): try : while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) except GeneratorExit : print ( 'Last issued Token is :' , tokenId ) t = TokenIssuer ( 100 ) t . send ( 'George' ) t . send ( 'Rosy' ) t . send ( 'Smith' ) t . close () * coroutine_decorator takes care of calling next on the created coroutine t . def nameFeeder (): while True : fname = yield print ( 'First Name:' , fname ) lname = yield print ( 'Last Name:' , lname ) n = nameFeeder () next ( n ) n . send ( 'George' ) n . send ( 'Williams' ) n . send ( 'John' ) First Name : George Last Name : Williams First Name : John","title":"Python"},{"location":"learning/terraform/","text":"Example using Docker images \u00b6 // Main.tf resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image.nginx.latest name = \"webserver\" ports { internal = 80 external = 8050 } } Infrastructure as Code \u00b6 IaC is an important DevOps cornerstone that enables you to define, automatically manage, and provision infrastructure through source code. Infrastructure is managed as a software system . IaC is frequently referred to as Programmable Infrastructure . Benefits of Iac \u00b6 In IaC, A tool will monitor the state of the infrastructure . A script will be sent automatically to fix the issue . A script can be written to add new instances, and it can be reused . Faster process , no/less human involvement. IaC Implemetation Approaches \u00b6 Declarative \u00b6 Focuses on the desired end state of infrastructure (Functional) . Tools perform the necessary actions to reach that state . Automatically takes care of the order and executes it . Examples are Terraform and CloudFormation. Imperative \u00b6 Focuses on how to achieve the desired state (Procedural) . Follows the order of tasks and it executes in that order . Examples are Chef and Ansible. Configuration Management \u00b6 It is a system that keeps the track of organization's hardware, software, and other related information. It includes the software updates and versions present in the system. Configuration management is also called as configuration control . E.g.: Chef and Ansible Orchestration: \u00b6 It automates the coordination, management, and configuration of systems and hardware. E.g.: Terraform and Nomad Installing Terraform \u00b6 sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip unzip terraform_0.11.11_linux_amd64.zip sudo mv terraform /usr/local/bin/ terraform \u2013version Terraform Lifecycle \u00b6 Terraform init - Initializes the working directory having Terraform configuration files. Terraform plan - Creates an execution plan and it mentions the changes it is going to make. Terraform apply - When you are not amazed by seeing the results of the terraform plan, you can go ahead and run terraform apply to apply the changes. The main difference between plan and apply is - apply asks for a prompt like Do you want to perform these actions? and then implement the changes, whereas, plan shows the possible end state of your infrastructure without actually implementing them. Terraform destroy - Destroys the created infrastructure. Terraform Configuration \u00b6 A set of files that describe the infrastructure in Terraform is called as Terraform Configuration. The entire configuration file is saved with .tf extension. Make sure to have all the configurations maintained in a single .tf file instead of many. Terraform loads all the .tf files present in the working directory. Creating Virtual Network \u00b6 create a virtual network without using the Azure portal and azure CLI commands. resource \"azurerm_virtual_network\"\"myterraformnetwork\" { name = \"myvnet\" address_space = [\"10.0.0.0/16\"] location=\"East US\" resource_group_name=\"er-tyjy\" } Resource blocks define the infrastructure (resource \"azurerm_virtual_network\" \"myterraformnetwork\"). It contains the two strings - type of the resource(azurerm_virtualnetwork) and name of the resource(myterraformnetwork). Terraform uses plugin based architecture to support the various services providers and infrastructure available. When you run Terraform init command, it downloads and installs provider binary for the corresponding providers. In this case, it is Azure. Now, you can run the terraform plan command. It shows all the changes it is going to make. If the output is as you expected and there are no errors, then you can run Terraform apply. Terraform Validate \u00b6 You can run this command whenever you like to check whether the code is correct. Once you run this command, if there is no output, it means that there are no errors in the code. Variables \u00b6 Terraform provides the following variable types: Strings - The default data type is a string . List - A list is like simple arrays Map - A map value is nothing but a lookup table for string keys to get the string values. In terraform, to reduce the complexity of code and for easiness , all the variables are created in the variables.tf file. variables.tf variable \"rg\" { default = \"terraform-lab2\" } variable \"locations\" { default = [\"ap-southeast-1\" , \"ap-southeast-2\"] } variable \"tags\" { type = \"map\" default = { environment = \"training\" source=\"citadel\" } } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-east-2 = \"image-4321\" } } For example, you have defined resource_group name in the variable.tf file. you can call them in main.tf file like this \"${var.resource_group}\" Sensitive Parameters \u00b6 There may be sensitive parameters which should be shown only when running the terraform plan but not terraform apply. output \"sensitiveoutput\" { sensitive = true value = VALUE } When you run terraform apply, the output is labeled as sensitive. If there is any sensitive information, then you can protect it by using a sensitive parameter. terraform.tfvars File \u00b6 In terraform.tfvars file, we mention the \"key/value\" pair. The main.tf file takes the input from a terraform. tfvars file. If it is not present, it gets the input from variables.tf file. resource_group = \"user-ybje\" location = \"East US\" terraform fmt \u00b6 It rewrites the confguration files to canonical style and format. State File - terraform.tfstate \u00b6 It is a JSON file that maps the real world resources to the configuration files. In terraform, there are three states. Desired state - Represents how the infrastructure to be present. Actual state - Represents the current state of infrastructure. Known state - To bridge desired state and actual state, there is a known state. You can get the details of known state from a tfstate file. When you run terraform plan command , terraform performs a quick refresh and lists the actions needed to achieve the desired state. When terraform apply command is run, it applies the changes and updates the actual state. Modules \u00b6 A module is like a reusable blueprint of infrastructure . A module is nothing but a folder of Terraform files. In terraform the modules are categorized into two types: Root modules - The current directory of Terraform files on which you are running the commands. Child modules - The modules sourced by the root module. To create a child module, add resources to it. Module syntax: module \"child\" { source = \"../child\" } The difference between adding the resources and module is for adding the resource (Eg: resource \"azurerm_virtual_network\" 'test\") you need type and name but for adding a module (Eg: module \"child\") only the name is enough. The name of the module should be unique within configurations because it is used as a reference to the module and outputs. main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } Updates \u00b6 terraform get - This command is used to download and install the modules for that configuration. terraform get -update - It checks the downloaded modules and checks for the recent versions if any. Module Outputs \u00b6 If you need output from any module, you can get it by passing the required parameter. For example, the output of virtual network location can be found by following the below syntax. You can add this syntax to .tf file(main.tf or variables.tf) . output \"virtual_network_id\" { value = \"${azurerm_virtual_network.test.location}\" } Where virtual_network_id is a unique name and \"${azurerm_virtual_network.test.location}\" is the location of the virtual network using string interpolation. Benefits of Modules \u00b6 Code reuse : When there is a need to provision the group of resources on another resource at the same time, a module can be used instead of copying the same code. It helps in resolving the bugs easily. If you want to make changes, changing the code in one place is enough. Abstraction layer : It makes complex configurations easier to conceptualize. For example, if you like to add vault(Another hashicorp's tool for managing secrets) cluster to the environment, it requires dozens of components. Instead of worrying about individual components, it gives ready-to-use vault cluster. Black box : To create a vault, you only need some configuration parameters without worrying about the complexity. You don't need any knowledge on how to install vault, configuring vault cluster and working of the vault. You can create a working cluster in a few minutes. Best practices in organization : When a module is created, you can give it to the other teams. It helps in easily developing the infrastructure. **Versioned artifacts: They are immutable artifacts and can be promoted from one environment to others. Introduction to Meta Parameters \u00b6 There are some difficulties with the declarative language. For example, since it is a declarative language, there is no concept of for loops. How do you repeat a piece of code without copy paste? * Terraform provides primitives like meta parameter called as count, life cycle blocks like create_before_destroy, ternary operator and a large number of interpolation functions . This helps in performing certain types of loops, if statements and zero downtime deployments. Count \u00b6 Count - It defines how many parameters you like to create. Modify main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } To increase Vnets to 5, update count = 5 . To decrease Vnets to 2, update count = 2 , last 3 Vnets will get deleted. Elements \u00b6 Suppose you like to create the three virtual networks with names vnet-A, vnet-B and vnet-C, you can do this easily with the help of list . Mention how many vnets you are going to create with the help of list and define it in variables.tf file variable \"name\" { type= \"list\" default = [\"A\",\"B\",\"C\"] } You can call this variable in the main.tf file in the following way count = \"${length(var.name)}\" - It returns number of elements present in the list. you should store it in meta parameter count. \"${element(var.name,count.index)}\" - It acts as a loop, it takes the input from list and repeats until there are no elements in list. variables.tf ** variable \"resource_group\" { default = \"user-nuqo\" } variable \"location\" { default = \"East US\" } variable \"name\" { type = \"list\" default = [\"A\",\"B\",\"C\"] } **main.tf resource \"azurerm_virtual_network\"\"multiple\" { name = \"vnet-${element(var.name,count.index)}\" resource_group_name = \"${var.resource_group}\" location = \"${var.location}\" address_space=[\"10.0.0.0/16\"] count=\"${length(var.name)}\" } Conditions \u00b6 For example, a vnet has to be created only, if the variable number of vnets is 3 or else no. You can do this by using the ternary operator. variables.tf variable \"no_of_vnets\" { default = 3 } main.tf count = \"${var.no_of_vnets == 3 ? 1 : 0}\" * First, it will execute the ternary operator. If it is true, it takes the output as 1 or else 0. * Now, in the above case, the output is 1, the count becomes one, and the vnet is created. Inheriting Variables \u00b6 Inheriting the variables between modules keeps your Terraform code DRY. Don't Repeat Yourself (DRY) It aims at reducing repeating patterns and code. There is no need to write the duplicate code for each environment you are deploying. You can write the minimum code and achieve the goal by having maximum re-usability. Module File Structure \u00b6 You will follow the below file structure to inherit the variables between modules. modules | |___mod | |__mod.tf | |___vnet |__vnet.tf |__vars.tf There is one vnet and you can configure the vnet by passing the parameters from module. The variables in the vnet are overriden by the values provided in mod.tf file. Create a folder with name Modules Create two subfolders of names vnet and mod in the nested_modules folder. In the vnet folder, create a virtual network using Terraform configuration. Maintain two files one for main configuration(vnet.tf) and other for declaring variables(vars.tf) . Now, create main.tf file like this resource \"azurerm_virtual_network\"\"vnet\" { name = \"${var.name}\" address_space = [\"10.0.0.0/16\"] resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" } Now, vars.tf file looks like this variable \"name\" { description = \"name of the vnet\" } variable \"resourcegroup\" { description = \"name of the resource group\" } variable \"location\" { description =\"name of the location\" } variable \"address\" { type =\"list\" description = \"specify the address\" } Create a folder with name mod in the modules directory. Create a file with name mod.tf and write the below code in it. We are passing the variables from the root module (mod) to the child module (vnet) using source path . module \"child\" { source = \"../vnet\" name = \"modvnet\" resourcegroup = \"user-vcom\" location = \"eastus\" address = \"10.0.1.0/16\" } Run terraform plan to visualize how the directories will be created. You can see the name of the vnet is taken from the module file instead of getting it from vars.tf Nested Modules \u00b6 For any module, root module should exist. Root module is basically a directory that holds the terraform configuration files for the desired infrastructure. They also provide the entry point to the nested modules you are going to utilize. For any module main.tf, vars.tf, and outputs.tf naming conventions are recommended. If you are using nested modules, create them under the subdirectory with name modules . checking_modules |__modules | |____main.tf | |______virtual_network | |_main.tf | |_vars.tf | |______storage_account |_main.tf |_vars.tf You will create three folders with names modules, virtual_network, and storage account. Make sure that you are following the terraform standards like writing the terraform configuration in the main file and defining the variables alone in the variables(vars.tf) file. stoacctmain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"${var.name}\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"${var.account_replication}\" account_tier = \"${var.account_tier}\" } stoacctvars.tf variable \"rg\" { default = \"user-wmle\" } variable \"loc\" { default = \"eastus\" } variable \"name\" { default =\"storagegh\" } variable \"account_replication\" { default = \"LRS\" } variable \"account_tier\" { default = \"Basic\" } Root Module (main.tf) file under checking_modules module \"vnet\" { source = \"../virtual_network\" name = \"mymodvnet\" } module \"stoacct\" { source = \"../storage_account\" name = \"mymodstorageaccount\" } Not only the parameters mentioned above, but you can also pass any parameter from root module to the child modules. You can pass the parameters from the module to avoid the duplicity between them. Remote Backends \u00b6 It is better to store the code in a central repository . However, is having some disadvantages of doing so: You have to maintain push, pull configurations . If one pushes the wrong configuration and commits it, it becomes a problem. State file consists of sensitive information . So state files are non-editable. Backend in terraform explains how the state file is loaded and operations are executed. Backend can get initialize only after running terraform init command. So, terraform init is required to be run every time when backend is configured when any changes made to the backend when the backend configuration is removed completely Terraform cannot perform auto-initialize because it may require additional info from the user, to perform state migrations, etc.. Below are the steps which you will be maintaining for creating remote backend in Azure. Create a storage account Create a storage container Create a backend Get the storage account access key and resource group name, give it as a parameter to the backend. Below is the complete file structure for sample backend. Backend |____stoacct.tf |____stocontainer.tf |____backend.tf |____vars.tf |____output.tf stoacct.tf resource \"azurerm_storage_account\" \"storageaccount\" { name = \"storageaccountname\" resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" account_tier = \"${var.accounttier}\" account_replication_type = \"GRS\" tags { environment = \"staging\" } } stocontainer.tf resource \"azurerm_storage_container\" \"storagecontainer\" { name = \"vhds\" resource_group_name = \"${var.resourcegroup}\" storage_account_name = \"${azurerm_storage_account.storageaccount.name}\" container_access_type = \"private\" } vars.tf variable \"resourcegroup\" { default = \"user-abcd\" } variable \"location\" { default = \"eastus\" } variable \"accounttier\" { default = \"Basic\" } output.tf output \"storageacctname\" { value = \"${azurerm_storage_account.storageaccount.name}\" } output \"storageacctcontainer\" { value = \"${azurerm_storage_account.storagecontainer.name}\" } output \"access_key\" { value = \"${azurerm_storage_account.storageaccount.primary_access_key\" } Before creating backend, run the command to get the storage account keys list( az storage account keys list \u2013account-name storageacctname ) and copy one of the key somewhere. It is useful for configuring the backend. backend.tf terraform{ backend \"azurerm\" { storage_account_name = \"storageacct21\" container_name = \"mycontainer\" key = \"terraform.tfstate.change\" resource_group_name = \"user-okvt\" access_key = \"aJlf+XjZhxwRp4gsU4hkGrQJzO7xBzz7B9rSzMLB/ozwcM6k/1N.......==\" } } where Parameters: storage_account_name - name of the storage account. container_name - name of the container. key- name of the tfstate blob. resource_group_name - name of the resource group. access_key - Storage account access key(any one). Points to Remember \u00b6 You cannot use interpolation syntax to configure backends. After creating backend run terraform init, it will be in the locked state. By running terraform apply command automatically, the lease status is changed to locked. After it is applied, it will come to an unlocked state. This backend supports consistency checking and state locking using the native capabilities of the Azure storage account. Terragrunt \u00b6 Terragrunt is referred to as a thin wrapper for Terraform which provides extra tools to keep the terraform configurations DRY, manage remote state and to work with multiple terraform modules. Terragrunt Commands \u00b6 terragrunt get terragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt supports the following use cases: Keeps terraform code DRY. Remote state configuration DRY. CLI flags DRY. Executing terraform commands on multiple modules at a time. Advantages of using terragrunt: Provides locking mechanism. Allows you to use remote state always. Build-In Functions \u00b6 lookup \u00b6 This helps in performing a dynamic lookup into a map variable. The syntax for lookup variable is lookup(map,key, [default]) . map: The map parameter is a variable like var.name. key: The key parameter includes the key from where it should find the environment. If the key doesn't exist in the map, interpolation will fail, if it doesn't find a third argument (default). The lookup will not work on nested lists or maps. It only works on flat maps. Local Values \u00b6 Local values help in assigning names to the expressions , which can be used multiple times within a module without rewriting it. Local values are defined in the local blocks. It is recommended to group the logically related local values together as a single block, if there is a dependency between them. locals { service_name = \"Fresco\" owner = \"Team\" } Example locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } If a single value or a result is used in many places and it is likely to be changed in the future, then you can go with locals. It is recommended to not use many local values because it makes the read configurations hard to the future maintainers. Data Source \u00b6 Data source allows the data to be computed or fetched for use in Terraform configuration. Data sources allow the terraform configurations to build on the information defined outside the Terraform or by another Terraform configuration. Providers play a major role in implementing and defining data sources. Data source helps in two major ways: It provides a read-only view for the pre-existing data. It can compute new values on the fly. Every data source in the Terraform is mapped to the provider depending on the longest-prefix-matching . data \"azurerm_resource_group\" \"passed\" { name = \"${var.resource_group_name}\" } Concat and Contains \u00b6 concat(list1,list2) * It combines two or more lists into a single list. contains(list, element) * It returns true if the element is present in the list or else false. E.g. contains(var.list_of_strings, \"an_element\") Workspaces \u00b6 Every terraform configurations has associate backends which defines how the operations are executed and where the persistent data like terraform state are stored. Persistent data stored in backend belongs to a workspace. Previously, the backend has only one workspace which is called as default and there will be only one state file associated with the configuration. Some of the backends support multiple workspaces, i.e. It allows multiple state files to be stored within a single configuration. The configuration is still having only one backend, and the multiple distinct instances can be deployed without configuring to a new backend or by changing authentication credentials. default workspace is special because you can't delete the default workspace. You cannot delete your active workspace (the workspace in which you are currently working). If you haven't created the workspace before, then you are working on the default workspace. Workspace Commands \u00b6 terraform workspace new - It creates the workspace with specified name and switches to it. terraform workspace list - It lists the workspaces available. terraform workspace select - It switches to the specified workspace. terraform workspace delete - It deletes the mentioned workspace. Configuring Multiple Providers \u00b6 Below is the file structure that you will maintain for working with multiple providers. multiple_providers | |___awsmain.tf | |___azuremain.tf | |___vars.tf | |___out.tf Create an S3 bucket in Azure which helps in storing the file in AWS. awsmain.tf resource \"aws_s3_bucket\" \"b\" { bucket = \"my-tf-test-bucket-21\" acl = \"private\" tags = { Name = \"My bucket test\" Environment = \"Dev\" } } Resource has two parameters: name(aws_s3_bucket) and type(\"b\"). Name of the resource may vary from provider to provider. Type of the resource depends on the user. Bucket indicates the name of the bucket. If it is not assigned, terraform assigns a random unique name. acl (access control list) - It helps to manage access to the buckets and objects. tag - It is a label which is assigned to the AWS resource by you or AWS. Environment - On which environment do you like to deploy S3 bucket (Dev, Prod or Stage). azuremain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"storageacct21\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"LRS\" account_tier = \"Standard\" } Parameters name - It indicates the name of the storage account. resource_group_name and location- name of the resource group and location. account_replication_type - Type of the account replication. account_tier - Account tier type out.tf output \"account_tier\" { value = \"azurerm_storage_Account.storagecct.account_tier\" }","title":"Terraform"},{"location":"learning/terraform/#example-using-docker-images","text":"// Main.tf resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image.nginx.latest name = \"webserver\" ports { internal = 80 external = 8050 } }","title":"Example using Docker images"},{"location":"learning/terraform/#infrastructure-as-code","text":"IaC is an important DevOps cornerstone that enables you to define, automatically manage, and provision infrastructure through source code. Infrastructure is managed as a software system . IaC is frequently referred to as Programmable Infrastructure .","title":"Infrastructure as Code"},{"location":"learning/terraform/#benefits-of-iac","text":"In IaC, A tool will monitor the state of the infrastructure . A script will be sent automatically to fix the issue . A script can be written to add new instances, and it can be reused . Faster process , no/less human involvement.","title":"Benefits of Iac"},{"location":"learning/terraform/#iac-implemetation-approaches","text":"","title":"IaC Implemetation Approaches"},{"location":"learning/terraform/#declarative","text":"Focuses on the desired end state of infrastructure (Functional) . Tools perform the necessary actions to reach that state . Automatically takes care of the order and executes it . Examples are Terraform and CloudFormation.","title":"Declarative"},{"location":"learning/terraform/#imperative","text":"Focuses on how to achieve the desired state (Procedural) . Follows the order of tasks and it executes in that order . Examples are Chef and Ansible.","title":"Imperative"},{"location":"learning/terraform/#configuration-management","text":"It is a system that keeps the track of organization's hardware, software, and other related information. It includes the software updates and versions present in the system. Configuration management is also called as configuration control . E.g.: Chef and Ansible","title":"Configuration Management"},{"location":"learning/terraform/#orchestration","text":"It automates the coordination, management, and configuration of systems and hardware. E.g.: Terraform and Nomad","title":"Orchestration:"},{"location":"learning/terraform/#installing-terraform","text":"sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip unzip terraform_0.11.11_linux_amd64.zip sudo mv terraform /usr/local/bin/ terraform \u2013version","title":"Installing Terraform"},{"location":"learning/terraform/#terraform-lifecycle","text":"Terraform init - Initializes the working directory having Terraform configuration files. Terraform plan - Creates an execution plan and it mentions the changes it is going to make. Terraform apply - When you are not amazed by seeing the results of the terraform plan, you can go ahead and run terraform apply to apply the changes. The main difference between plan and apply is - apply asks for a prompt like Do you want to perform these actions? and then implement the changes, whereas, plan shows the possible end state of your infrastructure without actually implementing them. Terraform destroy - Destroys the created infrastructure.","title":"Terraform Lifecycle"},{"location":"learning/terraform/#terraform-configuration","text":"A set of files that describe the infrastructure in Terraform is called as Terraform Configuration. The entire configuration file is saved with .tf extension. Make sure to have all the configurations maintained in a single .tf file instead of many. Terraform loads all the .tf files present in the working directory.","title":"Terraform Configuration"},{"location":"learning/terraform/#creating-virtual-network","text":"create a virtual network without using the Azure portal and azure CLI commands. resource \"azurerm_virtual_network\"\"myterraformnetwork\" { name = \"myvnet\" address_space = [\"10.0.0.0/16\"] location=\"East US\" resource_group_name=\"er-tyjy\" } Resource blocks define the infrastructure (resource \"azurerm_virtual_network\" \"myterraformnetwork\"). It contains the two strings - type of the resource(azurerm_virtualnetwork) and name of the resource(myterraformnetwork). Terraform uses plugin based architecture to support the various services providers and infrastructure available. When you run Terraform init command, it downloads and installs provider binary for the corresponding providers. In this case, it is Azure. Now, you can run the terraform plan command. It shows all the changes it is going to make. If the output is as you expected and there are no errors, then you can run Terraform apply.","title":"Creating Virtual Network"},{"location":"learning/terraform/#terraform-validate","text":"You can run this command whenever you like to check whether the code is correct. Once you run this command, if there is no output, it means that there are no errors in the code.","title":"Terraform Validate"},{"location":"learning/terraform/#variables","text":"Terraform provides the following variable types: Strings - The default data type is a string . List - A list is like simple arrays Map - A map value is nothing but a lookup table for string keys to get the string values. In terraform, to reduce the complexity of code and for easiness , all the variables are created in the variables.tf file. variables.tf variable \"rg\" { default = \"terraform-lab2\" } variable \"locations\" { default = [\"ap-southeast-1\" , \"ap-southeast-2\"] } variable \"tags\" { type = \"map\" default = { environment = \"training\" source=\"citadel\" } } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-east-2 = \"image-4321\" } } For example, you have defined resource_group name in the variable.tf file. you can call them in main.tf file like this \"${var.resource_group}\"","title":"Variables"},{"location":"learning/terraform/#sensitive-parameters","text":"There may be sensitive parameters which should be shown only when running the terraform plan but not terraform apply. output \"sensitiveoutput\" { sensitive = true value = VALUE } When you run terraform apply, the output is labeled as sensitive. If there is any sensitive information, then you can protect it by using a sensitive parameter.","title":"Sensitive Parameters"},{"location":"learning/terraform/#terraformtfvars-file","text":"In terraform.tfvars file, we mention the \"key/value\" pair. The main.tf file takes the input from a terraform. tfvars file. If it is not present, it gets the input from variables.tf file. resource_group = \"user-ybje\" location = \"East US\"","title":"terraform.tfvars File"},{"location":"learning/terraform/#terraform-fmt","text":"It rewrites the confguration files to canonical style and format.","title":"terraform fmt"},{"location":"learning/terraform/#state-file---terraformtfstate","text":"It is a JSON file that maps the real world resources to the configuration files. In terraform, there are three states. Desired state - Represents how the infrastructure to be present. Actual state - Represents the current state of infrastructure. Known state - To bridge desired state and actual state, there is a known state. You can get the details of known state from a tfstate file. When you run terraform plan command , terraform performs a quick refresh and lists the actions needed to achieve the desired state. When terraform apply command is run, it applies the changes and updates the actual state.","title":"State File - terraform.tfstate"},{"location":"learning/terraform/#modules","text":"A module is like a reusable blueprint of infrastructure . A module is nothing but a folder of Terraform files. In terraform the modules are categorized into two types: Root modules - The current directory of Terraform files on which you are running the commands. Child modules - The modules sourced by the root module. To create a child module, add resources to it. Module syntax: module \"child\" { source = \"../child\" } The difference between adding the resources and module is for adding the resource (Eg: resource \"azurerm_virtual_network\" 'test\") you need type and name but for adding a module (Eg: module \"child\") only the name is enough. The name of the module should be unique within configurations because it is used as a reference to the module and outputs. main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 }","title":"Modules"},{"location":"learning/terraform/#updates","text":"terraform get - This command is used to download and install the modules for that configuration. terraform get -update - It checks the downloaded modules and checks for the recent versions if any.","title":"Updates"},{"location":"learning/terraform/#module-outputs","text":"If you need output from any module, you can get it by passing the required parameter. For example, the output of virtual network location can be found by following the below syntax. You can add this syntax to .tf file(main.tf or variables.tf) . output \"virtual_network_id\" { value = \"${azurerm_virtual_network.test.location}\" } Where virtual_network_id is a unique name and \"${azurerm_virtual_network.test.location}\" is the location of the virtual network using string interpolation.","title":"Module Outputs"},{"location":"learning/terraform/#benefits-of-modules","text":"Code reuse : When there is a need to provision the group of resources on another resource at the same time, a module can be used instead of copying the same code. It helps in resolving the bugs easily. If you want to make changes, changing the code in one place is enough. Abstraction layer : It makes complex configurations easier to conceptualize. For example, if you like to add vault(Another hashicorp's tool for managing secrets) cluster to the environment, it requires dozens of components. Instead of worrying about individual components, it gives ready-to-use vault cluster. Black box : To create a vault, you only need some configuration parameters without worrying about the complexity. You don't need any knowledge on how to install vault, configuring vault cluster and working of the vault. You can create a working cluster in a few minutes. Best practices in organization : When a module is created, you can give it to the other teams. It helps in easily developing the infrastructure. **Versioned artifacts: They are immutable artifacts and can be promoted from one environment to others.","title":"Benefits of Modules"},{"location":"learning/terraform/#introduction-to-meta-parameters","text":"There are some difficulties with the declarative language. For example, since it is a declarative language, there is no concept of for loops. How do you repeat a piece of code without copy paste? * Terraform provides primitives like meta parameter called as count, life cycle blocks like create_before_destroy, ternary operator and a large number of interpolation functions . This helps in performing certain types of loops, if statements and zero downtime deployments.","title":"Introduction to Meta Parameters"},{"location":"learning/terraform/#count","text":"Count - It defines how many parameters you like to create. Modify main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } To increase Vnets to 5, update count = 5 . To decrease Vnets to 2, update count = 2 , last 3 Vnets will get deleted.","title":"Count"},{"location":"learning/terraform/#elements","text":"Suppose you like to create the three virtual networks with names vnet-A, vnet-B and vnet-C, you can do this easily with the help of list . Mention how many vnets you are going to create with the help of list and define it in variables.tf file variable \"name\" { type= \"list\" default = [\"A\",\"B\",\"C\"] } You can call this variable in the main.tf file in the following way count = \"${length(var.name)}\" - It returns number of elements present in the list. you should store it in meta parameter count. \"${element(var.name,count.index)}\" - It acts as a loop, it takes the input from list and repeats until there are no elements in list. variables.tf ** variable \"resource_group\" { default = \"user-nuqo\" } variable \"location\" { default = \"East US\" } variable \"name\" { type = \"list\" default = [\"A\",\"B\",\"C\"] } **main.tf resource \"azurerm_virtual_network\"\"multiple\" { name = \"vnet-${element(var.name,count.index)}\" resource_group_name = \"${var.resource_group}\" location = \"${var.location}\" address_space=[\"10.0.0.0/16\"] count=\"${length(var.name)}\" }","title":"Elements"},{"location":"learning/terraform/#conditions","text":"For example, a vnet has to be created only, if the variable number of vnets is 3 or else no. You can do this by using the ternary operator. variables.tf variable \"no_of_vnets\" { default = 3 } main.tf count = \"${var.no_of_vnets == 3 ? 1 : 0}\" * First, it will execute the ternary operator. If it is true, it takes the output as 1 or else 0. * Now, in the above case, the output is 1, the count becomes one, and the vnet is created.","title":"Conditions"},{"location":"learning/terraform/#inheriting-variables","text":"Inheriting the variables between modules keeps your Terraform code DRY. Don't Repeat Yourself (DRY) It aims at reducing repeating patterns and code. There is no need to write the duplicate code for each environment you are deploying. You can write the minimum code and achieve the goal by having maximum re-usability.","title":"Inheriting Variables"},{"location":"learning/terraform/#module-file-structure","text":"You will follow the below file structure to inherit the variables between modules. modules | |___mod | |__mod.tf | |___vnet |__vnet.tf |__vars.tf There is one vnet and you can configure the vnet by passing the parameters from module. The variables in the vnet are overriden by the values provided in mod.tf file. Create a folder with name Modules Create two subfolders of names vnet and mod in the nested_modules folder. In the vnet folder, create a virtual network using Terraform configuration. Maintain two files one for main configuration(vnet.tf) and other for declaring variables(vars.tf) . Now, create main.tf file like this resource \"azurerm_virtual_network\"\"vnet\" { name = \"${var.name}\" address_space = [\"10.0.0.0/16\"] resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" } Now, vars.tf file looks like this variable \"name\" { description = \"name of the vnet\" } variable \"resourcegroup\" { description = \"name of the resource group\" } variable \"location\" { description =\"name of the location\" } variable \"address\" { type =\"list\" description = \"specify the address\" } Create a folder with name mod in the modules directory. Create a file with name mod.tf and write the below code in it. We are passing the variables from the root module (mod) to the child module (vnet) using source path . module \"child\" { source = \"../vnet\" name = \"modvnet\" resourcegroup = \"user-vcom\" location = \"eastus\" address = \"10.0.1.0/16\" } Run terraform plan to visualize how the directories will be created. You can see the name of the vnet is taken from the module file instead of getting it from vars.tf","title":"Module File Structure"},{"location":"learning/terraform/#nested-modules","text":"For any module, root module should exist. Root module is basically a directory that holds the terraform configuration files for the desired infrastructure. They also provide the entry point to the nested modules you are going to utilize. For any module main.tf, vars.tf, and outputs.tf naming conventions are recommended. If you are using nested modules, create them under the subdirectory with name modules . checking_modules |__modules | |____main.tf | |______virtual_network | |_main.tf | |_vars.tf | |______storage_account |_main.tf |_vars.tf You will create three folders with names modules, virtual_network, and storage account. Make sure that you are following the terraform standards like writing the terraform configuration in the main file and defining the variables alone in the variables(vars.tf) file. stoacctmain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"${var.name}\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"${var.account_replication}\" account_tier = \"${var.account_tier}\" } stoacctvars.tf variable \"rg\" { default = \"user-wmle\" } variable \"loc\" { default = \"eastus\" } variable \"name\" { default =\"storagegh\" } variable \"account_replication\" { default = \"LRS\" } variable \"account_tier\" { default = \"Basic\" } Root Module (main.tf) file under checking_modules module \"vnet\" { source = \"../virtual_network\" name = \"mymodvnet\" } module \"stoacct\" { source = \"../storage_account\" name = \"mymodstorageaccount\" } Not only the parameters mentioned above, but you can also pass any parameter from root module to the child modules. You can pass the parameters from the module to avoid the duplicity between them.","title":"Nested Modules"},{"location":"learning/terraform/#remote-backends","text":"It is better to store the code in a central repository . However, is having some disadvantages of doing so: You have to maintain push, pull configurations . If one pushes the wrong configuration and commits it, it becomes a problem. State file consists of sensitive information . So state files are non-editable. Backend in terraform explains how the state file is loaded and operations are executed. Backend can get initialize only after running terraform init command. So, terraform init is required to be run every time when backend is configured when any changes made to the backend when the backend configuration is removed completely Terraform cannot perform auto-initialize because it may require additional info from the user, to perform state migrations, etc.. Below are the steps which you will be maintaining for creating remote backend in Azure. Create a storage account Create a storage container Create a backend Get the storage account access key and resource group name, give it as a parameter to the backend. Below is the complete file structure for sample backend. Backend |____stoacct.tf |____stocontainer.tf |____backend.tf |____vars.tf |____output.tf stoacct.tf resource \"azurerm_storage_account\" \"storageaccount\" { name = \"storageaccountname\" resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" account_tier = \"${var.accounttier}\" account_replication_type = \"GRS\" tags { environment = \"staging\" } } stocontainer.tf resource \"azurerm_storage_container\" \"storagecontainer\" { name = \"vhds\" resource_group_name = \"${var.resourcegroup}\" storage_account_name = \"${azurerm_storage_account.storageaccount.name}\" container_access_type = \"private\" } vars.tf variable \"resourcegroup\" { default = \"user-abcd\" } variable \"location\" { default = \"eastus\" } variable \"accounttier\" { default = \"Basic\" } output.tf output \"storageacctname\" { value = \"${azurerm_storage_account.storageaccount.name}\" } output \"storageacctcontainer\" { value = \"${azurerm_storage_account.storagecontainer.name}\" } output \"access_key\" { value = \"${azurerm_storage_account.storageaccount.primary_access_key\" } Before creating backend, run the command to get the storage account keys list( az storage account keys list \u2013account-name storageacctname ) and copy one of the key somewhere. It is useful for configuring the backend. backend.tf terraform{ backend \"azurerm\" { storage_account_name = \"storageacct21\" container_name = \"mycontainer\" key = \"terraform.tfstate.change\" resource_group_name = \"user-okvt\" access_key = \"aJlf+XjZhxwRp4gsU4hkGrQJzO7xBzz7B9rSzMLB/ozwcM6k/1N.......==\" } } where Parameters: storage_account_name - name of the storage account. container_name - name of the container. key- name of the tfstate blob. resource_group_name - name of the resource group. access_key - Storage account access key(any one).","title":"Remote Backends"},{"location":"learning/terraform/#points-to-remember","text":"You cannot use interpolation syntax to configure backends. After creating backend run terraform init, it will be in the locked state. By running terraform apply command automatically, the lease status is changed to locked. After it is applied, it will come to an unlocked state. This backend supports consistency checking and state locking using the native capabilities of the Azure storage account.","title":"Points to Remember"},{"location":"learning/terraform/#terragrunt","text":"Terragrunt is referred to as a thin wrapper for Terraform which provides extra tools to keep the terraform configurations DRY, manage remote state and to work with multiple terraform modules.","title":"Terragrunt"},{"location":"learning/terraform/#terragrunt-commands","text":"terragrunt get terragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt supports the following use cases: Keeps terraform code DRY. Remote state configuration DRY. CLI flags DRY. Executing terraform commands on multiple modules at a time. Advantages of using terragrunt: Provides locking mechanism. Allows you to use remote state always.","title":"Terragrunt Commands"},{"location":"learning/terraform/#build-in-functions","text":"","title":"Build-In Functions"},{"location":"learning/terraform/#lookup","text":"This helps in performing a dynamic lookup into a map variable. The syntax for lookup variable is lookup(map,key, [default]) . map: The map parameter is a variable like var.name. key: The key parameter includes the key from where it should find the environment. If the key doesn't exist in the map, interpolation will fail, if it doesn't find a third argument (default). The lookup will not work on nested lists or maps. It only works on flat maps.","title":"lookup"},{"location":"learning/terraform/#local-values","text":"Local values help in assigning names to the expressions , which can be used multiple times within a module without rewriting it. Local values are defined in the local blocks. It is recommended to group the logically related local values together as a single block, if there is a dependency between them. locals { service_name = \"Fresco\" owner = \"Team\" } Example locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } If a single value or a result is used in many places and it is likely to be changed in the future, then you can go with locals. It is recommended to not use many local values because it makes the read configurations hard to the future maintainers.","title":"Local Values"},{"location":"learning/terraform/#data-source","text":"Data source allows the data to be computed or fetched for use in Terraform configuration. Data sources allow the terraform configurations to build on the information defined outside the Terraform or by another Terraform configuration. Providers play a major role in implementing and defining data sources. Data source helps in two major ways: It provides a read-only view for the pre-existing data. It can compute new values on the fly. Every data source in the Terraform is mapped to the provider depending on the longest-prefix-matching . data \"azurerm_resource_group\" \"passed\" { name = \"${var.resource_group_name}\" }","title":"Data Source"},{"location":"learning/terraform/#concat-and-contains","text":"concat(list1,list2) * It combines two or more lists into a single list. contains(list, element) * It returns true if the element is present in the list or else false. E.g. contains(var.list_of_strings, \"an_element\")","title":"Concat and Contains"},{"location":"learning/terraform/#workspaces","text":"Every terraform configurations has associate backends which defines how the operations are executed and where the persistent data like terraform state are stored. Persistent data stored in backend belongs to a workspace. Previously, the backend has only one workspace which is called as default and there will be only one state file associated with the configuration. Some of the backends support multiple workspaces, i.e. It allows multiple state files to be stored within a single configuration. The configuration is still having only one backend, and the multiple distinct instances can be deployed without configuring to a new backend or by changing authentication credentials. default workspace is special because you can't delete the default workspace. You cannot delete your active workspace (the workspace in which you are currently working). If you haven't created the workspace before, then you are working on the default workspace.","title":"Workspaces"},{"location":"learning/terraform/#workspace-commands","text":"terraform workspace new - It creates the workspace with specified name and switches to it. terraform workspace list - It lists the workspaces available. terraform workspace select - It switches to the specified workspace. terraform workspace delete - It deletes the mentioned workspace.","title":"Workspace Commands"},{"location":"learning/terraform/#configuring-multiple-providers","text":"Below is the file structure that you will maintain for working with multiple providers. multiple_providers | |___awsmain.tf | |___azuremain.tf | |___vars.tf | |___out.tf Create an S3 bucket in Azure which helps in storing the file in AWS. awsmain.tf resource \"aws_s3_bucket\" \"b\" { bucket = \"my-tf-test-bucket-21\" acl = \"private\" tags = { Name = \"My bucket test\" Environment = \"Dev\" } } Resource has two parameters: name(aws_s3_bucket) and type(\"b\"). Name of the resource may vary from provider to provider. Type of the resource depends on the user. Bucket indicates the name of the bucket. If it is not assigned, terraform assigns a random unique name. acl (access control list) - It helps to manage access to the buckets and objects. tag - It is a label which is assigned to the AWS resource by you or AWS. Environment - On which environment do you like to deploy S3 bucket (Dev, Prod or Stage). azuremain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"storageacct21\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"LRS\" account_tier = \"Standard\" } Parameters name - It indicates the name of the storage account. resource_group_name and location- name of the resource group and location. account_replication_type - Type of the account replication. account_tier - Account tier type out.tf output \"account_tier\" { value = \"azurerm_storage_Account.storagecct.account_tier\" }","title":"Configuring Multiple Providers"},{"location":"learning/tmux/","text":"Config Setup Tmux Config Doc Example Tmux and Neovim tmux ls # List all sessions tmux attach -t 0 # -t #n indicates the session number from ls output tmux new -s work # Create a new session with a name tmux rename-session -t 0 work # Rename existing session tmux attach -t work # Attach to existing session after login tmux switch -t session_name # Switches to an existing session named session_name tmux list-sessions # Lists existing tmux sessions tmux detach # (prefix + d) detach the currently attached session # PREFIX CTRL + SPACE # reload ~/.tmux.conf using PREFIX r # New Window => PREFIX w # Rename Window => PREFIX n # Split window vertically => PREFIX | # Split window horizontally => PREFIX - # Toggle Window ALT j (previous), ALT k (next) # Toggle Panes => PREFIX h(Left),j(Down),k(Up),l(Right) # Synchronize-panes => PREFIX CTRL+y # Kill Pane => PREFIX x # Kill Window => PREFIX X Benefit \u00b6 Connect to your remote server via SSH. Launch tmux on the remote server. Run a script which takes hours. Close the SSH connection. The script will still run on the remote server, thanks to tmux! Switch off your computer and go home. Workflow Recommendations \u00b6 Use a single client \u2014 Although it is possible to run multiple tmux clients by opening multiple terminal tabs or windows, I find it better to focus on one client in a single terminal window. This provides a single high-level abstraction, which is easier to reason about and interact with. One project per session \u2014 I will open each project, roughly mapping to a git repository in its own session. Typically I will have Vim in the first window along with a pane for running tests and any background processes like the rails server running in additional windows. One Vim instance per session \u2014 In order to avoid conflicts with temp files and buffers getting out of sync, I will only use a single Vim instance per tmux session. Since each session maps to a specific project, this tends to keep me safe from conflicting edits and similar complications.","title":"Tmux"},{"location":"learning/tmux/#benefit","text":"Connect to your remote server via SSH. Launch tmux on the remote server. Run a script which takes hours. Close the SSH connection. The script will still run on the remote server, thanks to tmux! Switch off your computer and go home.","title":"Benefit"},{"location":"learning/tmux/#workflow-recommendations","text":"Use a single client \u2014 Although it is possible to run multiple tmux clients by opening multiple terminal tabs or windows, I find it better to focus on one client in a single terminal window. This provides a single high-level abstraction, which is easier to reason about and interact with. One project per session \u2014 I will open each project, roughly mapping to a git repository in its own session. Typically I will have Vim in the first window along with a pane for running tests and any background processes like the rails server running in additional windows. One Vim instance per session \u2014 In order to avoid conflicts with temp files and buffers getting out of sync, I will only use a single Vim instance per tmux session. Since each session maps to a specific project, this tends to keep me safe from conflicting edits and similar complications.","title":"Workflow Recommendations"},{"location":"learning/vagrant/","text":"Vagrant is not able to run on Windows 10 + WLS2 Windows setup \u00b6 vagrant plugin install vagrant-vbguest Image should have guest additions or it will not share folder from windows inside the VM Internal Network \u00b6 Go to VirtualBox \u2192 File \u2192 Host Network Manager \u2192 Check the enabled network DHCP address Windows Features Turn On Off \u00b6 Disable \"virtual machine platform\" and \"windows hypervisor platform Installation \u00b6 Install same version of Vagrant in Windows and WSL Verify vagrant --version in both to match In windows try downloading a box and start vagrant up \u2013provider=virtualbox Vagrant \u00b6 Init with a image in vagrant cloud \u00b6 vagrant init hashicorp/precise64 Start the vm \u00b6 vagrant up SSH into the vm \u00b6 vagrant ssh Hibernate the vm \u00b6 vagrant suspend Check the status of vagrant vm \u00b6 vagrant status Stop the vm \u00b6 vagrant halt Clean up the vm \u00b6 vagrant destroy Get Status of Vagrant Machines on host vagrant global-status Get SSH Settings vagrant ssh-config Reload Virtual Machine vagrant reload Make sure the ssh key you created is stored parallel to your Vagrantfile before you execute the vagrant up command. Vagrant commands Managing Vagrant boxes \u00b6 Download a box to a machine \u00b6 vagrant box add ubuntu/trusty64 vagrant box add centos/8 List boxes on machine \u00b6 vagrant box list Update an existing box on a machine \u00b6 vagrant box outdated vagrant box update Run a downloaded box \u2192 cd into a folder \u00b6 vagrant init ubuntu/trusty64 vagrant up Remove a downloaded box from a machine \u00b6 vagrant box remove ubuntu/trusty64 Finding boxes \u00b6 vagrantboxes.es & vagrantcloud \u2192 find a box and copy the url vagrant box add vagrant init vagrant up Using Plugins \u00b6 List existing plugins \u00b6 vagrant plugin list Install Plugins \u00b6 vagrant plugin install vagrant-vbguest Update Plugin version \u00b6 vagrant plugin update vagrant-vbguest Update all plugins \u00b6 vagrant plugin update Remove Plugins \u00b6 vagrant plugin uninstall vagrant-vbguest Adding services to startup boot \u00b6 sudo chkconfig \u2013add httpd sudo chkconfig httpd on sudo service httpd stop Create symbolic link which will serve file from local on vagrant machine, ensure index html file is there in local root \u00b6 cd /var/www/html cd .. && rm -rf html sudo ln -s /vagrant /var/www/html sudo service httpd start Packaging Vagrant after baking \u00b6 Imp that VM is running, check status \u00b6 vagrant status vagrant package \u2013output .box vagrant box add .box Custom base box packaging after customization / hardening \u00b6 vagrant package \u2013base Switching of guest additions checks if the plugin is available in local \u00b6 Add line in config \u00b6 config.vbguest.auto_update = false Adding a file from local machine not in the project folder to the vm \u00b6 config.vm.provision \"file\", source: \"~/vagrant/files/git-files\", destination: \"~/.gitconfig\" If VM is running when above provisioning is done, it is not reflected \u00b6 vagrant provision Adding software at provisioning \u00b6 config.vm.provision \"shell\", inline: \"yum install -y git nano\" Adding custom scripts not in the project folder to the vm \u00b6 config.vm.provision \"shell\", path: \"~/vagrant/scripts/provision.sh\" To restart vm \u00b6 sudo shutdown -r now Restart service \u00b6 sudo systemctl restart sshd.service Update centos kernal \u00b6 sudo yum update kernel* Check and delete old kernels \u00b6 rpm -qa kernel sudo package-cleanup \u2013old-kernels \u2013count=2 Debugging Vagrant \u00b6 During Vagrant Up your Windows system tries to connect to SSH. If you type on your command line: \u00b6 set VAGRANT_LOG=INFO Debug SSH \u00b6 set VAGRANT_PREFER_SYSTEM_BIN=0 vagrant ssh \u2013debug - Running this command will show which identity file is being used vagrant ssh-config | grep IdentityFile","title":"Vagrant"},{"location":"learning/vagrant/#windows-setup","text":"vagrant plugin install vagrant-vbguest Image should have guest additions or it will not share folder from windows inside the VM","title":"Windows setup"},{"location":"learning/vagrant/#internal-network","text":"Go to VirtualBox \u2192 File \u2192 Host Network Manager \u2192 Check the enabled network DHCP address","title":"Internal Network"},{"location":"learning/vagrant/#windows-features-turn-on-off","text":"Disable \"virtual machine platform\" and \"windows hypervisor platform","title":"Windows Features Turn On Off"},{"location":"learning/vagrant/#installation","text":"Install same version of Vagrant in Windows and WSL Verify vagrant --version in both to match In windows try downloading a box and start vagrant up \u2013provider=virtualbox","title":"Installation"},{"location":"learning/vagrant/#vagrant","text":"","title":"Vagrant"},{"location":"learning/vagrant/#init-with-a-image-in-vagrant-cloud","text":"vagrant init hashicorp/precise64","title":"Init with a image in vagrant cloud"},{"location":"learning/vagrant/#start-the-vm","text":"vagrant up","title":"Start the vm"},{"location":"learning/vagrant/#ssh-into-the-vm","text":"vagrant ssh","title":"SSH into the vm"},{"location":"learning/vagrant/#hibernate-the-vm","text":"vagrant suspend","title":"Hibernate the vm"},{"location":"learning/vagrant/#check-the-status-of-vagrant-vm","text":"vagrant status","title":"Check the status of vagrant vm"},{"location":"learning/vagrant/#stop-the-vm","text":"vagrant halt","title":"Stop the vm"},{"location":"learning/vagrant/#clean-up-the-vm","text":"vagrant destroy Get Status of Vagrant Machines on host vagrant global-status Get SSH Settings vagrant ssh-config Reload Virtual Machine vagrant reload Make sure the ssh key you created is stored parallel to your Vagrantfile before you execute the vagrant up command. Vagrant commands","title":"Clean up the vm"},{"location":"learning/vagrant/#managing-vagrant-boxes","text":"","title":"Managing Vagrant boxes"},{"location":"learning/vagrant/#download-a-box-to-a-machine","text":"vagrant box add ubuntu/trusty64 vagrant box add centos/8","title":"Download a box to a machine"},{"location":"learning/vagrant/#list-boxes-on-machine","text":"vagrant box list","title":"List boxes on machine"},{"location":"learning/vagrant/#update-an-existing-box-on-a-machine","text":"vagrant box outdated vagrant box update","title":"Update an existing box on a machine"},{"location":"learning/vagrant/#run-a-downloaded-box----cd-into-a-folder","text":"vagrant init ubuntu/trusty64 vagrant up","title":"Run a downloaded box --&gt; cd into a folder"},{"location":"learning/vagrant/#remove-a-downloaded-box--from-a-machine","text":"vagrant box remove ubuntu/trusty64","title":"Remove a downloaded box  from a machine"},{"location":"learning/vagrant/#finding-boxes","text":"vagrantboxes.es & vagrantcloud \u2192 find a box and copy the url vagrant box add vagrant init vagrant up","title":"Finding boxes"},{"location":"learning/vagrant/#using-plugins","text":"","title":"Using Plugins"},{"location":"learning/vagrant/#list-existing-plugins","text":"vagrant plugin list","title":"List existing plugins"},{"location":"learning/vagrant/#install-plugins","text":"vagrant plugin install vagrant-vbguest","title":"Install Plugins"},{"location":"learning/vagrant/#update-plugin-version","text":"vagrant plugin update vagrant-vbguest","title":"Update Plugin version"},{"location":"learning/vagrant/#update-all-plugins","text":"vagrant plugin update","title":"Update all plugins"},{"location":"learning/vagrant/#remove-plugins","text":"vagrant plugin uninstall vagrant-vbguest","title":"Remove Plugins"},{"location":"learning/vagrant/#adding-services-to-startup-boot","text":"sudo chkconfig \u2013add httpd sudo chkconfig httpd on sudo service httpd stop","title":"Adding services to startup boot"},{"location":"learning/vagrant/#create-symbolic-link-which-will-serve-file-from-local-on-vagrant-machine-ensure-index-html-file-is-there-in-local-root","text":"cd /var/www/html cd .. && rm -rf html sudo ln -s /vagrant /var/www/html sudo service httpd start","title":"Create symbolic link which will serve file from local on vagrant machine, ensure index html file is there in local root"},{"location":"learning/vagrant/#packaging-vagrant-after-baking","text":"","title":"Packaging Vagrant after baking"},{"location":"learning/vagrant/#imp-that-vm-is-running-check-status","text":"vagrant status vagrant package \u2013output .box vagrant box add .box","title":"Imp that VM is running, check status"},{"location":"learning/vagrant/#custom-base-box-packaging-after-customization--hardening","text":"vagrant package \u2013base","title":"Custom base box packaging after customization / hardening"},{"location":"learning/vagrant/#switching-of-guest-additions-checks-if-the-plugin-is-available-in-local","text":"","title":"Switching of guest additions checks if the plugin is available in local"},{"location":"learning/vagrant/#add-line-in-config","text":"config.vbguest.auto_update = false","title":"Add line in config"},{"location":"learning/vagrant/#adding-a-file-from-local-machine-not-in-the-project-folder-to-the-vm","text":"config.vm.provision \"file\", source: \"~/vagrant/files/git-files\", destination: \"~/.gitconfig\"","title":"Adding a file from local machine not in the project folder to the vm"},{"location":"learning/vagrant/#if-vm-is-running-when-above-provisioning-is-done-it-is-not-reflected","text":"vagrant provision","title":"If VM is running when above provisioning is done, it is not reflected"},{"location":"learning/vagrant/#adding-software-at-provisioning","text":"config.vm.provision \"shell\", inline: \"yum install -y git nano\"","title":"Adding software at provisioning"},{"location":"learning/vagrant/#adding-custom-scripts-not-in-the-project-folder-to-the-vm","text":"config.vm.provision \"shell\", path: \"~/vagrant/scripts/provision.sh\"","title":"Adding custom scripts not in the project folder to the vm"},{"location":"learning/vagrant/#to-restart-vm","text":"sudo shutdown -r now","title":"To restart vm"},{"location":"learning/vagrant/#restart-service","text":"sudo systemctl restart sshd.service","title":"Restart service"},{"location":"learning/vagrant/#update-centos-kernal","text":"sudo yum update kernel*","title":"Update centos kernal"},{"location":"learning/vagrant/#check-and-delete-old-kernels","text":"rpm -qa kernel sudo package-cleanup \u2013old-kernels \u2013count=2","title":"Check and delete old kernels"},{"location":"learning/vagrant/#debugging-vagrant","text":"","title":"Debugging Vagrant"},{"location":"learning/vagrant/#during-vagrant-up-your-windows-system-tries-to-connect-to-ssh-if-you-type-on-your-command-line","text":"set VAGRANT_LOG=INFO","title":"During Vagrant Up your Windows system tries to connect to SSH. If you type on your command line:"},{"location":"learning/vagrant/#debug-ssh","text":"set VAGRANT_PREFER_SYSTEM_BIN=0 vagrant ssh \u2013debug - Running this command will show which identity file is being used vagrant ssh-config | grep IdentityFile","title":"Debug SSH"},{"location":"learning/ansible/ansible/","text":"Introduction \u00b6 Note The rule of thumb is if you can script it, you can create a playbook for it. Ansible 101 Ansible Cheat Sheet App Security Ansible Tips and Tricks DO Practise examples & Explaination & Tutorials YAML Spec Ref Card Full Application on Cloud Maintaining Playbooks - Pitfalls Documentation on cmdline \u00b6 # System outputs the man page for debug module ansible-doc debug ansible-doc -l | grep aws # List all plugins and then grep for aws plugins ansible-doc -s shell # Shows snippets on how to use the plugins Setup Server \u00b6 Default Configuration \u00b6 ansible.cfg and hosts files are present inside /etc/ansible Testing ansible on Ubuntu WSL ansible localhost -m ping Enabling SSH on the VM \u00b6 If you need SSH enabled on the system, follow the below steps: Ensure the /etc/apt/sources.list file has been updated as per above Run the command: apt-get update Run the command: apt-get install openssh-server Run the command: service sshd start ssh-keygen -t rsa -C \"ansible\" #OR # Generate an SSH key pair for future connections to the VM instances (run the command exactly as it is): ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user -P \"\" #Add the SSH private key to the ssh-agent: ssh-add ~/.ssh/ansible-user #Verify that the key was added to the ssh-agent: $ ssh-add -l Access VM over SSH \u00b6 ssh vagrant@127.0.0.1 -p 2222 -i ~/.ssh/insecure_private_key Copy files recursively from local desktop to remote server \u00b6 scp -r ./scripts vagrant@127.0.0.1:/home/vagrant -p 2222 -i ~/.ssh/insecure_private_key Target Docker containers for Ansible controller \u00b6 The Docker file used to create the ubuntu-ssh-enabled Docker image is located here. Issues installing Ansible and its dependencies \u00b6 Once the Debian VM is up and running make the following changes to the /etc/apt/sources.list file to get the Ansible installation working right. deb http://security.debian.org/ jessie/updates main contrib deb-src http://security.debian.org/ jessie/updates main contrib deb http://ftp.debian.org/debian/ jessie-updates main contrib deb-src http://ftp.debian.org/debian/ jessie-updates main contrib deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main deb http://ftp.de.debian.org/debian sid main Ansible Directory Structure as per Best Practises \u00b6 This is the directory layout of this repository with explanation. production.ini # inventory file for production stage development.ini # inventory file for development stage test.ini # inventory file for test stage vpass # ansible-vault password file # This file should not be committed into the repository # therefore file is ignored by git ########################## This segration can be done if there are changes in variable values between environments \u251c\u2500\u2500 inventories \u2502 \u251c\u2500\u2500 development \u2502 \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u2502 \u2514\u2500\u2500 app.yml \u2502 \u2502 \u251c\u2500\u2500 hosts \u2502 \u2502 \u2514\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 production \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u2514\u2500\u2500 app.yml \u2502 \u251c\u2500\u2500 hosts \u2502 \u2514\u2500\u2500 host_vars ########################## group_vars/ all/ # variables under this directory belongs all the groups apt.yml # ansible-apt role variable file for all groups webservers/ # here we assign variables to webservers groups apt.yml # Each file will correspond to a role i.e. apt.yml nginx.yml # \"\" postgresql/ # here we assign variables to postgresql groups postgresql.yml # Each file will correspond to a role i.e. postgresql postgresql-password.yml # Encrypted password file plays/ ansible.cfg # Ansible.cfg file that holds all ansible config webservers.yml # playbook for webserver tier postgresql.yml # playbook for postgresql tier roles/ roles_requirements.yml# All the information about the roles external/ # All the roles that are in git or ansible galaxy # Roles that are in roles_requirements.yml file will be downloaded into this directory internal/ # All the roles that are not public common/ # common role tasks/ # main.yml # installing basic tasks scripts/ setup/ # All the setup files for updating roles and ansible dependencies Ansible Inventory \u00b6 Creating an inventory file and adding hosts \u00b6 Ansible supports two types of inventory\u2014static and dynamic Static inventories are by their very nature static; they are unchanging unless a human being goes and manually edits them. Even in small, closed environments, static inventories are a great way to manage your environment, especially when changes to the infrastructure are infrequent. # Sample inventory file in INI format target1.example.com ansible_host=192.168.81.142 ansible_port=3333 target2.example.com ansible_port=3333 ansible_user=danieloh target3.example.com ansible_host=192.168.81.143 ansible_port=5555 ansible_host: If the inventory hostname cannot be accessed directly\u2014perhaps because it is not in DNS, for example, this variable contains the hostname or IP address that Ansible will connect to instead. ansible_port: By default, Ansible attempts all communication over port 22 for SSH\u2014if you have an SSH daemon running on another port, you can tell Ansible about it using this variable. ansible_user: By default, Ansible will attempt to connect to the remote host using the current user account you are running the Ansible command from\u2014you can override this in several ways, of which this is one. Hence, the preceding three hosts can be summarized as follows: The target1.example.com host should be connected to using the 192.168.81.142 IP address, on port 3333. The target2.example.com host should be connected to on port 3333 also, but this time using the danieloh user rather than the account running the Ansible command. The target3.example.com host should be connected to using the 192.168.81.143 IP address, on port 5555. Using host groups \u00b6 Let's assume you have a simple three-tier web architecture, with multiple hosts in each tier for high availability and/or load balancing. The three tiers in this architecture might be the following: Frontend servers Application servers Database servers To keep the examples clear and concise, we'll assume that you can access all servers using their Fully Qualified Domain Names (FQDNs) , and hence won't add any host variables into these inventory files. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com We have created three groups called frontends, apps, and databases. Note that, in INI-formatted inventories, group names go inside square braces. Under each group name goes the server names that belong in each group, so the preceding example shows two servers in each group. Notice the outlier at the top, loadbalancer.example.com \u2014 this host isn't in any group. All ungrouped hosts must go at the very top of an INI-formatted file. The preceding inventory stands in its own right, but what if our frontend servers are built on Ubuntu, and the app and database servers are built on CentOS? There will be some fundamental differences in the ways we handle these hosts \u2014 for example, we might use the apt module to manage packages on Ubuntu and the yum module on CentOS. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends With the use of the children keyword in the group definition (inside the square braces), we can create groups of groups; hence, we can perform clever groupings to help our playbook design without having to specify each host more than once. ansible -i hostgroups-yml centos -m shell -a 'echo hello-yaml' -f 5 This is a powerful way of managing your inventory and making it easy to run commands on just the hosts you want to. The possibility of creating multiple groups makes life simple and easy, especially when you want to run different tasks on different groups of servers. Let's assume you have 100 app servers, all named sequentially, as follows: app01 to app100 [ apps ] app[01:100].prod.com The following inventory snippet actually produces an inventory with the same 100 app servers that we could create manually. Adding host and group variables to your inventory \u00b6 -Suppose that we need to set two variables for each of our two frontend servers. These are not special Ansible variables, but instead are variables entirely of our own choosing. - https_port, which defines the port that the frontend proxy should listen on - lb_vip, which defines the FQDN of the load-balancer in front of the frontend servers - You can assign variables to a host group as well as to hosts individually. [ frontends ] frt01.example.com frt02.example.com [frontends:vars] https_port=8443 lb_vip=lb.example.com - There will be times when you want to work with host variables for individual hosts, and times when group variables are more relevant. - It is also worth noting that host variables override group variables , so if we need to change the connection port to 8444 on the frt01.example.com one, we could do this as follows [ frontends ] frt01.example.com https_port=8444 frt02.example.com [frontends:vars] https_port=8443 lb_vip=lb.example.com - Right now, our examples are small and compact and only contain a handful of groups and variables; however, when you scale this up to a full infrastructure of servers, using a single flat inventory file could, once again, become unmanageable. - Luckily, Ansible also provides a solution to this. Two specially-named directories, host_vars and group_vars , are automatically searched for appropriate variable content if they exist within the playbook directory. - Under the host_vars directory, we'll create a file with the name of our host that needs the proxy setting, with .yml appended to it (that is, frt01.example.com.yml). --- https_port : 8444 - Under the group_vars directory, create a YAML file named after the group to which we want to assign variables (that is, frontends.yml) --- https_port : 8443 lb_vip : lb.example.com - Finally, we will create our inventory file as before, except that it contains no variables. # Final directory structure should look like this \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 frontends.yml \u251c\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 frt01.example.com.yml \u2514\u2500\u2500 inventory Note If you define the same variable at both a group level and a child group level, the variable at the child group level takes precedence. Consider our earlier inventory where we used child groups to differentiate between CentOS and Ubuntu hosts \u2014 if we add a variable with the same name to both the ubuntu child group and the frontends group (which is a child of the ubuntu group) as follows, what will the outcome be? loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [frontends:vars] testvar=childgroup [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends [ubuntu:vars] testvar=group Debugging variable at host level ansible -i hostgroups-children-vars-ini ubuntu -m debug -a \"var=testvar\" # Output frt01.example.com | SUCCESS = > { \"testvar\" : \"childgroup\" } frt02.example.com | SUCCESS = > { \"testvar\" : \"childgroup\" } It's important to note that the frontends group is a child of the ubuntu group in this inventory (hence, the group definition is [ubuntu:children]), and so the variable value we set at the frontends group level wins as this is the child group in this scenario. Special host management using patterns \u00b6 Let's look at how Ansible can work with patterns to figure out which hosts a command (or playbook) should be run against. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends We shall use the \u2013list-hosts switch with the ansible command to see which hosts Ansible would operate on. ansible -i hostgroups-children-ini all --list-hosts # The asterisk character has the same effect as all, but needs to be quoted in single quotes for the shell to interpret the command properly ansible -i hostgroups-children-ini '*' --list-hosts # Use : to specify a logical OR, meaning \"apply to hosts either in this group or that group,\" ansible -i hostgroups-children-ini frontends:apps --list-hosts # Use ! to exclude a specific group\u2014you can combine this with other characters such as : to show all hosts except those in the apps group. # Again, ! is a special character in the shell and so you must quote your pattern string in single quotes for it to work. ansible -i hostgroups-children-ini 'all:!apps' --list-hosts # Use :& to specify a logical AND between two groups, for example, if we want all hosts that are in the centos group and the apps group . ansible -i hostgroups-children-ini 'centos:&apps' --list-hosts # Use * wildcards ansible -i hostgroups-children-ini 'db*.example.com' --list-hosts # Another way you can limit which hosts a command is run on is to use the --limit switch with Ansible. ansible-playbook -i hostgroups-children-ini site.yml --limit frontends:apps WebApp Installation Instructions for Centos 7 \u00b6 Install Python Pip and dependencies on Centos 7 \u00b6 sudo yum install -y epel-release python python-pip sudo pip install flask flask-mysql If you come across a certification validation error while running the above command, please use the below command. sudo pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org flask flask-mysql Install MySQL Server on Centos 7 \u00b6 wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm sudo yum update sudo yum -y install mysql-server sudo service mysql start The complete playbook to get the same workin on CentOS is here: https://github.com/kodekloudhub/simple_web_application Setting up Ansible to run on localhost always \u00b6 Beneficial when testing roles or playbooks in docker images Remember: To set the hosts parameter matches in ansible.cfg This is useful for debugging ansible modules and syntax without having to use VMs or test in dev environments. Install ansible \u00b6 pip install ansible Make some relevant config files \u00b6 ~/.ansible.cfg : [ defaults ] inventory = ~/.ansible-hosts ~/.ansible-hosts : localhost ansible_connection=local Make a test playbook and run \u00b6 helloworld.yml : --- - hosts : all tasks : - shell : echo 'hello world' - run! ansible-playbook helloworld.yml Executing Ansible Playbook \u00b6 Launching Ansible situational commands \u00b6 # To check the inventory file ansible-inventory --list -y # Test Connection ansible all -m ping -u root # Ask for Sudo password ansible all -m ping --ask-pass # Using a specific SSH private key and a user ansible -m ping hosts --private-key = ~/.ssh/keys/id_rsa -u centos # Check the disk usage of all servers ansible all -a \"df -h\" -u root # Check the time of `uptime` each host in a group **servers** ansible servers -a \"uptime\" -u root # Specify multiple hosts by separating their names with colons ansible server1:server2 -m ping -u root # Get system dat in json format of target ansible target1 -i myhosts -m setup --private-key = ~/.ssh/ansible-user -u root # Filter json output ansible target1 -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root ansible all -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root Launching Ansible Playbook situational commands \u00b6 ansible-playbook -i myhosts site.yml # Ask for Sudo password ansible-playbook myplaybook.yml --ask-become-pass # Or use the -K option ansible-playbook -i inventory myplaybook.yml -u sammy -K # Execute a play without making any changes to the remote servers ansible-playbook myplaybook.yml --list-tasks # List all hosts that would be affected by a play ansible-playbook myplaybook.yml --list-hosts ansible-playbook -i myhosts playbooks/atmo_playbook.yml --user atmouser # Passing variables which executing playbooks ansible-playbook playbooks/atmo_playbook.yml -e \"ATMOUSERNAME=atmouser\" ansible host01 -i myhosts -m copy -a \"src=test.txt dest=/tmp/\" ansible host01 -i myhosts -m file -a \"dest=/tmp/test mode=644 state=directory\" ansible host01 -i myhosts -m apt -a \"name=sudo state=latest\" ansible host01 -i myhosts -m shell -a \"echo $TERM \" ansible host01 -i myhosts -m command -a \"mkdir folder1\" # Run playbook on one host ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1\" # Run playbook on multiple hosts ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1,host2\" # Flush Ansible memory f pevious runs ansible-playbook playbooks/PLAYBOOK_NAME.yml --flush-cache # Dry run mode ansible-playbook playbooks/PLAYBOOK_NAME.yml --check # Starts playbook execution from an intermediate task, task name should match ansible-playbook myplaybook.yml --start-at-task = \"Set Up Nginx\" # Increasing debug verbosity ansible-playbook myplaybook.yml -v ansible-playbook myplaybook.yml -vvvv Launching Ansible Vault situational commands \u00b6 # Create new encrypted file, enter password ansible-vault encrypt credentials.yml # View the contents of encrypted file ansible-vault view credentials.yml # Edit the encrypted file ansible-vault edit credentials.yml # Permanently decrypt the file ansible-vault decrypt credentials.yml # Creating multiple vaults per env like dev, prod # create a new vault ID named dev that uses prompt as password source. # Prompt will ask you to enter a password, or a valid path to a password file. ansible-vault create --vault-id dev@prompt credentials_dev.yml ansible-vault create --vault-id prod@prompt credentials_prod.yml # Editing , Decrypting multiple vaults ansible-vault edit credentials_dev.yml --vault-id dev@prompt # Using Password file when using 3rd party automation ansible-vault create --vault-id dev@path/to/passfile credentials_dev.yml # Running playbooks with encrypted password ansible-playbook myplaybook.yml --ask-vault-pass # Passing password file ansible-playbook myplaybook.yml --vault-password-file my_vault_password.py # Passing multi env password ansible-playbook myplaybook.yml --vault-id dev@prompt ansible-playbook myplaybook.yml --vault-id dev@vault_password.py --vault-id test@prompt --vault-id ci@prompt # To change the vault password for key rotation ansible-vault rekey credentials.yml Understanding the playbook framework \u00b6 A playbook allows you to manage multiple configurations and complex deployments on many machines simply and easily. This is one of the key benefits of using Ansible for the delivery of complex applications. With playbooks, you can organize your tasks in a logical structure as tasks are (generally) executed in the order they are written, allowing you to have a good deal of control over your automation processes. # Example inventory [ frontends ] frt01.example.com https_port=8443 frt02.example.com http_proxy=proxy.example.com [frontends:vars] ntp_server=ntp.frt.example.com proxy=proxy.frt.example.com [apps] app01.example.com app02.example.com [webapp:children] frontends apps [webapp:vars] proxy_server=proxy.webapp.example.com health_check_retry=3 health_check_interal=60 Create a simple playbook to run on the hosts in the frontends host group defined in our inventory file. We can set the user that will access the hosts using the remote_user directive in the playbook --- - hosts : frontends remote_user : danieloh tasks : - name : simple connection test ping : remote_user : danieloh The ignore_errors directive to this task to ensure that our playbook doesn't fail if the ls command fails (for example, if the directory we're trying to list doesn't exist). - name : run a simple command shell : /bin/ls -al /nonexistent ignore_errors : True Defining plays and tasks \u00b6 So far when we have worked with playbooks, we have been creating one single play per playbook (which logically is the minimum you can do). However, you can have more than one play in a playbook, and a \"play\" in Ansible terms is simply a set of tasks (and roles, handlers, and other Ansible facets) associated with a host (or group of hosts). A task is the smallest possible element of a play and is responsible for running a single module with a set of arguments to achieve a specific goal. Understanding roles \u00b6 Roles are designed to enable you to efficiently and effectively reuse Ansible code. They always follow a known structure and often will include sensible default values for variables, error handling, handlers, and so on. The process of creating roles is in fact very simple\u2014Ansible will (by default) look within the same directory as you are running your playbook from for a roles/ directory. The role name is derived from the subdirectory name\u2014there is no need to create complex metadata or anything else\u2014it really is that simple. Within each subdirectory goes a fixed directory structure that tells Ansible what the tasks, default variables, handlers, and so on are for each role. The roles/ directory is not the only play Ansible will look for roles\u2014this is the first directory it will look in, but it will then look in /etc/ansible/roles for any additional roles. Setting up role-based variables and dependencies \u00b6 The Ansible role directory structure allows for role-specific variables to be declared in two locations. Although, at first, the difference between these two locations may not seem obvious, it is of fundamental importance. Roles based variables can go in one of two locations: defaults/main.yml vars/main.yml Variables that go in the defaults/ directory are one of the lowest in terms of precedence and so are easily overwritten. This location is where you would put variables that you want to override easily, but where you don't want to leave a variable undefined. For example, if you are installing Apache Tomcat, you might build a role to install a specific version. However, you don't want the role to exit with an error if someone forgets to set the version\u2014rather, you would prefer to set a sensible default such as 7.0.76, which can then be overridden with inventory variables or on the command line (using the -e or \u2013extra-vars switches). In this way, you know the role will work even without someone explicitly setting this variable, but it can easily be changed to a newer Tomcat version if desired. Variables that go in the vars/ directory, however, come much higher up on Ansible's variable precedence ordering. This will not be overridden by inventory variables, and so should be used for variable data that it is more important to keep static. Of course, this is not to say they can't be overridden\u2014the -e or \u2013extra-vars switches are the highest order of precedence in Ansible and so will override anything else that you define. Most of the time, you will probably make use of the defaults/ based variables alone, but there will doubtless be times when having the option of variables higher up the precedence ordering becomes valuable to your automation, and so it is vital to know that this option is available to you. Note I recommend that you make extensive use of the debug statement and test your playbook design to make sure that you don't fall foul of this during your playbook development. Ansible Playbook Examples \u00b6 Install Software only if it doesn't exist - name : installing python2 minimal raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) Install latest software version - hosts : host01 --- become : true tasks : - name : ensure latest sysstat is installed apt : name : sysstat state : latest Install software on all hosts --- - name : install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name : apache2 update_cache : yes state : latest Copy file only when it does not exists --- - hosts : host01 tasks : - stat : path : /home/ubuntu/folder1/something.j2 register : st - name : Template to copy file template : src : ./something.j2 dest : /home/ubuntu/folder1/something.j2 mode : '0644' when : st.stat.exists == False Add users using Loops # Looping - name : add several users user : name : \"{{ item }}\" state : present groups : \"developer\" with_items : - raj - david - john - lauren Using Looping with debug # show all the hosts in the inventory - debug : msg : \"{{ item }}\" with_items : - \"{{ groups['all'] }}\" # show all the hosts in the current play - debug : msg : \"{{ item }}\" with_items : - \"{{ play_hosts }}\" Conditionals # This Playbook will add Java Packages to different systems (handling Ubuntu/Debian OS) - name : debian | ubuntu | add java ppa repo apt_repository : repo=ppa:webupd8team/java state=present become : yes when : ansible_distribution == 'Ubuntu' - name : debian | ensure the webupd8 launchpad apt repository is present apt_repository : repo=\"{{ item }} http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" update_cache=yes state=present with_items : - deb - deb-src become : yes when : ansible_distribution == 'Debian' Full Play --- - name : install software hosts : host01 sudo : yes tasks : - name : Update and upgrade apt packages apt : upgrade : dist update_cache : yes cache_valid_time : 86400 - name : install software apt : name : \"{{item}}\" update_cache : yes state : installed with_items : - nginx - postgresql - postgresql-contrib - libpq-dev - python-psycopg2 - name : Ensure the Nginx service is running service : name : nginx state : started enabled : yes - name : Ensure the PostgreSQL service is running service : name : postgresql state : started enabled : yes #file: vars.yml --- var : 20 #file: playbook.yml --- - hosts : all vars_files : - vars.yml tasks : - debug : msg=\"Variable 'var' is set to {{ var }}\" #host variables - Variables are defined inline for individual host [ group1 ] host1 http_port=80 host2 http_port=303 #group variables - Variables are applied to entire group of hosts [ group1 : vars ] ntp_server= example.com proxy=proxy.example.com Full Play - Deploying an Nginx static site on Ubuntu # playbook.yml --- - hosts : all become : yes vars : server_name : \"{{ ansible_default_ipv4.address }}\" document_root : /var/www app_root : html_demo_site-main tasks : - name : Update apt cache and install Nginx apt : name : nginx state : latest update_cache : yes - name : Copy website files to the server's document root copy : src : \"{{ app_root }}\" dest : \"{{ document_root }}\" mode : preserve - name : Apply Nginx template template : src : files/nginx.conf.j2 dest : /etc/nginx/sites-available/default notify : Restart Nginx - name : Enable new site file : src : /etc/nginx/sites-available/default dest : /etc/nginx/sites-enabled/default state : link notify : Restart Nginx - name : Allow all access to tcp port 80 ufw : rule : allow port : '80' proto : tcp handlers : - name : Restart Nginx service : name : nginx state : restarted # Copy the static files and unzip to folder root curl -L https://github.com/do-community/html_demo_site/archive/refs/heads/main.zip -o html_demo.zip # files/nginx.conf.j2 server { listen 80; root {{ document_root }}/{{ app_root }}; index index.html index.htm; server_name {{ server_name }}; location / { default_type \"text/html\"; try_files $uri.html $uri $uri/ =404; } } # Executing the playbook with sammy user and prompting for password ansible-playbook -i inventory playbook.yml -u sammy -K Using ansible system variables in Jinja2 Templates \u00b6 Whenever you run Playbook, Ansible by default collects information (facts) about each host like host IP address, CPU type, disk space, operating system information etc. ansible host01 -i myhosts -m setup Create Dynamic templates Consider you need the IP address of all the servers in you web group using 'group' variable { % for host in groups.web % } server {{ host.inventory_hostname }} {{ host.ansible_default_ipv4.address }} :8080 { % endfor % } Create a Webservice entry in Nginx { % for host in groups. [ 'jenkins' ] % } define host { use linux-server host_name {{ host }} alias {{ host }} address {{ hostvars [ host ] .ansible_default_ipv4.address }} hostgroups jenkins } { % endfor % } # service checks to be applied to the webserver { % if jenkins_uses_proxy == true % } define service { use local-service hostgroup_name jenkins service_description HTTP check_command check_jenkins_http notifications_enabled 1 } { % endif % } Get a list of all the variables associated with the current host with the help of hostvars and inventory_hostname variables. --- - name : built-in variables hosts : all tasks : - debug : var=hostvars[inventory_hostname] Using register variables # register variable stores the output, after executing command module, in contents variable # stdout is used to access string content of register variable --- - name : check registered variable for emptiness hosts : all tasks : - name : list contents of the directory in the host command : ls /home/ubuntu register : contents - name : check dir is empty debug : msg=\"Directory is empty\" when : contents.stdout == \"\" - name : check dir has contents debug : msg=\"Directory is not empty\" when : contents.stdout != \"\" Variable Precedence => Command Line > Playbook > Facts > Roles CLI: While running the playbook in Command Line redefine the variable # Passing runtime values in plays ansible-playbook -i myhosts test.yml --extra-vars \"ansible_bios_version=Ansible\" Async \u00b6 async - How long to run the task poll - How frequently to check the task status. Default is 10 seconds async_status - Check status of an async task - name : Deploy a mysql DB hosts : db_server roles : - python - mysql_db - name : Deploy a Web Server hosts : web_server roles : - python - flask_web # Below task will run the async in parallel as poll is 0 and register the output - name : Monitor Web Application for 6 Minutes hosts : web_server command : /opt/monitor_webapp.py async : 360 poll : 0 register : webapp_result - name : Monitor Database for 6 Minutes hosts : db_server command : /opt/monitor_database.py async : 360 poll : 0 register : database_result # To avoid job from completing, async_status can be used to poll all async jobs have completed - name : Check status of async task async_status : jid={{ webapp_result.ansible_job_id }} register : job_result until : job_result.finished retries : 30 Deployment Strategy and Forks \u00b6 Serial - Default: All tasks are run after the previous once completes Free: Once the task completes in a host, it continues next execution without waiting for other hosts Batch: Based on serial, but takes action on multiple host (Rolling Updates) Forks: Deployment on multiple servers # Runs playbook on 2 servers at a time - name : Deploy a web application hosts : app_servers serial : 2 vars : db_name : employee_db db_user : db_user db_password : Passw0rd tasks : - name : Install dependencies - name : Install MySQL database - name : Start Mysql Service - name : Create Application Database - name : Create Application DB User - name : Install Python Flask dependencies - name : Copy web-server code - name : Start web-application # Deploy based on random rolling strategy name : Deploy a web application hosts : app_servers serial : - 2 - 3 - 5 # Deploy based on percentage name : Deploy a web application hosts : app_servers serial : \"20%\" # Runs playbook to fail early, suppose there are 10 servers - name : Deploy a web application hosts : app_servers serial : 5 max_fail_percentage : 50 # The number of failed hosts must exceed the value of max_fail_percentage; if it is equal, the play continues. # So, in our example, if exactly 50% of our hosts failed, the play would still continue. # The first task has a special clause under it that we use to deliberately simulate a failure\u2014this line starts with failed_when and we use it to tell the task that if it runs this task on the first tow hosts in the batch, then it should deliberately fail this task regardless of the result; otherwise, it should allow the task to run as normal. tasks : - name : A task that will sometimes fail debug : msg : This might fail failed_when : inventory_hostname in ansible_play_batch[0:3] # We'll add a second task that will always succeed. - name : A task that will succeed debug : msg : Success! - We have also deliberately set up a failure condition that causes three of the hosts in the first batch of 5 (60%) to fail. ansible-playbook -i morehosts maxfail.yml - We deliberately failed three of the first batch of 5, exceeding the threshold for max_fail_percentage that we set. - This immediately causes the play to abort and the second task is not performed on the first batch of 5. - You will also notice that the second batch of 5, out of the 10 hosts, is never processed, so our play was truly aborted. - This is exactly the behavior you would want to see to prevent a failed update from rolling out across a cluster. - Through the careful use of batches and max_fail_percentage, you can safely run automated tasks across an entire cluster without the fear of breaking the entire cluster in the event of an issue. # Deploy based on completion name : Deploy a web application hosts : app_servers strategy : free Error Handling \u00b6 Playbook Error Handling We would like Ansible to stop execution of the entire playbook if a single server was to fail. # To fail playbook on any failure and stop processing on all servers name : Deploy a web application hosts : app_servers any_errors_fatal : true # This will stop all processing # To avoid failure of playbook due to an insignificant task name : Deploy a web application hosts : app_servers tasks : - mail : to : devops@abc.com subject : Server Deployed! body : Webserver is live! ignore_errors : yes # Add this to ignore task failure - command : cat /var/log/server.log register : command_output failed_when : \"'ERROR' in command_output.stdout\" # Conditional failure of task Jinja2 Templating \u00b6 Templating: A process a generating dynamic content or expressions String Manipulation - Filters # Substitution The name is {{ my_name }} # Uppercase The name is {{ my_name | upper }} # Lowercase The name is {{ my_name | lower }} # Titlecase The name is {{ my_name | title }} # Replace The name is {{ my_name | replace(\"Bond\", \"Bourne\") }} # Default value The name is {{ first_name | default(\"James\") }} {{ my_name }} Filters - List and Set # Min {{ [ 1 , 2 , 3 ] | min }} => 1 # Max {{ [1,2,3] | min }} => 3 # Unique {{ [1,2,3,2] | unique }} => 1,2,3 # Union {{ [1,2,3,4] | union([4,5]) }} => 1,2,3,4,5 # Intersect {{ [1,2,3,4] | intersect([4,5]) }} => 4 {{ 100 | random }} => generates random number between 1 to 100 # Join {{ [\"The\",\"name\",\"is\",\"Bond\"] | join(\" \")}} => The name is Bond Filters - File {{ \"/etc/hosts\" | basename }} => hosts Filters - expanduser tasks : - name : Ensure the SSH key is present on OpenStack os_keypair : state : present name : ansible_key public_key_file : \"{{ '~' | expanduser }}/.ssh/id_rsa.pub\" Lookups \u00b6 Lookups : To get data from another source on the system # Credentials File csv Hostname,Password web_server,Passw0rd db_server,Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('csvfile', 'web_server file=/tmp/credentials.csv delimiter=,') }}\" => Passw0rd # Credentials File ini [ web_server ] password = Passw0rd [ db_server ] password = Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('ini', 'password section=web_server file=/tmp/credentials.ini') }}\" => Passw0rd Tags \u00b6 Tags are names pinned on individual tasks, roles or an entire play, that allows you to run or skip parts of your Playbook. Tags can help you while testing certain parts of your Playbook. # tag.yml --- - name : Play1-install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name=apache2 update_cache=yes state=latest - name : displaying \"hello world\" debug : msg=\"hello world\" tags : - tag1 - name : Play2-install nginx hosts : all sudo : yes tags : - tag2 tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : debug module displays message in control machine debug : msg=\"have a good day\" tags : - mymessage - name : shell module displays message in host machine. shell : echo \"yet another task\" tags : - mymessage Executing above play using tags # displays the list of tasks in the Playbook ansible-playbook -i myhosts tag.yml --list-tasks # displays only tags in your Playbook ansible-playbook -i myhosts tag.yml --list-tags # executes only certain tasks which are tagged as tag1 and mymessage ansible-playbook -i myhosts tag.yml --tags \"tag1,mymessage\" Includes (Outdated after 2.0) \u00b6 Ansible gives you the flexibility of organizing your tasks through include keyword, that introduces more abstraction and make your Playbook more easily maintainable, reusable and powerful. --- - name : testing includes hosts : all sudo : yes tasks : - include : apache.yml - include : content.yml - include : create_folder.yml - include : content.yml - include : nginx.yml # apache.yml will not have hosts & tasks but nginx.yml as a separate play will have tasks and can run independently #nginx.yml --- - name : installing nginx hosts : all sudo : yes tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : displaying message debug : msg=\"yayy!! nginx installed\" Roles \u00b6 A Role is completely self contained or encapsulated and completely reusable # cat ansible.cfg [ defaults ] host_key_checking=False inventory = /etc/ansible/myhosts log_path = /home/scrapbook/tutorial/output.txt roles_path = /home/scrapbook/tutorial/roles/ # master-playbook.yml --- - name : my first role in ansible hosts : all sudo : yes roles : - sample_role - sample_role2 # sample_role/tasks/main.yml --- - include : nginx.yml - include : copy-template.yml - include : copy-static.yml # sample_role/tasks/nginx.yml --- - name : Installs nginx apt : pkg=nginx state=installed update_cache=true notify : - start nginx # sample_role/handlers/main.yml --- - name : start nginx service : name=nginx state=started # sample_role/tasks/copy-template.yml --- - name : sample template - x template : src : template-file.j2 dest : /home/ubuntu/copy-template-file.j2 with_items : var_x # sample_role/vars/main.yml var_x : - 'variable x' var_y : - 'variable y' # sample_role/tasks/copy-static.yml # Ensure some-file.txt is present under files folder of the role --- - name : Copy a file copy : src=some-file.txt dest=/home/ubuntu/file1.txt Executing the play # Let us run the master_playbook and check the output: ansible-playbook -i myhosts master_playbook.yml Ansible Galaxy \u00b6 ansible-galaxy is command line tool for scaffolding the creation of directory structure needed for organizing your code ansible-galaxy init sample_role ansible-galaxy useful commands \u00b6 Install a Role from Ansible Galaxy To use others role, visit https://galaxy.ansible.com/ and search for the role that will achieve your goal. Goal: Install Apache in the host machines. # Ensure that roles_path are defined in ansible.cfg for a role to successfully install. # Here, apache is role name and geerlingguy is name of the user in GitHub who created the role. ansible-galaxy install geerlingguy.apache # Forcefully Recreate Role ansible-galaxy init geerlingguy.apache --force # Listing Installed Roles ansible-galaxy list # Remove an Installed Role ansible-galaxy remove geerlingguy.apache Environment Variables \u00b6 Ansible recommends maintaining inventory file for each environment, instead of keeping all your hosts in a single inventory. Each environment directory has one inventory file (hosts) and group_vars directory. Ansible Best Practises \u00b6 Inventory Files \u00b6 To show you a great way of setting up your directory structure for a simple role-based playbook that has two different inventories \u2014 one for a development environment and one for a production environment. # inventories/development/hosts [ app ] app01.dev.example.com app02.dev.example.com # inventories/development/group_vars --- http_port : 8080 # inventories/production/hosts [ app ] app01.prod.example.com app02.prod.example.com # inventories/production/group_vars --- http_port : 80 # To run it on the development inventory ansible-playbook -i inventories/development/hosts site.yml # To run it on the production inventory ansible-playbook -i inventories/production/hosts site.yml However, there are always differences between the two environments, not just in the hostnames, but also sometimes in the parameters, the load balancer names, the port numbers, and so on\u2014the list can seem endless. Try and reuse the same playbooks for all of your environments that run the same code. For example, if you deploy a web app in your development environment, you should be confident that your playbooks will deploy the same app in the production environment This means that not only are you testing your application deployments and code, you are also testing your Ansible playbooks and roles as part of your overall testing process. Your inventories for each environment should be kept in separate directory trees, but all roles, playbooks, plugins, and modules (if used) should be in the same directory structure (this should be the case for both environments). It is normal for different environments to require different authentication credentials; you should keep these separate not only for security but also to ensure that playbooks are not accidentally run in the wrong environment. Your playbooks should be in your version control system, just as your code is. This enables you to track changes over time and ensure that everyone is working from the same copy of the automation code. The proper approach to defining group and host variables \u00b6 First and foremost, you should always pay attention to variable precedence. Host variables are always of a higher order of precedence than group variables; so, you can override any group variable with a host variable. This behavior is useful if you take advantage of it in a controlled manner, but can yield unexpected results if you are not aware of it. There is a special group variables definition called all, which is applied to all inventory groups. This has a lower order of precedence than specifically defined group variables. What happens if you define the same variable twice in two groups? If this happens, both groups have the same order of precedence, so which one wins? [ app ] app01.dev.example.com app02.dev.example.com # inventories/development/group_vars/all.yml --- http_port : 8080 # inventories/development/group_vars/app.yml --- http_port : 8081 # site.yml --- - name : Play using best practise directory structure hosts : all tasks : - name : Display the value of our inventory variable debug : var : http_port ansible-playbook -i inventories/development/hosts site.yml As expected, the variable definition in the specific group won, which is in line with the order of precedence documented for Ansible. Now, let's see what happens if we define the same variable twice in two specifically named groups. To complete this example, we'll create a child group, called centos, and another group that could notionally contain hosts built to a new build standard, called newcentos, which both application servers will be a member of. [ app ] app01.dev.example.com app02.dev.example.com [centos:children] app [newcentos:children] app # inventories/development/group_vars/centos.yml --- http_port : 8082 # inventories/development/group_vars/newcentos.yml --- http_port : 8083 We've now defined the same variable four times at the group level! ansible-playbook -i inventories/development/hosts site.yml The value we entered in newcentos.yml won\u2014but why? The Ansible documentation states that where identical variables are defined at the group level in the inventory (the one place you can do this), the one from the last-loaded group wins. Groups are processed in alphabetical order and newcentos is the group with the name beginning furthest down the alphabet\u2014so, its value of http_port was the value that won. Just for completeness, we can override all of this by leaving the group_vars directory untouched, but adding a file called inventories/development/host_vars/app01.dev.example.com.yml --- http_port : 9090 We will see that the value we defined at the host level completely overrides any value that we set at the group level for app01.dev.example.com. app02.dev.example.com is unaffected as we did not define a host variable for it, so the next highest level of precedence\u2014the group variable from the newcentos group\u2014won Using top-level playbooks \u00b6 Imagine handing a playbook directory structure with 100 different playbooks to a new system administrator\u2014how would they know which ones to run and in which circumstances? The task of training someone to use the playbooks would be immense and would simply move complexity from one area to another. The most important thing is that, on receipt of a new playbook directory structure, a new operator at least knows what the starting point for both running the playbooks, and understanding the code is. If the top-level playbook they encounter is always site.yml, then at least everyone knows where to start. Through the clever use of roles and the import_* and include_* statements, you can split your playbook up into logical portions of reusable code, all from one playbook file. Leveraging version control tools \u00b6 Any changes to your Ansible code could mean big changes to your environment, and possibly even whether an important production service works or not. As a result, it is vital that you maintain a version history of your Ansible code and that everyone works from the same version. Setting OS and distribution variances \u00b6 This playbook demonstrates how you can group differing plays using an Ansible fact so that the OS distribution determines which play in a playbook gets run. # osvariants.yml - It will also contain a single task. --- - name : Play to demonstrate group_by module hosts : all tasks : - name : Create inventory groups based on host facts group_by : key : os_{{ ansible_facts['distribution'] }} group_by module: It dynamically creates new inventory groups based on the key that we specify \u2014 in this example, we are creating groups based on a key comprised of the os_ fixed string, followed by the OS distribution fact obtained from the Gathering Facts stage. The original inventory group structure is preserved and unmodified, but all the hosts are also added to the newly created groups according to their facts. So, the two servers in our simple inventory remain in the app group, but if they are based on Ubuntu, they will be added to a newly created inventory group called os_Ubuntu. Similarly, if they are based on CentOS, they will be added to a group called os_CentOS. # Play definition to the same playbook file to install Apache on CentOS - name : Play to install Apache on CentOS hosts : os_CentOS # Refer to the Dynamic group become : true tasks : - name : Install Apache on CentOS yum : name : httpd state : present # Add a third Play definition, this time for installing the apache2 package on Ubuntu using the apt module - name : Play to install Apache on Ubuntu hosts : os_Ubuntu become : true tasks : - name : Install Apache on Ubuntu apt : name : apache2 state : present ansible-playbook -i hosts osvariants.yml Notice how the task to install Apache on CentOS was run. It was run this way because the group_by module created a group called os_CentOS and our second play only runs on hosts in the group called os_CentOS. As there were no servers running on Ubuntu in the inventory, the os_Ubuntu group was never created and so the third play does not run. We receive a warning about the fact that there is no host pattern that matches os_Ubuntu, but the playbook does not fail\u2014it simply skips this play. It is up to you to choose the coding style most appropriate to you. You can make use of the group_by module, as detailed here, or write your tasks in blocks and add a when clause to the blocks so that they only run when a certain fact-based condition is met (for example, the OS distribution is CentOS)\u2014or perhaps even a combination of the two. The choice is ultimately yours and these different examples are provided to empower you with multiple options that you can choose between to create the best possible solution for your scenario. Setting task execution delegation \u00b6 We have assumed that all the tasks are executed on each host in the inventory in turn. However, what if you need to run one or two tasks on a different host? For example, we have talked about the concept of automating upgrades on clusters. Logically, however, we would want to automate the entire process, including the removal of each host in turn from the load balancer and its return after the task is completed. Although we still want to run our play across our entire inventory, we certainly don't want to run the load balancer commands from those hosts. Imagine that you have a shell script (or other executables) that you can call that can add and remove hosts to and from a load balancer. # remove_from_loadbalancer.sh #!/bin/sh echo Removing $1 from load balancer... # add_to_loadbalancer.sh #!/bin/sh echo Adding $1 to load balancer... [ frontends ] frt01.example.com frt02.example.com --- - name : Play to demonstrate task delegation hosts : frontends tasks : - name : Remove host from the load balancer command : ./remove_from_loadbalancer.sh {{ inventory_hostname }} args : chdir : \"{{ playbook_dir }}\" delegate_to : localhost - name : Deploy code to host debug : msg : Deployment code would go here.... - name : Add host back to the load balancer command : ./add_to_loadbalancer.sh {{ inventory_hostname }} args : chdir : \"{{ playbook_dir }}\" delegate_to : localhost We are using the command module to call the script we created earlier, passing the hostname from the inventory being removed from the load balancer to the script. We use the chdir argument with the playbook_dir magic variable to tell Ansible that the script is to be run from the same directory as the playbook. The special part of this task is the delegate_to directive, which tells Ansible that even though we're iterating through an inventory that doesn't contain localhost, we should run this action on localhost (we aren't copying the script to our remote hosts, so it won't run if we attempt to run it from there). Deploy task has no delegate_to directive, and so it is actually run on the remote host from the inventory (as desired): Finally, we add the host back to the load balancer using the second script we created earlier. This task is almost identical to the first. ansible-playbook -i hosts delegate.yml Notice how even though Ansible is working through the inventory (which doesn't feature localhost), the load balancer-related scripts are actually run from localhost, while the upgrade task is performed directly on the remote host. In truth, you can delegate any task to localhost, or even another non-inventory host. You could, for example, run an rsync command delegated to localhost to copy files to remote hosts using a similar task definition to the previous one. This is useful because although Ansible has a copy module, it can't perform the advanced recursive copy and update functions that rsync is capable of. Note that you can choose to use a form of shorthand notation in your playbooks (and roles) for delegate_to, called local_action . This allows you to specify a task on a single line that would ordinarily be run with delegate_to: localhost added below it. --- - name : Second task delegation example hosts : frontends tasks : - name : Perform an rsync from localhost to inventory hosts local_action : command rsync -a /tmp/ {{ inventory_hostname }}:/tmp/target/ The preceding shorthand notation is equivalent to the following: tasks : - name : Perform an rsync from localhost to inventory hosts command : rsync -a /tmp/ {{ inventory_hostname }}:/tmp/target/ delegate_to : localhost If we run this playbook, we can see that local_action does indeed run rsync from localhost, enabling us to efficiently copy whole directory trees across to remote servers in the inventory. ansible-playbook -i hosts delegate2.yml Using the run_once option \u00b6 When working with clusters, you will sometimes encounter a task that should only be executed once for the entire cluster. For example, you might want to upgrade the schema of a clustered database. Instead, you can write your code as you normally would, but make use of the special run_once directive for any tasks you want to run only once on your inventory. For example, let's reuse the 10-host inventory. --- - name : Play to demonstrate the run_once directive hosts : frontends tasks : - name : Upgrade database schema debug : msg : Upgrading database schema... run_once : true ansible-playbook -i morehosts runonce.yml Notice that, just as desired, although the playbook was run on all 10 hosts (and, indeed, gathered facts from all 10 hosts), we only ran the upgrade task on one host. It's important to note that the run_once option applies per batch of servers, so if we add serial: 5 to our play definition (running our play in two batches of 5 on our inventory of 10 servers), the schema upgrade task actually runs twice! It runs once as requested, but once per batch of servers, not once for the entire inventory. Be careful of this nuance when working with this directive in a clustered environment. Running playbooks locally \u00b6 It is important to note that when we talk about running a playbook locally with Ansible, it is not the same as talking about running it on localhost. If we run a playbook on localhost, Ansible actually sets up an SSH connection to localhost (it doesn't differentiate its behavior or attempt to detect whether a host in the inventory is local or remote - it simply tries faithfully to connect). # Inventory file [ local ] localhost ansible_connection=local We've added a special variable to our localhost entry ansible_connection variable which defines which protocol is used to connect to this inventory host. So, we have told it to use a direct local connection instead of an SSH-based connectivity (which is the default). Note that this special value for the ansible_connection variable actually overrides the hostname you have put in your inventory. So, if we change our inventory to look as follows, Ansible will not even attempt to connect to the remote host called frt01.example.com it will connect locally to the machine running the playbook (without SSH). [ local ] frt01.example.com ansible_connection=local The presence of ansible_connection=local meant that this command was run on the local machine without using SSH. This ability to run commands locally without the need to set up SSH connectivity, SSH keys, and so on can be incredibly valuable, especially if you need to get things up and running quickly on your local machine. Working with proxies and jump hosts \u00b6 Often, when it comes to configuring core network devices, these are isolated from the main network via a proxy or jump host. Ansible lends itself well to automating network device configuration as most of it is performed over SSH: however, this is only helpful in a scenario where Ansible can either be installed and operated from the jump host or, better yet, can operate via a host such as this. Let's assume that you have two Cumulus Networks switches in your network (these are based on a special distribution of Linux for switching hardware, which is very similar to Debian). These two switches have the cmls01.example.com and cmls02.example.com hostnames, but both can only be accessed from a host called bastion.example.com. [ switches ] cmls01.example.com cmls02.example.com [switches:vars] ansible_ssh_common_args='-o ProxyCommand=\"ssh -W %h:%p -q bastion.example.com\"' This special variable content ansible_ssh_common_args tells Ansible to add extra options when it sets up an SSH connection, including to proxy via the bastion.example.com host. The -W %h:%p options tell SSH to proxy the connection and to connect to the host specified by %h (this is either cmls01.example.com or cmls02.example.com) on the port specified by %p (usually port 22). ansible -i switches -m ping all On the surface, Ansible works just as it normally does and connects successfully to the two hosts. However, behind the scenes it proxies via bastion.example.com. Note that this simple example assumes that you are connecting to both the bastion host and switches using the same username and SSH credentials (or in this case, keys). Configuring playbook prompts \u00b6 All of our playbooks have had their data specified for them at run time in variables we defined within the playbook. However, what if you actually want to obtain information from someone during a playbook run? Perhaps you want to obtain a password from a user for an authentication task without storing it anywhere. Ansible can prompt you for user input and store the input in a variable for future processing. We will prompt for two variables, one for a user ID and one for a password. One will be echoed to the screen, while the other won't be, by setting private: yes --- - name : A simple play to demonstrate prompting in a playbook hosts : frontends vars_prompt : - name : loginid prompt : \"Enter your username\" private : no - name : password prompt : \"Enter your password\" private : yes tasks : - name : Proceed with login debug : msg : \"Logging in as {{ loginid }}...\" ansible-playbook -i hosts prompt.yml Ansible Security Best Practices \u00b6 Working with Ansible Vault \u00b6 It's really important to use Ansible Vault to store all the secret information in our playbooks. Some of the really good use cases include how we can use these playbooks without changing our version control systems, CI/CD integration pipelines, and so on. How to use Ansible Vault with variables and files \u00b6 Let's take an example of installing MySQL server in an Ubuntu operating system using the following playbook. As per the Ansible documentation, it's easy and better to store Vault variables and normal variables differently. \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 mysql.yml # contains vault secret values \u251c\u2500\u2500 hosts \u251c\u2500\u2500 main.yml \u2514\u2500\u2500 roles \u2514\u2500\u2500 mysqlsetup \u2514\u2500\u2500 tasks \u2514\u2500\u2500 main.yml Now, if we see the group_vars/main.yml file, the content looks as shown in the codeblock. It contains the secrets variable to use in the playbook, called mysql_root_password. mysql_root_password : supersecretpassword\u200b To encrypt the vault file, we will use the following command and it then prompts for the password to protect ansible-vault encrypt group_vars/mysql.yml # Now, to execute the playbook run the following command, it will prompt for the vault password ansible-playbook --ask-vault-pass -i hosts main.yml We can also pass the ansible-vault password file with playbook execution by specifying flag, it helps in our continuous integration and pipeline platforms. The following file contains the password which used to encrypt the mysql.yml file. cat ~/.vaultpassword thisisvaultpassword # To pass the vault password file through the command line, use the following command when executing playbooks ansible-playbook --vault-password-file ~/.vaultpassword -i hosts main.yml Note Make sure to give proper permissions for this file, so others cannot access this file using chmod. Also, it's good practice to add this file to your .gitignore, so it will not be version controlled when pushing playbooks. Vault password file can be an executable script, which can retrieve data stored somewhere securely rather than having to keep the key in plain text on disk and relying on file permissions to keep it safe. We can also use system environment variables such as ANSIBLE_VAULT_PASSWORD_FILE=~/.vaultpassword and Ansible will use this while executing playbooks. Ansible Vault single encrypted variable \u00b6 It allows us to use vaulted variables with the !vault tag in YAML files This playbook is used to perform reverse IP lookups using the ViewDNS API. We want to secure api_key as it contains sensitive information. # We use the ansible-vault encrypt_string command to perform this encryption. # Here, we used echo with the -n flag to remove the new line echo -n '53ff4ad63849e6977cb652763g7b7c64e2fa42a' | ansible-vault encrypt_string --stdin-name 'api_key' We can place the variable, inside the playbook variables and execute the playbook as normal, using ansible-playbook with the \u2013ask-vault-pass option. - name : ViewDNS domain information hosts : localhost vars : domain : google.com api_key : !vault | $ANSIBLE_VAULT;1.1;AES256 36623761316238613461326466326162373764353437393733343334376161336630333532626465 6662383435303930303164353664643639303761353664330a393365633237306530653963353764 64626237313738656530373639653739656564316161663831653431623832336635393637653330 6632663563363264340a323537356166653338396135376161323435393730306133626635376539 37383861653239326336613837666237636463396465393662666561393132343166666334653465 6265386136386132363534336532623061646438363235383334 output_type : json tasks : - name : \"getting {{ domain }} server info\" uri : url : \"https://api.viewdns.info/reverseip/?host={{ domain }}&apikey={{ api_key }}&output={{ output_type }}\" method : GET register : results - debug : msg : \"{{ results.json }}\" Playbook being executed will be automatically decrypted after we provide it with the given password. ansible-playbook --ask-vault-pass -i hosts main.yml Setting up and using Ansible Galaxy \u00b6 Is an official centralized hub for finding, sharing, and reusing Ansible roles. This allows the community to share and collaborate on Ansible playbooks, and allows new users to quickly get started with using Ansible. To share our custom-written roles with the community, we can publish them to Ansible Galaxy using GitHub authentication. We can install or include roles direct from GitHub by specifying the GitHub URL. This allows the use of private version control systems as local inventories of playbook roles. ansible-galaxy install git+https://github.com/geerlingguy/ansible-role-composer.git Ansible controller machine security \u00b6 The controller machine for Ansible requires SSH and Python to be installed and configured. Ansible has a very low attack surface. Note In January 2017, multiple security issues were found by a company called Computest . This vulnerability was dubbed owning the farm, since compromising the controller would imply that all the nodes could potentially be compromised. The controller machine should be a hardened server and treated with all the seriousness that it deserves. In the vulnerability that was disclosed, if a node gets compromised attackers could leverage that to attack and gain access to the controller. - Once they have access, the could extend their control over all the other nodes being managed by the controller. Since the attack surface is already very limited, the best we can do is ensure that the server stays secure and hardened. Explanation of Ansible OS hardening playbook \u00b6 The following playbook is created by DevSec for Linux baselines. It covers most of the required hardening checks based on multiple standards, which includes Ubuntu Security Features, NSA Guide to Secure Configuration, ArchLinux System Hardening and other. This can be improved if required by adding more tasks (or) roles. Ansible OS Hardening Playbook covers Configures package management, that is, allows only signed packages Removes packages with known issues Configures pam and the pam_limits module Shadow password suite configuration Configures system path permissions Disables core dumps through soft limits Restricts root logins to system console Sets SUIDs Configures kernel parameters through sysctl # download the os-hardening role from Ansible Galaxy ansible-galaxy install dev-sec.os-hardening Call that role in your playbook and execute it to perform the baseline hardening, and also change the variables as required. Refer to https://galaxy.ansible.com/dev-sec/os-hardening for more detailed options. - hosts : localhost become : yes roles : - dev-sec.os-hardening # Execute the playbook ansible-playbook main.yml Best practices and reference playbook projects \u00b6 Projects such as Algo, DebOps, and OpenStack are large Ansible playbook projects that are well maintained and secure by default. DebOps \u2013 your Debian-based data center in a box \u00b6 DebOps is a project created by Maciej Delmanowski. It contains a collection of various Ansible playbooks that can be used for Debian and Ubuntu hosts. This project has more than 128 Ansible roles, which are customized for production use cases and work with multiple environments. We can see a list of available playbook services at debops/debops-playbooks There are two different ways we can quickly get started with a DebOps setup: Vagrant setup Docker setup Algo \u2013 set up a personal IPSEC VPN in the cloud \u00b6 Algo from Trail of Bits provides Ansible roles and scripts to automate the installation of a personal IPSEC VPN. By running the Ansible playbooks, you get a complete hardened VPN server, and deployments to all major cloud providers are already configured ( https://github.com/trailofbits/algo/blob/master/docs/deploy-from-ansible.md ). OpenStack-Ansible \u00b6 Not only does this project use Ansible playbooks extensively, but their security documentation is also worth reading and emulating. The best part is that all of the security configuration is declarative security codified in Ansible playbooks. Documentation on this project is available at https://docs.openstack.org/project-deploy-guide/openstack-ansible/ocata/app-security.html . AWX \u2013 open source version of Ansible Tower \u00b6 AWX provides a web-based user interface, REST API, and task engine built on top of Ansible. AWX can be used with the tower-CLI tool and client library. Get started with AWX here: ansible/awx . Get started with tower-cli here: ansible/tower-cli .","title":"Ansible"},{"location":"learning/ansible/ansible/#introduction","text":"Note The rule of thumb is if you can script it, you can create a playbook for it. Ansible 101 Ansible Cheat Sheet App Security Ansible Tips and Tricks DO Practise examples & Explaination & Tutorials YAML Spec Ref Card Full Application on Cloud Maintaining Playbooks - Pitfalls","title":"Introduction"},{"location":"learning/ansible/ansible/#documentation-on-cmdline","text":"# System outputs the man page for debug module ansible-doc debug ansible-doc -l | grep aws # List all plugins and then grep for aws plugins ansible-doc -s shell # Shows snippets on how to use the plugins","title":"Documentation on cmdline"},{"location":"learning/ansible/ansible/#setup-server","text":"","title":"Setup Server"},{"location":"learning/ansible/ansible/#default-configuration","text":"ansible.cfg and hosts files are present inside /etc/ansible Testing ansible on Ubuntu WSL ansible localhost -m ping","title":"Default Configuration"},{"location":"learning/ansible/ansible/#enabling-ssh-on-the-vm","text":"If you need SSH enabled on the system, follow the below steps: Ensure the /etc/apt/sources.list file has been updated as per above Run the command: apt-get update Run the command: apt-get install openssh-server Run the command: service sshd start ssh-keygen -t rsa -C \"ansible\" #OR # Generate an SSH key pair for future connections to the VM instances (run the command exactly as it is): ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user -P \"\" #Add the SSH private key to the ssh-agent: ssh-add ~/.ssh/ansible-user #Verify that the key was added to the ssh-agent: $ ssh-add -l","title":"Enabling SSH on the VM"},{"location":"learning/ansible/ansible/#access-vm-over-ssh","text":"ssh vagrant@127.0.0.1 -p 2222 -i ~/.ssh/insecure_private_key","title":"Access VM over SSH"},{"location":"learning/ansible/ansible/#copy-files-recursively-from-local-desktop-to-remote-server","text":"scp -r ./scripts vagrant@127.0.0.1:/home/vagrant -p 2222 -i ~/.ssh/insecure_private_key","title":"Copy files recursively from local desktop to remote server"},{"location":"learning/ansible/ansible/#target-docker-containers-for-ansible-controller","text":"The Docker file used to create the ubuntu-ssh-enabled Docker image is located here.","title":"Target Docker containers for Ansible controller"},{"location":"learning/ansible/ansible/#issues-installing-ansible-and-its-dependencies","text":"Once the Debian VM is up and running make the following changes to the /etc/apt/sources.list file to get the Ansible installation working right. deb http://security.debian.org/ jessie/updates main contrib deb-src http://security.debian.org/ jessie/updates main contrib deb http://ftp.debian.org/debian/ jessie-updates main contrib deb-src http://ftp.debian.org/debian/ jessie-updates main contrib deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main deb http://ftp.de.debian.org/debian sid main","title":"Issues installing Ansible and its dependencies"},{"location":"learning/ansible/ansible/#ansible-directory-structure-as-per-best-practises","text":"This is the directory layout of this repository with explanation. production.ini # inventory file for production stage development.ini # inventory file for development stage test.ini # inventory file for test stage vpass # ansible-vault password file # This file should not be committed into the repository # therefore file is ignored by git ########################## This segration can be done if there are changes in variable values between environments \u251c\u2500\u2500 inventories \u2502 \u251c\u2500\u2500 development \u2502 \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u2502 \u2514\u2500\u2500 app.yml \u2502 \u2502 \u251c\u2500\u2500 hosts \u2502 \u2502 \u2514\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 production \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u2514\u2500\u2500 app.yml \u2502 \u251c\u2500\u2500 hosts \u2502 \u2514\u2500\u2500 host_vars ########################## group_vars/ all/ # variables under this directory belongs all the groups apt.yml # ansible-apt role variable file for all groups webservers/ # here we assign variables to webservers groups apt.yml # Each file will correspond to a role i.e. apt.yml nginx.yml # \"\" postgresql/ # here we assign variables to postgresql groups postgresql.yml # Each file will correspond to a role i.e. postgresql postgresql-password.yml # Encrypted password file plays/ ansible.cfg # Ansible.cfg file that holds all ansible config webservers.yml # playbook for webserver tier postgresql.yml # playbook for postgresql tier roles/ roles_requirements.yml# All the information about the roles external/ # All the roles that are in git or ansible galaxy # Roles that are in roles_requirements.yml file will be downloaded into this directory internal/ # All the roles that are not public common/ # common role tasks/ # main.yml # installing basic tasks scripts/ setup/ # All the setup files for updating roles and ansible dependencies","title":"Ansible Directory Structure as per Best Practises"},{"location":"learning/ansible/ansible/#ansible-inventory","text":"","title":"Ansible Inventory"},{"location":"learning/ansible/ansible/#creating-an-inventory-file-and-adding-hosts","text":"Ansible supports two types of inventory\u2014static and dynamic Static inventories are by their very nature static; they are unchanging unless a human being goes and manually edits them. Even in small, closed environments, static inventories are a great way to manage your environment, especially when changes to the infrastructure are infrequent. # Sample inventory file in INI format target1.example.com ansible_host=192.168.81.142 ansible_port=3333 target2.example.com ansible_port=3333 ansible_user=danieloh target3.example.com ansible_host=192.168.81.143 ansible_port=5555 ansible_host: If the inventory hostname cannot be accessed directly\u2014perhaps because it is not in DNS, for example, this variable contains the hostname or IP address that Ansible will connect to instead. ansible_port: By default, Ansible attempts all communication over port 22 for SSH\u2014if you have an SSH daemon running on another port, you can tell Ansible about it using this variable. ansible_user: By default, Ansible will attempt to connect to the remote host using the current user account you are running the Ansible command from\u2014you can override this in several ways, of which this is one. Hence, the preceding three hosts can be summarized as follows: The target1.example.com host should be connected to using the 192.168.81.142 IP address, on port 3333. The target2.example.com host should be connected to on port 3333 also, but this time using the danieloh user rather than the account running the Ansible command. The target3.example.com host should be connected to using the 192.168.81.143 IP address, on port 5555.","title":"Creating an inventory file and adding hosts"},{"location":"learning/ansible/ansible/#using-host-groups","text":"Let's assume you have a simple three-tier web architecture, with multiple hosts in each tier for high availability and/or load balancing. The three tiers in this architecture might be the following: Frontend servers Application servers Database servers To keep the examples clear and concise, we'll assume that you can access all servers using their Fully Qualified Domain Names (FQDNs) , and hence won't add any host variables into these inventory files. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com We have created three groups called frontends, apps, and databases. Note that, in INI-formatted inventories, group names go inside square braces. Under each group name goes the server names that belong in each group, so the preceding example shows two servers in each group. Notice the outlier at the top, loadbalancer.example.com \u2014 this host isn't in any group. All ungrouped hosts must go at the very top of an INI-formatted file. The preceding inventory stands in its own right, but what if our frontend servers are built on Ubuntu, and the app and database servers are built on CentOS? There will be some fundamental differences in the ways we handle these hosts \u2014 for example, we might use the apt module to manage packages on Ubuntu and the yum module on CentOS. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends With the use of the children keyword in the group definition (inside the square braces), we can create groups of groups; hence, we can perform clever groupings to help our playbook design without having to specify each host more than once. ansible -i hostgroups-yml centos -m shell -a 'echo hello-yaml' -f 5 This is a powerful way of managing your inventory and making it easy to run commands on just the hosts you want to. The possibility of creating multiple groups makes life simple and easy, especially when you want to run different tasks on different groups of servers. Let's assume you have 100 app servers, all named sequentially, as follows: app01 to app100 [ apps ] app[01:100].prod.com The following inventory snippet actually produces an inventory with the same 100 app servers that we could create manually.","title":"Using host groups"},{"location":"learning/ansible/ansible/#adding-host-and-group-variables-to-your-inventory","text":"-Suppose that we need to set two variables for each of our two frontend servers. These are not special Ansible variables, but instead are variables entirely of our own choosing. - https_port, which defines the port that the frontend proxy should listen on - lb_vip, which defines the FQDN of the load-balancer in front of the frontend servers - You can assign variables to a host group as well as to hosts individually. [ frontends ] frt01.example.com frt02.example.com [frontends:vars] https_port=8443 lb_vip=lb.example.com - There will be times when you want to work with host variables for individual hosts, and times when group variables are more relevant. - It is also worth noting that host variables override group variables , so if we need to change the connection port to 8444 on the frt01.example.com one, we could do this as follows [ frontends ] frt01.example.com https_port=8444 frt02.example.com [frontends:vars] https_port=8443 lb_vip=lb.example.com - Right now, our examples are small and compact and only contain a handful of groups and variables; however, when you scale this up to a full infrastructure of servers, using a single flat inventory file could, once again, become unmanageable. - Luckily, Ansible also provides a solution to this. Two specially-named directories, host_vars and group_vars , are automatically searched for appropriate variable content if they exist within the playbook directory. - Under the host_vars directory, we'll create a file with the name of our host that needs the proxy setting, with .yml appended to it (that is, frt01.example.com.yml). --- https_port : 8444 - Under the group_vars directory, create a YAML file named after the group to which we want to assign variables (that is, frontends.yml) --- https_port : 8443 lb_vip : lb.example.com - Finally, we will create our inventory file as before, except that it contains no variables. # Final directory structure should look like this \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 frontends.yml \u251c\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 frt01.example.com.yml \u2514\u2500\u2500 inventory Note If you define the same variable at both a group level and a child group level, the variable at the child group level takes precedence. Consider our earlier inventory where we used child groups to differentiate between CentOS and Ubuntu hosts \u2014 if we add a variable with the same name to both the ubuntu child group and the frontends group (which is a child of the ubuntu group) as follows, what will the outcome be? loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [frontends:vars] testvar=childgroup [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends [ubuntu:vars] testvar=group Debugging variable at host level ansible -i hostgroups-children-vars-ini ubuntu -m debug -a \"var=testvar\" # Output frt01.example.com | SUCCESS = > { \"testvar\" : \"childgroup\" } frt02.example.com | SUCCESS = > { \"testvar\" : \"childgroup\" } It's important to note that the frontends group is a child of the ubuntu group in this inventory (hence, the group definition is [ubuntu:children]), and so the variable value we set at the frontends group level wins as this is the child group in this scenario.","title":"Adding host and group variables to your inventory"},{"location":"learning/ansible/ansible/#special-host-management-using-patterns","text":"Let's look at how Ansible can work with patterns to figure out which hosts a command (or playbook) should be run against. loadbalancer.example.com [frontends] frt01.example.com frt02.example.com [apps] app01.example.com app02.example.com [databases] dbms01.example.com dbms02.example.com [centos:children] apps databases [ubuntu:children] frontends We shall use the \u2013list-hosts switch with the ansible command to see which hosts Ansible would operate on. ansible -i hostgroups-children-ini all --list-hosts # The asterisk character has the same effect as all, but needs to be quoted in single quotes for the shell to interpret the command properly ansible -i hostgroups-children-ini '*' --list-hosts # Use : to specify a logical OR, meaning \"apply to hosts either in this group or that group,\" ansible -i hostgroups-children-ini frontends:apps --list-hosts # Use ! to exclude a specific group\u2014you can combine this with other characters such as : to show all hosts except those in the apps group. # Again, ! is a special character in the shell and so you must quote your pattern string in single quotes for it to work. ansible -i hostgroups-children-ini 'all:!apps' --list-hosts # Use :& to specify a logical AND between two groups, for example, if we want all hosts that are in the centos group and the apps group . ansible -i hostgroups-children-ini 'centos:&apps' --list-hosts # Use * wildcards ansible -i hostgroups-children-ini 'db*.example.com' --list-hosts # Another way you can limit which hosts a command is run on is to use the --limit switch with Ansible. ansible-playbook -i hostgroups-children-ini site.yml --limit frontends:apps","title":"Special host management using patterns"},{"location":"learning/ansible/ansible/#webapp--installation-instructions-for-centos-7","text":"","title":"WebApp  Installation Instructions for Centos 7"},{"location":"learning/ansible/ansible/#install-python-pip-and-dependencies-on-centos-7","text":"sudo yum install -y epel-release python python-pip sudo pip install flask flask-mysql If you come across a certification validation error while running the above command, please use the below command. sudo pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org flask flask-mysql","title":"Install Python Pip and dependencies on Centos 7"},{"location":"learning/ansible/ansible/#install-mysql-server-on-centos-7","text":"wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm sudo yum update sudo yum -y install mysql-server sudo service mysql start The complete playbook to get the same workin on CentOS is here: https://github.com/kodekloudhub/simple_web_application","title":"Install MySQL Server on Centos 7"},{"location":"learning/ansible/ansible/#setting-up-ansible-to-run-on-localhost-always","text":"Beneficial when testing roles or playbooks in docker images Remember: To set the hosts parameter matches in ansible.cfg This is useful for debugging ansible modules and syntax without having to use VMs or test in dev environments.","title":"Setting up Ansible to run on localhost always"},{"location":"learning/ansible/ansible/#install-ansible","text":"pip install ansible","title":"Install ansible"},{"location":"learning/ansible/ansible/#make-some-relevant-config-files","text":"~/.ansible.cfg : [ defaults ] inventory = ~/.ansible-hosts ~/.ansible-hosts : localhost ansible_connection=local","title":"Make some relevant config files"},{"location":"learning/ansible/ansible/#make-a-test-playbook-and-run","text":"helloworld.yml : --- - hosts : all tasks : - shell : echo 'hello world' - run! ansible-playbook helloworld.yml","title":"Make a test playbook and run"},{"location":"learning/ansible/ansible/#executing-ansible-playbook","text":"","title":"Executing Ansible Playbook"},{"location":"learning/ansible/ansible/#launching-ansible-situational-commands","text":"# To check the inventory file ansible-inventory --list -y # Test Connection ansible all -m ping -u root # Ask for Sudo password ansible all -m ping --ask-pass # Using a specific SSH private key and a user ansible -m ping hosts --private-key = ~/.ssh/keys/id_rsa -u centos # Check the disk usage of all servers ansible all -a \"df -h\" -u root # Check the time of `uptime` each host in a group **servers** ansible servers -a \"uptime\" -u root # Specify multiple hosts by separating their names with colons ansible server1:server2 -m ping -u root # Get system dat in json format of target ansible target1 -i myhosts -m setup --private-key = ~/.ssh/ansible-user -u root # Filter json output ansible target1 -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root ansible all -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root","title":"Launching Ansible situational commands"},{"location":"learning/ansible/ansible/#launching-ansible-playbook-situational-commands","text":"ansible-playbook -i myhosts site.yml # Ask for Sudo password ansible-playbook myplaybook.yml --ask-become-pass # Or use the -K option ansible-playbook -i inventory myplaybook.yml -u sammy -K # Execute a play without making any changes to the remote servers ansible-playbook myplaybook.yml --list-tasks # List all hosts that would be affected by a play ansible-playbook myplaybook.yml --list-hosts ansible-playbook -i myhosts playbooks/atmo_playbook.yml --user atmouser # Passing variables which executing playbooks ansible-playbook playbooks/atmo_playbook.yml -e \"ATMOUSERNAME=atmouser\" ansible host01 -i myhosts -m copy -a \"src=test.txt dest=/tmp/\" ansible host01 -i myhosts -m file -a \"dest=/tmp/test mode=644 state=directory\" ansible host01 -i myhosts -m apt -a \"name=sudo state=latest\" ansible host01 -i myhosts -m shell -a \"echo $TERM \" ansible host01 -i myhosts -m command -a \"mkdir folder1\" # Run playbook on one host ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1\" # Run playbook on multiple hosts ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1,host2\" # Flush Ansible memory f pevious runs ansible-playbook playbooks/PLAYBOOK_NAME.yml --flush-cache # Dry run mode ansible-playbook playbooks/PLAYBOOK_NAME.yml --check # Starts playbook execution from an intermediate task, task name should match ansible-playbook myplaybook.yml --start-at-task = \"Set Up Nginx\" # Increasing debug verbosity ansible-playbook myplaybook.yml -v ansible-playbook myplaybook.yml -vvvv","title":"Launching Ansible Playbook situational commands"},{"location":"learning/ansible/ansible/#launching-ansible-vault-situational-commands","text":"# Create new encrypted file, enter password ansible-vault encrypt credentials.yml # View the contents of encrypted file ansible-vault view credentials.yml # Edit the encrypted file ansible-vault edit credentials.yml # Permanently decrypt the file ansible-vault decrypt credentials.yml # Creating multiple vaults per env like dev, prod # create a new vault ID named dev that uses prompt as password source. # Prompt will ask you to enter a password, or a valid path to a password file. ansible-vault create --vault-id dev@prompt credentials_dev.yml ansible-vault create --vault-id prod@prompt credentials_prod.yml # Editing , Decrypting multiple vaults ansible-vault edit credentials_dev.yml --vault-id dev@prompt # Using Password file when using 3rd party automation ansible-vault create --vault-id dev@path/to/passfile credentials_dev.yml # Running playbooks with encrypted password ansible-playbook myplaybook.yml --ask-vault-pass # Passing password file ansible-playbook myplaybook.yml --vault-password-file my_vault_password.py # Passing multi env password ansible-playbook myplaybook.yml --vault-id dev@prompt ansible-playbook myplaybook.yml --vault-id dev@vault_password.py --vault-id test@prompt --vault-id ci@prompt # To change the vault password for key rotation ansible-vault rekey credentials.yml","title":"Launching Ansible Vault situational commands"},{"location":"learning/ansible/ansible/#understanding-the-playbook-framework","text":"A playbook allows you to manage multiple configurations and complex deployments on many machines simply and easily. This is one of the key benefits of using Ansible for the delivery of complex applications. With playbooks, you can organize your tasks in a logical structure as tasks are (generally) executed in the order they are written, allowing you to have a good deal of control over your automation processes. # Example inventory [ frontends ] frt01.example.com https_port=8443 frt02.example.com http_proxy=proxy.example.com [frontends:vars] ntp_server=ntp.frt.example.com proxy=proxy.frt.example.com [apps] app01.example.com app02.example.com [webapp:children] frontends apps [webapp:vars] proxy_server=proxy.webapp.example.com health_check_retry=3 health_check_interal=60 Create a simple playbook to run on the hosts in the frontends host group defined in our inventory file. We can set the user that will access the hosts using the remote_user directive in the playbook --- - hosts : frontends remote_user : danieloh tasks : - name : simple connection test ping : remote_user : danieloh The ignore_errors directive to this task to ensure that our playbook doesn't fail if the ls command fails (for example, if the directory we're trying to list doesn't exist). - name : run a simple command shell : /bin/ls -al /nonexistent ignore_errors : True","title":"Understanding the playbook framework"},{"location":"learning/ansible/ansible/#defining-plays-and-tasks","text":"So far when we have worked with playbooks, we have been creating one single play per playbook (which logically is the minimum you can do). However, you can have more than one play in a playbook, and a \"play\" in Ansible terms is simply a set of tasks (and roles, handlers, and other Ansible facets) associated with a host (or group of hosts). A task is the smallest possible element of a play and is responsible for running a single module with a set of arguments to achieve a specific goal.","title":"Defining plays and tasks"},{"location":"learning/ansible/ansible/#understanding-roles","text":"Roles are designed to enable you to efficiently and effectively reuse Ansible code. They always follow a known structure and often will include sensible default values for variables, error handling, handlers, and so on. The process of creating roles is in fact very simple\u2014Ansible will (by default) look within the same directory as you are running your playbook from for a roles/ directory. The role name is derived from the subdirectory name\u2014there is no need to create complex metadata or anything else\u2014it really is that simple. Within each subdirectory goes a fixed directory structure that tells Ansible what the tasks, default variables, handlers, and so on are for each role. The roles/ directory is not the only play Ansible will look for roles\u2014this is the first directory it will look in, but it will then look in /etc/ansible/roles for any additional roles.","title":"Understanding roles"},{"location":"learning/ansible/ansible/#setting-up-role-based-variables-and-dependencies","text":"The Ansible role directory structure allows for role-specific variables to be declared in two locations. Although, at first, the difference between these two locations may not seem obvious, it is of fundamental importance. Roles based variables can go in one of two locations: defaults/main.yml vars/main.yml Variables that go in the defaults/ directory are one of the lowest in terms of precedence and so are easily overwritten. This location is where you would put variables that you want to override easily, but where you don't want to leave a variable undefined. For example, if you are installing Apache Tomcat, you might build a role to install a specific version. However, you don't want the role to exit with an error if someone forgets to set the version\u2014rather, you would prefer to set a sensible default such as 7.0.76, which can then be overridden with inventory variables or on the command line (using the -e or \u2013extra-vars switches). In this way, you know the role will work even without someone explicitly setting this variable, but it can easily be changed to a newer Tomcat version if desired. Variables that go in the vars/ directory, however, come much higher up on Ansible's variable precedence ordering. This will not be overridden by inventory variables, and so should be used for variable data that it is more important to keep static. Of course, this is not to say they can't be overridden\u2014the -e or \u2013extra-vars switches are the highest order of precedence in Ansible and so will override anything else that you define. Most of the time, you will probably make use of the defaults/ based variables alone, but there will doubtless be times when having the option of variables higher up the precedence ordering becomes valuable to your automation, and so it is vital to know that this option is available to you. Note I recommend that you make extensive use of the debug statement and test your playbook design to make sure that you don't fall foul of this during your playbook development.","title":"Setting up role-based variables and dependencies"},{"location":"learning/ansible/ansible/#ansible-playbook-examples","text":"Install Software only if it doesn't exist - name : installing python2 minimal raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) Install latest software version - hosts : host01 --- become : true tasks : - name : ensure latest sysstat is installed apt : name : sysstat state : latest Install software on all hosts --- - name : install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name : apache2 update_cache : yes state : latest Copy file only when it does not exists --- - hosts : host01 tasks : - stat : path : /home/ubuntu/folder1/something.j2 register : st - name : Template to copy file template : src : ./something.j2 dest : /home/ubuntu/folder1/something.j2 mode : '0644' when : st.stat.exists == False Add users using Loops # Looping - name : add several users user : name : \"{{ item }}\" state : present groups : \"developer\" with_items : - raj - david - john - lauren Using Looping with debug # show all the hosts in the inventory - debug : msg : \"{{ item }}\" with_items : - \"{{ groups['all'] }}\" # show all the hosts in the current play - debug : msg : \"{{ item }}\" with_items : - \"{{ play_hosts }}\" Conditionals # This Playbook will add Java Packages to different systems (handling Ubuntu/Debian OS) - name : debian | ubuntu | add java ppa repo apt_repository : repo=ppa:webupd8team/java state=present become : yes when : ansible_distribution == 'Ubuntu' - name : debian | ensure the webupd8 launchpad apt repository is present apt_repository : repo=\"{{ item }} http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" update_cache=yes state=present with_items : - deb - deb-src become : yes when : ansible_distribution == 'Debian' Full Play --- - name : install software hosts : host01 sudo : yes tasks : - name : Update and upgrade apt packages apt : upgrade : dist update_cache : yes cache_valid_time : 86400 - name : install software apt : name : \"{{item}}\" update_cache : yes state : installed with_items : - nginx - postgresql - postgresql-contrib - libpq-dev - python-psycopg2 - name : Ensure the Nginx service is running service : name : nginx state : started enabled : yes - name : Ensure the PostgreSQL service is running service : name : postgresql state : started enabled : yes #file: vars.yml --- var : 20 #file: playbook.yml --- - hosts : all vars_files : - vars.yml tasks : - debug : msg=\"Variable 'var' is set to {{ var }}\" #host variables - Variables are defined inline for individual host [ group1 ] host1 http_port=80 host2 http_port=303 #group variables - Variables are applied to entire group of hosts [ group1 : vars ] ntp_server= example.com proxy=proxy.example.com Full Play - Deploying an Nginx static site on Ubuntu # playbook.yml --- - hosts : all become : yes vars : server_name : \"{{ ansible_default_ipv4.address }}\" document_root : /var/www app_root : html_demo_site-main tasks : - name : Update apt cache and install Nginx apt : name : nginx state : latest update_cache : yes - name : Copy website files to the server's document root copy : src : \"{{ app_root }}\" dest : \"{{ document_root }}\" mode : preserve - name : Apply Nginx template template : src : files/nginx.conf.j2 dest : /etc/nginx/sites-available/default notify : Restart Nginx - name : Enable new site file : src : /etc/nginx/sites-available/default dest : /etc/nginx/sites-enabled/default state : link notify : Restart Nginx - name : Allow all access to tcp port 80 ufw : rule : allow port : '80' proto : tcp handlers : - name : Restart Nginx service : name : nginx state : restarted # Copy the static files and unzip to folder root curl -L https://github.com/do-community/html_demo_site/archive/refs/heads/main.zip -o html_demo.zip # files/nginx.conf.j2 server { listen 80; root {{ document_root }}/{{ app_root }}; index index.html index.htm; server_name {{ server_name }}; location / { default_type \"text/html\"; try_files $uri.html $uri $uri/ =404; } } # Executing the playbook with sammy user and prompting for password ansible-playbook -i inventory playbook.yml -u sammy -K","title":"Ansible Playbook Examples"},{"location":"learning/ansible/ansible/#using-ansible-system-variables-in-jinja2-templates","text":"Whenever you run Playbook, Ansible by default collects information (facts) about each host like host IP address, CPU type, disk space, operating system information etc. ansible host01 -i myhosts -m setup Create Dynamic templates Consider you need the IP address of all the servers in you web group using 'group' variable { % for host in groups.web % } server {{ host.inventory_hostname }} {{ host.ansible_default_ipv4.address }} :8080 { % endfor % } Create a Webservice entry in Nginx { % for host in groups. [ 'jenkins' ] % } define host { use linux-server host_name {{ host }} alias {{ host }} address {{ hostvars [ host ] .ansible_default_ipv4.address }} hostgroups jenkins } { % endfor % } # service checks to be applied to the webserver { % if jenkins_uses_proxy == true % } define service { use local-service hostgroup_name jenkins service_description HTTP check_command check_jenkins_http notifications_enabled 1 } { % endif % } Get a list of all the variables associated with the current host with the help of hostvars and inventory_hostname variables. --- - name : built-in variables hosts : all tasks : - debug : var=hostvars[inventory_hostname] Using register variables # register variable stores the output, after executing command module, in contents variable # stdout is used to access string content of register variable --- - name : check registered variable for emptiness hosts : all tasks : - name : list contents of the directory in the host command : ls /home/ubuntu register : contents - name : check dir is empty debug : msg=\"Directory is empty\" when : contents.stdout == \"\" - name : check dir has contents debug : msg=\"Directory is not empty\" when : contents.stdout != \"\" Variable Precedence => Command Line > Playbook > Facts > Roles CLI: While running the playbook in Command Line redefine the variable # Passing runtime values in plays ansible-playbook -i myhosts test.yml --extra-vars \"ansible_bios_version=Ansible\"","title":"Using ansible system variables in Jinja2 Templates"},{"location":"learning/ansible/ansible/#async","text":"async - How long to run the task poll - How frequently to check the task status. Default is 10 seconds async_status - Check status of an async task - name : Deploy a mysql DB hosts : db_server roles : - python - mysql_db - name : Deploy a Web Server hosts : web_server roles : - python - flask_web # Below task will run the async in parallel as poll is 0 and register the output - name : Monitor Web Application for 6 Minutes hosts : web_server command : /opt/monitor_webapp.py async : 360 poll : 0 register : webapp_result - name : Monitor Database for 6 Minutes hosts : db_server command : /opt/monitor_database.py async : 360 poll : 0 register : database_result # To avoid job from completing, async_status can be used to poll all async jobs have completed - name : Check status of async task async_status : jid={{ webapp_result.ansible_job_id }} register : job_result until : job_result.finished retries : 30","title":"Async"},{"location":"learning/ansible/ansible/#deployment-strategy-and-forks","text":"Serial - Default: All tasks are run after the previous once completes Free: Once the task completes in a host, it continues next execution without waiting for other hosts Batch: Based on serial, but takes action on multiple host (Rolling Updates) Forks: Deployment on multiple servers # Runs playbook on 2 servers at a time - name : Deploy a web application hosts : app_servers serial : 2 vars : db_name : employee_db db_user : db_user db_password : Passw0rd tasks : - name : Install dependencies - name : Install MySQL database - name : Start Mysql Service - name : Create Application Database - name : Create Application DB User - name : Install Python Flask dependencies - name : Copy web-server code - name : Start web-application # Deploy based on random rolling strategy name : Deploy a web application hosts : app_servers serial : - 2 - 3 - 5 # Deploy based on percentage name : Deploy a web application hosts : app_servers serial : \"20%\" # Runs playbook to fail early, suppose there are 10 servers - name : Deploy a web application hosts : app_servers serial : 5 max_fail_percentage : 50 # The number of failed hosts must exceed the value of max_fail_percentage; if it is equal, the play continues. # So, in our example, if exactly 50% of our hosts failed, the play would still continue. # The first task has a special clause under it that we use to deliberately simulate a failure\u2014this line starts with failed_when and we use it to tell the task that if it runs this task on the first tow hosts in the batch, then it should deliberately fail this task regardless of the result; otherwise, it should allow the task to run as normal. tasks : - name : A task that will sometimes fail debug : msg : This might fail failed_when : inventory_hostname in ansible_play_batch[0:3] # We'll add a second task that will always succeed. - name : A task that will succeed debug : msg : Success! - We have also deliberately set up a failure condition that causes three of the hosts in the first batch of 5 (60%) to fail. ansible-playbook -i morehosts maxfail.yml - We deliberately failed three of the first batch of 5, exceeding the threshold for max_fail_percentage that we set. - This immediately causes the play to abort and the second task is not performed on the first batch of 5. - You will also notice that the second batch of 5, out of the 10 hosts, is never processed, so our play was truly aborted. - This is exactly the behavior you would want to see to prevent a failed update from rolling out across a cluster. - Through the careful use of batches and max_fail_percentage, you can safely run automated tasks across an entire cluster without the fear of breaking the entire cluster in the event of an issue. # Deploy based on completion name : Deploy a web application hosts : app_servers strategy : free","title":"Deployment Strategy and Forks"},{"location":"learning/ansible/ansible/#error-handling","text":"Playbook Error Handling We would like Ansible to stop execution of the entire playbook if a single server was to fail. # To fail playbook on any failure and stop processing on all servers name : Deploy a web application hosts : app_servers any_errors_fatal : true # This will stop all processing # To avoid failure of playbook due to an insignificant task name : Deploy a web application hosts : app_servers tasks : - mail : to : devops@abc.com subject : Server Deployed! body : Webserver is live! ignore_errors : yes # Add this to ignore task failure - command : cat /var/log/server.log register : command_output failed_when : \"'ERROR' in command_output.stdout\" # Conditional failure of task","title":"Error Handling"},{"location":"learning/ansible/ansible/#jinja2-templating","text":"Templating: A process a generating dynamic content or expressions String Manipulation - Filters # Substitution The name is {{ my_name }} # Uppercase The name is {{ my_name | upper }} # Lowercase The name is {{ my_name | lower }} # Titlecase The name is {{ my_name | title }} # Replace The name is {{ my_name | replace(\"Bond\", \"Bourne\") }} # Default value The name is {{ first_name | default(\"James\") }} {{ my_name }} Filters - List and Set # Min {{ [ 1 , 2 , 3 ] | min }} => 1 # Max {{ [1,2,3] | min }} => 3 # Unique {{ [1,2,3,2] | unique }} => 1,2,3 # Union {{ [1,2,3,4] | union([4,5]) }} => 1,2,3,4,5 # Intersect {{ [1,2,3,4] | intersect([4,5]) }} => 4 {{ 100 | random }} => generates random number between 1 to 100 # Join {{ [\"The\",\"name\",\"is\",\"Bond\"] | join(\" \")}} => The name is Bond Filters - File {{ \"/etc/hosts\" | basename }} => hosts Filters - expanduser tasks : - name : Ensure the SSH key is present on OpenStack os_keypair : state : present name : ansible_key public_key_file : \"{{ '~' | expanduser }}/.ssh/id_rsa.pub\"","title":"Jinja2 Templating"},{"location":"learning/ansible/ansible/#lookups","text":"Lookups : To get data from another source on the system # Credentials File csv Hostname,Password web_server,Passw0rd db_server,Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('csvfile', 'web_server file=/tmp/credentials.csv delimiter=,') }}\" => Passw0rd # Credentials File ini [ web_server ] password = Passw0rd [ db_server ] password = Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('ini', 'password section=web_server file=/tmp/credentials.ini') }}\" => Passw0rd","title":"Lookups"},{"location":"learning/ansible/ansible/#tags","text":"Tags are names pinned on individual tasks, roles or an entire play, that allows you to run or skip parts of your Playbook. Tags can help you while testing certain parts of your Playbook. # tag.yml --- - name : Play1-install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name=apache2 update_cache=yes state=latest - name : displaying \"hello world\" debug : msg=\"hello world\" tags : - tag1 - name : Play2-install nginx hosts : all sudo : yes tags : - tag2 tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : debug module displays message in control machine debug : msg=\"have a good day\" tags : - mymessage - name : shell module displays message in host machine. shell : echo \"yet another task\" tags : - mymessage Executing above play using tags # displays the list of tasks in the Playbook ansible-playbook -i myhosts tag.yml --list-tasks # displays only tags in your Playbook ansible-playbook -i myhosts tag.yml --list-tags # executes only certain tasks which are tagged as tag1 and mymessage ansible-playbook -i myhosts tag.yml --tags \"tag1,mymessage\"","title":"Tags"},{"location":"learning/ansible/ansible/#includes-outdated-after-20","text":"Ansible gives you the flexibility of organizing your tasks through include keyword, that introduces more abstraction and make your Playbook more easily maintainable, reusable and powerful. --- - name : testing includes hosts : all sudo : yes tasks : - include : apache.yml - include : content.yml - include : create_folder.yml - include : content.yml - include : nginx.yml # apache.yml will not have hosts & tasks but nginx.yml as a separate play will have tasks and can run independently #nginx.yml --- - name : installing nginx hosts : all sudo : yes tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : displaying message debug : msg=\"yayy!! nginx installed\"","title":"Includes (Outdated after 2.0)"},{"location":"learning/ansible/ansible/#roles","text":"A Role is completely self contained or encapsulated and completely reusable # cat ansible.cfg [ defaults ] host_key_checking=False inventory = /etc/ansible/myhosts log_path = /home/scrapbook/tutorial/output.txt roles_path = /home/scrapbook/tutorial/roles/ # master-playbook.yml --- - name : my first role in ansible hosts : all sudo : yes roles : - sample_role - sample_role2 # sample_role/tasks/main.yml --- - include : nginx.yml - include : copy-template.yml - include : copy-static.yml # sample_role/tasks/nginx.yml --- - name : Installs nginx apt : pkg=nginx state=installed update_cache=true notify : - start nginx # sample_role/handlers/main.yml --- - name : start nginx service : name=nginx state=started # sample_role/tasks/copy-template.yml --- - name : sample template - x template : src : template-file.j2 dest : /home/ubuntu/copy-template-file.j2 with_items : var_x # sample_role/vars/main.yml var_x : - 'variable x' var_y : - 'variable y' # sample_role/tasks/copy-static.yml # Ensure some-file.txt is present under files folder of the role --- - name : Copy a file copy : src=some-file.txt dest=/home/ubuntu/file1.txt Executing the play # Let us run the master_playbook and check the output: ansible-playbook -i myhosts master_playbook.yml","title":"Roles"},{"location":"learning/ansible/ansible/#ansible-galaxy","text":"ansible-galaxy is command line tool for scaffolding the creation of directory structure needed for organizing your code ansible-galaxy init sample_role","title":"Ansible Galaxy"},{"location":"learning/ansible/ansible/#ansible-galaxy-useful-commands","text":"Install a Role from Ansible Galaxy To use others role, visit https://galaxy.ansible.com/ and search for the role that will achieve your goal. Goal: Install Apache in the host machines. # Ensure that roles_path are defined in ansible.cfg for a role to successfully install. # Here, apache is role name and geerlingguy is name of the user in GitHub who created the role. ansible-galaxy install geerlingguy.apache # Forcefully Recreate Role ansible-galaxy init geerlingguy.apache --force # Listing Installed Roles ansible-galaxy list # Remove an Installed Role ansible-galaxy remove geerlingguy.apache","title":"ansible-galaxy useful commands"},{"location":"learning/ansible/ansible/#environment-variables","text":"Ansible recommends maintaining inventory file for each environment, instead of keeping all your hosts in a single inventory. Each environment directory has one inventory file (hosts) and group_vars directory.","title":"Environment Variables"},{"location":"learning/ansible/ansible/#ansible-best-practises","text":"","title":"Ansible Best Practises"},{"location":"learning/ansible/ansible/#inventory-files","text":"To show you a great way of setting up your directory structure for a simple role-based playbook that has two different inventories \u2014 one for a development environment and one for a production environment. # inventories/development/hosts [ app ] app01.dev.example.com app02.dev.example.com # inventories/development/group_vars --- http_port : 8080 # inventories/production/hosts [ app ] app01.prod.example.com app02.prod.example.com # inventories/production/group_vars --- http_port : 80 # To run it on the development inventory ansible-playbook -i inventories/development/hosts site.yml # To run it on the production inventory ansible-playbook -i inventories/production/hosts site.yml However, there are always differences between the two environments, not just in the hostnames, but also sometimes in the parameters, the load balancer names, the port numbers, and so on\u2014the list can seem endless. Try and reuse the same playbooks for all of your environments that run the same code. For example, if you deploy a web app in your development environment, you should be confident that your playbooks will deploy the same app in the production environment This means that not only are you testing your application deployments and code, you are also testing your Ansible playbooks and roles as part of your overall testing process. Your inventories for each environment should be kept in separate directory trees, but all roles, playbooks, plugins, and modules (if used) should be in the same directory structure (this should be the case for both environments). It is normal for different environments to require different authentication credentials; you should keep these separate not only for security but also to ensure that playbooks are not accidentally run in the wrong environment. Your playbooks should be in your version control system, just as your code is. This enables you to track changes over time and ensure that everyone is working from the same copy of the automation code.","title":"Inventory Files"},{"location":"learning/ansible/ansible/#the-proper-approach-to-defining-group-and-host-variables","text":"First and foremost, you should always pay attention to variable precedence. Host variables are always of a higher order of precedence than group variables; so, you can override any group variable with a host variable. This behavior is useful if you take advantage of it in a controlled manner, but can yield unexpected results if you are not aware of it. There is a special group variables definition called all, which is applied to all inventory groups. This has a lower order of precedence than specifically defined group variables. What happens if you define the same variable twice in two groups? If this happens, both groups have the same order of precedence, so which one wins? [ app ] app01.dev.example.com app02.dev.example.com # inventories/development/group_vars/all.yml --- http_port : 8080 # inventories/development/group_vars/app.yml --- http_port : 8081 # site.yml --- - name : Play using best practise directory structure hosts : all tasks : - name : Display the value of our inventory variable debug : var : http_port ansible-playbook -i inventories/development/hosts site.yml As expected, the variable definition in the specific group won, which is in line with the order of precedence documented for Ansible. Now, let's see what happens if we define the same variable twice in two specifically named groups. To complete this example, we'll create a child group, called centos, and another group that could notionally contain hosts built to a new build standard, called newcentos, which both application servers will be a member of. [ app ] app01.dev.example.com app02.dev.example.com [centos:children] app [newcentos:children] app # inventories/development/group_vars/centos.yml --- http_port : 8082 # inventories/development/group_vars/newcentos.yml --- http_port : 8083 We've now defined the same variable four times at the group level! ansible-playbook -i inventories/development/hosts site.yml The value we entered in newcentos.yml won\u2014but why? The Ansible documentation states that where identical variables are defined at the group level in the inventory (the one place you can do this), the one from the last-loaded group wins. Groups are processed in alphabetical order and newcentos is the group with the name beginning furthest down the alphabet\u2014so, its value of http_port was the value that won. Just for completeness, we can override all of this by leaving the group_vars directory untouched, but adding a file called inventories/development/host_vars/app01.dev.example.com.yml --- http_port : 9090 We will see that the value we defined at the host level completely overrides any value that we set at the group level for app01.dev.example.com. app02.dev.example.com is unaffected as we did not define a host variable for it, so the next highest level of precedence\u2014the group variable from the newcentos group\u2014won","title":"The proper approach to defining group and host variables"},{"location":"learning/ansible/ansible/#using-top-level-playbooks","text":"Imagine handing a playbook directory structure with 100 different playbooks to a new system administrator\u2014how would they know which ones to run and in which circumstances? The task of training someone to use the playbooks would be immense and would simply move complexity from one area to another. The most important thing is that, on receipt of a new playbook directory structure, a new operator at least knows what the starting point for both running the playbooks, and understanding the code is. If the top-level playbook they encounter is always site.yml, then at least everyone knows where to start. Through the clever use of roles and the import_* and include_* statements, you can split your playbook up into logical portions of reusable code, all from one playbook file.","title":"Using top-level playbooks"},{"location":"learning/ansible/ansible/#leveraging-version-control-tools","text":"Any changes to your Ansible code could mean big changes to your environment, and possibly even whether an important production service works or not. As a result, it is vital that you maintain a version history of your Ansible code and that everyone works from the same version.","title":"Leveraging version control tools"},{"location":"learning/ansible/ansible/#setting-os-and-distribution-variances","text":"This playbook demonstrates how you can group differing plays using an Ansible fact so that the OS distribution determines which play in a playbook gets run. # osvariants.yml - It will also contain a single task. --- - name : Play to demonstrate group_by module hosts : all tasks : - name : Create inventory groups based on host facts group_by : key : os_{{ ansible_facts['distribution'] }} group_by module: It dynamically creates new inventory groups based on the key that we specify \u2014 in this example, we are creating groups based on a key comprised of the os_ fixed string, followed by the OS distribution fact obtained from the Gathering Facts stage. The original inventory group structure is preserved and unmodified, but all the hosts are also added to the newly created groups according to their facts. So, the two servers in our simple inventory remain in the app group, but if they are based on Ubuntu, they will be added to a newly created inventory group called os_Ubuntu. Similarly, if they are based on CentOS, they will be added to a group called os_CentOS. # Play definition to the same playbook file to install Apache on CentOS - name : Play to install Apache on CentOS hosts : os_CentOS # Refer to the Dynamic group become : true tasks : - name : Install Apache on CentOS yum : name : httpd state : present # Add a third Play definition, this time for installing the apache2 package on Ubuntu using the apt module - name : Play to install Apache on Ubuntu hosts : os_Ubuntu become : true tasks : - name : Install Apache on Ubuntu apt : name : apache2 state : present ansible-playbook -i hosts osvariants.yml Notice how the task to install Apache on CentOS was run. It was run this way because the group_by module created a group called os_CentOS and our second play only runs on hosts in the group called os_CentOS. As there were no servers running on Ubuntu in the inventory, the os_Ubuntu group was never created and so the third play does not run. We receive a warning about the fact that there is no host pattern that matches os_Ubuntu, but the playbook does not fail\u2014it simply skips this play. It is up to you to choose the coding style most appropriate to you. You can make use of the group_by module, as detailed here, or write your tasks in blocks and add a when clause to the blocks so that they only run when a certain fact-based condition is met (for example, the OS distribution is CentOS)\u2014or perhaps even a combination of the two. The choice is ultimately yours and these different examples are provided to empower you with multiple options that you can choose between to create the best possible solution for your scenario.","title":"Setting OS and distribution variances"},{"location":"learning/ansible/ansible/#setting-task-execution-delegation","text":"We have assumed that all the tasks are executed on each host in the inventory in turn. However, what if you need to run one or two tasks on a different host? For example, we have talked about the concept of automating upgrades on clusters. Logically, however, we would want to automate the entire process, including the removal of each host in turn from the load balancer and its return after the task is completed. Although we still want to run our play across our entire inventory, we certainly don't want to run the load balancer commands from those hosts. Imagine that you have a shell script (or other executables) that you can call that can add and remove hosts to and from a load balancer. # remove_from_loadbalancer.sh #!/bin/sh echo Removing $1 from load balancer... # add_to_loadbalancer.sh #!/bin/sh echo Adding $1 to load balancer... [ frontends ] frt01.example.com frt02.example.com --- - name : Play to demonstrate task delegation hosts : frontends tasks : - name : Remove host from the load balancer command : ./remove_from_loadbalancer.sh {{ inventory_hostname }} args : chdir : \"{{ playbook_dir }}\" delegate_to : localhost - name : Deploy code to host debug : msg : Deployment code would go here.... - name : Add host back to the load balancer command : ./add_to_loadbalancer.sh {{ inventory_hostname }} args : chdir : \"{{ playbook_dir }}\" delegate_to : localhost We are using the command module to call the script we created earlier, passing the hostname from the inventory being removed from the load balancer to the script. We use the chdir argument with the playbook_dir magic variable to tell Ansible that the script is to be run from the same directory as the playbook. The special part of this task is the delegate_to directive, which tells Ansible that even though we're iterating through an inventory that doesn't contain localhost, we should run this action on localhost (we aren't copying the script to our remote hosts, so it won't run if we attempt to run it from there). Deploy task has no delegate_to directive, and so it is actually run on the remote host from the inventory (as desired): Finally, we add the host back to the load balancer using the second script we created earlier. This task is almost identical to the first. ansible-playbook -i hosts delegate.yml Notice how even though Ansible is working through the inventory (which doesn't feature localhost), the load balancer-related scripts are actually run from localhost, while the upgrade task is performed directly on the remote host. In truth, you can delegate any task to localhost, or even another non-inventory host. You could, for example, run an rsync command delegated to localhost to copy files to remote hosts using a similar task definition to the previous one. This is useful because although Ansible has a copy module, it can't perform the advanced recursive copy and update functions that rsync is capable of. Note that you can choose to use a form of shorthand notation in your playbooks (and roles) for delegate_to, called local_action . This allows you to specify a task on a single line that would ordinarily be run with delegate_to: localhost added below it. --- - name : Second task delegation example hosts : frontends tasks : - name : Perform an rsync from localhost to inventory hosts local_action : command rsync -a /tmp/ {{ inventory_hostname }}:/tmp/target/ The preceding shorthand notation is equivalent to the following: tasks : - name : Perform an rsync from localhost to inventory hosts command : rsync -a /tmp/ {{ inventory_hostname }}:/tmp/target/ delegate_to : localhost If we run this playbook, we can see that local_action does indeed run rsync from localhost, enabling us to efficiently copy whole directory trees across to remote servers in the inventory. ansible-playbook -i hosts delegate2.yml","title":"Setting task execution delegation"},{"location":"learning/ansible/ansible/#using-the-run_once-option","text":"When working with clusters, you will sometimes encounter a task that should only be executed once for the entire cluster. For example, you might want to upgrade the schema of a clustered database. Instead, you can write your code as you normally would, but make use of the special run_once directive for any tasks you want to run only once on your inventory. For example, let's reuse the 10-host inventory. --- - name : Play to demonstrate the run_once directive hosts : frontends tasks : - name : Upgrade database schema debug : msg : Upgrading database schema... run_once : true ansible-playbook -i morehosts runonce.yml Notice that, just as desired, although the playbook was run on all 10 hosts (and, indeed, gathered facts from all 10 hosts), we only ran the upgrade task on one host. It's important to note that the run_once option applies per batch of servers, so if we add serial: 5 to our play definition (running our play in two batches of 5 on our inventory of 10 servers), the schema upgrade task actually runs twice! It runs once as requested, but once per batch of servers, not once for the entire inventory. Be careful of this nuance when working with this directive in a clustered environment.","title":"Using the run_once option"},{"location":"learning/ansible/ansible/#running-playbooks-locally","text":"It is important to note that when we talk about running a playbook locally with Ansible, it is not the same as talking about running it on localhost. If we run a playbook on localhost, Ansible actually sets up an SSH connection to localhost (it doesn't differentiate its behavior or attempt to detect whether a host in the inventory is local or remote - it simply tries faithfully to connect). # Inventory file [ local ] localhost ansible_connection=local We've added a special variable to our localhost entry ansible_connection variable which defines which protocol is used to connect to this inventory host. So, we have told it to use a direct local connection instead of an SSH-based connectivity (which is the default). Note that this special value for the ansible_connection variable actually overrides the hostname you have put in your inventory. So, if we change our inventory to look as follows, Ansible will not even attempt to connect to the remote host called frt01.example.com it will connect locally to the machine running the playbook (without SSH). [ local ] frt01.example.com ansible_connection=local The presence of ansible_connection=local meant that this command was run on the local machine without using SSH. This ability to run commands locally without the need to set up SSH connectivity, SSH keys, and so on can be incredibly valuable, especially if you need to get things up and running quickly on your local machine.","title":"Running playbooks locally"},{"location":"learning/ansible/ansible/#working-with-proxies-and-jump-hosts","text":"Often, when it comes to configuring core network devices, these are isolated from the main network via a proxy or jump host. Ansible lends itself well to automating network device configuration as most of it is performed over SSH: however, this is only helpful in a scenario where Ansible can either be installed and operated from the jump host or, better yet, can operate via a host such as this. Let's assume that you have two Cumulus Networks switches in your network (these are based on a special distribution of Linux for switching hardware, which is very similar to Debian). These two switches have the cmls01.example.com and cmls02.example.com hostnames, but both can only be accessed from a host called bastion.example.com. [ switches ] cmls01.example.com cmls02.example.com [switches:vars] ansible_ssh_common_args='-o ProxyCommand=\"ssh -W %h:%p -q bastion.example.com\"' This special variable content ansible_ssh_common_args tells Ansible to add extra options when it sets up an SSH connection, including to proxy via the bastion.example.com host. The -W %h:%p options tell SSH to proxy the connection and to connect to the host specified by %h (this is either cmls01.example.com or cmls02.example.com) on the port specified by %p (usually port 22). ansible -i switches -m ping all On the surface, Ansible works just as it normally does and connects successfully to the two hosts. However, behind the scenes it proxies via bastion.example.com. Note that this simple example assumes that you are connecting to both the bastion host and switches using the same username and SSH credentials (or in this case, keys).","title":"Working with proxies and jump hosts"},{"location":"learning/ansible/ansible/#configuring-playbook-prompts","text":"All of our playbooks have had their data specified for them at run time in variables we defined within the playbook. However, what if you actually want to obtain information from someone during a playbook run? Perhaps you want to obtain a password from a user for an authentication task without storing it anywhere. Ansible can prompt you for user input and store the input in a variable for future processing. We will prompt for two variables, one for a user ID and one for a password. One will be echoed to the screen, while the other won't be, by setting private: yes --- - name : A simple play to demonstrate prompting in a playbook hosts : frontends vars_prompt : - name : loginid prompt : \"Enter your username\" private : no - name : password prompt : \"Enter your password\" private : yes tasks : - name : Proceed with login debug : msg : \"Logging in as {{ loginid }}...\" ansible-playbook -i hosts prompt.yml","title":"Configuring playbook prompts"},{"location":"learning/ansible/ansible/#ansible-security-best-practices","text":"","title":"Ansible Security Best Practices"},{"location":"learning/ansible/ansible/#working-with-ansible-vault","text":"It's really important to use Ansible Vault to store all the secret information in our playbooks. Some of the really good use cases include how we can use these playbooks without changing our version control systems, CI/CD integration pipelines, and so on.","title":"Working with Ansible Vault"},{"location":"learning/ansible/ansible/#how-to-use-ansible-vault-with-variables-and-files","text":"Let's take an example of installing MySQL server in an Ubuntu operating system using the following playbook. As per the Ansible documentation, it's easy and better to store Vault variables and normal variables differently. \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 mysql.yml # contains vault secret values \u251c\u2500\u2500 hosts \u251c\u2500\u2500 main.yml \u2514\u2500\u2500 roles \u2514\u2500\u2500 mysqlsetup \u2514\u2500\u2500 tasks \u2514\u2500\u2500 main.yml Now, if we see the group_vars/main.yml file, the content looks as shown in the codeblock. It contains the secrets variable to use in the playbook, called mysql_root_password. mysql_root_password : supersecretpassword\u200b To encrypt the vault file, we will use the following command and it then prompts for the password to protect ansible-vault encrypt group_vars/mysql.yml # Now, to execute the playbook run the following command, it will prompt for the vault password ansible-playbook --ask-vault-pass -i hosts main.yml We can also pass the ansible-vault password file with playbook execution by specifying flag, it helps in our continuous integration and pipeline platforms. The following file contains the password which used to encrypt the mysql.yml file. cat ~/.vaultpassword thisisvaultpassword # To pass the vault password file through the command line, use the following command when executing playbooks ansible-playbook --vault-password-file ~/.vaultpassword -i hosts main.yml Note Make sure to give proper permissions for this file, so others cannot access this file using chmod. Also, it's good practice to add this file to your .gitignore, so it will not be version controlled when pushing playbooks. Vault password file can be an executable script, which can retrieve data stored somewhere securely rather than having to keep the key in plain text on disk and relying on file permissions to keep it safe. We can also use system environment variables such as ANSIBLE_VAULT_PASSWORD_FILE=~/.vaultpassword and Ansible will use this while executing playbooks.","title":"How to use Ansible Vault with variables and files"},{"location":"learning/ansible/ansible/#ansible-vault-single-encrypted-variable","text":"It allows us to use vaulted variables with the !vault tag in YAML files This playbook is used to perform reverse IP lookups using the ViewDNS API. We want to secure api_key as it contains sensitive information. # We use the ansible-vault encrypt_string command to perform this encryption. # Here, we used echo with the -n flag to remove the new line echo -n '53ff4ad63849e6977cb652763g7b7c64e2fa42a' | ansible-vault encrypt_string --stdin-name 'api_key' We can place the variable, inside the playbook variables and execute the playbook as normal, using ansible-playbook with the \u2013ask-vault-pass option. - name : ViewDNS domain information hosts : localhost vars : domain : google.com api_key : !vault | $ANSIBLE_VAULT;1.1;AES256 36623761316238613461326466326162373764353437393733343334376161336630333532626465 6662383435303930303164353664643639303761353664330a393365633237306530653963353764 64626237313738656530373639653739656564316161663831653431623832336635393637653330 6632663563363264340a323537356166653338396135376161323435393730306133626635376539 37383861653239326336613837666237636463396465393662666561393132343166666334653465 6265386136386132363534336532623061646438363235383334 output_type : json tasks : - name : \"getting {{ domain }} server info\" uri : url : \"https://api.viewdns.info/reverseip/?host={{ domain }}&apikey={{ api_key }}&output={{ output_type }}\" method : GET register : results - debug : msg : \"{{ results.json }}\" Playbook being executed will be automatically decrypted after we provide it with the given password. ansible-playbook --ask-vault-pass -i hosts main.yml","title":"Ansible Vault single encrypted variable"},{"location":"learning/ansible/ansible/#setting-up-and-using-ansible-galaxy","text":"Is an official centralized hub for finding, sharing, and reusing Ansible roles. This allows the community to share and collaborate on Ansible playbooks, and allows new users to quickly get started with using Ansible. To share our custom-written roles with the community, we can publish them to Ansible Galaxy using GitHub authentication. We can install or include roles direct from GitHub by specifying the GitHub URL. This allows the use of private version control systems as local inventories of playbook roles. ansible-galaxy install git+https://github.com/geerlingguy/ansible-role-composer.git","title":"Setting up and using Ansible Galaxy"},{"location":"learning/ansible/ansible/#ansible-controller-machine-security","text":"The controller machine for Ansible requires SSH and Python to be installed and configured. Ansible has a very low attack surface. Note In January 2017, multiple security issues were found by a company called Computest . This vulnerability was dubbed owning the farm, since compromising the controller would imply that all the nodes could potentially be compromised. The controller machine should be a hardened server and treated with all the seriousness that it deserves. In the vulnerability that was disclosed, if a node gets compromised attackers could leverage that to attack and gain access to the controller. - Once they have access, the could extend their control over all the other nodes being managed by the controller. Since the attack surface is already very limited, the best we can do is ensure that the server stays secure and hardened.","title":"Ansible controller machine security"},{"location":"learning/ansible/ansible/#explanation-of-ansible-os-hardening-playbook","text":"The following playbook is created by DevSec for Linux baselines. It covers most of the required hardening checks based on multiple standards, which includes Ubuntu Security Features, NSA Guide to Secure Configuration, ArchLinux System Hardening and other. This can be improved if required by adding more tasks (or) roles. Ansible OS Hardening Playbook covers Configures package management, that is, allows only signed packages Removes packages with known issues Configures pam and the pam_limits module Shadow password suite configuration Configures system path permissions Disables core dumps through soft limits Restricts root logins to system console Sets SUIDs Configures kernel parameters through sysctl # download the os-hardening role from Ansible Galaxy ansible-galaxy install dev-sec.os-hardening Call that role in your playbook and execute it to perform the baseline hardening, and also change the variables as required. Refer to https://galaxy.ansible.com/dev-sec/os-hardening for more detailed options. - hosts : localhost become : yes roles : - dev-sec.os-hardening # Execute the playbook ansible-playbook main.yml","title":"Explanation of Ansible OS hardening playbook"},{"location":"learning/ansible/ansible/#best-practices-and-reference-playbook-projects","text":"Projects such as Algo, DebOps, and OpenStack are large Ansible playbook projects that are well maintained and secure by default.","title":"Best practices and reference playbook projects"},{"location":"learning/ansible/ansible/#debops--your-debian-based-data-center-in-a-box","text":"DebOps is a project created by Maciej Delmanowski. It contains a collection of various Ansible playbooks that can be used for Debian and Ubuntu hosts. This project has more than 128 Ansible roles, which are customized for production use cases and work with multiple environments. We can see a list of available playbook services at debops/debops-playbooks There are two different ways we can quickly get started with a DebOps setup: Vagrant setup Docker setup","title":"DebOps \u2013 your Debian-based data center in a box"},{"location":"learning/ansible/ansible/#algo--set-up-a-personal-ipsec-vpn-in-the-cloud","text":"Algo from Trail of Bits provides Ansible roles and scripts to automate the installation of a personal IPSEC VPN. By running the Ansible playbooks, you get a complete hardened VPN server, and deployments to all major cloud providers are already configured ( https://github.com/trailofbits/algo/blob/master/docs/deploy-from-ansible.md ).","title":"Algo \u2013 set up a personal IPSEC VPN in the cloud"},{"location":"learning/ansible/ansible/#openstack-ansible","text":"Not only does this project use Ansible playbooks extensively, but their security documentation is also worth reading and emulating. The best part is that all of the security configuration is declarative security codified in Ansible playbooks. Documentation on this project is available at https://docs.openstack.org/project-deploy-guide/openstack-ansible/ocata/app-security.html .","title":"OpenStack-Ansible"},{"location":"learning/ansible/ansible/#awx--open-source-version-of-ansible-tower","text":"AWX provides a web-based user interface, REST API, and task engine built on top of Ansible. AWX can be used with the tower-CLI tool and client library. Get started with AWX here: ansible/awx . Get started with tower-cli here: ansible/tower-cli .","title":"AWX \u2013 open source version of Ansible Tower"},{"location":"learning/ansible/container_cloud/","text":"Container and Cloud Management \u00b6 In the last few years, container-based workloads and cloud workloads have become more and more popular, and for this reason, we are going to look at how you can automate tasks related to those kinds of workloads with Ansible. First of all, even if you are in a very good place in your automation path and you have a lot of Ansible roles written for your infrastructure, you can't leverage them in Dockerfiles, so you would end up replicating your work to create containers. If this is not enough of a problem, this situation quickly deteriorates when you start considering cloud environments. All cloud environments have their own control planes and native automation languages, so in a very short time, you would find yourself rewriting the automation for the same operation over and over, thus wasting time and deteriorating the consistency of your environments. Designing and building containers with playbooks \u00b6 Ansible provides ansible-container so that you can create containers using the same components you would use for creating machines. The first thing you should do is ensure that you have ansible-container installed. sudo pip install ansible-container [ docker,k8s ] The ansible-container tool comes with three supported engines at the time of writing: docker: This is needed if you want to use it with Docker Engine (that is, on your local machine). k8s: This is needed if you want to use it with a Kubernetes cluster, both local (that is, MiniKube) or remote (that is, a production cluster). openshift: This is needed if you want to use it with an OpenShift cluster, both local (that is, MiniShift) or remote (that is, a production cluster). Follow these steps to build the container using playbooks Issuing the ansible-container init command ansible-container init Running this command will also create the following files: ansible.cfg: An empty file to be (eventually) used to override Ansible system configurations ansible-requirements.txt: An empty file to (eventually) list the Python requirements for the building process of your containers container.yml: A file that contains the Ansible code for the build meta.yml: A file that contains the metadata for Ansible Galaxy requirements.yml: An empty file to (eventually) list the Ansible roles that are required for your build Let's try building our own container using this tool \u2013 replace the contents of container.yml with the following version : \"2\" settings : conductor : base : centos:7 project_name : http-server services : web : from : \"centos:7\" roles : - geerlingguy.apache ports : - \"80:80\" command : - \"/usr/bin/dumb-init\" - \"/usr/sbin/apache2ctl\" - \"-D\" - \"FOREGROUND\" dev_overrides : environment : - \"DEBUG=1\" We can now run ansible-container build to initiate the build. At the end of the building process, we will have a container built with the geerlingguy.apache role applied to it. The ansible-container tool performs a multi-stage build capability, spinning up an Ansible container that is then used to build the real container. If we specified more than one role to be applied, the output would be an image with more layers, since Ansible will create a layer for every specified role. In this way, containers can easily be built using your existing Ansible roles rather than Dockerfiles. Managing multiple container platforms \u00b6 To be able to call a deployment \"production-ready,\" you need to be able to demonstrate that the service your application is delivering will run reasonably, even in the case of a single application crash, as well as hardware failure. Often, you'll have even more reliability constraints from your customer. Today, the most successful one is Kubernetes due to its various distributions/versions, so we are going to focus on it primarily. The idea of Kubernetes is that you inform the Kubernetes Control Plane that you want X number of instances of your Y application, and Kubernetes will count how many instances of the Y application are running on the Kubernetes Nodes to ensure that the number of instances are X. If there are too few instances, Kubernetes will take care to start more instances, while if there are too many instances, the exceeding instances will be stopped. Due to the complexity of installing and managing Kubernetes, multiple companies have started to sell distributions of Kubernetes that simplify their operations and that they are willing to support. Deploying to Kubernetes with ansible-container \u00b6 We will assume that you have access to either a Kubernetes cluster for testing. To deploy your application to your cluster, you need to change the container.yml file so that you can add some additional information. We will need to add a section called settings and a section called k8s_namespace to declare our deployment settings. k8s_namespace : name : http-server description : An HTTP server display_name : HTTP server We can proceed with the deployment- ansible-container --engine kubernetes deploy As soon as Ansible has completed its execution, you will be able to find the http-server deployment on your Kubernetes cluster. Based on the image that we built in the previous section and the additional information we added at the beginning of this section, Ansible is able to populate a deployment template and then deploy it using the k8s module. Managing Kubernetes Objects with Ansible \u00b6 You can do kubectl get namespaces with Ansible by creating a file called k8s-ns-show.yaml --- - hosts : localhost tasks : - name : Get information from K8s k8s_info : api_version : v1 kind : Namespace # Specify the k8s object Deployments, Services, Pods register : ns - name : Print info debug : var : ns ansible-playbook k8s-ns-show.yaml # Create a new namespace --- - hosts : localhost tasks : - name : Ensure the myns namespace exists k8s : api_version : v1 kind : Namespace name : myns state : present # Creates a new service --- - hosts : localhost tasks : - name : Ensure the Service mysvc is present k8s : state : present definition : apiVersion : v1 kind : Service metadata : name : mysvc namespace : myns spec : selector : app : myapp service : mysvc ports : - protocol : TCP targetPort : 800 name : port-80-tcp port : 80 Ansible allows you to manage your Kubernetes clusters with some modules: k8s: Allows you to manage any kind of Kubernetes object k8s_auth: Allows you to authenticate to Kubernetes clusters that require an explicit login step k8s_facts: Allows you to inspect Kubernetes objects k8s_scale: Allows you to set a new size for a Deployment, ReplicaSet, Replication Controller, or Job k8s_service: Allows you to manage Services on Kubernetes Automating Docker with Ansible \u00b6 With Ansible, you can easily manage your Docker instance in Development environments. First of all, we need to create a playbook called start-docker-container.yaml that will contain the following code - hosts : localhost tasks : - name : Start a container with a command docker_container : name : test-container image : alpine command : - echo - \"Hello, World!\" Other modules include the following: docker_config: Used to change the configurations of the Docker daemon docker_container_info: Used to gather information from (inspect) a container docker_network: Used to manage Docker networking configuration Automating against Amazon Web Services \u00b6 To be able to use Ansible to automate your Amazon Web Service estate, you'll need to install the boto library. pip install boto Authentication \u00b6 The boto library looks up the necessary credentials in the ~/.aws/credentials file. There are two different ways to ensure that the credentials file is configured properly. It is possible to use the AWS CLI tool. Alternatively, this can be done with a text editor of your choice by creating a file with the following structure [ default ] aws_access_key_id = [ YOUR_KEY_HERE ] aws_secret_access_key = [ YOUR_SECRET_ACCESS_KEY_HERE ] Now that you've created the file with the necessary credentials, boto will be able to work against your AWS environment. Since Ansible uses boto for every single communication with AWS systems, this means that Ansible will be appropriately configured, even without you have to change any Ansible-specific configuration. Creating your first machine \u00b6 To launch a virtual machine in AWS, we need a few things to be in place, as follows: An SSH key pair A network A subnetwork A security group By default, a network and a subnetwork are already available in your accounts, but you need to retrieve their IDs. Create the aws.yaml Playbook with the following content - hosts : localhost tasks : - name : Ensure key pair is present ec2_key : name : fale key_material : \"{{ lookup('file', '~/.ssh/fale.pub') }}\" - name : Gather information of the EC2 VPC net in eu-west-1 ec2_vpc_net_facts : region : eu-west-1 register : aws_simple_net - name : Gather information of the EC2 VPC subnet in eu-west-1 ec2_vpc_subnet_facts : region : eu-west-1 filters : vpc-id : '{{ aws_simple_net.vpcs.0.id }}' register : aws_simple_subnet - name : Ensure wssg Security Group is present ec2_group : name : wssg description : Web Security Group region : eu-west-1 vpc_id : '{{ aws_simple_net.vpcs.0.id }}' rules : - proto : tcp from_port : 22 to_port : 22 cidr_ip : 0.0.0.0/0 - proto : tcp from_port : 80 to_port : 80 cidr_ip : 0.0.0.0/0 - proto : tcp from_port : 443 to_port : 443 cidr_ip : 0.0.0.0/0 rules_egress : - proto : all cidr_ip : 0.0.0.0/0 register : aws_simple_wssg - name : Setup instance ec2 : assign_public_ip : true image : ami-3548444c region : eu-west-1 exact_count : 1 key_name : fale count_tag : Name : ws01.ansible2cookbook.com instance_tags : Name : ws01.ansible2cookbook.coms instance_type : t2.micro group_id : '{{ aws_simple_wssg.group_id }}' vpc_subnet_id : '{{ aws_simple_subnet.subnets.0.id }}' volumes : - device_name : /dev/sda1 volume_type : gp2 volume_size : 10 delete_on_termination : True ansible-playbook aws.yaml We started by uploading the public part of an SSH keypair to AWS, then queried for information about the network and the subnetwork, then ensured that the Security Group we wanted to use was present, and lastly triggered the machine build. Automating against Azure \u00b6 To let Ansible manage the Azure cloud, you need to install the Azure SDK for Python. pip install 'ansible[azure]' ``` ## Authentication - There are different ways to ensure that Ansible is able to manage Azure for you, based on the way your Azure account is set up, but they can all be configured in the ` ~/.azure/credentials ` file. ``` BASH [ default ] subscription_id = [ YOUR_SUBSCIRPTION_ID_HERE ] client_id = [ YOUR_CLIENT_ID_HERE ] secret = [ YOUR_SECRET_HERE ] tenant = [ YOUR_TENANT_HERE ] If you prefer to use Active Directories with a username and password. [ default ] ad_user = [ YOUR_AD_USER_HERE ] password = [ YOUR_AD_PASSWORD_HERE ] You can opt for an Active Directory login with ADFS. [ default ] ad_user = [ YOUR_AD_USER_HERE ] password = [ YOUR_AD_PASSWORD_HERE ] client_id = [ YOUR_CLIENT_ID_HERE ] tenant = [ YOUR_TENANT_HERE ] adfs_authority_url = [ YOUR_ADFS_AUTHORITY_URL_HERE ] Creating your first machine \u00b6 Create the azure.yaml Playbook with the following content. In Azure, you will need all the resources to be ready before you can issue the machine creation command. This is the reason you create the Storage Account, the Virtual Network, the Subnet, the Public IP, the security Group, and the NIC first, and only at that point, the machine itself. - hosts : localhost tasks : - name : Ensure the Storage Account is present azure_rm_storageaccount : resource_group : Testing name : mysa account_type : Standard_LRS - name : Ensure the Virtual Network is present azure_rm_virtualnetwork : resource_group : Testing name : myvn address_prefixes : \"10.10.0.0/16\" - name : Ensure the Subnet is present azure_rm_subnet : resource_group : Testing name : mysn address_prefix : \"10.10.0.0/24\" virtual_network : myvn - name : Ensure that the Public IP is set azure_rm_publicipaddress : resource_group : Testing allocation_method : Static name : myip - name : Ensure a Security Group allowing SSH is present azure_rm_securitygroup : resource_group : Testing name : mysg rules : - name : SSH protocol : Tcp destination_port_range : 22 access : Allow priority : 101 direction : Inbound - name : Ensure the NIC is present azure_rm_networkinterface : resource_group : Testing name : testnic001 virtual_network : myvn subnet : mysn public_ip_name : myip security_group : mysg - name : Ensure the Virtual Machine is present azure_rm_virtualmachine : resource_group : Testing name : myvm01 vm_size : Standard_D1 storage_account : mysa storage_container : myvm01 storage_blob : myvm01.vhd admin_username : admin admin_password : Password! network_interfaces : testnic001 image : offer : CentOS publisher : OpenLogic sku : '8.0' version : latest ansible-playbook azure.yaml","title":"Container and Cloud Management"},{"location":"learning/ansible/container_cloud/#container-and-cloud-management","text":"In the last few years, container-based workloads and cloud workloads have become more and more popular, and for this reason, we are going to look at how you can automate tasks related to those kinds of workloads with Ansible. First of all, even if you are in a very good place in your automation path and you have a lot of Ansible roles written for your infrastructure, you can't leverage them in Dockerfiles, so you would end up replicating your work to create containers. If this is not enough of a problem, this situation quickly deteriorates when you start considering cloud environments. All cloud environments have their own control planes and native automation languages, so in a very short time, you would find yourself rewriting the automation for the same operation over and over, thus wasting time and deteriorating the consistency of your environments.","title":"Container and Cloud Management"},{"location":"learning/ansible/container_cloud/#designing-and-building-containers-with-playbooks","text":"Ansible provides ansible-container so that you can create containers using the same components you would use for creating machines. The first thing you should do is ensure that you have ansible-container installed. sudo pip install ansible-container [ docker,k8s ] The ansible-container tool comes with three supported engines at the time of writing: docker: This is needed if you want to use it with Docker Engine (that is, on your local machine). k8s: This is needed if you want to use it with a Kubernetes cluster, both local (that is, MiniKube) or remote (that is, a production cluster). openshift: This is needed if you want to use it with an OpenShift cluster, both local (that is, MiniShift) or remote (that is, a production cluster). Follow these steps to build the container using playbooks Issuing the ansible-container init command ansible-container init Running this command will also create the following files: ansible.cfg: An empty file to be (eventually) used to override Ansible system configurations ansible-requirements.txt: An empty file to (eventually) list the Python requirements for the building process of your containers container.yml: A file that contains the Ansible code for the build meta.yml: A file that contains the metadata for Ansible Galaxy requirements.yml: An empty file to (eventually) list the Ansible roles that are required for your build Let's try building our own container using this tool \u2013 replace the contents of container.yml with the following version : \"2\" settings : conductor : base : centos:7 project_name : http-server services : web : from : \"centos:7\" roles : - geerlingguy.apache ports : - \"80:80\" command : - \"/usr/bin/dumb-init\" - \"/usr/sbin/apache2ctl\" - \"-D\" - \"FOREGROUND\" dev_overrides : environment : - \"DEBUG=1\" We can now run ansible-container build to initiate the build. At the end of the building process, we will have a container built with the geerlingguy.apache role applied to it. The ansible-container tool performs a multi-stage build capability, spinning up an Ansible container that is then used to build the real container. If we specified more than one role to be applied, the output would be an image with more layers, since Ansible will create a layer for every specified role. In this way, containers can easily be built using your existing Ansible roles rather than Dockerfiles.","title":"Designing and building containers with playbooks"},{"location":"learning/ansible/container_cloud/#managing-multiple-container-platforms","text":"To be able to call a deployment \"production-ready,\" you need to be able to demonstrate that the service your application is delivering will run reasonably, even in the case of a single application crash, as well as hardware failure. Often, you'll have even more reliability constraints from your customer. Today, the most successful one is Kubernetes due to its various distributions/versions, so we are going to focus on it primarily. The idea of Kubernetes is that you inform the Kubernetes Control Plane that you want X number of instances of your Y application, and Kubernetes will count how many instances of the Y application are running on the Kubernetes Nodes to ensure that the number of instances are X. If there are too few instances, Kubernetes will take care to start more instances, while if there are too many instances, the exceeding instances will be stopped. Due to the complexity of installing and managing Kubernetes, multiple companies have started to sell distributions of Kubernetes that simplify their operations and that they are willing to support.","title":"Managing multiple container platforms"},{"location":"learning/ansible/container_cloud/#deploying-to-kubernetes-with-ansible-container","text":"We will assume that you have access to either a Kubernetes cluster for testing. To deploy your application to your cluster, you need to change the container.yml file so that you can add some additional information. We will need to add a section called settings and a section called k8s_namespace to declare our deployment settings. k8s_namespace : name : http-server description : An HTTP server display_name : HTTP server We can proceed with the deployment- ansible-container --engine kubernetes deploy As soon as Ansible has completed its execution, you will be able to find the http-server deployment on your Kubernetes cluster. Based on the image that we built in the previous section and the additional information we added at the beginning of this section, Ansible is able to populate a deployment template and then deploy it using the k8s module.","title":"Deploying to Kubernetes with ansible-container"},{"location":"learning/ansible/container_cloud/#managing-kubernetes-objects-with-ansible","text":"You can do kubectl get namespaces with Ansible by creating a file called k8s-ns-show.yaml --- - hosts : localhost tasks : - name : Get information from K8s k8s_info : api_version : v1 kind : Namespace # Specify the k8s object Deployments, Services, Pods register : ns - name : Print info debug : var : ns ansible-playbook k8s-ns-show.yaml # Create a new namespace --- - hosts : localhost tasks : - name : Ensure the myns namespace exists k8s : api_version : v1 kind : Namespace name : myns state : present # Creates a new service --- - hosts : localhost tasks : - name : Ensure the Service mysvc is present k8s : state : present definition : apiVersion : v1 kind : Service metadata : name : mysvc namespace : myns spec : selector : app : myapp service : mysvc ports : - protocol : TCP targetPort : 800 name : port-80-tcp port : 80 Ansible allows you to manage your Kubernetes clusters with some modules: k8s: Allows you to manage any kind of Kubernetes object k8s_auth: Allows you to authenticate to Kubernetes clusters that require an explicit login step k8s_facts: Allows you to inspect Kubernetes objects k8s_scale: Allows you to set a new size for a Deployment, ReplicaSet, Replication Controller, or Job k8s_service: Allows you to manage Services on Kubernetes","title":"Managing Kubernetes Objects with Ansible"},{"location":"learning/ansible/container_cloud/#automating-docker-with-ansible","text":"With Ansible, you can easily manage your Docker instance in Development environments. First of all, we need to create a playbook called start-docker-container.yaml that will contain the following code - hosts : localhost tasks : - name : Start a container with a command docker_container : name : test-container image : alpine command : - echo - \"Hello, World!\" Other modules include the following: docker_config: Used to change the configurations of the Docker daemon docker_container_info: Used to gather information from (inspect) a container docker_network: Used to manage Docker networking configuration","title":"Automating Docker with Ansible"},{"location":"learning/ansible/container_cloud/#automating-against-amazon-web-services","text":"To be able to use Ansible to automate your Amazon Web Service estate, you'll need to install the boto library. pip install boto","title":"Automating against Amazon Web Services"},{"location":"learning/ansible/container_cloud/#authentication","text":"The boto library looks up the necessary credentials in the ~/.aws/credentials file. There are two different ways to ensure that the credentials file is configured properly. It is possible to use the AWS CLI tool. Alternatively, this can be done with a text editor of your choice by creating a file with the following structure [ default ] aws_access_key_id = [ YOUR_KEY_HERE ] aws_secret_access_key = [ YOUR_SECRET_ACCESS_KEY_HERE ] Now that you've created the file with the necessary credentials, boto will be able to work against your AWS environment. Since Ansible uses boto for every single communication with AWS systems, this means that Ansible will be appropriately configured, even without you have to change any Ansible-specific configuration.","title":"Authentication"},{"location":"learning/ansible/container_cloud/#creating-your-first-machine","text":"To launch a virtual machine in AWS, we need a few things to be in place, as follows: An SSH key pair A network A subnetwork A security group By default, a network and a subnetwork are already available in your accounts, but you need to retrieve their IDs. Create the aws.yaml Playbook with the following content - hosts : localhost tasks : - name : Ensure key pair is present ec2_key : name : fale key_material : \"{{ lookup('file', '~/.ssh/fale.pub') }}\" - name : Gather information of the EC2 VPC net in eu-west-1 ec2_vpc_net_facts : region : eu-west-1 register : aws_simple_net - name : Gather information of the EC2 VPC subnet in eu-west-1 ec2_vpc_subnet_facts : region : eu-west-1 filters : vpc-id : '{{ aws_simple_net.vpcs.0.id }}' register : aws_simple_subnet - name : Ensure wssg Security Group is present ec2_group : name : wssg description : Web Security Group region : eu-west-1 vpc_id : '{{ aws_simple_net.vpcs.0.id }}' rules : - proto : tcp from_port : 22 to_port : 22 cidr_ip : 0.0.0.0/0 - proto : tcp from_port : 80 to_port : 80 cidr_ip : 0.0.0.0/0 - proto : tcp from_port : 443 to_port : 443 cidr_ip : 0.0.0.0/0 rules_egress : - proto : all cidr_ip : 0.0.0.0/0 register : aws_simple_wssg - name : Setup instance ec2 : assign_public_ip : true image : ami-3548444c region : eu-west-1 exact_count : 1 key_name : fale count_tag : Name : ws01.ansible2cookbook.com instance_tags : Name : ws01.ansible2cookbook.coms instance_type : t2.micro group_id : '{{ aws_simple_wssg.group_id }}' vpc_subnet_id : '{{ aws_simple_subnet.subnets.0.id }}' volumes : - device_name : /dev/sda1 volume_type : gp2 volume_size : 10 delete_on_termination : True ansible-playbook aws.yaml We started by uploading the public part of an SSH keypair to AWS, then queried for information about the network and the subnetwork, then ensured that the Security Group we wanted to use was present, and lastly triggered the machine build.","title":"Creating your first machine"},{"location":"learning/ansible/container_cloud/#automating-against-azure","text":"To let Ansible manage the Azure cloud, you need to install the Azure SDK for Python. pip install 'ansible[azure]' ``` ## Authentication - There are different ways to ensure that Ansible is able to manage Azure for you, based on the way your Azure account is set up, but they can all be configured in the ` ~/.azure/credentials ` file. ``` BASH [ default ] subscription_id = [ YOUR_SUBSCIRPTION_ID_HERE ] client_id = [ YOUR_CLIENT_ID_HERE ] secret = [ YOUR_SECRET_HERE ] tenant = [ YOUR_TENANT_HERE ] If you prefer to use Active Directories with a username and password. [ default ] ad_user = [ YOUR_AD_USER_HERE ] password = [ YOUR_AD_PASSWORD_HERE ] You can opt for an Active Directory login with ADFS. [ default ] ad_user = [ YOUR_AD_USER_HERE ] password = [ YOUR_AD_PASSWORD_HERE ] client_id = [ YOUR_CLIENT_ID_HERE ] tenant = [ YOUR_TENANT_HERE ] adfs_authority_url = [ YOUR_ADFS_AUTHORITY_URL_HERE ]","title":"Automating against Azure"},{"location":"learning/ansible/container_cloud/#creating-your-first-machine_1","text":"Create the azure.yaml Playbook with the following content. In Azure, you will need all the resources to be ready before you can issue the machine creation command. This is the reason you create the Storage Account, the Virtual Network, the Subnet, the Public IP, the security Group, and the NIC first, and only at that point, the machine itself. - hosts : localhost tasks : - name : Ensure the Storage Account is present azure_rm_storageaccount : resource_group : Testing name : mysa account_type : Standard_LRS - name : Ensure the Virtual Network is present azure_rm_virtualnetwork : resource_group : Testing name : myvn address_prefixes : \"10.10.0.0/16\" - name : Ensure the Subnet is present azure_rm_subnet : resource_group : Testing name : mysn address_prefix : \"10.10.0.0/24\" virtual_network : myvn - name : Ensure that the Public IP is set azure_rm_publicipaddress : resource_group : Testing allocation_method : Static name : myip - name : Ensure a Security Group allowing SSH is present azure_rm_securitygroup : resource_group : Testing name : mysg rules : - name : SSH protocol : Tcp destination_port_range : 22 access : Allow priority : 101 direction : Inbound - name : Ensure the NIC is present azure_rm_networkinterface : resource_group : Testing name : testnic001 virtual_network : myvn subnet : mysn public_ip_name : myip security_group : mysg - name : Ensure the Virtual Machine is present azure_rm_virtualmachine : resource_group : Testing name : myvm01 vm_size : Standard_D1 storage_account : mysa storage_container : myvm01 storage_blob : myvm01.vhd admin_username : admin admin_password : Password! network_interfaces : testnic001 image : offer : CentOS publisher : OpenLogic sku : '8.0' version : latest ansible-playbook azure.yaml","title":"Creating your first machine"},{"location":"learning/ansible/security_basics/","text":"Introduction \u00b6 Ansible Security Dev-Sec Community Playbooks Ansible Security Automation Why Ansible for this setup? \u00b6 Ansible is made for security automation and hardening. It uses YAML syntax, which helps us to codify our entire process of repeated tasks. By using this, we can automate the process of continuous delivery and deployment of infrastructure using roles and playbooks. The modular approach enables us to perform tasks very simply. For example, the operations teams can write a playbook to set up a WordPress site and the security team can create another role which can harden the WordPress site. It is very easy to use the modules for repeatability, and the output is idempotent, which means creating standards for the servers, applications, and infrastructure. Some use cases include creating base images for organizations using internal policy standards. Ansible uses SSH protocol, which is by default secured with encrypted transmission and host encryption. Also, there are no dependency issues while dealing with different types of operating systems. It uses Python to perform; this can be easily extended, based on our use case. Setting up nginx web server \u00b6 We are adding the signing key, then adding the repository, then installing. This ensures that we can also perform integrity checks while downloading packages from the repositories. Hardening SSH service \u00b6 # Disabling the root user login, and instead creating a different user, and, if required, providing the sudo privilege - name : create new user user : name : \"{{ new_user_name }}\" password : \"{{ new_user_password }}\" shell : /bin/bash groups : sudo append : yes # Using key-based authentication to log in. Unlike with password-based authentication, we can generate SSH keys and add the public key to the authorized keys - name : add ssh key for new user authorized_key : user : \"{{ new_user_name }}\" key : \"{{ lookup('file', '/home/user/.ssh/id_rsa.pub') }}\" state : present # Some of the configuration tweaks using the SSH configuration file; for example, PermitRootLogin, PubkeyAuthentication, and PasswordAuthentication - name : ssh configuration tweaks lineinfile : dest : /etc/ssh/sshd_config state : present line : \"{{ item }}\" backups : yes with_items : - \"PermitRootLogin no\" - \"PasswordAuthentication no\" notify : - restart ssh - We can also set up services like fail2ban for protecting against basic attacks. - Also, we can enable MFA, if required to log in. Digitial Ocean - The following playbook will provide more advanced features for SSH hardening by dev-sec team Hardening nginx \u00b6 We can start looking at things like disabling server tokens to not display version information, adding headers like X-XSS-Protection, and many other configuration tweaks. Most of these changes are done via configuration changes, and Ansible allows us to version and control and automate these changes based on user requirements. The nginx server version information can be blocked by adding the server_tokens off; value to the configuration add_header X-XSS-Protection \"1; mode=block\"; will enable the cross-site scripting (XSS) filter SSLv3 can be disabled by adding ssl_protocols TLSv1 TLSv1.1 TLSv1.2; - name : update the hardened nginx configuration changes template : src : \"hardened-nginx-config.j2\" dest : \"/etc/nginx/sites-available/default\" notify : - restart nginx Mozilla runs an updated web page on guidance for SSL/TLS . The guidance offers an opinion on what cipher suites to use, and other security measures. Additionally, if you trust their judgment, you can also use their SSL/TLS configuration generator to quickly generate a configuration for your web server configuration . Whichever configuration you decide to use, the template needs to be named as hardened-nginx-config.j2 . Hardening WordPress \u00b6 This includes basic checks for WordPress security misconfigurations. Some of them include: # Directory and file permissions - name : update the file permissions file : path : \"{{ WordPress_install_directory }}\" recurse : yes owner : \"{{ new_user_name }}\" group : www-data - name : updating file and directory permissions shell : \"{{ item }}\" with_items : - \"find {{ WordPress_install_directory }} -type d -exec chmod 755 {} \\ ;\" - \"find {{ WordPress_install_directory }} -type f -exec chmod 644 {} \\ ;\" # Username and attachment enumeration blocking. The following code snippet is part of nginx's configuration # Username enumeration block if ($args ~ \"^/?author=([0-9]*)\"){ return 403; } # Attachment enumeration block if ($query_string ~ \"attachment_id=([0-9]*)\"){ return 403; } # Disallowing file edits in the WordPress editor - name : update the WordPress configuration lineinfile : path : /var/www/html/wp-config.php line : \"{{ item }}\" with_items : - define('FS_METHOD', 'direct'); - define('DISALLOW_FILE_EDIT', true); Hardening a database service \u00b6 We can harden the MySQL service by binding it to localhost and the required interfaces for interacting with the application. It then removes the anonymous user and test databases - name : delete anonymous mysql user for localhost mysql_user : user : \"\" state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : secure mysql root user mysql_user : user : \"root\" password : \"{{ mysql_root_password }}\" host : \"{{ item }}\" login_password : \"{{ mysql_root_password }}\" login_user : root with_items : - 127.0.0.1 - localhost - ::1 - \"{{ ansible_fqdn }}\" - name : removes mysql test database mysql_db : db : test state : absent login_password : \"{{ mysql_root_password }}\" login_user : root Hardening a host firewall service \u00b6 Ansible even has a module for UFW, so the following snippet starts with installing this and enabling logging. It follows this by adding default policies, like default denying all incoming and allowing outgoing. Then it will add SSH, HTTP, and HTTPS services to allow incoming. These options are completely configurable, as required. - name : installing ufw package apt : name : \"ufw\" update_cache : yes state : present - name : enable ufw logging ufw : logging : on - name : default ufw setting ufw : direction : \"{{ item.direction }}\" policy : \"{{ item.policy }}\" with_items : - { direction : 'incoming' , policy : 'deny' } - { direction : 'outgoing' , policy : 'allow' } - name : allow required ports to access server ufw : rule : \"{{ item.policy }}\" port : \"{{ item.port }}\" proto : \"{{ item.protocol }}\" with_items : - { port : \"22\" , protocol : \"tcp\" , policy : \"allow\" } - { port : \"80\" , protocol : \"tcp\" , policy : \"allow\" } - { port : \"443\" , protocol : \"tcp\" , policy : \"allow\" } - name : enable ufw ufw : state : enabled - name : restart ufw and add to start up programs service : name : ufw state : restarted enabled : yes Setting up automated encrypted backups in AWS S3 \u00b6 Backups are always something that most of us feel should be done, but they seem quite a chore. Over the years, people have done extensive work to ensure we can have simple enough ways to back up and restore our data. In today's day and age, a great backup solution/software should be able to do the following: Automated: Automation allows for process around it Incremental: While storage is cheap overall, if we want backups at five minute intervals, what has changed should be backed up Encrypted before it leaves our server: This is to ensure that we have security of data at rest and in motion Cheap: While we care about our data, a good back up solution will be much cheaper than the server which needs to be backed up For our backup solution, we will pick up the following stack: Software: Duply - A wrapper over duplicity, a Python script Storage: While duply offers many backends, it works really well with AWS S3 Encryption: By using GPG, we can use asymmetric public and private key pairs - name : installing duply apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-boto - duply - name : check if we already have backup directory stat : path : \"/root/.duply/{{ new_backup_name }}\" register : duply_dir_stats - name : create backup directories shell : duply {{ new_backup_name }} create when : duply_dir_stats.stat.exists == False - name : update the duply configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dest }}\" with_items : - { src : conf.j2 , dest : /root/.duply/ {{ new_backup_name }} /conf } - { src : exclude.j2 , dest : /root/.duply/ {{ new_backup_name }} /exclude } - name : create cron job for automated backups template : src : duply-backup.j2 dest : /etc/cron.hourly/duply-backup LAMP stack playbook \u00b6 The high-level hierarchy structure of the entire playbook: \u00b6 inventory # inventory file group_vars/ # all.yml # variables site.yml # master playbook (contains list of roles) roles/ # common/ # common role tasks/ # main.yml # installing basic tasks web/ # apache2 role tasks/ # main.yml # install apache templates/ # web.conf.j2 # apache2 custom configuration vars/ # main.yml # variables for web role handlers/ # main.yml # start apache2 php/ # php role tasks/ # main.yml # installing php and restart apache2 db/ # db role tasks/ # main.yml # install mysql and include harden.yml harden.yml # security hardening for mysql handlers/ # main.yml # start db and restart apache2 vars/ # main.yml # variables for db role Playbook Files \u00b6 Here is a very basic static inventory file where we will define a since host and set the IP address used to connect to it. Configure the following inventory file as required: [ lamp ] lampstack ansible_host=192.168.56.10 # group_vars/lamp.yml, which has the configuration of all the global variables remote_username : \"hodor\" # site.yml, which is the main playbook file to start - name : LAMP stack setup on Ubuntu 16.04 hosts : lamp gather_facts : False remote_user : \"{{ remote_username }}\" become : True roles : - common - web - db - php # roles/common/tasks/main.yml file, which will install python2, curl, and git # In ubuntu 16.04 by default there is no python2 - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : install curl and git apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - curl - git # roles/web/tasks/main.yml, performs multiple operations, such as installation and configuration of apache2. # It also adds the service to the startup process - name : install apache2 server apt : name : apache2 state : present - name : update the apache2 server configuration template : src : web.conf.j2 dest : /etc/apache2/sites-available/000-default.conf owner : root group : root mode : 0644 - name : enable apache2 on startup systemd : name : apache2 enabled : yes notify : - start apache2 # notify parameter will trigger the handlers found in roles/web/handlers/main.yml - name : start apache2 systemd : state : started name : apache2 - name : stop apache2 systemd : state : stopped name : apache2 - name : restart apache2 systemd : state : restarted name : apache2 daemon_reload : yes # The template files will be taken from role/web/templates/web.conf.j2, which uses Jinja templating, it also takes values from local variables <VirtualHost *:80><VirtualHost *:80> ServerAdmin {{server_admin_email}} DocumentRoot {{server_document_root}} ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined </VirtualHost> # The local variables file is located in roles/web/vars/main.yml server_admin_email : hodor@localhost.local server_document_root : /var/www/html # File roles/db/tasks/main.yml includes installation of the database server with assigned passwords when prompted. # At the end of the file, we included harden.yml, which executes another set of tasks - name : set mysql root password debconf : name : mysql-server question : mysql-server/root_password value : \"{{ mysql_root_password | quote }}\" vtype : password - name : confirm mysql root password debconf : name : mysql-server question : mysql-server/root_password_again value : \"{{ mysql_root_password | quote }}\" vtype : password - name : install mysqlserver apt : name : \"{{ item }}\" state : present with_items : - mysql-server - mysql-client - include : harden.yml # harden.yml performs hardening of MySQL server configuration - name : deletes anonymous mysql user mysql_user : user : \"\" state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : secures the mysql root user mysql_user : user : root password : \"{{ mysql_root_password }}\" host : \"{{ item }}\" login_password : \"{{mysql_root_password}}\" login_user : root with_items : - 127.0.0.1 - localhost - ::1 - \"{{ ansible_fqdn }}\" - name : removes the mysql test database mysql_db : db : test state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : enable mysql on startup systemd : name : mysql enabled : yes notify : - start mysql # db server role also has roles/db/handlers/main.yml and local variables similar to the web role - name : start mysql systemd : state : started name : mysql - name : stop mysql systemd : state : stopped name : mysql - name : restart mysql systemd : state : restarted name : mysql daemon_reload : yes # roles/db/vars/main.yml, which has the mysql_root_password while configuring the server. # we can secure these plaintext passwords using ansible-vault in future. mysql_root_password : R4nd0mP4$$w0rd # we will install PHP and configure it to work with apache2 by restarting the roles/php/tasks/main.yml service - name : install php7 apt : name : \"{{ item }}\" state : present with_items : - php7.0-mysql - php7.0-curl - php7.0-json - php7.0-cgi - php7.0 - libapache2-mod-php7 - name : restart apache2 systemd : state : restarted name : apache2 daemon_reload : yes # Execute the following command against the Ubuntu 16.04 server to set up LAMP stack. # Provide the password when it prompts for system access for user hodor. ansible-playbook -i inventory site.yml Setting up Ansible Tower \u00b6 # Make sure you have Vagrant installed in your host system before running the following command: vagrant init ansible/tower vagrant up vagrant ssh # It will prompt you to enter IP address, username, and password to login to the Ansible Tower dashboard. - Then navigate the browser to https://10.42.0.42 and accept the SSL error to proceed. - This SSL error can be fixed by providing the valid certificates in the configuration at /etc/tower and need to restart the Ansible Tower service. - Enter the login credentials to access the Ansible Tower dashboard. - Once you log in, it will prompt you for the Ansible Tower license. - Ansible Tower also provides Role-Based Authentication Control (RBAC) , which provides a granular level of control for different users and groups to manage Tower. - Create a new user with the System Administrator privilege - To add inventory into Ansible Tower, we can simply enter it manually - Add credentials (or) keys to the tower by providing them in credential management, which can be reused as well. - Secrets store in Ansible Tower are encrypted with a symmetric key unique to each Ansible Tower cluster. - Once stored in the Ansible Tower database, the credentials may only be used, not viewed, in the web interface. - The types of credentials that Ansible Tower can store are passwords, SSH keys, Ansible Vault keys, and cloud credentials. - Once we have the inventory gathered, we can create jobs to perform the playbook or ad-hoc command operations. - The Ansible Tower REST API is a pretty powerful way to interact with the system - Get started with the pip install ansible-tower-cli command. Setting up AWX \u00b6 Ansible is very powerful, but it does require the user to use the CLI. In some situations, this is not the best option, such as in cases where you need to trigger an Ansible job from another job (where APIs would be better) or in cases where the person that should trigger a job should only be able to trigger that specific job. For these cases, AWX or Ansible Tower are better options to use. The only differences between AWX and Ansible Tower are that AWX is the upstream and open source version, while Ansible Tower is the Red Hat and downstream product that is officially supported but for a price, and also the delivery method. We will use AWX and talk about AWX, but everything we discuss also applies to Ansible Tower. Although there are several ways to install AWX, we are going to use the suggested AWX installation, which is container-based. For this reason, the following software needs to be installed on your machine: Ansible 2.4+. Docker. The docker Python module. The docker-compose Python module. If your system uses Security-Enhanced Linux (SELinux), you also need the libselinux Python module. Installing AWX \u00b6 We need to clone the AWX Git repository. git clone https://github.com/ansible/awx.git Modify the installer/inventory file by setting sensible values for the passwords and secrets (such as pg_password, rabbitmq_password, admin_password, and secret_key) Now that we have downloaded the Ansible AWX code and installer, we can move into the installer folder and execute the installation by running the following code. cd awx/installer ansible-playbook -i inventory install.yml The install.yml playbook performs the whole installation for us. It starts by checking the environment for possible misconfigurations or missing dependencies. If everything seems to be correct, it moves on to downloading several Docker images (including PostgreSQL, memcached, RabbitMQ, AWX Web, and AWX workers) and then runs them all. As soon as the playbook completes, you can check the installation by issuing the docker ps command. As you can see from docker ps output, our system now has a container called awx_web, which has bound itself to port 80. You can now access AWX by browsing to http://<ip address of your AWX host>/ and using the credentials you specified in the inventory file earlier on and that the default administrator username is admin unless you change it in the inventory. Running your first playbook from AWX \u00b6 As in Ansible, in AWX, the goal is running an Ansible playbook and each playbook that is run is called a job. Since AWX gives you more flexibility and automation than Ansible, it requires a little bit more configuration before you can run your first job. Creating an AWX project \u00b6 AWX uses the term project to identify a repository of Ansible playbooks. AWX projects support the placement of playbooks in all major Source Control Management (SCM) systems, such as Git, Mercurial, and SVN, but also support playbooks on the filesystem or playbooks provided by Red Hat Insights. Projects are the system to store and use playbooks in AWX. As you can imagine, there are many interesting additional configurations for AWX projects\u2014and the most interesting one, in my view\u2014is update revision on launch. If flagged, this option instructs Ansible to always update the playbook's repository before running any playbook from that project. This ensures it always executes the latest version of the playbook. This is an important feature to enable as if you don't have it checked, there is the possibility (and sooner or later, this will happen in your environment) that someone notices that there is a problem in a playbook and fixes it, then they run the playbook feeling sure that they are running the latest version. They then forget to run the synchronization task before running the playbook, effectively running the older version of the playbook. This could lead to major problems if the previous version was fairly buggy. The downside of using this option is that every time you execute a playbook, two playbooks are effectively run, adding time to your task execution. I think this is a very small downside and one that does not offset the benefits of using this option. Creating an inventory \u00b6 As with Ansible Core, to make AWX aware of the machines present in your environment, we use inventories. Inventories, in the AWX world, are not that different from their equivalents in Ansible Core. Since an empty inventory is not useful in any way, we are going to add localhost to it. We then need to add the hostname (localhost) and instruct Ansible to use the local connection by adding the following code to the VARIABLES box. --- ansible_connection : local ansible_python_interpreter : '{{ ansible_playbook_python }}' Creating a job template \u00b6 A job template in AWX is a collection of the configurations that are needed to perform a job. This is very similar to the ansible-playbook command-line options. The reason why we need to create a job template is so that playbook runs can be launched with little or no user input, meaning they can be delegated to teams who might not know all the details of how a playbook works, or can even be run on a scheduled basis without anyone present. Note that because we are running it using the local connection to localhost, we don't need to create or specify any credentials. However, if you were running a job template against one or more remote hosts, you would need to create a machine credential and associate it with your job template. A machine credential is, for example, an SSH username and password or an SSH username and a private key\u2014these are stored securely in the backend database of AWX, meaning you can again delegate playbook-related tasks to other teams without actually giving them passwords or SSH keys. The first thing we had to choose was whether we are creating a job template or a workflow template. We chose Job Template since we want to be able to create simple jobs out of this template. It's also possible to create more complex jobs, which are the composition of multiple job templates, with flow control features between one job and the next. This allows more complex situations and scenarios where you might want to have multiple jobs (such as the creation of an instance, company customization, the setup of Oracle Database, the setup of a MySQL database, and so on), but you also want to have a one-click deployment that would, for instance, set up the machine, apply all the company customization, and install the MySQL database. Obviously, you might also have another deployment that uses all the same components except the last one and in its place, it uses the Oracle Database piece to create an Oracle Database machine. This allows you to have extreme flexibility and to reuse a lot of components, creating multiple, nearly identical playbooks. It's interesting to note that many fields in the Job Template creation window have an option with the Prompt on launch caption. This is to be able to set this value optionally during the creation of the job template, but also allow the user running the job to enter/override it at runtime. This can be incredibly valuable when you have a field that changes on each run (perhaps the limit field, which operates in the same way as \u2013limit when used with the ansible-playbook command) or can also be used as a sanity check, as it prompts the user with the value (and gives them a chance to modify it) before the playbook is actually run. However, it could potentially block scheduled job runs, so exercise caution when enabling this feature. Running a job \u00b6 A job is an instance of a job template. This means that to perform any action on our machine, we have to create a job template instance or, more simply, a job. One of the great things about AWX and Ansible Tower is that they archive this job execution output in the backend database, meaning you can, at any point in the future, come back and query a job run to see what changed and what happened. This is incredibly powerful and useful for occasions such as auditing and policy enforcement. Controlling access to AWX \u00b6 In my opinion, one of the biggest advantages of AWX compared to Ansible is the fact that AWX allows multiple users to connect and control/perform actions. This allows a company to have a single AWX installation for different teams, a whole organization, or even multiple organizations. A Role-Based Access Control (RBAC) system is in place to manage the users' permissions. Both AWX and Ansible Tower can link to central directories, such as Lightweight Directory Access Protocol (LDAP) and Azure Active Directory however, we can also create user accounts locally on the AWX server itself. Creating a user \u00b6 One of the big advantages of AWX is the ability to manage multiple users. This allows us to create a user in AWX for each person that is using the AWX system so that we can ensure they are only granted the permissions that they need. Also, by using individual accounts, we can ensure that we can see who carried out what action by using the audit logs. By adding the email address, the username, and the password (with confirmation), you can create the new user. Users can be of three types: A normal user: Users of this type do not have any inherited permissions and they need to be awarded specific permissions to be able to do anything. A system auditor: Users of this type have full read-only privileges on the whole AWX installation. A system administrator: Users of this type have full privileges on the whole AWX installation. Creating a team \u00b6 Although having individual user accounts is an incredibly powerful tool, especially for enterprise use cases, it would be incredibly inconvenient and cumbersome to have to set permissions for each object (such as a job template or an inventory) on an individual basis. Every time someone joins a team, their user account has to be manually configured with the correct permissions against every object and, similarly, be removed if they leave. AWX and Ansible Tower have the same concept of user grouping that you would find in most other RBAC systems. The only slight difference is that in the user interface, they are referred to as teams, rather than groups. However, you can create teams simply and easily and then add and remove users as you need to. Doing this through the user interface is very straightforward and you will find the process similar to the way that most RBAC systems handle user groups, so we won't go into any more specific details here. Once you have your teams set up, I recommend that you assign your permissions to teams, rather than through individual users, as this will make your management of AWX object permissions much easier as your organization grows. Creating an organization \u00b6 Sometimes, you have multiple independent groups of people that you need to manage independent machines. For those kinds of scenarios, the use of organizations can help you. An organization is basically a tenant of AWX, with its own unique user accounts, teams, projects, inventories, and job templates\u2014it's almost like having a separate instance of AWX! After you create the organization, you can assign any kind of resource to an organization, such as projects, templates, inventories, users, and so on. Organizations are a simple concept to grasp, but also powerful in terms of segregating roles and responsibilities in AWX. Assigning permissions in AWX \u00b6 Individual users (or the teams that they belong to) can be granted permissions on a per-object basis. So, for example, you could have a team of database administrators who only have access to see and execute playbooks on an inventory of database servers, using job templates that are specific to their role. Linux system administrators could then have access to the inventories, projects, and job templates that are specific to their role. AWX hides objects that users don't have the privileges to, which means the database administrators never see the Linux system administrator objects and vice versa. There are a number of different privilege levels that you can award users (or teams) with, which include the following: Admin: This is the organization-level equivalent of a system administrator. Execute: This kind of user can only execute templates that are part of the organization. Project admin: This kind of user can alter any project that is part of the organization. Inventory admin: This kind of user can alter any inventory that is part of the organization. Credential admin: This kind of user can alter any credential that is part of the organization. Workflow admin: This kind of user can alter any workflow that is part of the organization. Notification admin: This kind of user can alter any notification that is part of the organization. Job template admin: This kind of user can alter any job template that is part of the organization. Auditor: This is the organization-level equivalent to a system auditor. Member: This is the organization-level equivalent of a normal user. Read: This kind of user is able to view non-sensible objects that are part of the organization. AWX is a great addition to the power of Ansible in an enterprise setting and really helps ensure that your users can run Ansible playbooks in a manner that is well managed, secure, and auditable. Setting up Jenkins \u00b6 - name : installing jenkins in ubuntu 16.04 hosts : \"192.168.1.7\" remote_user : ubuntu gather_facts : False become : True tasks : - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : install curl and git apt : name={{ item }} state=present update_cache=yes with_items : - curl - git - name : adding jenkins gpg key apt_key : url : https://pkg.jenkins.io/debian/jenkins-ci.org.key state : present - name : jeknins repository to system apt_repository : repo : http://pkg.jenkins.io/debian-stable binary/ state : present - name : installing jenkins apt : name : jenkins state : present update_cache : yes - name : adding jenkins to startup service : name : jenkins state : started enabled : yes - name : printing jenkins default administration password command : cat /var/lib/jenkins/secrets/initialAdminPassword register : jenkins_default_admin_password - debug : msg : \"{{ jenkins_default_admin_password.stdout }}\" # To set up Jenkins, run the following command. Where 192.168.1.7 is the server IP address where Jenkins will be installed: ansible-playbook -i '192.168.1.7,' site.yml --ask-sudo-pass # we have to navigate to the Jenkins dashboard by browsing to http://192.168.1.7:8080 and providing the auto-generated password. # If the playbook runs without any errors, it will display the password at the end of the play. - Create the new user by filling in the details and confirming to log in to the Jenkins console. - We can install custom plugins in Jenkins, navigate to the Manage Jenkins tab, select Manage Plugins, then navigate to the Available tab. - In the Filter: enter the plugin name as Ansible. Then select the checkbox and click Install without restart. - Now we are ready to work with the Ansible plugin for Jenkins. - Create a new project in the main dashboard, give it a name, and select Freestyle project to proceed: - We can configure the build options, this is where Jenkins will give us more flexibility to define our own triggers, build instructions, and post build scripts - Once the build triggers based on an event, this can be sent to some artifact storage, it can also be available in the Jenkins build console output - This is a really very powerful way to perform dynamic operations such as triggering automated server and stacks setup based on a code push to the repository, as well as scheduled scans and automated reporting. Security automation use cases \u00b6 Here is a list of tasks that will prepare you to build layers of automation for the stuff that is important to you: Adding playbooks or connecting your source code management (SCM) tools, such as GitHub/GitLab/BitBucket Authentication and data security Logging output and managing reports for the automation jobs Job scheduling Alerting, notifications, and webhooks Ansible Tower configuration \u00b6 To add playbooks into Ansible Tower, we have to start by creating projects, then select the SCM TYPE as Manual We can choose the SCM TYPE set to Git and provide a github.com URL pointing to a playbook Git SCM to add playbooks into projects We can also change the PROJECTS_ROOT under CONFIGURE TOWER to change this location. The added playbooks are executed by creating a job template. Then we can schedule these jobs (or) we can launch directly. Jenkins Ansible integration configuration \u00b6 Jenkins supports SCM to use playbooks and local directories for manual playbooks too. This can be configured with the build options. Jenkins supports both ad-hoc commands and playbooks to trigger as a build (or) post-build action. We can also specify credentials if we want to access private repositories We can add the Playbook path by specifying the location of the playbook and defining inventory and variables as required Authentication and data security \u00b6 Some of the security features the tools offer include: RBAC (authentication and authorization) Web application over TLS/SSL (security for data in motion) Encryption for storing secrets (security for data at rest) RBAC for Ansible Tower \u00b6 Ansible Tower supports RBAC to manage multiple users with different permissions and roles. It also supports Lightweight Directory Access Protocol (LDAP) integration in the enterprise version to support Active Directory. This feature allows us to create different levels of users for accessing Ansible Tower. For example: The operations team requires a system administrator role to perform playbook execution and other activities like monitoring The security team requires a system auditor role to perform audit check for compliance standards such as Payment Card Industry Data Security Standard (PCI DSS) or even internal policy validation Normal users, such as team members, might just want to see how things are going, in the form of status updates and failure (or) success of job status TLS/SSL for Ansible Tower \u00b6 By default, Ansible Tower uses HTTPS using self-signed certificates at /etc/tower/tower.cert and /etc/tower/tower.key. Encryption and data security for Ansible Tower \u00b6 Ansible Tower has been created with built-in security for handling encryption of credentials that includes passwords and keys. It uses Ansible Vault to perform this operation. It encrypts passwords and key information in the database. TLS/SSL for Jenkins \u00b6 By default, Jenkins runs as plain old HTTP. To enable HTTPS, we can use a reverse proxy, such as Nginx, in front of Jenkins to serve as HTTPS. Digital Ocean Reference for TLS Encryption and data security for Jenkins \u00b6 We are using Jenkins' default credential feature. This will store the keys and passwords in the local filesystem. Output of the playbooks \u00b6 We would like to know where can we see the output of the playbooks executing and if any other logs that get created. Report management for Ansible Tower \u00b6 By default, Ansible Tower itself is a reporting platform for the status of the playbooks, job executions, and inventory collection. The Ansible Tower dashboard gives an overview of the total projects, inventories, hosts, and status of the jobs. The output can be consumed in the dashboard, standard out, or by using the REST API and we can get this via tower-cli command line tool as well, which is just a pre-built command line tool for interfacing with the REST API. Scheduling of jobs \u00b6 The scheduling of jobs is simple and straightforward in Ansible Tower. For a job, you can specify a schedule and the options are mostly like cron. For example, you can say that you have a daily scan template and would like it to be executed at 4 a.m. every day for the next three months. This kind of schedule makes our meta automation very flexible and powerful. Alerting, notifications, and webhooks \u00b6 Tower supports multiple ways of alerting and notifying users as per configuration. This can even be configured to make an HTTP POST request to a URL of your choice using a webhook. Ansible Tower notification using slack webhook is a popular option. Setting Up a Hardened WordPress with Encrypted Automated Backups \u00b6 Automating our server's patches is the most obvious, and possibly popular, requirement. We will apply security automation techniques and approaches to set up a hardened WordPress and enable encrypted backups. Everyone would agree that setting up a secure website and keeping it secured is a fairly common security requirement. And since it is so common, it would be useful for a lot of people who are tasked with building and managing websites to stay secure to look at that specific scenario. Note Are you aware that, according to Wikipedia, 27.5% of the top 10 million websites use WordPress? According to another statistic, 58.7% of all websites with known software on the entire web run WordPress. For us, setting up a hardened WordPress with encrypted automated backups can be broken down into the following steps: Setting up a Linux/Windows server with security measures in place. Setting up a web server (Apache/Nginx on Linux and IIS on Windows). Setting up a database server (MySQL) on the same host. Setting up WordPress using a command-line utility called WP-CLI. Setting up backup for the site files and the database which is incremental, encrypted, and most importantly, automated. Note We will assume that the server that we plan to deploy our WordPress website on is already up and running and we are able to connect to it. We will store the backup in an already configured AWS S3 bucket, for which the access key and secret access key is already provisioned. Refer the examples in Introduction to set this up. Secure automated the WordPress updates \u00b6 Run the backups and update WordPress core, themes, and plugins. This can be scheduled via an Ansible Tower job for every day - name : running backup using duply command : /etc/cron.hourly/duply-backup - name : updating WordPress core command : wp core update register : wp_core_update_output ignore_errors : yes - name : wp core update output debug : msg : \"{{ wp_core_update_output.stdout }}\" - name : updating WordPress themes command : wp theme update --all register : wp_theme_update_output ignore_errors : yes - name : wp themes update output debug : msg : \"{{ wp_theme_update_output.stdout }}\" - name : updating WordPress plugins command : wp plugin update --all register : wp_plugin_update_output ignore_errors : yes - name : wp plugins update output debug : msg : \"{{ wp_plugin_update_output.stdout }}\" Scheduling via Ansible Tower for daily updates \u00b6 Ansible Tower job scheduling for automated WordPress updates OR We can use the cron job template to perform this daily and add this template while deploying the WordPress setup #!/bin/bash /etc/cron.hourly/duply-backup wp core update wp theme update --all wp plugin update --all Setting up Apache2 web server \u00b6 Shows how we can use templating to perform configuration updates in the server - name : installing apache2 server apt : name : \"apache2\" update_cache : yes state : present - name : updating customized templates for apache2 configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" mode : 0644 with_tems : - { src : apache2.conf.j2 , dst : /etc/apache2/conf.d/apache2.conf } - { src : 000-default.conf.j2 , dst : /etc/apache2/sites-available/000-default.conf } - { src : default-ssl.conf.j2 , dst : /etc/apache2/sites-available/default-ssl.conf } - name : adding custom link for sites-enabled from sites-available file : src : \"{{ item.src }}\" dest : \"{{ item.dest }}\" state : link with_items : - { src : '/etc/apache2/sites-available/000-default.conf' , dest : '/etc/apache2/sites-enabled/000-default.conf' } - { src : '/etc/apache2/sites-available/default-ssl.conf' , dest : '/etc/apache2/sites-enabled/default-ssl.conf' } notify : - start apache2 - startup apache2 Enabling TLS/SSL with Let's Encrypt \u00b6 We can use a command-line tool offered by Let's Encrypt to get free SSL/TLS certificates in an open, automated manner. The tool is capable of reading and understanding an nginx virtual host file and generating the relevant certificates completely automatically, without any kind of manual intervention. - name : adding certbot ppa apt_repository : repo : \"ppa:certbot/certbot\" - name : install certbot apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-certbot-nginx - name : check if we have generated a cert already stat : path : \"/etc/letsencrypt/live/{{ website_domain_name }}/fullchain.pem\" register : cert_stats - name : run certbot to generate the certificates shell : \"certbot certonly --standalone -d {{ website_domain_name }} --email {{ service_admin_email }} --non-interactive --agree-tos\" when : cert_stats.stat.exists == False - name : configuring site files template : src : website.conf dest : \"/etc/nginx/sites-available/{{ website_domain_name }}\" - name : restart nginx service : name : nginx state : restarted Log Monitoring \u00b6 Log monitoring is the perfect place to think about security automation. For monitoring to be effective, a few things need to happen. We should be able to move logs from different devices to a central location. We should be able to make sense of what a regular log entry is and what could possibly be an attack. We should be able to store the logs, and also operate on them for things such as aggregation, normalization, and eventually, analysis. Traditional logging systems find it difficult to log for all applications, systems, and devices. The variety of time formats, log output formats, and so on, makes the task pretty complicated. The biggest roadblock is finding a way to be able to centralize logs. This gets in the way of being able to process log entries in real time, or near real time effectively. Some of the problematic points are as follows: Access is often difficult High expertise in mined data is required Logs can be difficult to find Log data is immense in size Introduction to Elastic Stack \u00b6 Elastic Stack is a group of open source products from the Elastic company. It takes data from any type of source and in any format and searches, analyzes, and visualizes that data in real time. It consists of four major components, as follows: Elasticsearch Logstash Kibana Beats It helps users/admins to collect, analyze, and visualize data in (near) real time. Each module fits based on your use case and environment. Elasticsearch \u00b6 Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected Main plus points of Elastic Stack: Distributed and highly available search engine, written in Java, and uses Groovy Built on top of Lucene Multi-tenant, with multi types and a set of APIs Document-oriented, providing (near) real-time search Logstash \u00b6 Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources, simultaneously transforms it, and then sends it to your favorite stash. Centralized data processing of all types of logs Consists of the following three main components: Input: Passing logs to process them into machine-understandable format Filter: A set of conditions to perform a specific action on an event Output: The decision maker for processed events/logs Kibana \u00b6 Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack, so you can do anything from learning why you're getting paged at 2:00 a.m. to understanding the impact rain might have on your quarterly numbers. Kibana's list of features: Powerful frontend dashboard is written in JavaScript Browser-based analytics and search dashboard for Elasticsearch A flexible analytics and visualization platform Provides data in the form of charts, graphs, counts, maps, and so on, in real time Beats \u00b6 Beats is the platform for single-purpose data shippers. They install as lightweight agents and send data from hundreds or thousands of machines to Logstash or Elasticsearch. Beats are: Lightweight shippers for Elasticsearch and Logstash Capture all sorts of operational data, like logs or network packet data They can send logs to either Elasticsearch or Logstash ElastAlert \u00b6 ElastAlert is a Python tool which also bundles with the different types of integrations to support with alerting and notifications. Some of them include Command, Email, JIRA, OpsGenie, AWS SNS, HipChat, Slack, Telegram, and so on. It also provides a modular approach to creating our own integrations. Why should we use Elastic Stack for security monitoring and alerting? \u00b6 The Elastic Stack solves most of the problems that we have discussed before, such as: Ability to store large amounts of data Ability to understand and read a variety of log formats Ability to ship the log information from a variety of devices in near real time to one central location A visualization dashboard for log analysis Prerequisites for setting up Elastic Stack \u00b6 - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : accepting oracle java license agreement debconf : name : 'oracle-java8-installer' question : 'shared/accepted-oracle-license-v1-1' value : 'true' vtype : 'select' - name : adding ppa repo for oracle java by webupd8team apt_repository : repo : 'ppa:webupd8team/java' state : present update_cache : yes - name : installing java nginx apache2-utils and git apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-software-properties - oracle-java8-installer - nginx - apache2-utils - python-pip - python-passlib Setting up the Elastic Stack \u00b6 The stack is a combination of: The Elasticsearch service The Logstash service The Kibana service The Beats service on all the devices We are going to set up Elasticsearch, Logstash, and Kibana on a single machine. This is the main log collection machine: It requires a minimum of 4 GB RAM, as we are using a single machine to serve three services (Elasticsearch, Logstash, and Kibana) It requires a minimum of 20 GB disk space, and, based on your log size, you can add the disk space Installing Elasticsearch \u00b6 Install Elasticsearch from the repository with gpg key and add it to the startup programs - name : adding elastic gpg key for elasticsearch apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing elasticsearch apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - elasticsearch - name : adding elasticsearch to the startup programs service : name : elasticsearch enabled : yes notify : - start elasticsearch Configure the Elasticsearch cluster with the required settings. Also, set up the JVM options for the Elasticsearch cluster. Also, create a backup directory for Elasticsearch cluster backups and snapshots - name : creating elasticsearch backup repo directory at {{ elasticsearch_backups_repo_path }} file : path : \"{{ elasticsearch_backups_repo_path }}\" state : directory mode : 0755 owner : elasticsearch group : elasticsearch - name : configuring elasticsearch.yml file template : src : \"{{ item.src }}\" dest : /etc/elasticsearch/\"{{ item.dst }}\" with_items : - { src : 'elasticsearch.yml.j2' , dst : 'elasticsearch.yml' } - { src : 'jvm.options.j2' , dst : 'jvm.options' } notify : - restart elasticsearch The notify part will trigger the restart elasticsearch handler and the handler file will look as follows. - name : start elasticsearch service : name : elasticsearch state : started - name : restart elasticsearch service : name : elasticsearch state : restarted Installing Logstash \u00b6 Install Logstash from the repository with gpg key and add it to the startup programs - name : adding elastic gpg key for logstash apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing logstash apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - logstash - name : adding logstash to the startup programs service : name : logstash enabled : yes notify : - start logstash Configure the Logstash service with input, output, and filter settings. This enables receiving logs, processing logs, and sending logs to the Elasticsearch cluster - name : logstash configuration files template : src : \"{{ item.src }}\" dest : /etc/logstash/conf.d/\"{{ item.dst }}\" with_items : - { src : '02-beats-input.conf.j2' , dst : '02-beats-input.conf' } - { src : '10-sshlog-filter.conf.j2' , dst : '10-sshlog-filter.conf' } - { src : '11-weblog-filter.conf.j2' , dst : '11-weblog-filter.conf' } - { src : '30-elasticsearch-output.conf.j2' , dst : '10-elasticsearch-output.conf' } notify : - restart logstash Logstash configuration \u00b6 To receive logs from different systems, we use the Beats service from Elastic. The following configuration is to receive logs from different servers to the Logstash server. Logstash runs on port 5044 and we can use SSL certificates to ensure logs are transferred via an encrypted channel. # 02-beats-input.conf.j2 input { beats { port => 5044 ssl => true ssl_certificate => \"/etc/pki/tls/certs/logstash-forwarder.crt\" ssl_key => \"/etc/pki/tls/private/logstash-forwarder.key\" } } The following configuration is to parse the system SSH service logs (auth.log) using grok filters. It also applies filters like geoip, while providing additional information like country, location, longitude, latitude, and so on. #10-sshlog-filter.conf.j2 filter { if [type] == \"sshlog\" { grok { match => [ \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : message repeated 2 times : \\[ %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } password for invalid user %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } % { WORD : auth_method } for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\" ] } date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] locale => en } geoip { source => \"ip\" } } } The following configuration is to parse web server logs (nginx, apache2). We will also apply filters for geoip and useragent. The useragent filter allows us to get information about the agent, OS type, version information, and so on. #11-weblog-filter.conf.j2 filter { if [type] == \"weblog\" { grok { match => { \"message\" => '%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}' } } date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] locale => en } geoip { source => \"clientip\" } useragent { source => \"agent\" target => \"useragent\" } } } The following configuration will send the log output into the Elasticsearch cluster with daily index formats. #30-elasticsearch-output.conf.j2 output { elasticsearch { hosts => [\"localhost:9200\"] manage_template => false index => \"%{[@metadata][beat]}-%{+YYYY.MM.dd}\" document_type => \"%{[@metadata][type]}\" } } Installing Kibana \u00b6 By default we are not making any changes in Kibana, as it works out of the box with Elasticsearch. - name : adding elastic gpg key for kibana apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing kibana apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - kibana - name : adding kibana to the startup programs service : name : kibana enabled : yes notify : - start kibana By default Kibana doesn't have any authentication, X-Pack is the commercial plug-in by Elastic for RBAC (role-based access control) with security. Also, some open source options include https://readonlyrest.com/ and Search Guard ( https://floragunn.com ) to interact with Elasticsearch. Using TLS/SSL and custom authentication and aauthorization is highly recommended. Some of the open source options includes Oauth2 Proxy ( bitly/oauth2_proxy ) and Auth0, and so on. Setting up nginx reverse proxy \u00b6 The following configuration is to enable basic authentication for Kibana using nginx reverse proxy. server { listen 80; server_name localhost; auth_basic \"Restricted Access\"; auth_basic_user_file /etc/nginx/htpasswd.users; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } Setting up and configuring the nginx service looks as follows. #command: htpasswd -c /etc/nginx/htpasswd.users - name : htpasswd generation htpasswd : path : \"/etc/nginx/htpasswd.users\" name : \"{{ basic_auth_username }}\" password : \"{{ basic_auth_password }}\" owner : root group : root mode : 0644 - name : nginx virtualhost configuration template : src : \"templates/nginxdefault.j2\" dest : \"/etc/nginx/sites-available/default\" notify : - restart nginx Installing Beats to send logs to Elastic Stack \u00b6 We are going to install Filebeat to send SSH and web server logs to the Elastic Stack: - name : adding elastic gpg key for filebeat apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing filebeat apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - apt-transport-https - filebeat - name : adding filebeat to the startup programs service : name : filebeat enabled : yes notify : - start filebeat Configure the Filebeat to send both SSH and web server logs to Elastic Stack, to process and index in near real-time. filebeat : prospectors : - paths : - /var/log/auth.log # - /var/log/syslog # - /var/log/*.log document_type : sshlog - paths : - /var/log/nginx/access.log document_type : weblog registry_file : /var/lib/filebeat/registry output : logstash : hosts : [ \"{{ logstash_server_ip }}:5044\" ] bulk_max_size : 1024 ssl : certificate_authorities : [ \"/etc/pki/tls/certs/logstash-forwarder.crt\" ] logging : files : rotateeverybytes : 10485760 # = 10MB ElastAlert for alerting \u00b6 First, we need to install the prerequisites for setting up ElastAlert. - name : installing pre requisuites for elastalert apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-pip - python-dev - libffi-dev - libssl-dev - python-setuptools - build-essential - name : installing elastalert pip : name : elastalert - name : creating elastalert directories file : path : \"{{ item }}\" state : directory mode : 0755 with_items : - /opt/elastalert/rules - /opt/elastalert/config - name : creating elastalert configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : 'elastalert-config.j2' , dst : '/opt/elastalert/config/config.yml' } - { src : 'elastalert-service.j2' , dst : '/lib/systemd/system/elastalert.service' } - { src : 'elastalert-sshrule.j2' , dst : '/opt/elastalert/rules/ssh-bruteforce.yml' } - name : enable elastalert service service : name : elastalert state : started enabled : yes Creating a simple startup script so that ElastAlert will be used as a system service. [ Unit ] Description = elastalert After = multi-user.target [ Service ] Type = simple WorkingDirectory = /opt/elastalert ExecStart = /usr/local/bin/elastalert --config /opt/elastalert/config/config.yml [ Install ] WantedBy = multi-user.target Configuring the Let's Encrypt service \u00b6 - name : adding certbot ppa apt_repository : repo : \"ppa:certbot/certbot\" - name : install certbot apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-certbot-nginx - name : check if we have generated a cert already stat : path : \"/etc/letsencrypt/live/{{ website_domain_name }}/fullchain.pem\" register : cert_stats - name : run certbot to generate the certificates shell : \"certbot certonly --standalone -d {{ website_domain_name }} --email {{ service_admin_email }} --non-interactive --agree-tos\" when : cert_stats.stat.exists == False - name : configuring site files template : src : website.conf dest : \"/etc/nginx/sites-available/{{ website_domain_name }}\" - name : restart nginx service : name : nginx state : restarted ElastAlert rule configuration \u00b6 Assuming that you already have Elastic Stack installed and logging SSH logs, use the following ElastAlert rule to trigger SSH attack IP blacklisting. es_host : localhost es_port : 9200 name : \"SSH Bruteforce attack alert\" type : frequency index : filebeat-* num_events : 20 timeframe : minutes : 1 # For more info: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.html filter : - query : query_string : query : '_type:sshlog AND login:failed AND (username: \"ubuntu\" OR username: \"root\")' alert : - slack : slack_webhook_url : \"https://hooks.slack.com/services/xxxxx\" slack_username_override : \"attack-bot\" slack_emoji_override : \"robot_face\" - command : [ \"/usr/bin/curl\" , \"https://xxxxxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/zzzzzzzzzzzzzz/ip/inframonitor/%(ip)s\" ] realert : minutes : 0 For more references, visit https://elastalert.readthedocs.io/en/latest/running_elastalert.html . Serverless Automated Defense \u00b6 If we can get a notification for an attack, we can set up and do the following: Call an AWS Lambda function Send the attacker's IP address information to this AWS Lambda function endpoint Use the code deployed in the Lambda function to call the VPC network access list API and block the attacker's IP address To ensure that we don't fill up the ACLs with attacker IPs, we can combine this approach with AWS DynamoDB to store this information for a short duration and remove it from the block list. As soon as an attack is detected, the alerter sends the IP to the blacklist lambda endpoint via an HTTPS request. The IP is blocked using the network ACL and the record of it is maintained in DynamoDB. If the IP is currently blocked already, then the expiry time for the rule will be extended in the DynamoDB. An expiry handler function is periodically triggered, which removes expired rules from DynamoDB and ACL accordingly. Setup \u00b6 The setup involves the following steps: Obtain IAM credentials Create a table in DynamoDB Configure the lambda function based on requirement Deploy code to AWS Lambda Configure Cloudwatch to periodic invocation The entire setup is automated, except for obtaining the IAM credentials and configuring the function based on requirements. Configuration \u00b6 The following parameters are configurable before deployment: region: AWS region to deploy in. This needs to be the same as the region where the VPC network resides. accessToken: The accessToken that will be used to authenticate the requests to the blacklist endpoint. aclLimit: The maximum number of rules an ACL can handle. The maximum limit in AWS is 20 by default. ruleStartId: The starting ID for rules in the ACL. aclID: The ACL ID of the network where the rules will be applied. tableName: The unique table name in DynamoDB, created for each VPC to be defended. ruleValidity: The duration for which a rule is valid, after which the IP will be unblocked. // Configure the following in the config.js file module . exports = { region : \"us-east-1\" , // AWS Region to deploy in accessToken : \"YOUR_R4NDOM_S3CR3T_ACCESS_TOKEN_GOES_HERE\" , // Accesstoken to make requests to blacklist aclLimit : 20 , // Maximum number of acl rules ruleStartId : 10 , // Starting id for acl entries aclId : \"YOUR_ACL_ID\" , // AclId that you want to be managed tableName : \"blacklist_ip\" , // DynamoDB table that will be created ruleValidity : 5 // Validity of Blacklist rule in minutes } Make sure to modify at least the aclId, accessToken, and region based on your setup. To modify the lambda deployment configuration use the serverless.yml file ... functions : blacklist : handler : handler.blacklistip events : - http : path : blacklistip method : get handleexpiry : handler : handler.handleexpiry events : - schedule : rate(1 minute) ... For example, the rate at which the expiry function is triggered and the endpoint URL for the blacklist function can be modified using the YML file. But the defaults are already optimal. # The playbook looks as follows: - name : installing node run time and npm apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - nodejs - npm - name : installing serverless package npm : name : \"{{ item }}\" global : yes state : present with_items : - serverless - aws-sdk - name : copy the setup files template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : 'config.js.j2' , dst : '/opt/serverless/config.js' } - { src : 'handler.js.j2' , dst : '/opt/serverless/handler.js' } - { src : 'iamRoleStatements.json.j2' , dst : '/opt/serverless/iamRoleStatements.json' } - { src : 'initDb.js.j2' , dst : '/opt/serverless/initDb.js' } - { src : 'serverless.yml.j2' , dst : '/opt/serverless/serverless.yml' } - { src : 'aws-credentials.j2' , dst : '~/.aws/credentials' } - name : create dynamo db table command : node initDb.js args : chdir : /opt/serverless/ - name : deploy the serverless command : serverless deploy args : chdir : /opt/serverless/ The current setup for AWS Lambda is to block the IP address against network ACL. This can be reused with other API endpoints, like a firewall dynamic block list and other security devices. The blacklist endpoint is responsible for blocking an IP address. The URL looks like the following: https://lambda_url/blacklistipaccessToken=ACCESS_TOKEN&ip=IP_ADDRESS The query parameters are as follows: IP_ADDRESS: This is the IP address to be blocked ACCESS_TOKEN: The accessToken to authenticate the request Automated defense lambda in action \u00b6 When the ElastAlert detects an SSH brute force attack, it will trigger a request to lambda endpoint by providing the attacker's IP address. Then our automated defense platform will trigger a network ACL blocklist rule. This can be configurable to say for how much time it should be blocked.","title":"Introduction"},{"location":"learning/ansible/security_basics/#introduction","text":"Ansible Security Dev-Sec Community Playbooks Ansible Security Automation","title":"Introduction"},{"location":"learning/ansible/security_basics/#why-ansible-for-this-setup","text":"Ansible is made for security automation and hardening. It uses YAML syntax, which helps us to codify our entire process of repeated tasks. By using this, we can automate the process of continuous delivery and deployment of infrastructure using roles and playbooks. The modular approach enables us to perform tasks very simply. For example, the operations teams can write a playbook to set up a WordPress site and the security team can create another role which can harden the WordPress site. It is very easy to use the modules for repeatability, and the output is idempotent, which means creating standards for the servers, applications, and infrastructure. Some use cases include creating base images for organizations using internal policy standards. Ansible uses SSH protocol, which is by default secured with encrypted transmission and host encryption. Also, there are no dependency issues while dealing with different types of operating systems. It uses Python to perform; this can be easily extended, based on our use case.","title":"Why Ansible for this setup?"},{"location":"learning/ansible/security_basics/#setting-up-nginx-web-server","text":"We are adding the signing key, then adding the repository, then installing. This ensures that we can also perform integrity checks while downloading packages from the repositories.","title":"Setting up nginx web server"},{"location":"learning/ansible/security_basics/#hardening-ssh-service","text":"# Disabling the root user login, and instead creating a different user, and, if required, providing the sudo privilege - name : create new user user : name : \"{{ new_user_name }}\" password : \"{{ new_user_password }}\" shell : /bin/bash groups : sudo append : yes # Using key-based authentication to log in. Unlike with password-based authentication, we can generate SSH keys and add the public key to the authorized keys - name : add ssh key for new user authorized_key : user : \"{{ new_user_name }}\" key : \"{{ lookup('file', '/home/user/.ssh/id_rsa.pub') }}\" state : present # Some of the configuration tweaks using the SSH configuration file; for example, PermitRootLogin, PubkeyAuthentication, and PasswordAuthentication - name : ssh configuration tweaks lineinfile : dest : /etc/ssh/sshd_config state : present line : \"{{ item }}\" backups : yes with_items : - \"PermitRootLogin no\" - \"PasswordAuthentication no\" notify : - restart ssh - We can also set up services like fail2ban for protecting against basic attacks. - Also, we can enable MFA, if required to log in. Digitial Ocean - The following playbook will provide more advanced features for SSH hardening by dev-sec team","title":"Hardening SSH service"},{"location":"learning/ansible/security_basics/#hardening-nginx","text":"We can start looking at things like disabling server tokens to not display version information, adding headers like X-XSS-Protection, and many other configuration tweaks. Most of these changes are done via configuration changes, and Ansible allows us to version and control and automate these changes based on user requirements. The nginx server version information can be blocked by adding the server_tokens off; value to the configuration add_header X-XSS-Protection \"1; mode=block\"; will enable the cross-site scripting (XSS) filter SSLv3 can be disabled by adding ssl_protocols TLSv1 TLSv1.1 TLSv1.2; - name : update the hardened nginx configuration changes template : src : \"hardened-nginx-config.j2\" dest : \"/etc/nginx/sites-available/default\" notify : - restart nginx Mozilla runs an updated web page on guidance for SSL/TLS . The guidance offers an opinion on what cipher suites to use, and other security measures. Additionally, if you trust their judgment, you can also use their SSL/TLS configuration generator to quickly generate a configuration for your web server configuration . Whichever configuration you decide to use, the template needs to be named as hardened-nginx-config.j2 .","title":"Hardening nginx"},{"location":"learning/ansible/security_basics/#hardening-wordpress","text":"This includes basic checks for WordPress security misconfigurations. Some of them include: # Directory and file permissions - name : update the file permissions file : path : \"{{ WordPress_install_directory }}\" recurse : yes owner : \"{{ new_user_name }}\" group : www-data - name : updating file and directory permissions shell : \"{{ item }}\" with_items : - \"find {{ WordPress_install_directory }} -type d -exec chmod 755 {} \\ ;\" - \"find {{ WordPress_install_directory }} -type f -exec chmod 644 {} \\ ;\" # Username and attachment enumeration blocking. The following code snippet is part of nginx's configuration # Username enumeration block if ($args ~ \"^/?author=([0-9]*)\"){ return 403; } # Attachment enumeration block if ($query_string ~ \"attachment_id=([0-9]*)\"){ return 403; } # Disallowing file edits in the WordPress editor - name : update the WordPress configuration lineinfile : path : /var/www/html/wp-config.php line : \"{{ item }}\" with_items : - define('FS_METHOD', 'direct'); - define('DISALLOW_FILE_EDIT', true);","title":"Hardening WordPress"},{"location":"learning/ansible/security_basics/#hardening-a-database-service","text":"We can harden the MySQL service by binding it to localhost and the required interfaces for interacting with the application. It then removes the anonymous user and test databases - name : delete anonymous mysql user for localhost mysql_user : user : \"\" state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : secure mysql root user mysql_user : user : \"root\" password : \"{{ mysql_root_password }}\" host : \"{{ item }}\" login_password : \"{{ mysql_root_password }}\" login_user : root with_items : - 127.0.0.1 - localhost - ::1 - \"{{ ansible_fqdn }}\" - name : removes mysql test database mysql_db : db : test state : absent login_password : \"{{ mysql_root_password }}\" login_user : root","title":"Hardening a database service"},{"location":"learning/ansible/security_basics/#hardening-a-host-firewall-service","text":"Ansible even has a module for UFW, so the following snippet starts with installing this and enabling logging. It follows this by adding default policies, like default denying all incoming and allowing outgoing. Then it will add SSH, HTTP, and HTTPS services to allow incoming. These options are completely configurable, as required. - name : installing ufw package apt : name : \"ufw\" update_cache : yes state : present - name : enable ufw logging ufw : logging : on - name : default ufw setting ufw : direction : \"{{ item.direction }}\" policy : \"{{ item.policy }}\" with_items : - { direction : 'incoming' , policy : 'deny' } - { direction : 'outgoing' , policy : 'allow' } - name : allow required ports to access server ufw : rule : \"{{ item.policy }}\" port : \"{{ item.port }}\" proto : \"{{ item.protocol }}\" with_items : - { port : \"22\" , protocol : \"tcp\" , policy : \"allow\" } - { port : \"80\" , protocol : \"tcp\" , policy : \"allow\" } - { port : \"443\" , protocol : \"tcp\" , policy : \"allow\" } - name : enable ufw ufw : state : enabled - name : restart ufw and add to start up programs service : name : ufw state : restarted enabled : yes","title":"Hardening a host firewall service"},{"location":"learning/ansible/security_basics/#setting-up-automated-encrypted-backups-in-aws-s3","text":"Backups are always something that most of us feel should be done, but they seem quite a chore. Over the years, people have done extensive work to ensure we can have simple enough ways to back up and restore our data. In today's day and age, a great backup solution/software should be able to do the following: Automated: Automation allows for process around it Incremental: While storage is cheap overall, if we want backups at five minute intervals, what has changed should be backed up Encrypted before it leaves our server: This is to ensure that we have security of data at rest and in motion Cheap: While we care about our data, a good back up solution will be much cheaper than the server which needs to be backed up For our backup solution, we will pick up the following stack: Software: Duply - A wrapper over duplicity, a Python script Storage: While duply offers many backends, it works really well with AWS S3 Encryption: By using GPG, we can use asymmetric public and private key pairs - name : installing duply apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-boto - duply - name : check if we already have backup directory stat : path : \"/root/.duply/{{ new_backup_name }}\" register : duply_dir_stats - name : create backup directories shell : duply {{ new_backup_name }} create when : duply_dir_stats.stat.exists == False - name : update the duply configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dest }}\" with_items : - { src : conf.j2 , dest : /root/.duply/ {{ new_backup_name }} /conf } - { src : exclude.j2 , dest : /root/.duply/ {{ new_backup_name }} /exclude } - name : create cron job for automated backups template : src : duply-backup.j2 dest : /etc/cron.hourly/duply-backup","title":"Setting up automated encrypted backups in AWS S3"},{"location":"learning/ansible/security_basics/#lamp-stack-playbook","text":"","title":"LAMP stack playbook"},{"location":"learning/ansible/security_basics/#the-high-level-hierarchy-structure-of-the-entire-playbook","text":"inventory # inventory file group_vars/ # all.yml # variables site.yml # master playbook (contains list of roles) roles/ # common/ # common role tasks/ # main.yml # installing basic tasks web/ # apache2 role tasks/ # main.yml # install apache templates/ # web.conf.j2 # apache2 custom configuration vars/ # main.yml # variables for web role handlers/ # main.yml # start apache2 php/ # php role tasks/ # main.yml # installing php and restart apache2 db/ # db role tasks/ # main.yml # install mysql and include harden.yml harden.yml # security hardening for mysql handlers/ # main.yml # start db and restart apache2 vars/ # main.yml # variables for db role","title":"The high-level hierarchy structure of the entire playbook:"},{"location":"learning/ansible/security_basics/#playbook-files","text":"Here is a very basic static inventory file where we will define a since host and set the IP address used to connect to it. Configure the following inventory file as required: [ lamp ] lampstack ansible_host=192.168.56.10 # group_vars/lamp.yml, which has the configuration of all the global variables remote_username : \"hodor\" # site.yml, which is the main playbook file to start - name : LAMP stack setup on Ubuntu 16.04 hosts : lamp gather_facts : False remote_user : \"{{ remote_username }}\" become : True roles : - common - web - db - php # roles/common/tasks/main.yml file, which will install python2, curl, and git # In ubuntu 16.04 by default there is no python2 - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : install curl and git apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - curl - git # roles/web/tasks/main.yml, performs multiple operations, such as installation and configuration of apache2. # It also adds the service to the startup process - name : install apache2 server apt : name : apache2 state : present - name : update the apache2 server configuration template : src : web.conf.j2 dest : /etc/apache2/sites-available/000-default.conf owner : root group : root mode : 0644 - name : enable apache2 on startup systemd : name : apache2 enabled : yes notify : - start apache2 # notify parameter will trigger the handlers found in roles/web/handlers/main.yml - name : start apache2 systemd : state : started name : apache2 - name : stop apache2 systemd : state : stopped name : apache2 - name : restart apache2 systemd : state : restarted name : apache2 daemon_reload : yes # The template files will be taken from role/web/templates/web.conf.j2, which uses Jinja templating, it also takes values from local variables <VirtualHost *:80><VirtualHost *:80> ServerAdmin {{server_admin_email}} DocumentRoot {{server_document_root}} ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined </VirtualHost> # The local variables file is located in roles/web/vars/main.yml server_admin_email : hodor@localhost.local server_document_root : /var/www/html # File roles/db/tasks/main.yml includes installation of the database server with assigned passwords when prompted. # At the end of the file, we included harden.yml, which executes another set of tasks - name : set mysql root password debconf : name : mysql-server question : mysql-server/root_password value : \"{{ mysql_root_password | quote }}\" vtype : password - name : confirm mysql root password debconf : name : mysql-server question : mysql-server/root_password_again value : \"{{ mysql_root_password | quote }}\" vtype : password - name : install mysqlserver apt : name : \"{{ item }}\" state : present with_items : - mysql-server - mysql-client - include : harden.yml # harden.yml performs hardening of MySQL server configuration - name : deletes anonymous mysql user mysql_user : user : \"\" state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : secures the mysql root user mysql_user : user : root password : \"{{ mysql_root_password }}\" host : \"{{ item }}\" login_password : \"{{mysql_root_password}}\" login_user : root with_items : - 127.0.0.1 - localhost - ::1 - \"{{ ansible_fqdn }}\" - name : removes the mysql test database mysql_db : db : test state : absent login_password : \"{{ mysql_root_password }}\" login_user : root - name : enable mysql on startup systemd : name : mysql enabled : yes notify : - start mysql # db server role also has roles/db/handlers/main.yml and local variables similar to the web role - name : start mysql systemd : state : started name : mysql - name : stop mysql systemd : state : stopped name : mysql - name : restart mysql systemd : state : restarted name : mysql daemon_reload : yes # roles/db/vars/main.yml, which has the mysql_root_password while configuring the server. # we can secure these plaintext passwords using ansible-vault in future. mysql_root_password : R4nd0mP4$$w0rd # we will install PHP and configure it to work with apache2 by restarting the roles/php/tasks/main.yml service - name : install php7 apt : name : \"{{ item }}\" state : present with_items : - php7.0-mysql - php7.0-curl - php7.0-json - php7.0-cgi - php7.0 - libapache2-mod-php7 - name : restart apache2 systemd : state : restarted name : apache2 daemon_reload : yes # Execute the following command against the Ubuntu 16.04 server to set up LAMP stack. # Provide the password when it prompts for system access for user hodor. ansible-playbook -i inventory site.yml","title":"Playbook Files"},{"location":"learning/ansible/security_basics/#setting-up-ansible-tower","text":"# Make sure you have Vagrant installed in your host system before running the following command: vagrant init ansible/tower vagrant up vagrant ssh # It will prompt you to enter IP address, username, and password to login to the Ansible Tower dashboard. - Then navigate the browser to https://10.42.0.42 and accept the SSL error to proceed. - This SSL error can be fixed by providing the valid certificates in the configuration at /etc/tower and need to restart the Ansible Tower service. - Enter the login credentials to access the Ansible Tower dashboard. - Once you log in, it will prompt you for the Ansible Tower license. - Ansible Tower also provides Role-Based Authentication Control (RBAC) , which provides a granular level of control for different users and groups to manage Tower. - Create a new user with the System Administrator privilege - To add inventory into Ansible Tower, we can simply enter it manually - Add credentials (or) keys to the tower by providing them in credential management, which can be reused as well. - Secrets store in Ansible Tower are encrypted with a symmetric key unique to each Ansible Tower cluster. - Once stored in the Ansible Tower database, the credentials may only be used, not viewed, in the web interface. - The types of credentials that Ansible Tower can store are passwords, SSH keys, Ansible Vault keys, and cloud credentials. - Once we have the inventory gathered, we can create jobs to perform the playbook or ad-hoc command operations. - The Ansible Tower REST API is a pretty powerful way to interact with the system - Get started with the pip install ansible-tower-cli command.","title":"Setting up Ansible Tower"},{"location":"learning/ansible/security_basics/#setting-up-awx","text":"Ansible is very powerful, but it does require the user to use the CLI. In some situations, this is not the best option, such as in cases where you need to trigger an Ansible job from another job (where APIs would be better) or in cases where the person that should trigger a job should only be able to trigger that specific job. For these cases, AWX or Ansible Tower are better options to use. The only differences between AWX and Ansible Tower are that AWX is the upstream and open source version, while Ansible Tower is the Red Hat and downstream product that is officially supported but for a price, and also the delivery method. We will use AWX and talk about AWX, but everything we discuss also applies to Ansible Tower. Although there are several ways to install AWX, we are going to use the suggested AWX installation, which is container-based. For this reason, the following software needs to be installed on your machine: Ansible 2.4+. Docker. The docker Python module. The docker-compose Python module. If your system uses Security-Enhanced Linux (SELinux), you also need the libselinux Python module.","title":"Setting up AWX"},{"location":"learning/ansible/security_basics/#installing-awx","text":"We need to clone the AWX Git repository. git clone https://github.com/ansible/awx.git Modify the installer/inventory file by setting sensible values for the passwords and secrets (such as pg_password, rabbitmq_password, admin_password, and secret_key) Now that we have downloaded the Ansible AWX code and installer, we can move into the installer folder and execute the installation by running the following code. cd awx/installer ansible-playbook -i inventory install.yml The install.yml playbook performs the whole installation for us. It starts by checking the environment for possible misconfigurations or missing dependencies. If everything seems to be correct, it moves on to downloading several Docker images (including PostgreSQL, memcached, RabbitMQ, AWX Web, and AWX workers) and then runs them all. As soon as the playbook completes, you can check the installation by issuing the docker ps command. As you can see from docker ps output, our system now has a container called awx_web, which has bound itself to port 80. You can now access AWX by browsing to http://<ip address of your AWX host>/ and using the credentials you specified in the inventory file earlier on and that the default administrator username is admin unless you change it in the inventory.","title":"Installing AWX"},{"location":"learning/ansible/security_basics/#running-your-first-playbook-from-awx","text":"As in Ansible, in AWX, the goal is running an Ansible playbook and each playbook that is run is called a job. Since AWX gives you more flexibility and automation than Ansible, it requires a little bit more configuration before you can run your first job.","title":"Running your first playbook from AWX"},{"location":"learning/ansible/security_basics/#creating-an-awx-project","text":"AWX uses the term project to identify a repository of Ansible playbooks. AWX projects support the placement of playbooks in all major Source Control Management (SCM) systems, such as Git, Mercurial, and SVN, but also support playbooks on the filesystem or playbooks provided by Red Hat Insights. Projects are the system to store and use playbooks in AWX. As you can imagine, there are many interesting additional configurations for AWX projects\u2014and the most interesting one, in my view\u2014is update revision on launch. If flagged, this option instructs Ansible to always update the playbook's repository before running any playbook from that project. This ensures it always executes the latest version of the playbook. This is an important feature to enable as if you don't have it checked, there is the possibility (and sooner or later, this will happen in your environment) that someone notices that there is a problem in a playbook and fixes it, then they run the playbook feeling sure that they are running the latest version. They then forget to run the synchronization task before running the playbook, effectively running the older version of the playbook. This could lead to major problems if the previous version was fairly buggy. The downside of using this option is that every time you execute a playbook, two playbooks are effectively run, adding time to your task execution. I think this is a very small downside and one that does not offset the benefits of using this option.","title":"Creating an AWX project"},{"location":"learning/ansible/security_basics/#creating-an-inventory","text":"As with Ansible Core, to make AWX aware of the machines present in your environment, we use inventories. Inventories, in the AWX world, are not that different from their equivalents in Ansible Core. Since an empty inventory is not useful in any way, we are going to add localhost to it. We then need to add the hostname (localhost) and instruct Ansible to use the local connection by adding the following code to the VARIABLES box. --- ansible_connection : local ansible_python_interpreter : '{{ ansible_playbook_python }}'","title":"Creating an inventory"},{"location":"learning/ansible/security_basics/#creating-a-job-template","text":"A job template in AWX is a collection of the configurations that are needed to perform a job. This is very similar to the ansible-playbook command-line options. The reason why we need to create a job template is so that playbook runs can be launched with little or no user input, meaning they can be delegated to teams who might not know all the details of how a playbook works, or can even be run on a scheduled basis without anyone present. Note that because we are running it using the local connection to localhost, we don't need to create or specify any credentials. However, if you were running a job template against one or more remote hosts, you would need to create a machine credential and associate it with your job template. A machine credential is, for example, an SSH username and password or an SSH username and a private key\u2014these are stored securely in the backend database of AWX, meaning you can again delegate playbook-related tasks to other teams without actually giving them passwords or SSH keys. The first thing we had to choose was whether we are creating a job template or a workflow template. We chose Job Template since we want to be able to create simple jobs out of this template. It's also possible to create more complex jobs, which are the composition of multiple job templates, with flow control features between one job and the next. This allows more complex situations and scenarios where you might want to have multiple jobs (such as the creation of an instance, company customization, the setup of Oracle Database, the setup of a MySQL database, and so on), but you also want to have a one-click deployment that would, for instance, set up the machine, apply all the company customization, and install the MySQL database. Obviously, you might also have another deployment that uses all the same components except the last one and in its place, it uses the Oracle Database piece to create an Oracle Database machine. This allows you to have extreme flexibility and to reuse a lot of components, creating multiple, nearly identical playbooks. It's interesting to note that many fields in the Job Template creation window have an option with the Prompt on launch caption. This is to be able to set this value optionally during the creation of the job template, but also allow the user running the job to enter/override it at runtime. This can be incredibly valuable when you have a field that changes on each run (perhaps the limit field, which operates in the same way as \u2013limit when used with the ansible-playbook command) or can also be used as a sanity check, as it prompts the user with the value (and gives them a chance to modify it) before the playbook is actually run. However, it could potentially block scheduled job runs, so exercise caution when enabling this feature.","title":"Creating a job template"},{"location":"learning/ansible/security_basics/#running-a-job","text":"A job is an instance of a job template. This means that to perform any action on our machine, we have to create a job template instance or, more simply, a job. One of the great things about AWX and Ansible Tower is that they archive this job execution output in the backend database, meaning you can, at any point in the future, come back and query a job run to see what changed and what happened. This is incredibly powerful and useful for occasions such as auditing and policy enforcement.","title":"Running a job"},{"location":"learning/ansible/security_basics/#controlling-access-to-awx","text":"In my opinion, one of the biggest advantages of AWX compared to Ansible is the fact that AWX allows multiple users to connect and control/perform actions. This allows a company to have a single AWX installation for different teams, a whole organization, or even multiple organizations. A Role-Based Access Control (RBAC) system is in place to manage the users' permissions. Both AWX and Ansible Tower can link to central directories, such as Lightweight Directory Access Protocol (LDAP) and Azure Active Directory however, we can also create user accounts locally on the AWX server itself.","title":"Controlling access to AWX"},{"location":"learning/ansible/security_basics/#creating-a-user","text":"One of the big advantages of AWX is the ability to manage multiple users. This allows us to create a user in AWX for each person that is using the AWX system so that we can ensure they are only granted the permissions that they need. Also, by using individual accounts, we can ensure that we can see who carried out what action by using the audit logs. By adding the email address, the username, and the password (with confirmation), you can create the new user. Users can be of three types: A normal user: Users of this type do not have any inherited permissions and they need to be awarded specific permissions to be able to do anything. A system auditor: Users of this type have full read-only privileges on the whole AWX installation. A system administrator: Users of this type have full privileges on the whole AWX installation.","title":"Creating a user"},{"location":"learning/ansible/security_basics/#creating-a-team","text":"Although having individual user accounts is an incredibly powerful tool, especially for enterprise use cases, it would be incredibly inconvenient and cumbersome to have to set permissions for each object (such as a job template or an inventory) on an individual basis. Every time someone joins a team, their user account has to be manually configured with the correct permissions against every object and, similarly, be removed if they leave. AWX and Ansible Tower have the same concept of user grouping that you would find in most other RBAC systems. The only slight difference is that in the user interface, they are referred to as teams, rather than groups. However, you can create teams simply and easily and then add and remove users as you need to. Doing this through the user interface is very straightforward and you will find the process similar to the way that most RBAC systems handle user groups, so we won't go into any more specific details here. Once you have your teams set up, I recommend that you assign your permissions to teams, rather than through individual users, as this will make your management of AWX object permissions much easier as your organization grows.","title":"Creating a team"},{"location":"learning/ansible/security_basics/#creating-an-organization","text":"Sometimes, you have multiple independent groups of people that you need to manage independent machines. For those kinds of scenarios, the use of organizations can help you. An organization is basically a tenant of AWX, with its own unique user accounts, teams, projects, inventories, and job templates\u2014it's almost like having a separate instance of AWX! After you create the organization, you can assign any kind of resource to an organization, such as projects, templates, inventories, users, and so on. Organizations are a simple concept to grasp, but also powerful in terms of segregating roles and responsibilities in AWX.","title":"Creating an organization"},{"location":"learning/ansible/security_basics/#assigning-permissions-in-awx","text":"Individual users (or the teams that they belong to) can be granted permissions on a per-object basis. So, for example, you could have a team of database administrators who only have access to see and execute playbooks on an inventory of database servers, using job templates that are specific to their role. Linux system administrators could then have access to the inventories, projects, and job templates that are specific to their role. AWX hides objects that users don't have the privileges to, which means the database administrators never see the Linux system administrator objects and vice versa. There are a number of different privilege levels that you can award users (or teams) with, which include the following: Admin: This is the organization-level equivalent of a system administrator. Execute: This kind of user can only execute templates that are part of the organization. Project admin: This kind of user can alter any project that is part of the organization. Inventory admin: This kind of user can alter any inventory that is part of the organization. Credential admin: This kind of user can alter any credential that is part of the organization. Workflow admin: This kind of user can alter any workflow that is part of the organization. Notification admin: This kind of user can alter any notification that is part of the organization. Job template admin: This kind of user can alter any job template that is part of the organization. Auditor: This is the organization-level equivalent to a system auditor. Member: This is the organization-level equivalent of a normal user. Read: This kind of user is able to view non-sensible objects that are part of the organization. AWX is a great addition to the power of Ansible in an enterprise setting and really helps ensure that your users can run Ansible playbooks in a manner that is well managed, secure, and auditable.","title":"Assigning permissions in AWX"},{"location":"learning/ansible/security_basics/#setting-up-jenkins","text":"- name : installing jenkins in ubuntu 16.04 hosts : \"192.168.1.7\" remote_user : ubuntu gather_facts : False become : True tasks : - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : install curl and git apt : name={{ item }} state=present update_cache=yes with_items : - curl - git - name : adding jenkins gpg key apt_key : url : https://pkg.jenkins.io/debian/jenkins-ci.org.key state : present - name : jeknins repository to system apt_repository : repo : http://pkg.jenkins.io/debian-stable binary/ state : present - name : installing jenkins apt : name : jenkins state : present update_cache : yes - name : adding jenkins to startup service : name : jenkins state : started enabled : yes - name : printing jenkins default administration password command : cat /var/lib/jenkins/secrets/initialAdminPassword register : jenkins_default_admin_password - debug : msg : \"{{ jenkins_default_admin_password.stdout }}\" # To set up Jenkins, run the following command. Where 192.168.1.7 is the server IP address where Jenkins will be installed: ansible-playbook -i '192.168.1.7,' site.yml --ask-sudo-pass # we have to navigate to the Jenkins dashboard by browsing to http://192.168.1.7:8080 and providing the auto-generated password. # If the playbook runs without any errors, it will display the password at the end of the play. - Create the new user by filling in the details and confirming to log in to the Jenkins console. - We can install custom plugins in Jenkins, navigate to the Manage Jenkins tab, select Manage Plugins, then navigate to the Available tab. - In the Filter: enter the plugin name as Ansible. Then select the checkbox and click Install without restart. - Now we are ready to work with the Ansible plugin for Jenkins. - Create a new project in the main dashboard, give it a name, and select Freestyle project to proceed: - We can configure the build options, this is where Jenkins will give us more flexibility to define our own triggers, build instructions, and post build scripts - Once the build triggers based on an event, this can be sent to some artifact storage, it can also be available in the Jenkins build console output - This is a really very powerful way to perform dynamic operations such as triggering automated server and stacks setup based on a code push to the repository, as well as scheduled scans and automated reporting.","title":"Setting up Jenkins"},{"location":"learning/ansible/security_basics/#security-automation-use-cases","text":"Here is a list of tasks that will prepare you to build layers of automation for the stuff that is important to you: Adding playbooks or connecting your source code management (SCM) tools, such as GitHub/GitLab/BitBucket Authentication and data security Logging output and managing reports for the automation jobs Job scheduling Alerting, notifications, and webhooks","title":"Security automation use cases"},{"location":"learning/ansible/security_basics/#ansible-tower-configuration","text":"To add playbooks into Ansible Tower, we have to start by creating projects, then select the SCM TYPE as Manual We can choose the SCM TYPE set to Git and provide a github.com URL pointing to a playbook Git SCM to add playbooks into projects We can also change the PROJECTS_ROOT under CONFIGURE TOWER to change this location. The added playbooks are executed by creating a job template. Then we can schedule these jobs (or) we can launch directly.","title":"Ansible Tower configuration"},{"location":"learning/ansible/security_basics/#jenkins-ansible-integration-configuration","text":"Jenkins supports SCM to use playbooks and local directories for manual playbooks too. This can be configured with the build options. Jenkins supports both ad-hoc commands and playbooks to trigger as a build (or) post-build action. We can also specify credentials if we want to access private repositories We can add the Playbook path by specifying the location of the playbook and defining inventory and variables as required","title":"Jenkins Ansible integration configuration"},{"location":"learning/ansible/security_basics/#authentication-and--data-security","text":"Some of the security features the tools offer include: RBAC (authentication and authorization) Web application over TLS/SSL (security for data in motion) Encryption for storing secrets (security for data at rest)","title":"Authentication and  data security"},{"location":"learning/ansible/security_basics/#rbac-for-ansible-tower","text":"Ansible Tower supports RBAC to manage multiple users with different permissions and roles. It also supports Lightweight Directory Access Protocol (LDAP) integration in the enterprise version to support Active Directory. This feature allows us to create different levels of users for accessing Ansible Tower. For example: The operations team requires a system administrator role to perform playbook execution and other activities like monitoring The security team requires a system auditor role to perform audit check for compliance standards such as Payment Card Industry Data Security Standard (PCI DSS) or even internal policy validation Normal users, such as team members, might just want to see how things are going, in the form of status updates and failure (or) success of job status","title":"RBAC for Ansible Tower"},{"location":"learning/ansible/security_basics/#tlsssl-for-ansible-tower","text":"By default, Ansible Tower uses HTTPS using self-signed certificates at /etc/tower/tower.cert and /etc/tower/tower.key.","title":"TLS/SSL for Ansible Tower"},{"location":"learning/ansible/security_basics/#encryption-and-data-security-for-ansible-tower","text":"Ansible Tower has been created with built-in security for handling encryption of credentials that includes passwords and keys. It uses Ansible Vault to perform this operation. It encrypts passwords and key information in the database.","title":"Encryption and data security for Ansible Tower"},{"location":"learning/ansible/security_basics/#tlsssl-for-jenkins","text":"By default, Jenkins runs as plain old HTTP. To enable HTTPS, we can use a reverse proxy, such as Nginx, in front of Jenkins to serve as HTTPS. Digital Ocean Reference for TLS","title":"TLS/SSL for Jenkins"},{"location":"learning/ansible/security_basics/#encryption-and-data-security-for-jenkins","text":"We are using Jenkins' default credential feature. This will store the keys and passwords in the local filesystem.","title":"Encryption and data security for Jenkins"},{"location":"learning/ansible/security_basics/#output-of-the-playbooks","text":"We would like to know where can we see the output of the playbooks executing and if any other logs that get created.","title":"Output of the playbooks"},{"location":"learning/ansible/security_basics/#report-management-for-ansible-tower","text":"By default, Ansible Tower itself is a reporting platform for the status of the playbooks, job executions, and inventory collection. The Ansible Tower dashboard gives an overview of the total projects, inventories, hosts, and status of the jobs. The output can be consumed in the dashboard, standard out, or by using the REST API and we can get this via tower-cli command line tool as well, which is just a pre-built command line tool for interfacing with the REST API.","title":"Report management for Ansible Tower"},{"location":"learning/ansible/security_basics/#scheduling-of-jobs","text":"The scheduling of jobs is simple and straightforward in Ansible Tower. For a job, you can specify a schedule and the options are mostly like cron. For example, you can say that you have a daily scan template and would like it to be executed at 4 a.m. every day for the next three months. This kind of schedule makes our meta automation very flexible and powerful.","title":"Scheduling of jobs"},{"location":"learning/ansible/security_basics/#alerting-notifications-and-webhooks","text":"Tower supports multiple ways of alerting and notifying users as per configuration. This can even be configured to make an HTTP POST request to a URL of your choice using a webhook. Ansible Tower notification using slack webhook is a popular option.","title":"Alerting, notifications, and webhooks"},{"location":"learning/ansible/security_basics/#setting-up-a-hardened-wordpress-with-encrypted-automated-backups","text":"Automating our server's patches is the most obvious, and possibly popular, requirement. We will apply security automation techniques and approaches to set up a hardened WordPress and enable encrypted backups. Everyone would agree that setting up a secure website and keeping it secured is a fairly common security requirement. And since it is so common, it would be useful for a lot of people who are tasked with building and managing websites to stay secure to look at that specific scenario. Note Are you aware that, according to Wikipedia, 27.5% of the top 10 million websites use WordPress? According to another statistic, 58.7% of all websites with known software on the entire web run WordPress. For us, setting up a hardened WordPress with encrypted automated backups can be broken down into the following steps: Setting up a Linux/Windows server with security measures in place. Setting up a web server (Apache/Nginx on Linux and IIS on Windows). Setting up a database server (MySQL) on the same host. Setting up WordPress using a command-line utility called WP-CLI. Setting up backup for the site files and the database which is incremental, encrypted, and most importantly, automated. Note We will assume that the server that we plan to deploy our WordPress website on is already up and running and we are able to connect to it. We will store the backup in an already configured AWS S3 bucket, for which the access key and secret access key is already provisioned. Refer the examples in Introduction to set this up.","title":"Setting Up a Hardened WordPress with Encrypted Automated Backups"},{"location":"learning/ansible/security_basics/#secure-automated-the-wordpress-updates","text":"Run the backups and update WordPress core, themes, and plugins. This can be scheduled via an Ansible Tower job for every day - name : running backup using duply command : /etc/cron.hourly/duply-backup - name : updating WordPress core command : wp core update register : wp_core_update_output ignore_errors : yes - name : wp core update output debug : msg : \"{{ wp_core_update_output.stdout }}\" - name : updating WordPress themes command : wp theme update --all register : wp_theme_update_output ignore_errors : yes - name : wp themes update output debug : msg : \"{{ wp_theme_update_output.stdout }}\" - name : updating WordPress plugins command : wp plugin update --all register : wp_plugin_update_output ignore_errors : yes - name : wp plugins update output debug : msg : \"{{ wp_plugin_update_output.stdout }}\"","title":"Secure automated the WordPress updates"},{"location":"learning/ansible/security_basics/#scheduling-via-ansible-tower-for-daily-updates","text":"Ansible Tower job scheduling for automated WordPress updates OR We can use the cron job template to perform this daily and add this template while deploying the WordPress setup #!/bin/bash /etc/cron.hourly/duply-backup wp core update wp theme update --all wp plugin update --all","title":"Scheduling via Ansible Tower for daily updates"},{"location":"learning/ansible/security_basics/#setting-up-apache2-web-server","text":"Shows how we can use templating to perform configuration updates in the server - name : installing apache2 server apt : name : \"apache2\" update_cache : yes state : present - name : updating customized templates for apache2 configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" mode : 0644 with_tems : - { src : apache2.conf.j2 , dst : /etc/apache2/conf.d/apache2.conf } - { src : 000-default.conf.j2 , dst : /etc/apache2/sites-available/000-default.conf } - { src : default-ssl.conf.j2 , dst : /etc/apache2/sites-available/default-ssl.conf } - name : adding custom link for sites-enabled from sites-available file : src : \"{{ item.src }}\" dest : \"{{ item.dest }}\" state : link with_items : - { src : '/etc/apache2/sites-available/000-default.conf' , dest : '/etc/apache2/sites-enabled/000-default.conf' } - { src : '/etc/apache2/sites-available/default-ssl.conf' , dest : '/etc/apache2/sites-enabled/default-ssl.conf' } notify : - start apache2 - startup apache2","title":"Setting up Apache2 web server"},{"location":"learning/ansible/security_basics/#enabling-tlsssl-with-lets-encrypt","text":"We can use a command-line tool offered by Let's Encrypt to get free SSL/TLS certificates in an open, automated manner. The tool is capable of reading and understanding an nginx virtual host file and generating the relevant certificates completely automatically, without any kind of manual intervention. - name : adding certbot ppa apt_repository : repo : \"ppa:certbot/certbot\" - name : install certbot apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-certbot-nginx - name : check if we have generated a cert already stat : path : \"/etc/letsencrypt/live/{{ website_domain_name }}/fullchain.pem\" register : cert_stats - name : run certbot to generate the certificates shell : \"certbot certonly --standalone -d {{ website_domain_name }} --email {{ service_admin_email }} --non-interactive --agree-tos\" when : cert_stats.stat.exists == False - name : configuring site files template : src : website.conf dest : \"/etc/nginx/sites-available/{{ website_domain_name }}\" - name : restart nginx service : name : nginx state : restarted","title":"Enabling TLS/SSL with Let's Encrypt"},{"location":"learning/ansible/security_basics/#log-monitoring","text":"Log monitoring is the perfect place to think about security automation. For monitoring to be effective, a few things need to happen. We should be able to move logs from different devices to a central location. We should be able to make sense of what a regular log entry is and what could possibly be an attack. We should be able to store the logs, and also operate on them for things such as aggregation, normalization, and eventually, analysis. Traditional logging systems find it difficult to log for all applications, systems, and devices. The variety of time formats, log output formats, and so on, makes the task pretty complicated. The biggest roadblock is finding a way to be able to centralize logs. This gets in the way of being able to process log entries in real time, or near real time effectively. Some of the problematic points are as follows: Access is often difficult High expertise in mined data is required Logs can be difficult to find Log data is immense in size","title":"Log Monitoring"},{"location":"learning/ansible/security_basics/#introduction-to-elastic-stack","text":"Elastic Stack is a group of open source products from the Elastic company. It takes data from any type of source and in any format and searches, analyzes, and visualizes that data in real time. It consists of four major components, as follows: Elasticsearch Logstash Kibana Beats It helps users/admins to collect, analyze, and visualize data in (near) real time. Each module fits based on your use case and environment.","title":"Introduction to Elastic Stack"},{"location":"learning/ansible/security_basics/#elasticsearch","text":"Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected Main plus points of Elastic Stack: Distributed and highly available search engine, written in Java, and uses Groovy Built on top of Lucene Multi-tenant, with multi types and a set of APIs Document-oriented, providing (near) real-time search","title":"Elasticsearch"},{"location":"learning/ansible/security_basics/#logstash","text":"Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources, simultaneously transforms it, and then sends it to your favorite stash. Centralized data processing of all types of logs Consists of the following three main components: Input: Passing logs to process them into machine-understandable format Filter: A set of conditions to perform a specific action on an event Output: The decision maker for processed events/logs","title":"Logstash"},{"location":"learning/ansible/security_basics/#kibana","text":"Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack, so you can do anything from learning why you're getting paged at 2:00 a.m. to understanding the impact rain might have on your quarterly numbers. Kibana's list of features: Powerful frontend dashboard is written in JavaScript Browser-based analytics and search dashboard for Elasticsearch A flexible analytics and visualization platform Provides data in the form of charts, graphs, counts, maps, and so on, in real time","title":"Kibana"},{"location":"learning/ansible/security_basics/#beats","text":"Beats is the platform for single-purpose data shippers. They install as lightweight agents and send data from hundreds or thousands of machines to Logstash or Elasticsearch. Beats are: Lightweight shippers for Elasticsearch and Logstash Capture all sorts of operational data, like logs or network packet data They can send logs to either Elasticsearch or Logstash","title":"Beats"},{"location":"learning/ansible/security_basics/#elastalert","text":"ElastAlert is a Python tool which also bundles with the different types of integrations to support with alerting and notifications. Some of them include Command, Email, JIRA, OpsGenie, AWS SNS, HipChat, Slack, Telegram, and so on. It also provides a modular approach to creating our own integrations.","title":"ElastAlert"},{"location":"learning/ansible/security_basics/#why-should-we-use-elastic-stack-for-security-monitoring-and-alerting","text":"The Elastic Stack solves most of the problems that we have discussed before, such as: Ability to store large amounts of data Ability to understand and read a variety of log formats Ability to ship the log information from a variety of devices in near real time to one central location A visualization dashboard for log analysis","title":"Why should we use Elastic Stack for security monitoring and alerting?"},{"location":"learning/ansible/security_basics/#prerequisites-for-setting-up-elastic-stack","text":"- name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : accepting oracle java license agreement debconf : name : 'oracle-java8-installer' question : 'shared/accepted-oracle-license-v1-1' value : 'true' vtype : 'select' - name : adding ppa repo for oracle java by webupd8team apt_repository : repo : 'ppa:webupd8team/java' state : present update_cache : yes - name : installing java nginx apache2-utils and git apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-software-properties - oracle-java8-installer - nginx - apache2-utils - python-pip - python-passlib","title":"Prerequisites for setting up Elastic Stack"},{"location":"learning/ansible/security_basics/#setting-up-the-elastic-stack","text":"The stack is a combination of: The Elasticsearch service The Logstash service The Kibana service The Beats service on all the devices We are going to set up Elasticsearch, Logstash, and Kibana on a single machine. This is the main log collection machine: It requires a minimum of 4 GB RAM, as we are using a single machine to serve three services (Elasticsearch, Logstash, and Kibana) It requires a minimum of 20 GB disk space, and, based on your log size, you can add the disk space","title":"Setting up the Elastic Stack"},{"location":"learning/ansible/security_basics/#installing-elasticsearch","text":"Install Elasticsearch from the repository with gpg key and add it to the startup programs - name : adding elastic gpg key for elasticsearch apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing elasticsearch apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - elasticsearch - name : adding elasticsearch to the startup programs service : name : elasticsearch enabled : yes notify : - start elasticsearch Configure the Elasticsearch cluster with the required settings. Also, set up the JVM options for the Elasticsearch cluster. Also, create a backup directory for Elasticsearch cluster backups and snapshots - name : creating elasticsearch backup repo directory at {{ elasticsearch_backups_repo_path }} file : path : \"{{ elasticsearch_backups_repo_path }}\" state : directory mode : 0755 owner : elasticsearch group : elasticsearch - name : configuring elasticsearch.yml file template : src : \"{{ item.src }}\" dest : /etc/elasticsearch/\"{{ item.dst }}\" with_items : - { src : 'elasticsearch.yml.j2' , dst : 'elasticsearch.yml' } - { src : 'jvm.options.j2' , dst : 'jvm.options' } notify : - restart elasticsearch The notify part will trigger the restart elasticsearch handler and the handler file will look as follows. - name : start elasticsearch service : name : elasticsearch state : started - name : restart elasticsearch service : name : elasticsearch state : restarted","title":"Installing Elasticsearch"},{"location":"learning/ansible/security_basics/#installing-logstash","text":"Install Logstash from the repository with gpg key and add it to the startup programs - name : adding elastic gpg key for logstash apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing logstash apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - logstash - name : adding logstash to the startup programs service : name : logstash enabled : yes notify : - start logstash Configure the Logstash service with input, output, and filter settings. This enables receiving logs, processing logs, and sending logs to the Elasticsearch cluster - name : logstash configuration files template : src : \"{{ item.src }}\" dest : /etc/logstash/conf.d/\"{{ item.dst }}\" with_items : - { src : '02-beats-input.conf.j2' , dst : '02-beats-input.conf' } - { src : '10-sshlog-filter.conf.j2' , dst : '10-sshlog-filter.conf' } - { src : '11-weblog-filter.conf.j2' , dst : '11-weblog-filter.conf' } - { src : '30-elasticsearch-output.conf.j2' , dst : '10-elasticsearch-output.conf' } notify : - restart logstash","title":"Installing Logstash"},{"location":"learning/ansible/security_basics/#logstash-configuration","text":"To receive logs from different systems, we use the Beats service from Elastic. The following configuration is to receive logs from different servers to the Logstash server. Logstash runs on port 5044 and we can use SSL certificates to ensure logs are transferred via an encrypted channel. # 02-beats-input.conf.j2 input { beats { port => 5044 ssl => true ssl_certificate => \"/etc/pki/tls/certs/logstash-forwarder.crt\" ssl_key => \"/etc/pki/tls/private/logstash-forwarder.key\" } } The following configuration is to parse the system SSH service logs (auth.log) using grok filters. It also applies filters like geoip, while providing additional information like country, location, longitude, latitude, and so on. #10-sshlog-filter.conf.j2 filter { if [type] == \"sshlog\" { grok { match => [ \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : message repeated 2 times : \\[ %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } password for invalid user %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\", \"message\", \"%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\\[%{POSINT}\\])? : % { WORD : login } % { WORD : auth_method } for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}\" ] } date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] locale => en } geoip { source => \"ip\" } } } The following configuration is to parse web server logs (nginx, apache2). We will also apply filters for geoip and useragent. The useragent filter allows us to get information about the agent, OS type, version information, and so on. #11-weblog-filter.conf.j2 filter { if [type] == \"weblog\" { grok { match => { \"message\" => '%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}' } } date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] locale => en } geoip { source => \"clientip\" } useragent { source => \"agent\" target => \"useragent\" } } } The following configuration will send the log output into the Elasticsearch cluster with daily index formats. #30-elasticsearch-output.conf.j2 output { elasticsearch { hosts => [\"localhost:9200\"] manage_template => false index => \"%{[@metadata][beat]}-%{+YYYY.MM.dd}\" document_type => \"%{[@metadata][type]}\" } }","title":"Logstash configuration"},{"location":"learning/ansible/security_basics/#installing-kibana","text":"By default we are not making any changes in Kibana, as it works out of the box with Elasticsearch. - name : adding elastic gpg key for kibana apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing kibana apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - kibana - name : adding kibana to the startup programs service : name : kibana enabled : yes notify : - start kibana By default Kibana doesn't have any authentication, X-Pack is the commercial plug-in by Elastic for RBAC (role-based access control) with security. Also, some open source options include https://readonlyrest.com/ and Search Guard ( https://floragunn.com ) to interact with Elasticsearch. Using TLS/SSL and custom authentication and aauthorization is highly recommended. Some of the open source options includes Oauth2 Proxy ( bitly/oauth2_proxy ) and Auth0, and so on.","title":"Installing Kibana"},{"location":"learning/ansible/security_basics/#setting-up-nginx-reverse-proxy","text":"The following configuration is to enable basic authentication for Kibana using nginx reverse proxy. server { listen 80; server_name localhost; auth_basic \"Restricted Access\"; auth_basic_user_file /etc/nginx/htpasswd.users; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } Setting up and configuring the nginx service looks as follows. #command: htpasswd -c /etc/nginx/htpasswd.users - name : htpasswd generation htpasswd : path : \"/etc/nginx/htpasswd.users\" name : \"{{ basic_auth_username }}\" password : \"{{ basic_auth_password }}\" owner : root group : root mode : 0644 - name : nginx virtualhost configuration template : src : \"templates/nginxdefault.j2\" dest : \"/etc/nginx/sites-available/default\" notify : - restart nginx","title":"Setting up nginx reverse proxy"},{"location":"learning/ansible/security_basics/#installing-beats-to-send-logs-to-elastic-stack","text":"We are going to install Filebeat to send SSH and web server logs to the Elastic Stack: - name : adding elastic gpg key for filebeat apt_key : url : \"https://artifacts.elastic.co/GPG-KEY-elasticsearch\" state : present - name : adding the elastic repository apt_repository : repo : \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" state : present - name : installing filebeat apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - apt-transport-https - filebeat - name : adding filebeat to the startup programs service : name : filebeat enabled : yes notify : - start filebeat Configure the Filebeat to send both SSH and web server logs to Elastic Stack, to process and index in near real-time. filebeat : prospectors : - paths : - /var/log/auth.log # - /var/log/syslog # - /var/log/*.log document_type : sshlog - paths : - /var/log/nginx/access.log document_type : weblog registry_file : /var/lib/filebeat/registry output : logstash : hosts : [ \"{{ logstash_server_ip }}:5044\" ] bulk_max_size : 1024 ssl : certificate_authorities : [ \"/etc/pki/tls/certs/logstash-forwarder.crt\" ] logging : files : rotateeverybytes : 10485760 # = 10MB","title":"Installing Beats to send logs to Elastic Stack"},{"location":"learning/ansible/security_basics/#elastalert-for-alerting","text":"First, we need to install the prerequisites for setting up ElastAlert. - name : installing pre requisuites for elastalert apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-pip - python-dev - libffi-dev - libssl-dev - python-setuptools - build-essential - name : installing elastalert pip : name : elastalert - name : creating elastalert directories file : path : \"{{ item }}\" state : directory mode : 0755 with_items : - /opt/elastalert/rules - /opt/elastalert/config - name : creating elastalert configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : 'elastalert-config.j2' , dst : '/opt/elastalert/config/config.yml' } - { src : 'elastalert-service.j2' , dst : '/lib/systemd/system/elastalert.service' } - { src : 'elastalert-sshrule.j2' , dst : '/opt/elastalert/rules/ssh-bruteforce.yml' } - name : enable elastalert service service : name : elastalert state : started enabled : yes Creating a simple startup script so that ElastAlert will be used as a system service. [ Unit ] Description = elastalert After = multi-user.target [ Service ] Type = simple WorkingDirectory = /opt/elastalert ExecStart = /usr/local/bin/elastalert --config /opt/elastalert/config/config.yml [ Install ] WantedBy = multi-user.target","title":"ElastAlert for alerting"},{"location":"learning/ansible/security_basics/#configuring-the-lets-encrypt-service","text":"- name : adding certbot ppa apt_repository : repo : \"ppa:certbot/certbot\" - name : install certbot apt : name : \"{{ item }}\" update_cache : yes state : present with_items : - python-certbot-nginx - name : check if we have generated a cert already stat : path : \"/etc/letsencrypt/live/{{ website_domain_name }}/fullchain.pem\" register : cert_stats - name : run certbot to generate the certificates shell : \"certbot certonly --standalone -d {{ website_domain_name }} --email {{ service_admin_email }} --non-interactive --agree-tos\" when : cert_stats.stat.exists == False - name : configuring site files template : src : website.conf dest : \"/etc/nginx/sites-available/{{ website_domain_name }}\" - name : restart nginx service : name : nginx state : restarted","title":"Configuring the Let's Encrypt service"},{"location":"learning/ansible/security_basics/#elastalert-rule-configuration","text":"Assuming that you already have Elastic Stack installed and logging SSH logs, use the following ElastAlert rule to trigger SSH attack IP blacklisting. es_host : localhost es_port : 9200 name : \"SSH Bruteforce attack alert\" type : frequency index : filebeat-* num_events : 20 timeframe : minutes : 1 # For more info: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.html filter : - query : query_string : query : '_type:sshlog AND login:failed AND (username: \"ubuntu\" OR username: \"root\")' alert : - slack : slack_webhook_url : \"https://hooks.slack.com/services/xxxxx\" slack_username_override : \"attack-bot\" slack_emoji_override : \"robot_face\" - command : [ \"/usr/bin/curl\" , \"https://xxxxxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/zzzzzzzzzzzzzz/ip/inframonitor/%(ip)s\" ] realert : minutes : 0 For more references, visit https://elastalert.readthedocs.io/en/latest/running_elastalert.html .","title":"ElastAlert rule configuration"},{"location":"learning/ansible/security_basics/#serverless-automated-defense","text":"If we can get a notification for an attack, we can set up and do the following: Call an AWS Lambda function Send the attacker's IP address information to this AWS Lambda function endpoint Use the code deployed in the Lambda function to call the VPC network access list API and block the attacker's IP address To ensure that we don't fill up the ACLs with attacker IPs, we can combine this approach with AWS DynamoDB to store this information for a short duration and remove it from the block list. As soon as an attack is detected, the alerter sends the IP to the blacklist lambda endpoint via an HTTPS request. The IP is blocked using the network ACL and the record of it is maintained in DynamoDB. If the IP is currently blocked already, then the expiry time for the rule will be extended in the DynamoDB. An expiry handler function is periodically triggered, which removes expired rules from DynamoDB and ACL accordingly.","title":"Serverless Automated Defense"},{"location":"learning/ansible/security_basics/#setup","text":"The setup involves the following steps: Obtain IAM credentials Create a table in DynamoDB Configure the lambda function based on requirement Deploy code to AWS Lambda Configure Cloudwatch to periodic invocation The entire setup is automated, except for obtaining the IAM credentials and configuring the function based on requirements.","title":"Setup"},{"location":"learning/ansible/security_basics/#configuration","text":"The following parameters are configurable before deployment: region: AWS region to deploy in. This needs to be the same as the region where the VPC network resides. accessToken: The accessToken that will be used to authenticate the requests to the blacklist endpoint. aclLimit: The maximum number of rules an ACL can handle. The maximum limit in AWS is 20 by default. ruleStartId: The starting ID for rules in the ACL. aclID: The ACL ID of the network where the rules will be applied. tableName: The unique table name in DynamoDB, created for each VPC to be defended. ruleValidity: The duration for which a rule is valid, after which the IP will be unblocked. // Configure the following in the config.js file module . exports = { region : \"us-east-1\" , // AWS Region to deploy in accessToken : \"YOUR_R4NDOM_S3CR3T_ACCESS_TOKEN_GOES_HERE\" , // Accesstoken to make requests to blacklist aclLimit : 20 , // Maximum number of acl rules ruleStartId : 10 , // Starting id for acl entries aclId : \"YOUR_ACL_ID\" , // AclId that you want to be managed tableName : \"blacklist_ip\" , // DynamoDB table that will be created ruleValidity : 5 // Validity of Blacklist rule in minutes } Make sure to modify at least the aclId, accessToken, and region based on your setup. To modify the lambda deployment configuration use the serverless.yml file ... functions : blacklist : handler : handler.blacklistip events : - http : path : blacklistip method : get handleexpiry : handler : handler.handleexpiry events : - schedule : rate(1 minute) ... For example, the rate at which the expiry function is triggered and the endpoint URL for the blacklist function can be modified using the YML file. But the defaults are already optimal. # The playbook looks as follows: - name : installing node run time and npm apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - nodejs - npm - name : installing serverless package npm : name : \"{{ item }}\" global : yes state : present with_items : - serverless - aws-sdk - name : copy the setup files template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : 'config.js.j2' , dst : '/opt/serverless/config.js' } - { src : 'handler.js.j2' , dst : '/opt/serverless/handler.js' } - { src : 'iamRoleStatements.json.j2' , dst : '/opt/serverless/iamRoleStatements.json' } - { src : 'initDb.js.j2' , dst : '/opt/serverless/initDb.js' } - { src : 'serverless.yml.j2' , dst : '/opt/serverless/serverless.yml' } - { src : 'aws-credentials.j2' , dst : '~/.aws/credentials' } - name : create dynamo db table command : node initDb.js args : chdir : /opt/serverless/ - name : deploy the serverless command : serverless deploy args : chdir : /opt/serverless/ The current setup for AWS Lambda is to block the IP address against network ACL. This can be reused with other API endpoints, like a firewall dynamic block list and other security devices. The blacklist endpoint is responsible for blocking an IP address. The URL looks like the following: https://lambda_url/blacklistipaccessToken=ACCESS_TOKEN&ip=IP_ADDRESS The query parameters are as follows: IP_ADDRESS: This is the IP address to be blocked ACCESS_TOKEN: The accessToken to authenticate the request","title":"Configuration"},{"location":"learning/ansible/security_basics/#automated-defense-lambda-in-action","text":"When the ElastAlert detects an SSH brute force attack, it will trigger a request to lambda endpoint by providing the attacker's IP address. Then our automated defense platform will trigger a network ACL blocklist rule. This can be configurable to say for how much time it should be blocked.","title":"Automated defense lambda in action"},{"location":"learning/ansible/security_hardening/","text":"Security Hardening for Applications and Networks \u00b6 Security hardening is the most obvious task for any security-conscious endeavor. By doing the effort of securing systems, applications, and networks, one can achieve multiple security goals given as follows: Ensuring that applications and networks are not compromised (sometimes) Making it difficult for compromises to stay hidden for long Securing by default ensures that compromises in one part of the network don't propagate further and more We will build playbooks that will allow us to do the following things: Secure our master images so that as soon as the applications and systems are part of the network, they offer decent security Execute audit processes so that we can verify and measure periodically if the applications, systems, and networks are in line with the security policies that are required by the organization Security hardening with benchmarks such as Center for Internet Security (CIS), Security Technical Implementation Guides (STIG), and National Institute of Standards and Technology (NIST) Automating security audit checks for networking devices using Ansible Automating security audit checks for applications using Ansible Automated patching approaches using Ansible Security hardening with benchmarks such as CIS, STIGs, and NIST \u00b6 Benchmarks provide a great way for anyone to gain assurance of their individual security efforts. Hardening for security mostly boils down to do the following: Agreeing on what is the minimal set of configuration that qualifies as secure configuration. This is usually defined as a hardening benchmark or framework. Making changes to all the aspects of the system that are touched by such configuration. Measuring periodically if the application and system are still in line with the configuration or if there is any deviation. If any deviation is found, take corrective action to fix that. If no deviation is found, log that. Since software is always getting upgraded, staying on top of the latest configuration guidelines and benchmarks is most important. Operating system hardening for baseline using an Ansible playbook \u00b6 We will see how we can use existing playbooks from the community (Ansible Galaxy). The following playbook provides multiple security configurations, standards, and ways to protect operating system against different attacks and security vulnerabilities. Some of the tasks it will perform include the following: Configures package management, for example, allows only signed packages Remove packages with known issues Configures pam and pam_limits modules Shadow password suite configuration Configures system path permissions Disable core dumps via soft limits Restrict root logins to system console Set SUIDs Configures kernel parameters via sysctl Downloading and executing Ansible playbooks from galaxy is as simple as follows: ansible-galaxy install dev-sec.os-hardening - hosts : localhost become : yes roles : - dev-sec.os-hardening The preceding playbook will detect the operating system and perform hardening steps based on the different guidelines. This can be configured as required by updating the default variables values. Refer to dev-sec/ansible-collection-hardening for more details about the playbook. STIGs Ansible role for automated security hardening for Linux hosts \u00b6 OpenStack has an awesome project named ansible-hardening , which applies the security configuration changes as per the STIGs standards. It performs security hardening for the following domains: accounts: User account security controls aide: Advanced Intrusion Detection Environment auditd: Audit daemon auth: Authentication file_perms: Filesystem permissions graphical: Graphical login security controls kernel: Kernel parameters lsm: Linux Security Modules misc: Miscellaneous security controls packages: Package managers sshd: SSH daemon Download the role from the GitHub repository itself using ansible-galaxy as follows: ansible-galaxy install git+https://github.com/openstack/ansible-hardening - name : STIGs ansible-hardening for automated security hardening hosts : servers become : yes remote_user : \"{{ remote_user_name }}\" vars : remote_user_name : vagrant security_ntp_servers : - time.nist.gov - time.google.com roles : - ansible-hardening Continuous security scans and reports for OpenSCAP using Ansible Tower \u00b6 OpenSCAP is a set of security tools, policies, and standards to perform security compliance checks against the systems by following SCAP. SCAP is the U.S. standard maintained by NIST. OpenSCAP follows these steps to perform scanning on your system: Install SCAP Workbench or OpenSCAP Base (for more information, visit https://www.open-scap.org ) Choose a policy Adjust your settings Evaluate the system Check playbook reference at https://medium.com/@jackprice/ansible-openscap-for-compliance-automation-14200fe70663 . - hosts : all become : yes vars : oscap_profile : xccdf_org.ssgproject.content_profile_pci-dss oscap_policy : ssg-rhel7-ds tasks : - name : install openscap scanner package : name : \"{{ item }}\" state : latest with_items : - openscap-scanner - scap-security-guide - block : - name : run openscap command : > oscap xccdf eval --profile {{ oscap_profile }} --results-arf /tmp/oscap-arf.xml --report /tmp/oscap-report.html --fetch-remote-resources /usr/share/xml/scap/ssg/content/{{ oscap_policy }}.xml always : - name : download report fetch : src : /tmp/oscap-report.html dest : ./{{ inventory_hostname }}.html flat : yes We can use this playbook to perform continuously automated checks using Ansible Tower First, we need to create a directory in Ansible Tower server in order to store this playbook with the awx user permission to add the custom playbook. Create a new project in Ansible Tower to perform the OpenSCAP setup and scan against the checks. Then, we have to create a new job to execute the playbook. Here, we can include the list of hosts, credentials for login, and other details required to perform the execution. This audit can be scheduled to perform frequently. We can also launch this job on demand when required. The output of the playbook will generate the OpenSCAP report, and it will be fetched to Ansible Tower. We can access this playbook at the /tmp/ location. Also, we can send this report to the other centralized reporting server if required. We can also set up notifications based on playbook execution results. By doing this, we can send this notifications to respective channels, such as email, slack, and message. CIS Benchmarks \u00b6 CIS has benchmarks for different type OS, software, and services. The following are some high-level categories: Desktops and web browsers Mobile devices Network devices Security metrics Servers \u2013 operating systems Servers \u2013 other Virtualization platforms, cloud, and other Ubuntu CIS Benchmarks (server level) \u00b6 CIS Benchmarks Ubuntu provides prescriptive guidance to establish a secure configuration posture for Ubuntu Linux systems running on x86 and x64 platforms. This benchmark is intended for system and application administrators, security specialists, auditors, help desk, and platform deployment personnel who plan to develop, deploy, assess, or secure solutions that incorporate Linux platform. Here are the high-level six domains that are part of CIS Ubuntu 16.04 LTS benchmarks: Initial setup: Filesystem configuration Configure software updates Filesystem integrity checking Secure boot settings Additional process hardening Mandatory access control Warning banners Services: Inted Services Special purpose services Service clients Network configuration: Network parameters (host only) Network parameters (host and router) IPv6 TCP wrappers Uncommon network protocols Logging and auditing: Configure system accounting (auditd) Configure logging Access, authentication, and authorization: Configure cron SSH server configuration Configure PAM User accounts and environment System maintenance: System file permissions User and group settings # Playbooks git clone https://github.com/oguya/cis-ubuntu-14-ansible.git cd cis-ubuntu-14-ansible Then, update the variables and inventory and execute the playbook using the following command. The variables are not required mostly, as this performs against different CIS checks unless, if we wanted to customize the benchmarks as per the organization. ansible-playbook -i inventory cis.yml The preceding playbook will execute the CIS security benchmark against an Ubuntu server and performs all the checks listed in the CIS guidelines. AWS benchmarks (cloud provider level) \u00b6 AWS CIS Benchmarks provides prescriptive guidance to configure security options for a subset of AWS with an emphasis on foundational, testable, and architecture agnostic settings. Here are the high-level domains, which are part of AWS CIS Benchmarks: Identity and access management Logging Monitoring Networking Extra Currently, there is a tool named prowler ( Alfresco/prowler ) based on AWS-CLI commands for AWS account security assessment and hardening. Before running the playbook, we have to provide AWS API keys to perform security audit. This can be created using IAM role in AWS service. If you have an already existing account with required privileges, these steps can be skipped. Create a new user in your AWS account with programmatic access. Apply the SecurityAudit policy for the user from existing policies in IAM console. Create the new user by following the steps. Make sure that you safely save the Access key ID and Secret access key for later use. The following playbook assume that you already have installed python and pip in your local system. - name : AWS CIS Benchmarks playbook hosts : localhost become : yes vars : aws_access_key : XXXXXXXX aws_secret_key : XXXXXXXX tasks : - name : installing aws cli and ansi2html pip : name : \"{{ item }}\" with_items : - awscli - ansi2html - name : downloading and setting up prowler get_url : url : https://raw.githubusercontent.com/Alfresco/prowler/master/prowler dest : /usr/bin/prowler mode : 0755 - name : running prowler full scan shell : \"prowler | ansi2html -la > ./aws-cis-report-{{ ansible_date_time.epoch }}.html\" environment : AWS_ACCESS_KEY_ID : \"{{ aws_access_key }}\" AWS_SECRET_ACCESS_KEY : \"{{ aws_secret_key }}\" - name : AWS CIS Benchmarks report downloaded debug : msg : \"Report can be found at ./aws-cis-report-{{ ansible_date_time.epoch }}.html\" The playbook will trigger the setup and security audit scan for AWS CIS Benchmarks using the prowler tool. Prowler-generated HTML report can be downloaded in different formats as required and also scanning checks can be configured as required. More reference about the tool can be found at Alfresco/prowler . Automating security audit checks for networking devices using Ansible \u00b6 We can use this to do security audit checks for networking devices. Nmap scanning and NSE \u00b6 Network Mapper (Nmap) is a free open source software to perform network discovery, scanning, audit, and many others. It has a various amount of features such as OS detection, system fingerprinting, firewall detection, and many other features. Nmap Scripting Engine (Nmap NSE) provides advanced capabilities like scanning for particular vulnerabilities and attacks. We can also write and extend Nmap using our own custom script. Nmap is a swiss army knife for pen testers (security testers) and network security teams. - name : Basic NMAP Scan Playbook hosts : localhost gather_facts : false vars : top_ports : 1000 network_hosts : - 192.168.1.1 - scanme.nmap.org - 127.0.0.1 - 192.168.11.0/24 tasks : - name : check if nmap installed and install apt : name : nmap update_cache : yes state : present become : yes - name : top ports scan shell : \"nmap --top-ports {{ top_ports }} -Pn -oA nmap-scan-%Y-%m-%d {{ network_hosts|join(' ') }}\" {{ network_hosts|join(' ') }} is a Jinja2 feature named filter arguments to parse the given network_hosts by space delimited network_hosts variable holds the list of IPs, network range (CIDR), hosts, and so on to perform scan using Nmap top_ports is the number that is ranging from 0 to 65535. Nmap by default picks commonly opened top ports -Pn specifies that scans the host if ping (ICMP) doesn't work also -oA gets the output in all formats, which includes gnmap (greppable format), Nmap, and XML Nmap NSE scanning playbook \u00b6 This playbook will perform enumeration of directories used by popular web applications and servers using http-enum and finds options that are supported by an HTTP server using http-methods using Nmap scripts. - name : Advanced NMAP Scan using NSE hosts : localhost vars : ports : - 80 - 443 scan_host : scanme.nmap.org tasks : - name : Running Nmap NSE scan shell : \"nmap -Pn -p {{ ports|join(',') }} --script {{ item }} -oA nmap-{{ item }}-results-%Y-%m-%d {{ scan_host }}\" with_items : - http-methods - http-enum The http-enum script runs additional tests against network ports where web servers are detected. We can see that two folders were discovered by the script and additionally all HTTP methods that are supported got enumerated as well. Automation security audit checks for applications using Ansible \u00b6 Modern applications can get pretty complex fairly quickly. Having the ability to run automation to do security tasks is almost a mandatory requirement. The different types of application security scanning we can do can range from the following: Run CI/CD scanning against the source code (for example, RIPS and brakeman). Dependency checking scanners (for example, OWASP dependency checker and snyk.io ( https://snyk.io/ )). Once deployed then run the web application scanner (for example, Nikto, Arachni, and w3af). Framework-specific security scanners (for example, WPScan and Droopscan) and many other. Source code analysis scanners \u00b6 This is one of the first and common way to minimize the security risk while applications going to production. Source code analysis scanner also known as Static Application Security Testing (SAST) will help to find security issues by analyzing the source code of the application. Source code analysis is kind of white box testing and looking through code. This kind of testing methodology may not find 100% coverage of security vulnerabilities, and it requires manual testing as well. For example, finding logical vulnerabilities requires some kind of user interactions such as dynamic functionalities. For example, if you are scanning PHP code, then RIPS ; if it's Ruby on Rails code, then it's Brakeman ; and if it's python, then Bandit Dependency-checking scanners \u00b6 Most of the developers use third-party libraries while developing applications, and it's very common to see using open source plugins and modules inside their code. So dependency checks will allow us to find using components with known vulnerabilities (OWASP A9) issues in application code by scanning the libraries against the CVE and NIST vulnerability database. There are multiple projects out there in the market for performing these checks, and some of them includes the following: OWASP Dependency-Check Snyk.io ( https://snyk.io/ ) Retire.js [:] SourceClear and many other Running web application security scanners \u00b6 This is the phase where the application went live to QA, stage, (or) Production. Then, we wanted to perform security scans like an attacker (black box view). At this stage, an application will have all the dynamic functionalities and server configurations applied. These scanner results tell us how good the server configured and any other application security issues before releasing the replica copy into the production. There are many tools in the market to do these jobs for you in both open source and commercial world. Nikto Arachni w3af Acunetix Framework-specific security scanners \u00b6 This kind of check and scanning is to perform against specific to framework, CMS, and platforms. It allows to get more detailed results by validating against multiple security test cases and checks. Scanning against WordPress CMS using WPScan Scanning against JavaScript libraries using Retire.js Scanning against Drupal CMS using Droopescan Automated patching approaches using Ansible \u00b6 Patching and updating is a task that everyone who has to manage production systems has to deal with. There are two approaches that we will look are as follows: Rolling updates BlueGreen deployments Rolling updates \u00b6 Imagine that we have five web servers behind a load balancer. What we would like to do is a zero downtime upgrade of our web application. We want to achieve the following: Tell the load balancer that web server node is down Bring down the web server on that node Copy the updated application files to that node Bring up the web server on that node The first keyword for us to look at is serial. This ensures that the execution of the playbook is done serially rather than in parallel. We can choose to provide a percentage value or numeric value to serial. A great example for this way of doing updates is given in the following link Episode #47 - Zero-downtime Deployments with Ansible BlueGreen deployments \u00b6 The concept of BlueGreen is attributed to Martin Fowler . The idea is to consider our current live production workload as blue. Now what we want to do is upgrade the application. So a replica of blue is brought up behind the same load balancer. The replica of the infrastructure has the updated application. Once it is up and running, the load balancer configuration is switched from current blue to point to green. Blue keeps running in case there are any operational issues. Once we are happy with the progress, we can tear down the older host. BlueGreen deployment setup playbook \u00b6 The following playbook will set up three nodes, which includes load balancer and two web server nodes. The first playbook brings up three hosts. Two web servers running nginx behind a load balancer The second playbook switches what is live (blue) to green # inventory.yml [ proxyserver ] proxy ansible_host=192.168.100.100 ansible_user=ubuntu ansible_password=passwordgoeshere [blue] blueserver ansible_host=192.168.100.10 ansible_user=ubuntu ansible_password=passwordgoeshere [green] greenserver ansible_host=192.168.100.20 ansible_user=ubuntu ansible_password=passwordgoeshere [webservers:children] blue green [prod:children] webservers proxyserver # main.yml - name : running common role hosts : prod gather_facts : false become : yes serial : 100% roles : - common - name : running haproxy role hosts : proxyserver become : yes roles : - haproxy - name : running webserver role hosts : webservers become : yes serial : 100% roles : - nginx - name : updating blue code hosts : blue become : yes roles : - bluecode - name : updating green code hosts : green become : yes roles : - greencode # common role - name : installing python if not installed raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : updating and installing git, curl apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - git - curl # Also we can include common any monitoring and security hardening tasks # haproxy role - name : adding haproxy repo apt_repository : repo : ppa:vbernat/haproxy-1.7 - name : updating and installing haproxy apt : name : haproxy state : present update_cache : yes - name : updating the haproxy configuration template : src : haproxy.cfg.j2 dest : /etc/haproxy/haproxy.cfg - name : starting the haproxy service service : name : haproxy state : started enabled : yes # haproxy.cfg.j2 global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). This list is from : # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ # An alternative list with additional directives can be obtained from # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http frontend http_front bind *:80 stats uri /haproxy?stats default_backend http_back backend http_back balance roundrobin server {{ hostvars.blueserver.ansible_host }} {{ hostvars.blueserver.ansible_host }}:80 check #server {{ hostvars.greenserver.ansible_host }} {{ hostvars.greenserver.ansible_host }}:80 check # nginx role - name : installing nginx apt : name : nginx state : present update_cache : yes - name : starting the nginx service service : name : nginx state : started enabled : yes # Code snipet for blue <html> <body bgcolor=\"blue\"> <h1 align=\"center\">Welcome to Blue Deployment</h1> </body> </html> # Code snipet for green <html> <body bgcolor=\"green\"> <h1 align=\"center\">Welcome to Green Deployment</h1> </body> </html> We want to deploy the new version of production site with green deployment. The playbook looks very simple as follows, it will update the configuration and reloads the haproxy service to serve the new production deployment. - name : Updating to GREEN deployment hosts : proxyserver become : yes tasks : - name : updating proxy configuration template : src : haproxy.cfg.j2 dest : /etc/haproxy/haproxy.cfg - name : updating the service service : name : haproxy state : reloaded - debug : msg : \"GREEN deployment successful. Please check your server :)\" Continuous Security Scanning for Docker Containers \u00b6 Docker containers are the new way developers package applications. The best feature of containers is the fact that they contain the code, runtime, system libraries, and all the settings that are required for the application to work. Due to the ease of use and deployment, more and more applications are getting deployed in containers for production use. With so many moving parts, it becomes imperative that we have the capability to continuously scan Docker containers for security issues. Understanding continuous security concepts \u00b6 One of the key approaches to emerge out of DevOps is the idea of immutable infrastructure. It means that every time there needs to be a runtime change , either in application code or configuration, the containers are built and deployed again and the existing running ones are torn down. Since that allows for predictability, resilience, and simplifies deployment choices at runtime, it is no surprise that many operations teams are moving toward it. With that comes the question of when these containers should be tested for security and compliance. By embracing the process of continuous security scanning and monitoring, you can automate for a variety of workloads and workflows. Automating vulnerability assessments of Docker containers using Ansible \u00b6 Tool: Description There are many different ways of evaluating the security of containers. Docker Bench: A security shell script to perform checks based on CIS Clair: A tool to perform vulnerability analysis based on the CVE database Anchore: A tool to perform security evaluation and make runtime policy decisions vuls: An agent-less vulnerability scanner with CVE, OVAL database osquery: OS instrumentation framework for OS analytics to do HIDS-type activities Docker Bench for Security \u00b6 Docker Bench for Security is a shell script to perform multiple checks against the Docker container environment. It will give a more detailed view of the security configuration based on CIS benchmarks. This script supports most of the Unix operating systems as it was built based on the POSIX 2004 compliant. The following are the high-level areas of checks this script will perform: Host configuration Docker daemon configuration and files Docker container images Docker runtime Docker security operations Docker swarm configuration - name : Docker bench security playbook hosts : docker remote_user : ubuntu become : yes tasks : - name : make sure git installed apt : name : git state : present - name : download the docker bench security git : repo : https://github.com/docker/docker-bench-security.git dest : /opt/docker-bench-security - name : running docker-bench-security scan command : docker-bench-security.sh -l /tmp/output.log args : chdir : /opt/docker-bench-security/ - name : downloading report locally fetch : src : /tmp/output.log dest : \"{{ playbook_dir }}/{{ inventory_hostname }}-docker-report-{{ ansible_date_time.date }}.log\" flat : yes - name : report location debug : msg : \"Report can be found at {{ playbook_dir }}/{{ inventory_hostname }}-docker-report-{{ ansible_date_time.date }}.log\" </mark> The output of the playbook will download and scan the containers based on the CIS benchmark and store the results in a log file Clair \u00b6 Clair allows us to perform static vulnerability analysis against containers by checking with the existing vulnerability database. It allows us to perform vulnerability analysis checks against our Docker container images using the Clair database. Setting up Clair itself is really difficult and scanning using the API with Docker images makes more difficult. Here comes clair-scanner , it makes really simple to set up and perform scans using the REST API. Clair-scanner can trigger a simple scan against a container based on certain events, to check for existing vulnerabilities. Furthermore, this report can be forwarded to perform the team responsible for fixes and so on. # It assumes that the target system has Docker and the required libraries installed - name : Clair Scanner Server Setup hosts : docker remote_user : ubuntu become : yes tasks : - name : setting up clair-db docker_container : name : clair_db image : arminc/clair-db exposed_ports : - 5432 - name : setting up clair-local-scan docker_container : name : clair image : arminc/clair-local-scan:v2.0.1 ports : - \"6060:6060\" links : - \"clair_db:postgres\" # Setting up clair-scanner with Docker containers using Ansible # It will take a while to download and setup the CVE database after playbook execution. This playbook will be used to run clair-scanner to perform an analysis on the containers by making an API request to the server. - name : Scanning containers using clair-scanner hosts : docker remote_user : ubuntu become : yes vars : image_to_scan : \"debian:sid\" # container to scan for vulnerabilities clair_server : \"http://192.168.1.10:6060\" # clair server api endpoint tasks : - name : downloading and setting up clair-scanner binary get_url : url : https://github.com/arminc/clair-scanner/releases/download/v6/clair-scanner_linux_amd64 dest : /usr/local/bin/clair-scanner mode : 0755 - name : scanning {{ image_to_scan }} container for vulnerabilities command : clair-scanner -r /tmp/{{ image_to_scan }}-scan-report.json -c {{ clair_server }} --ip 0.0.0.0 {{ image_to_scan }} register : scan_output ignore_errors : yes - name : downloading the report locally fetch : src : /tmp/{{ image_to_scan }}-scan-report.json dest : {{ playbook_dir }} /{{ image_to_scan }}-scan-report.json flat : yes Scheduled scans using Ansible Tower for Docker security \u00b6 Continuous security processes are all about the loop of planning, doing, measuring, and acting By following standard checklists and benchmarks and using Ansible to execute them on containers, we can check for security issues and act on them. Anchore \u2013 open container compliance platform \u00b6 Anchore is one of the most popular tools and services to perform analysis, inspection, and certification of container images. Anchore is an analysis and inspection platform for containers. It provides multiple services and platforms to set up, the most stable and powerful way is to set up the local service using Anchore Engine, which can be accessed via the REST API. High level operations Anchore can perform: Policy evaluation operations Image operations Policy operations Registry operations Subscription operations System operations Anchore Engine service setup \u00b6 This playbook will set up the Anchore Engine service, which contains the engine container as well as the postgres to store database information. The admin_password variable is the admin user password to access the REST API of Anchore. - name : anchore server setup hosts : anchore become : yes vars : db_password : changeme admin_password : secretpassword tasks : - name : creating volumes file : path : \"{{ item }}\" recurse : yes state : directory with_items : - /root/aevolume/db - /root/aevolume/config - name : copying anchore-engine configuration template : src : config.yaml.j2 dest : /root/aevolume/config/config.yaml - name : starting anchore-db container docker_container : name : anchore-db image : postgres:9 volumes : - \"/root/aevolume/db/:/var/lib/postgresql/data/pgdata/\" env : POSTGRES_PASSWORD : \"{{ db_password }}\" PGDATA : \"/var/lib/postgresql/data/pgdata/\" - name : starting anchore-engine container docker_container : name : anchore-engine image : anchore/anchore-engine ports : - 8228:8228 - 8338:8338 volumes : - \"/root/aevolume/config/config.yaml:/config/config.yaml:ro\" - \"/var/run/docker.sock:/var/run/docker.sock:ro\" links : - anchore-db:anchore-db Anchore CLI scanner \u00b6 Now that we have the Anchore Engine service REST API with access details, we can use this to perform the scanning of container images in any host. The following steps are the Ansible Tower setup to perform continuous scanning of container images for vulnerabilities. - name : anchore-cli scan hosts : anchore become : yes vars : scan_image_name : \"docker.io/library/ubuntu:latest\" anchore_vars : ANCHORE_CLI_URL : http://localhost:8228/v1 ANCHORE_CLI_USER : admin ANCHORE_CLI_PASS : secretpassword tasks : - name : installing anchore-cli pip : name : \"{{ item }}\" with_items : - anchorecli - pyyaml - name : downloading image docker_image : name : \"{{ scan_image_name }}\" - name : adding image for analysis command : \"anchore-cli image add {{ scan_image_name }}\" environment : \"{{anchore_vars}}\" - name : wait for analysis to compelte command : \"anchore-cli image content {{ scan_image_name }} os\" register : analysis until : analysis.rc != 1 retries : 10 delay : 30 ignore_errors : yes environment : \"{{anchore_vars}}\" - name : vulnerabilities results command : \"anchore-cli image vuln {{ scan_image_name }} os\" register : vuln_output environment : \"{{anchore_vars}}\" - name : \"vulnerabilities in {{ scan_image_name }}\" debug : msg : \"{{ vuln_output.stdout_lines }}\" Scheduled scans using Ansible Tower for operating systems and kernel security \u00b6 While most of the discussed tools can be used for scanning and maintaining a benchmark for security, we should think about the entire process of the incident response and threat detection workflow: Preparation Detection and analysis Containment, eradication, and recovery Post-incident activity Setting up all such scanners is our preparation. Using the output of these scanners gives us the ability to detect and analyze. Both containment and recovery are beyond the scope of such tools. For the process of recovery and post-incident activity, you may want to consider playbooks that can trash the current infrastructure and recreate it as it is. As part of our preparation, it may be useful to get familiar with the following terms as you will see them being used repeatedly in the world of vulnerability scanners and vulnerability management tools: Term: Full form (if any): Description of the term CVE: Common Vulnerabilities and Exposures: It is a list of cybersecurity vulnerability identifiers. Usage typically includes CVE IDs. OVAL: Open Vulnerability and Assessment Language: A language for finding out and naming vulnerabilities and configuration issues in computer systems. CWE: Common Weakness Enumeration: A common list of software security weaknesses. NVD: National Vulnerability Database: A US government vulnerability management database available for public use in XML format. Vuls \u2013 vulnerability scanner \u00b6 Vuls is an agent-less scanner written in golang. It supports a different variety of Linux operating systems. It performs the complete end-to-end security system administrative tasks such as scanning for security vulnerabilities and security software updates. It analyzes the system for required security vulnerabilities, performs security risk analysis based on the CVE score, sends notifications via Slack and email, and also provides a simple web report with historical data. The playbook has mainly two roles for setting up vuls using Docker containers. vuls_containers_download vuls_database_download - name : setting up vuls using docker containers hosts : vuls become : yes roles : - vuls_containers_download - vuls_database_download # Pulling the Docker containers locally using the docker_image module: - name : pulling containers locally docker_image : name : \"{{ item }}\" pull : yes with_items : - vuls/go-cve-dictionary - vuls/goval-dictionary - vuls/vuls # Then downloading the CVE and OVAL databases for the required operating systems and distributions versions - name : fetching NVD database locally docker_container : name : \"cve-{{ item }}\" image : vuls/go-cve-dictionary auto_remove : yes interactive : yes state : started command : fetchnvd -years \"{{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/go-cve-dictionary-log:/var/log/vuls\" with_sequence : start=2002 end=\"{{ nvd_database_years }}\" - name : fetching redhat oval data docker_container : name : \"redhat-oval-{{ item }}\" image : vuls/goval-dictionary auto_remove : yes interactive : yes state : started command : fetch-redhat \"{{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/goval-dictionary-log:/var/log/vuls\" with_items : \"{{ redhat_oval_versions }}\" - name : fetching ubuntu oval data docker_container : name : \"ubuntu-oval-{{ item }}\" image : vuls/goval-dictionary auto_remove : yes interactive : yes state : started command : \"fetch-ubuntu {{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/goval-dictionary-log:/var/log/vuls\" with_items : \"{{ ubuntu_oval_versions }}\" The global variables file looks as follows. We can add more redhat_oval_versions, such as 5. The nvd_database_years will download the CVE database up until the end of 2017: vuls_data_directory : \"/vuls_data\" nvd_database_years : 2017 redhat_oval_versions : - 6 - 7 ubuntu_oval_versions : - 12 - 14 - 16 Now, it's time to perform the scanning and reporting using the vuls Docker containers. The following playbook contains simple steps to perform the vuls scan against virtual machines and containers, and send the report to slack and web: - name : scanning and reporting using vuls hosts : vuls become : yes vars : vuls_data_directory : \"/vuls_data\" slack_web_hook_url : https://hooks.slack.com/services/XXXXXXX/XXXXXXXXXXXXXXXXXXXXX slack_channel : \"#vuls\" slack_emoji : \":ghost:\" server_to_scan : 192.168.33.80 server_username : vagrant server_key_file_name : 192-168-33-80 tasks : - name : copying configuraiton file and ssh keys template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" mode : 0400 with_items : - { src : 'config.toml' , dst : '/root/config.toml' } - { src : '192-168-33-80' , dst : '/root/.ssh/192-168-33-80' } - name : running config test docker_container : name : configtest image : vuls/vuls auto_remove : yes interactive : yes state : started command : configtest -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - name : running vuls scanner docker_container : name : vulsscan image : vuls/vuls auto_remove : yes interactive : yes state : started command : scan -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - \"/etc/localtime:/etc/localtime:ro\" env : TZ : \"Asia/Kolkata\" - name : sending slack report docker_container : name : vulsreport image : vuls/vuls auto_remove : yes interactive : yes state : started command : report -cvedb-path=/vuls/cve.sqlite3 -ovaldb-path=/vuls/oval.sqlite3 --to-slack -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - \"/etc/localtime:/etc/localtime:ro\" - name : vuls webui report docker_container : name : vulswebui image : vuls/vulsrepo interactive : yes volumes : - \"{{ vuls_data_directory }}:/vuls\" ports : - \"80:5111\" The following file is the configuration file for vuls to perform the scanning. This holds the configuration for slack alerting and also the server to perform scanning. This can be configured very effectively as required using vuls documentation: [ slack ] hookURL = \"{{ slack_web_hook_url}}\" channel = \"{{ slack_channel }}\" iconEmoji = \"{{ slack_emoji }}\" [servers] [servers.{{ server_key_file_name }}] host = \"{{ server_to_scan }}\" user = \"{{ server_username }}\" keyPath = \"/root/.ssh/{{ server_key_file_name }}\" We can also visit the web UI interface of the vuls server IP address to see the detailed results in tabular and portable format. This is very useful to manage large amount of servers and patches at scale. This can be part of the CI/CD life cycle as an infrastructure code and then we can run this as a scheduled scan using Ansible Tower or Jenkins. Scheduled scans for file integrity checks, host-level monitoring using Ansible for various compliance initiatives \u00b6 One of the many advantages of being able to execute commands on the host using Ansible is the ability to get internal system information, such as: File hashes Network connections List of running processes It can act as a lightweight Host-Based Intrusion Detection System (HIDS). While this may not eliminate the case for a purpose-built HIDS in many cases, we can execute the same kind of security tasks using a tool such as Facebook's osquery along with Ansible. osquery \u00b6 osquery is an operating system instrumentation framework by Facebook and written in C++, that supports Windows, Linux, OS X (macOS), and other operating systems. It provides an interface to query an operating system using an SQL like syntax. By using this, we can perform low-level activities such as running processes, kernel configurations, network connections, and file integrity checks. Overall it's like a host-based intrusion detection system (HIDS) endpoint security. It provides osquery as a service, system interactive shell, and so on. Hence we can use this to perform centralized monitoring and security management solutions. This playbook is to set up and configure the osquery agent in your Linux servers to monitor and look for vulnerabilities, file integrity monitoring, and many other compliance activities, and then log them for sending to a centralized logging monitoring system. The reference tutorial can be followed at DigitalOcean . - name : setting up osquery hosts : linuxservers become : yes tasks : - name : installing osquery apt : deb : https://pkg.osquery.io/deb/osquery_2.10.2_1.linux.amd64.deb update_cache : yes - name : adding osquery configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : fim.conf , dst : /usr/share/osquery/packs/fim.conf } - { src : osquery.conf , dst : /etc/osquery/osquery.conf } - name : starting and enabling osquery service service : name : osqueryd state : started enabled : yes The following fim.conf code snippet is the pack for file integrity monitoring and it monitors for file events in the /home, /etc, and /tmp directories every 300 seconds. It uses Secure Hash Algorithm (SHA) checksum to validate the changes. This can be used to find out whether attackers add their own SSH keys or audit log changes against system configuration changes for compliance and other activities. { \"queries\" : { \"file_events\" : { \"query\" : \"select * from file_events;\" , \"removed\" : false , \"interval\" : 300 } }, \"file_paths\" : { \"homes\" : [ \"/root/.ssh/%%\" , \"/home/%/.ssh/%%\" ], \"etc\" : [ \"/etc/%%\" ], \"home\" : [ \"/home/%%\" ], \"tmp\" : [ \"/tmp/%%\" ] } } The following code snippet is the osquery service configuration. This can be modified as required to monitor and log by osquery service. { \"options\" : { \"config_plugin\" : \"filesystem\" , \"logger_plugin\" : \"filesystem\" , \"logger_path\" : \"/var/log/osquery\" , \"disable_logging\" : \"false\" , \"log_result_events\" : \"true\" , \"schedule_splay_percent\" : \"10\" , \"pidfile\" : \"/var/osquery/osquery.pidfile\" , \"events_expiry\" : \"3600\" , \"database_path\" : \"/var/osquery/osquery.db\" , \"verbose\" : \"false\" , \"worker_threads\" : \"2\" , \"enable_monitor\" : \"true\" , \"disable_events\" : \"false\" , \"disable_audit\" : \"false\" , \"audit_allow_config\" : \"true\" , \"host_identifier\" : \"hostname\" , \"enable_syslog\" : \"true\" , \"audit_allow_sockets\" : \"true\" , \"schedule_default_interval\" : \"3600\" }, \"schedule\" : { \"crontab\" : { \"query\" : \"SELECT * FROM crontab;\" , \"interval\" : 300 }, \"system_profile\" : { \"query\" : \"SELECT * FROM osquery_schedule;\" }, \"system_info\" : { \"query\" : \"SELECT hostname, cpu_brand, physical_memory FROM system_info;\" , \"interval\" : 3600 } }, \"decorators\" : { \"load\" : [ \"SELECT uuid AS host_uuid FROM system_info;\" , \"SELECT user AS username FROM logged_in_users ORDER BY time DESC LIMIT 1;\" ] }, \"packs\" : { \"fim\" : \"/usr/share/osquery/packs/fim.conf\" , \"osquery-monitoring\" : \"/usr/share/osquery/packs/osquery-monitoring.conf\" , \"incident-response\" : \"/usr/share/osquery/packs/incident-response.conf\" , \"it-compliance\" : \"/usr/share/osquery/packs/it-compliance.conf\" , \"vuln-management\" : \"/usr/share/osquery/packs/vuln-management.conf\" } } The goal is not just setting up osquery, we can use the logs to build a centralized real-time monitoring system using our Elastic stack. We can use the Filebeat agent to forward these logs to our Elastic stack and we can view them and build a centralized dashboard for alerting and monitoring. This idea can be extended for building some automated defences by taking actions against attacks by using automated Ansible playbooks for known actions. The world is moving toward containers and this kind of monitoring gives us a look at low-level things such as kernel security checks, and file integrity checks on host level. When attackers try to bypass containers and get access to hosts to escalate privileges, we can detect and defend them using this kind of setup. Summary \u00b6 Containers are rapidly changing the world of developers and operations teams. The rate of change is accelerating, and in this new world, security automation gets to play a front and center role. By leveraging our knowledge of using Ansible for scripting play-by-play commands along with excellent tools such as Anchore and osquery , we can measure, analyze, and benchmark our containers for security. This allows us to build end-to-end automatic processes of securing, scanning and remediating containers. Automating Lab Setups for Forensics Collection and Malware Analysis \u00b6 Malware is one of the biggest challenges faced by the security community. It impacts everyone who gets to interact with information systems. While there is a massive effort required in keeping computers safe from malware for operational systems, a big chunk of work in malware defenses is about understanding where they come from and what they are capable of. Another important aspect of malware analysis is the ability to collaborate and share threats using the Malware Information Sharing Platform (MISP). One of the initial phases of malware analysis is identification and classification. The most popular source is using VirusTotal to scan and get the results of the malware samples, domain information, and so on. It has a very rich API and a lot of people have written custom apps that leverage the API to perform the automated scans using the API key for identifying the malware type. It generally checks using more than 60 antivirus scanners and tools and provides detailed information. VirusTotal API tool set up \u00b6 The following playbook will set up the VirusTotal API tool - name : setting up VirusTotal hosts : malware remote_user : ubuntu become : yes tasks : - name : installing pip apt : name : \"{{ item }}\" with_items : - python-pip - unzip - name : checking if vt already exists stat : path : /usr/local/bin/vt register : vt_status - name : downloading VirusTotal api tool repo unarchive : src : \"https://github.com/doomedraven/VirusTotalApi/archive/master.zip\" dest : /tmp/ remote_src : yes when : vt_status.stat.exists == False - name : installing the dependencies pip : requirements : /tmp/VirusTotalApi-master/requirements.txt when : vt_status.stat.exists == False - name : installing vt command : python /tmp/VirusTotalApi-master/setup.py install when : vt_status.stat.exists == False The playbook execution will download the repository and set up the VirusTotal API tool. The following playbook will find and copy the local malware samples to a remote system and scan them recursively and return the results. Once the scan has been completed, it will remove the samples from the remote system. - name : scanning file in VirusTotal hosts : malware remote_user : ubuntu vars : vt_api_key : XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX #use Ansible-vault vt_api_type : public # public/private vt_intelligence_access : False # True/False files_in_local_system : /tmp/samples/ files_in_remote_system : /tmp/sample-file/ tasks : - name : creating samples directory file : path : \"{{ files_in_remote_system }}\" state : directory - name : copying file to remote system copy : src : \"{{ files_in_local_system }}\" dest : \"{{ files_in_remote_system }}\" directory_mode : yes - name : copying configuration template : src : config.j2 dest : \"{{ files_in_remote_system }}/.vtapi\" - name : running VirusTotal scan command : \"vt -fr {{ files_in_remote_system }}\" args : chdir : \"{{ files_in_remote_system }}\" register : vt_scan - name : removing the samples file : path : \"{{ files_in_remote_system }}\" state : absent - name : VirusTotal scan results debug : msg : \"{{ vt_scan.stdout_lines }}\" Creating Ansible playbooks for collection and storage with secure backup of forensic artifacts \u00b6 Ansible is an apt replacement for all kinds of bash scripts. Typically, for most activities that require analysis, we follow a set pattern: Collect logs from running processes into files with a path we already know Copy the content from these log files periodically to a secure storage locally or accessible remotely over SSH or a network file share Once copied successfully, rotate the logs Since there is a bit of network activity involved, our bash scripts are usually written to be fault tolerant with regard to network connections and become complex very soon. Ansible playbooks can be used to do all of that while being simple to read for everyone. Collecting log artifacts for incident response \u00b6 The key phase in incident response is log analysis. This playbook will collect the logs from all the hosts and store it locally. This allows responders to perform the further analysis. # Reference https://www.Ansible.com/security-automation-with-Ansible - name : Gather log files hosts : servers become : yes tasks : - name : List files to grab find : paths : - /var/log patterns : - '*.log*' recurse : yes register : log_files - name : Grab files fetch : src : \"{{ item.path }}\" dest : \"/tmp/LOGS_{{ Ansible_fqdn }}/\" with_items : \"{{ log_files.files }}\" This playbook execution will collect a list of logs in specified locations in remote hosts using Ansible modules and store them in the local system. Secure backups for data collection \u00b6 When collecting multiple sets of data from servers, it's important to store them securely with encrypted backups. This can be achieved by backing up the data to storage services such as S3. This Ansible playbook allows us to install and copy the collected data to the AWS S3 service with encryption enabled. - name : backing up the log data hosts : localhost gather_facts : false become : yes vars : s3_access_key : XXXXXXX # Use Ansible-vault to encrypt s3_access_secret : XXXXXXX # Use Ansible-vault to encrypt localfolder : /tmp/LOGS/ # Trailing slash is important remotebucket : secretforensicsdatausingAnsible # This should be unique in s3 tasks : - name : installing s3cmd if not installed apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-magic - python-dateutil - s3cmd - name : create s3cmd config file template : src : s3cmd.j2 dest : /root/.s3cfg owner : root group : root mode : 0640 - name : make sure \"{{ remotebucket }}\" is avilable command : \"s3cmd mb s3://{{ remotebucket }}/ -c /root/.s3cfg\" - name : running the s3 backup to \"{{ remotebucket }}\" command : \"s3cmd sync {{ localfolder }} --preserve s3://{{ remotebucket }}/ -c /root/.s3cfg\" The Ansible playbook installing s3cmd, creating the new bucket called secretforensicsdatausingAnsible, and copying the local log data to the remote S3 bucket. The configuration file looks like the following for the s3cmd configuration [ default ] access_key = {{ s3_access_key }} secret_key = {{ s3_access_secret }} host_base = s3.amazonaws.com host_bucket = %(bucket)s.s3.amazonaws.com website_endpoint = http://%(bucket)s.s3-website-%(location)s.amazonaws.com/ use_https = True signature_v2 = True We can see that the logs are successfully uploaded into the secretforensicsdatausingAnsible S3 bucket in AWS S3.","title":"Security Hardening for Applications and Networks"},{"location":"learning/ansible/security_hardening/#security-hardening-for-applications-and-networks","text":"Security hardening is the most obvious task for any security-conscious endeavor. By doing the effort of securing systems, applications, and networks, one can achieve multiple security goals given as follows: Ensuring that applications and networks are not compromised (sometimes) Making it difficult for compromises to stay hidden for long Securing by default ensures that compromises in one part of the network don't propagate further and more We will build playbooks that will allow us to do the following things: Secure our master images so that as soon as the applications and systems are part of the network, they offer decent security Execute audit processes so that we can verify and measure periodically if the applications, systems, and networks are in line with the security policies that are required by the organization Security hardening with benchmarks such as Center for Internet Security (CIS), Security Technical Implementation Guides (STIG), and National Institute of Standards and Technology (NIST) Automating security audit checks for networking devices using Ansible Automating security audit checks for applications using Ansible Automated patching approaches using Ansible","title":"Security Hardening for Applications and Networks"},{"location":"learning/ansible/security_hardening/#security-hardening-with-benchmarks-such-as-cis-stigs-and-nist","text":"Benchmarks provide a great way for anyone to gain assurance of their individual security efforts. Hardening for security mostly boils down to do the following: Agreeing on what is the minimal set of configuration that qualifies as secure configuration. This is usually defined as a hardening benchmark or framework. Making changes to all the aspects of the system that are touched by such configuration. Measuring periodically if the application and system are still in line with the configuration or if there is any deviation. If any deviation is found, take corrective action to fix that. If no deviation is found, log that. Since software is always getting upgraded, staying on top of the latest configuration guidelines and benchmarks is most important.","title":"Security hardening with benchmarks such as CIS, STIGs, and NIST"},{"location":"learning/ansible/security_hardening/#operating-system-hardening-for-baseline-using-an-ansible-playbook","text":"We will see how we can use existing playbooks from the community (Ansible Galaxy). The following playbook provides multiple security configurations, standards, and ways to protect operating system against different attacks and security vulnerabilities. Some of the tasks it will perform include the following: Configures package management, for example, allows only signed packages Remove packages with known issues Configures pam and pam_limits modules Shadow password suite configuration Configures system path permissions Disable core dumps via soft limits Restrict root logins to system console Set SUIDs Configures kernel parameters via sysctl Downloading and executing Ansible playbooks from galaxy is as simple as follows: ansible-galaxy install dev-sec.os-hardening - hosts : localhost become : yes roles : - dev-sec.os-hardening The preceding playbook will detect the operating system and perform hardening steps based on the different guidelines. This can be configured as required by updating the default variables values. Refer to dev-sec/ansible-collection-hardening for more details about the playbook.","title":"Operating system hardening for baseline using an Ansible playbook"},{"location":"learning/ansible/security_hardening/#stigs-ansible-role-for-automated-security-hardening-for-linux-hosts","text":"OpenStack has an awesome project named ansible-hardening , which applies the security configuration changes as per the STIGs standards. It performs security hardening for the following domains: accounts: User account security controls aide: Advanced Intrusion Detection Environment auditd: Audit daemon auth: Authentication file_perms: Filesystem permissions graphical: Graphical login security controls kernel: Kernel parameters lsm: Linux Security Modules misc: Miscellaneous security controls packages: Package managers sshd: SSH daemon Download the role from the GitHub repository itself using ansible-galaxy as follows: ansible-galaxy install git+https://github.com/openstack/ansible-hardening - name : STIGs ansible-hardening for automated security hardening hosts : servers become : yes remote_user : \"{{ remote_user_name }}\" vars : remote_user_name : vagrant security_ntp_servers : - time.nist.gov - time.google.com roles : - ansible-hardening","title":"STIGs Ansible role for automated security hardening for Linux hosts"},{"location":"learning/ansible/security_hardening/#continuous-security-scans-and-reports-for-openscap-using-ansible-tower","text":"OpenSCAP is a set of security tools, policies, and standards to perform security compliance checks against the systems by following SCAP. SCAP is the U.S. standard maintained by NIST. OpenSCAP follows these steps to perform scanning on your system: Install SCAP Workbench or OpenSCAP Base (for more information, visit https://www.open-scap.org ) Choose a policy Adjust your settings Evaluate the system Check playbook reference at https://medium.com/@jackprice/ansible-openscap-for-compliance-automation-14200fe70663 . - hosts : all become : yes vars : oscap_profile : xccdf_org.ssgproject.content_profile_pci-dss oscap_policy : ssg-rhel7-ds tasks : - name : install openscap scanner package : name : \"{{ item }}\" state : latest with_items : - openscap-scanner - scap-security-guide - block : - name : run openscap command : > oscap xccdf eval --profile {{ oscap_profile }} --results-arf /tmp/oscap-arf.xml --report /tmp/oscap-report.html --fetch-remote-resources /usr/share/xml/scap/ssg/content/{{ oscap_policy }}.xml always : - name : download report fetch : src : /tmp/oscap-report.html dest : ./{{ inventory_hostname }}.html flat : yes We can use this playbook to perform continuously automated checks using Ansible Tower First, we need to create a directory in Ansible Tower server in order to store this playbook with the awx user permission to add the custom playbook. Create a new project in Ansible Tower to perform the OpenSCAP setup and scan against the checks. Then, we have to create a new job to execute the playbook. Here, we can include the list of hosts, credentials for login, and other details required to perform the execution. This audit can be scheduled to perform frequently. We can also launch this job on demand when required. The output of the playbook will generate the OpenSCAP report, and it will be fetched to Ansible Tower. We can access this playbook at the /tmp/ location. Also, we can send this report to the other centralized reporting server if required. We can also set up notifications based on playbook execution results. By doing this, we can send this notifications to respective channels, such as email, slack, and message.","title":"Continuous security scans and reports for OpenSCAP using Ansible Tower"},{"location":"learning/ansible/security_hardening/#cis-benchmarks","text":"CIS has benchmarks for different type OS, software, and services. The following are some high-level categories: Desktops and web browsers Mobile devices Network devices Security metrics Servers \u2013 operating systems Servers \u2013 other Virtualization platforms, cloud, and other","title":"CIS Benchmarks"},{"location":"learning/ansible/security_hardening/#ubuntu-cis-benchmarks-server-level","text":"CIS Benchmarks Ubuntu provides prescriptive guidance to establish a secure configuration posture for Ubuntu Linux systems running on x86 and x64 platforms. This benchmark is intended for system and application administrators, security specialists, auditors, help desk, and platform deployment personnel who plan to develop, deploy, assess, or secure solutions that incorporate Linux platform. Here are the high-level six domains that are part of CIS Ubuntu 16.04 LTS benchmarks: Initial setup: Filesystem configuration Configure software updates Filesystem integrity checking Secure boot settings Additional process hardening Mandatory access control Warning banners Services: Inted Services Special purpose services Service clients Network configuration: Network parameters (host only) Network parameters (host and router) IPv6 TCP wrappers Uncommon network protocols Logging and auditing: Configure system accounting (auditd) Configure logging Access, authentication, and authorization: Configure cron SSH server configuration Configure PAM User accounts and environment System maintenance: System file permissions User and group settings # Playbooks git clone https://github.com/oguya/cis-ubuntu-14-ansible.git cd cis-ubuntu-14-ansible Then, update the variables and inventory and execute the playbook using the following command. The variables are not required mostly, as this performs against different CIS checks unless, if we wanted to customize the benchmarks as per the organization. ansible-playbook -i inventory cis.yml The preceding playbook will execute the CIS security benchmark against an Ubuntu server and performs all the checks listed in the CIS guidelines.","title":"Ubuntu CIS Benchmarks (server level)"},{"location":"learning/ansible/security_hardening/#aws-benchmarks-cloud-provider-level","text":"AWS CIS Benchmarks provides prescriptive guidance to configure security options for a subset of AWS with an emphasis on foundational, testable, and architecture agnostic settings. Here are the high-level domains, which are part of AWS CIS Benchmarks: Identity and access management Logging Monitoring Networking Extra Currently, there is a tool named prowler ( Alfresco/prowler ) based on AWS-CLI commands for AWS account security assessment and hardening. Before running the playbook, we have to provide AWS API keys to perform security audit. This can be created using IAM role in AWS service. If you have an already existing account with required privileges, these steps can be skipped. Create a new user in your AWS account with programmatic access. Apply the SecurityAudit policy for the user from existing policies in IAM console. Create the new user by following the steps. Make sure that you safely save the Access key ID and Secret access key for later use. The following playbook assume that you already have installed python and pip in your local system. - name : AWS CIS Benchmarks playbook hosts : localhost become : yes vars : aws_access_key : XXXXXXXX aws_secret_key : XXXXXXXX tasks : - name : installing aws cli and ansi2html pip : name : \"{{ item }}\" with_items : - awscli - ansi2html - name : downloading and setting up prowler get_url : url : https://raw.githubusercontent.com/Alfresco/prowler/master/prowler dest : /usr/bin/prowler mode : 0755 - name : running prowler full scan shell : \"prowler | ansi2html -la > ./aws-cis-report-{{ ansible_date_time.epoch }}.html\" environment : AWS_ACCESS_KEY_ID : \"{{ aws_access_key }}\" AWS_SECRET_ACCESS_KEY : \"{{ aws_secret_key }}\" - name : AWS CIS Benchmarks report downloaded debug : msg : \"Report can be found at ./aws-cis-report-{{ ansible_date_time.epoch }}.html\" The playbook will trigger the setup and security audit scan for AWS CIS Benchmarks using the prowler tool. Prowler-generated HTML report can be downloaded in different formats as required and also scanning checks can be configured as required. More reference about the tool can be found at Alfresco/prowler .","title":"AWS benchmarks (cloud provider level)"},{"location":"learning/ansible/security_hardening/#automating-security-audit-checks-for-networking-devices-using-ansible","text":"We can use this to do security audit checks for networking devices.","title":"Automating security audit checks for networking devices using Ansible"},{"location":"learning/ansible/security_hardening/#nmap-scanning-and-nse","text":"Network Mapper (Nmap) is a free open source software to perform network discovery, scanning, audit, and many others. It has a various amount of features such as OS detection, system fingerprinting, firewall detection, and many other features. Nmap Scripting Engine (Nmap NSE) provides advanced capabilities like scanning for particular vulnerabilities and attacks. We can also write and extend Nmap using our own custom script. Nmap is a swiss army knife for pen testers (security testers) and network security teams. - name : Basic NMAP Scan Playbook hosts : localhost gather_facts : false vars : top_ports : 1000 network_hosts : - 192.168.1.1 - scanme.nmap.org - 127.0.0.1 - 192.168.11.0/24 tasks : - name : check if nmap installed and install apt : name : nmap update_cache : yes state : present become : yes - name : top ports scan shell : \"nmap --top-ports {{ top_ports }} -Pn -oA nmap-scan-%Y-%m-%d {{ network_hosts|join(' ') }}\" {{ network_hosts|join(' ') }} is a Jinja2 feature named filter arguments to parse the given network_hosts by space delimited network_hosts variable holds the list of IPs, network range (CIDR), hosts, and so on to perform scan using Nmap top_ports is the number that is ranging from 0 to 65535. Nmap by default picks commonly opened top ports -Pn specifies that scans the host if ping (ICMP) doesn't work also -oA gets the output in all formats, which includes gnmap (greppable format), Nmap, and XML","title":"Nmap scanning and NSE"},{"location":"learning/ansible/security_hardening/#nmap-nse-scanning-playbook","text":"This playbook will perform enumeration of directories used by popular web applications and servers using http-enum and finds options that are supported by an HTTP server using http-methods using Nmap scripts. - name : Advanced NMAP Scan using NSE hosts : localhost vars : ports : - 80 - 443 scan_host : scanme.nmap.org tasks : - name : Running Nmap NSE scan shell : \"nmap -Pn -p {{ ports|join(',') }} --script {{ item }} -oA nmap-{{ item }}-results-%Y-%m-%d {{ scan_host }}\" with_items : - http-methods - http-enum The http-enum script runs additional tests against network ports where web servers are detected. We can see that two folders were discovered by the script and additionally all HTTP methods that are supported got enumerated as well.","title":"Nmap NSE scanning playbook"},{"location":"learning/ansible/security_hardening/#automation-security-audit-checks-for-applications-using-ansible","text":"Modern applications can get pretty complex fairly quickly. Having the ability to run automation to do security tasks is almost a mandatory requirement. The different types of application security scanning we can do can range from the following: Run CI/CD scanning against the source code (for example, RIPS and brakeman). Dependency checking scanners (for example, OWASP dependency checker and snyk.io ( https://snyk.io/ )). Once deployed then run the web application scanner (for example, Nikto, Arachni, and w3af). Framework-specific security scanners (for example, WPScan and Droopscan) and many other.","title":"Automation security audit checks for applications using Ansible"},{"location":"learning/ansible/security_hardening/#source-code-analysis-scanners","text":"This is one of the first and common way to minimize the security risk while applications going to production. Source code analysis scanner also known as Static Application Security Testing (SAST) will help to find security issues by analyzing the source code of the application. Source code analysis is kind of white box testing and looking through code. This kind of testing methodology may not find 100% coverage of security vulnerabilities, and it requires manual testing as well. For example, finding logical vulnerabilities requires some kind of user interactions such as dynamic functionalities. For example, if you are scanning PHP code, then RIPS ; if it's Ruby on Rails code, then it's Brakeman ; and if it's python, then Bandit","title":"Source code analysis scanners"},{"location":"learning/ansible/security_hardening/#dependency-checking-scanners","text":"Most of the developers use third-party libraries while developing applications, and it's very common to see using open source plugins and modules inside their code. So dependency checks will allow us to find using components with known vulnerabilities (OWASP A9) issues in application code by scanning the libraries against the CVE and NIST vulnerability database. There are multiple projects out there in the market for performing these checks, and some of them includes the following: OWASP Dependency-Check Snyk.io ( https://snyk.io/ ) Retire.js [:] SourceClear and many other","title":"Dependency-checking scanners"},{"location":"learning/ansible/security_hardening/#running-web-application-security-scanners","text":"This is the phase where the application went live to QA, stage, (or) Production. Then, we wanted to perform security scans like an attacker (black box view). At this stage, an application will have all the dynamic functionalities and server configurations applied. These scanner results tell us how good the server configured and any other application security issues before releasing the replica copy into the production. There are many tools in the market to do these jobs for you in both open source and commercial world. Nikto Arachni w3af Acunetix","title":"Running web application security scanners"},{"location":"learning/ansible/security_hardening/#framework-specific-security-scanners","text":"This kind of check and scanning is to perform against specific to framework, CMS, and platforms. It allows to get more detailed results by validating against multiple security test cases and checks. Scanning against WordPress CMS using WPScan Scanning against JavaScript libraries using Retire.js Scanning against Drupal CMS using Droopescan","title":"Framework-specific security scanners"},{"location":"learning/ansible/security_hardening/#automated-patching-approaches-using-ansible","text":"Patching and updating is a task that everyone who has to manage production systems has to deal with. There are two approaches that we will look are as follows: Rolling updates BlueGreen deployments","title":"Automated patching approaches using Ansible"},{"location":"learning/ansible/security_hardening/#rolling-updates","text":"Imagine that we have five web servers behind a load balancer. What we would like to do is a zero downtime upgrade of our web application. We want to achieve the following: Tell the load balancer that web server node is down Bring down the web server on that node Copy the updated application files to that node Bring up the web server on that node The first keyword for us to look at is serial. This ensures that the execution of the playbook is done serially rather than in parallel. We can choose to provide a percentage value or numeric value to serial. A great example for this way of doing updates is given in the following link Episode #47 - Zero-downtime Deployments with Ansible","title":"Rolling updates"},{"location":"learning/ansible/security_hardening/#bluegreen-deployments","text":"The concept of BlueGreen is attributed to Martin Fowler . The idea is to consider our current live production workload as blue. Now what we want to do is upgrade the application. So a replica of blue is brought up behind the same load balancer. The replica of the infrastructure has the updated application. Once it is up and running, the load balancer configuration is switched from current blue to point to green. Blue keeps running in case there are any operational issues. Once we are happy with the progress, we can tear down the older host.","title":"BlueGreen deployments"},{"location":"learning/ansible/security_hardening/#bluegreen-deployment-setup-playbook","text":"The following playbook will set up three nodes, which includes load balancer and two web server nodes. The first playbook brings up three hosts. Two web servers running nginx behind a load balancer The second playbook switches what is live (blue) to green # inventory.yml [ proxyserver ] proxy ansible_host=192.168.100.100 ansible_user=ubuntu ansible_password=passwordgoeshere [blue] blueserver ansible_host=192.168.100.10 ansible_user=ubuntu ansible_password=passwordgoeshere [green] greenserver ansible_host=192.168.100.20 ansible_user=ubuntu ansible_password=passwordgoeshere [webservers:children] blue green [prod:children] webservers proxyserver # main.yml - name : running common role hosts : prod gather_facts : false become : yes serial : 100% roles : - common - name : running haproxy role hosts : proxyserver become : yes roles : - haproxy - name : running webserver role hosts : webservers become : yes serial : 100% roles : - nginx - name : updating blue code hosts : blue become : yes roles : - bluecode - name : updating green code hosts : green become : yes roles : - greencode # common role - name : installing python if not installed raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : updating and installing git, curl apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - git - curl # Also we can include common any monitoring and security hardening tasks # haproxy role - name : adding haproxy repo apt_repository : repo : ppa:vbernat/haproxy-1.7 - name : updating and installing haproxy apt : name : haproxy state : present update_cache : yes - name : updating the haproxy configuration template : src : haproxy.cfg.j2 dest : /etc/haproxy/haproxy.cfg - name : starting the haproxy service service : name : haproxy state : started enabled : yes # haproxy.cfg.j2 global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). This list is from : # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ # An alternative list with additional directives can be obtained from # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http frontend http_front bind *:80 stats uri /haproxy?stats default_backend http_back backend http_back balance roundrobin server {{ hostvars.blueserver.ansible_host }} {{ hostvars.blueserver.ansible_host }}:80 check #server {{ hostvars.greenserver.ansible_host }} {{ hostvars.greenserver.ansible_host }}:80 check # nginx role - name : installing nginx apt : name : nginx state : present update_cache : yes - name : starting the nginx service service : name : nginx state : started enabled : yes # Code snipet for blue <html> <body bgcolor=\"blue\"> <h1 align=\"center\">Welcome to Blue Deployment</h1> </body> </html> # Code snipet for green <html> <body bgcolor=\"green\"> <h1 align=\"center\">Welcome to Green Deployment</h1> </body> </html> We want to deploy the new version of production site with green deployment. The playbook looks very simple as follows, it will update the configuration and reloads the haproxy service to serve the new production deployment. - name : Updating to GREEN deployment hosts : proxyserver become : yes tasks : - name : updating proxy configuration template : src : haproxy.cfg.j2 dest : /etc/haproxy/haproxy.cfg - name : updating the service service : name : haproxy state : reloaded - debug : msg : \"GREEN deployment successful. Please check your server :)\"","title":"BlueGreen deployment setup playbook"},{"location":"learning/ansible/security_hardening/#continuous-security-scanning-for-docker-containers","text":"Docker containers are the new way developers package applications. The best feature of containers is the fact that they contain the code, runtime, system libraries, and all the settings that are required for the application to work. Due to the ease of use and deployment, more and more applications are getting deployed in containers for production use. With so many moving parts, it becomes imperative that we have the capability to continuously scan Docker containers for security issues.","title":"Continuous Security Scanning for Docker Containers"},{"location":"learning/ansible/security_hardening/#understanding-continuous-security-concepts","text":"One of the key approaches to emerge out of DevOps is the idea of immutable infrastructure. It means that every time there needs to be a runtime change , either in application code or configuration, the containers are built and deployed again and the existing running ones are torn down. Since that allows for predictability, resilience, and simplifies deployment choices at runtime, it is no surprise that many operations teams are moving toward it. With that comes the question of when these containers should be tested for security and compliance. By embracing the process of continuous security scanning and monitoring, you can automate for a variety of workloads and workflows.","title":"Understanding continuous security concepts"},{"location":"learning/ansible/security_hardening/#automating-vulnerability-assessments-of-docker-containers-using-ansible","text":"Tool: Description There are many different ways of evaluating the security of containers. Docker Bench: A security shell script to perform checks based on CIS Clair: A tool to perform vulnerability analysis based on the CVE database Anchore: A tool to perform security evaluation and make runtime policy decisions vuls: An agent-less vulnerability scanner with CVE, OVAL database osquery: OS instrumentation framework for OS analytics to do HIDS-type activities","title":"Automating vulnerability assessments of Docker containers using Ansible"},{"location":"learning/ansible/security_hardening/#docker-bench-for-security","text":"Docker Bench for Security is a shell script to perform multiple checks against the Docker container environment. It will give a more detailed view of the security configuration based on CIS benchmarks. This script supports most of the Unix operating systems as it was built based on the POSIX 2004 compliant. The following are the high-level areas of checks this script will perform: Host configuration Docker daemon configuration and files Docker container images Docker runtime Docker security operations Docker swarm configuration - name : Docker bench security playbook hosts : docker remote_user : ubuntu become : yes tasks : - name : make sure git installed apt : name : git state : present - name : download the docker bench security git : repo : https://github.com/docker/docker-bench-security.git dest : /opt/docker-bench-security - name : running docker-bench-security scan command : docker-bench-security.sh -l /tmp/output.log args : chdir : /opt/docker-bench-security/ - name : downloading report locally fetch : src : /tmp/output.log dest : \"{{ playbook_dir }}/{{ inventory_hostname }}-docker-report-{{ ansible_date_time.date }}.log\" flat : yes - name : report location debug : msg : \"Report can be found at {{ playbook_dir }}/{{ inventory_hostname }}-docker-report-{{ ansible_date_time.date }}.log\" </mark> The output of the playbook will download and scan the containers based on the CIS benchmark and store the results in a log file","title":"Docker Bench for Security"},{"location":"learning/ansible/security_hardening/#clair","text":"Clair allows us to perform static vulnerability analysis against containers by checking with the existing vulnerability database. It allows us to perform vulnerability analysis checks against our Docker container images using the Clair database. Setting up Clair itself is really difficult and scanning using the API with Docker images makes more difficult. Here comes clair-scanner , it makes really simple to set up and perform scans using the REST API. Clair-scanner can trigger a simple scan against a container based on certain events, to check for existing vulnerabilities. Furthermore, this report can be forwarded to perform the team responsible for fixes and so on. # It assumes that the target system has Docker and the required libraries installed - name : Clair Scanner Server Setup hosts : docker remote_user : ubuntu become : yes tasks : - name : setting up clair-db docker_container : name : clair_db image : arminc/clair-db exposed_ports : - 5432 - name : setting up clair-local-scan docker_container : name : clair image : arminc/clair-local-scan:v2.0.1 ports : - \"6060:6060\" links : - \"clair_db:postgres\" # Setting up clair-scanner with Docker containers using Ansible # It will take a while to download and setup the CVE database after playbook execution. This playbook will be used to run clair-scanner to perform an analysis on the containers by making an API request to the server. - name : Scanning containers using clair-scanner hosts : docker remote_user : ubuntu become : yes vars : image_to_scan : \"debian:sid\" # container to scan for vulnerabilities clair_server : \"http://192.168.1.10:6060\" # clair server api endpoint tasks : - name : downloading and setting up clair-scanner binary get_url : url : https://github.com/arminc/clair-scanner/releases/download/v6/clair-scanner_linux_amd64 dest : /usr/local/bin/clair-scanner mode : 0755 - name : scanning {{ image_to_scan }} container for vulnerabilities command : clair-scanner -r /tmp/{{ image_to_scan }}-scan-report.json -c {{ clair_server }} --ip 0.0.0.0 {{ image_to_scan }} register : scan_output ignore_errors : yes - name : downloading the report locally fetch : src : /tmp/{{ image_to_scan }}-scan-report.json dest : {{ playbook_dir }} /{{ image_to_scan }}-scan-report.json flat : yes","title":"Clair"},{"location":"learning/ansible/security_hardening/#scheduled-scans-using-ansible-tower-for-docker-security","text":"Continuous security processes are all about the loop of planning, doing, measuring, and acting By following standard checklists and benchmarks and using Ansible to execute them on containers, we can check for security issues and act on them.","title":"Scheduled scans using Ansible Tower for Docker security"},{"location":"learning/ansible/security_hardening/#anchore--open-container-compliance-platform","text":"Anchore is one of the most popular tools and services to perform analysis, inspection, and certification of container images. Anchore is an analysis and inspection platform for containers. It provides multiple services and platforms to set up, the most stable and powerful way is to set up the local service using Anchore Engine, which can be accessed via the REST API. High level operations Anchore can perform: Policy evaluation operations Image operations Policy operations Registry operations Subscription operations System operations","title":"Anchore \u2013 open container compliance platform"},{"location":"learning/ansible/security_hardening/#anchore-engine-service-setup","text":"This playbook will set up the Anchore Engine service, which contains the engine container as well as the postgres to store database information. The admin_password variable is the admin user password to access the REST API of Anchore. - name : anchore server setup hosts : anchore become : yes vars : db_password : changeme admin_password : secretpassword tasks : - name : creating volumes file : path : \"{{ item }}\" recurse : yes state : directory with_items : - /root/aevolume/db - /root/aevolume/config - name : copying anchore-engine configuration template : src : config.yaml.j2 dest : /root/aevolume/config/config.yaml - name : starting anchore-db container docker_container : name : anchore-db image : postgres:9 volumes : - \"/root/aevolume/db/:/var/lib/postgresql/data/pgdata/\" env : POSTGRES_PASSWORD : \"{{ db_password }}\" PGDATA : \"/var/lib/postgresql/data/pgdata/\" - name : starting anchore-engine container docker_container : name : anchore-engine image : anchore/anchore-engine ports : - 8228:8228 - 8338:8338 volumes : - \"/root/aevolume/config/config.yaml:/config/config.yaml:ro\" - \"/var/run/docker.sock:/var/run/docker.sock:ro\" links : - anchore-db:anchore-db","title":"Anchore Engine service setup"},{"location":"learning/ansible/security_hardening/#anchore-cli-scanner","text":"Now that we have the Anchore Engine service REST API with access details, we can use this to perform the scanning of container images in any host. The following steps are the Ansible Tower setup to perform continuous scanning of container images for vulnerabilities. - name : anchore-cli scan hosts : anchore become : yes vars : scan_image_name : \"docker.io/library/ubuntu:latest\" anchore_vars : ANCHORE_CLI_URL : http://localhost:8228/v1 ANCHORE_CLI_USER : admin ANCHORE_CLI_PASS : secretpassword tasks : - name : installing anchore-cli pip : name : \"{{ item }}\" with_items : - anchorecli - pyyaml - name : downloading image docker_image : name : \"{{ scan_image_name }}\" - name : adding image for analysis command : \"anchore-cli image add {{ scan_image_name }}\" environment : \"{{anchore_vars}}\" - name : wait for analysis to compelte command : \"anchore-cli image content {{ scan_image_name }} os\" register : analysis until : analysis.rc != 1 retries : 10 delay : 30 ignore_errors : yes environment : \"{{anchore_vars}}\" - name : vulnerabilities results command : \"anchore-cli image vuln {{ scan_image_name }} os\" register : vuln_output environment : \"{{anchore_vars}}\" - name : \"vulnerabilities in {{ scan_image_name }}\" debug : msg : \"{{ vuln_output.stdout_lines }}\"","title":"Anchore CLI scanner"},{"location":"learning/ansible/security_hardening/#scheduled-scans-using-ansible-tower-for-operating-systems-and-kernel-security","text":"While most of the discussed tools can be used for scanning and maintaining a benchmark for security, we should think about the entire process of the incident response and threat detection workflow: Preparation Detection and analysis Containment, eradication, and recovery Post-incident activity Setting up all such scanners is our preparation. Using the output of these scanners gives us the ability to detect and analyze. Both containment and recovery are beyond the scope of such tools. For the process of recovery and post-incident activity, you may want to consider playbooks that can trash the current infrastructure and recreate it as it is. As part of our preparation, it may be useful to get familiar with the following terms as you will see them being used repeatedly in the world of vulnerability scanners and vulnerability management tools: Term: Full form (if any): Description of the term CVE: Common Vulnerabilities and Exposures: It is a list of cybersecurity vulnerability identifiers. Usage typically includes CVE IDs. OVAL: Open Vulnerability and Assessment Language: A language for finding out and naming vulnerabilities and configuration issues in computer systems. CWE: Common Weakness Enumeration: A common list of software security weaknesses. NVD: National Vulnerability Database: A US government vulnerability management database available for public use in XML format.","title":"Scheduled scans using Ansible Tower for operating systems and kernel security"},{"location":"learning/ansible/security_hardening/#vuls--vulnerability-scanner","text":"Vuls is an agent-less scanner written in golang. It supports a different variety of Linux operating systems. It performs the complete end-to-end security system administrative tasks such as scanning for security vulnerabilities and security software updates. It analyzes the system for required security vulnerabilities, performs security risk analysis based on the CVE score, sends notifications via Slack and email, and also provides a simple web report with historical data. The playbook has mainly two roles for setting up vuls using Docker containers. vuls_containers_download vuls_database_download - name : setting up vuls using docker containers hosts : vuls become : yes roles : - vuls_containers_download - vuls_database_download # Pulling the Docker containers locally using the docker_image module: - name : pulling containers locally docker_image : name : \"{{ item }}\" pull : yes with_items : - vuls/go-cve-dictionary - vuls/goval-dictionary - vuls/vuls # Then downloading the CVE and OVAL databases for the required operating systems and distributions versions - name : fetching NVD database locally docker_container : name : \"cve-{{ item }}\" image : vuls/go-cve-dictionary auto_remove : yes interactive : yes state : started command : fetchnvd -years \"{{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/go-cve-dictionary-log:/var/log/vuls\" with_sequence : start=2002 end=\"{{ nvd_database_years }}\" - name : fetching redhat oval data docker_container : name : \"redhat-oval-{{ item }}\" image : vuls/goval-dictionary auto_remove : yes interactive : yes state : started command : fetch-redhat \"{{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/goval-dictionary-log:/var/log/vuls\" with_items : \"{{ redhat_oval_versions }}\" - name : fetching ubuntu oval data docker_container : name : \"ubuntu-oval-{{ item }}\" image : vuls/goval-dictionary auto_remove : yes interactive : yes state : started command : \"fetch-ubuntu {{ item }}\" volumes : - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/goval-dictionary-log:/var/log/vuls\" with_items : \"{{ ubuntu_oval_versions }}\" The global variables file looks as follows. We can add more redhat_oval_versions, such as 5. The nvd_database_years will download the CVE database up until the end of 2017: vuls_data_directory : \"/vuls_data\" nvd_database_years : 2017 redhat_oval_versions : - 6 - 7 ubuntu_oval_versions : - 12 - 14 - 16 Now, it's time to perform the scanning and reporting using the vuls Docker containers. The following playbook contains simple steps to perform the vuls scan against virtual machines and containers, and send the report to slack and web: - name : scanning and reporting using vuls hosts : vuls become : yes vars : vuls_data_directory : \"/vuls_data\" slack_web_hook_url : https://hooks.slack.com/services/XXXXXXX/XXXXXXXXXXXXXXXXXXXXX slack_channel : \"#vuls\" slack_emoji : \":ghost:\" server_to_scan : 192.168.33.80 server_username : vagrant server_key_file_name : 192-168-33-80 tasks : - name : copying configuraiton file and ssh keys template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" mode : 0400 with_items : - { src : 'config.toml' , dst : '/root/config.toml' } - { src : '192-168-33-80' , dst : '/root/.ssh/192-168-33-80' } - name : running config test docker_container : name : configtest image : vuls/vuls auto_remove : yes interactive : yes state : started command : configtest -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - name : running vuls scanner docker_container : name : vulsscan image : vuls/vuls auto_remove : yes interactive : yes state : started command : scan -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - \"/etc/localtime:/etc/localtime:ro\" env : TZ : \"Asia/Kolkata\" - name : sending slack report docker_container : name : vulsreport image : vuls/vuls auto_remove : yes interactive : yes state : started command : report -cvedb-path=/vuls/cve.sqlite3 -ovaldb-path=/vuls/oval.sqlite3 --to-slack -config=/root/config.toml volumes : - \"/root/.ssh:/root/.ssh:ro\" - \"{{ vuls_data_directory }}:/vuls\" - \"{{ vuls_data_directory }}/vuls-log:/var/log/vuls\" - \"/root/config.toml:/root/config.toml:ro\" - \"/etc/localtime:/etc/localtime:ro\" - name : vuls webui report docker_container : name : vulswebui image : vuls/vulsrepo interactive : yes volumes : - \"{{ vuls_data_directory }}:/vuls\" ports : - \"80:5111\" The following file is the configuration file for vuls to perform the scanning. This holds the configuration for slack alerting and also the server to perform scanning. This can be configured very effectively as required using vuls documentation: [ slack ] hookURL = \"{{ slack_web_hook_url}}\" channel = \"{{ slack_channel }}\" iconEmoji = \"{{ slack_emoji }}\" [servers] [servers.{{ server_key_file_name }}] host = \"{{ server_to_scan }}\" user = \"{{ server_username }}\" keyPath = \"/root/.ssh/{{ server_key_file_name }}\" We can also visit the web UI interface of the vuls server IP address to see the detailed results in tabular and portable format. This is very useful to manage large amount of servers and patches at scale. This can be part of the CI/CD life cycle as an infrastructure code and then we can run this as a scheduled scan using Ansible Tower or Jenkins.","title":"Vuls \u2013 vulnerability scanner"},{"location":"learning/ansible/security_hardening/#scheduled-scans-for-file-integrity-checks-host-level-monitoring-using-ansible-for-various-compliance-initiatives","text":"One of the many advantages of being able to execute commands on the host using Ansible is the ability to get internal system information, such as: File hashes Network connections List of running processes It can act as a lightweight Host-Based Intrusion Detection System (HIDS). While this may not eliminate the case for a purpose-built HIDS in many cases, we can execute the same kind of security tasks using a tool such as Facebook's osquery along with Ansible.","title":"Scheduled scans for file integrity checks, host-level monitoring using Ansible for various compliance initiatives"},{"location":"learning/ansible/security_hardening/#osquery","text":"osquery is an operating system instrumentation framework by Facebook and written in C++, that supports Windows, Linux, OS X (macOS), and other operating systems. It provides an interface to query an operating system using an SQL like syntax. By using this, we can perform low-level activities such as running processes, kernel configurations, network connections, and file integrity checks. Overall it's like a host-based intrusion detection system (HIDS) endpoint security. It provides osquery as a service, system interactive shell, and so on. Hence we can use this to perform centralized monitoring and security management solutions. This playbook is to set up and configure the osquery agent in your Linux servers to monitor and look for vulnerabilities, file integrity monitoring, and many other compliance activities, and then log them for sending to a centralized logging monitoring system. The reference tutorial can be followed at DigitalOcean . - name : setting up osquery hosts : linuxservers become : yes tasks : - name : installing osquery apt : deb : https://pkg.osquery.io/deb/osquery_2.10.2_1.linux.amd64.deb update_cache : yes - name : adding osquery configuration template : src : \"{{ item.src }}\" dest : \"{{ item.dst }}\" with_items : - { src : fim.conf , dst : /usr/share/osquery/packs/fim.conf } - { src : osquery.conf , dst : /etc/osquery/osquery.conf } - name : starting and enabling osquery service service : name : osqueryd state : started enabled : yes The following fim.conf code snippet is the pack for file integrity monitoring and it monitors for file events in the /home, /etc, and /tmp directories every 300 seconds. It uses Secure Hash Algorithm (SHA) checksum to validate the changes. This can be used to find out whether attackers add their own SSH keys or audit log changes against system configuration changes for compliance and other activities. { \"queries\" : { \"file_events\" : { \"query\" : \"select * from file_events;\" , \"removed\" : false , \"interval\" : 300 } }, \"file_paths\" : { \"homes\" : [ \"/root/.ssh/%%\" , \"/home/%/.ssh/%%\" ], \"etc\" : [ \"/etc/%%\" ], \"home\" : [ \"/home/%%\" ], \"tmp\" : [ \"/tmp/%%\" ] } } The following code snippet is the osquery service configuration. This can be modified as required to monitor and log by osquery service. { \"options\" : { \"config_plugin\" : \"filesystem\" , \"logger_plugin\" : \"filesystem\" , \"logger_path\" : \"/var/log/osquery\" , \"disable_logging\" : \"false\" , \"log_result_events\" : \"true\" , \"schedule_splay_percent\" : \"10\" , \"pidfile\" : \"/var/osquery/osquery.pidfile\" , \"events_expiry\" : \"3600\" , \"database_path\" : \"/var/osquery/osquery.db\" , \"verbose\" : \"false\" , \"worker_threads\" : \"2\" , \"enable_monitor\" : \"true\" , \"disable_events\" : \"false\" , \"disable_audit\" : \"false\" , \"audit_allow_config\" : \"true\" , \"host_identifier\" : \"hostname\" , \"enable_syslog\" : \"true\" , \"audit_allow_sockets\" : \"true\" , \"schedule_default_interval\" : \"3600\" }, \"schedule\" : { \"crontab\" : { \"query\" : \"SELECT * FROM crontab;\" , \"interval\" : 300 }, \"system_profile\" : { \"query\" : \"SELECT * FROM osquery_schedule;\" }, \"system_info\" : { \"query\" : \"SELECT hostname, cpu_brand, physical_memory FROM system_info;\" , \"interval\" : 3600 } }, \"decorators\" : { \"load\" : [ \"SELECT uuid AS host_uuid FROM system_info;\" , \"SELECT user AS username FROM logged_in_users ORDER BY time DESC LIMIT 1;\" ] }, \"packs\" : { \"fim\" : \"/usr/share/osquery/packs/fim.conf\" , \"osquery-monitoring\" : \"/usr/share/osquery/packs/osquery-monitoring.conf\" , \"incident-response\" : \"/usr/share/osquery/packs/incident-response.conf\" , \"it-compliance\" : \"/usr/share/osquery/packs/it-compliance.conf\" , \"vuln-management\" : \"/usr/share/osquery/packs/vuln-management.conf\" } } The goal is not just setting up osquery, we can use the logs to build a centralized real-time monitoring system using our Elastic stack. We can use the Filebeat agent to forward these logs to our Elastic stack and we can view them and build a centralized dashboard for alerting and monitoring. This idea can be extended for building some automated defences by taking actions against attacks by using automated Ansible playbooks for known actions. The world is moving toward containers and this kind of monitoring gives us a look at low-level things such as kernel security checks, and file integrity checks on host level. When attackers try to bypass containers and get access to hosts to escalate privileges, we can detect and defend them using this kind of setup.","title":"osquery"},{"location":"learning/ansible/security_hardening/#summary","text":"Containers are rapidly changing the world of developers and operations teams. The rate of change is accelerating, and in this new world, security automation gets to play a front and center role. By leveraging our knowledge of using Ansible for scripting play-by-play commands along with excellent tools such as Anchore and osquery , we can measure, analyze, and benchmark our containers for security. This allows us to build end-to-end automatic processes of securing, scanning and remediating containers.","title":"Summary"},{"location":"learning/ansible/security_hardening/#automating-lab-setups-for-forensics-collection-and-malware-analysis","text":"Malware is one of the biggest challenges faced by the security community. It impacts everyone who gets to interact with information systems. While there is a massive effort required in keeping computers safe from malware for operational systems, a big chunk of work in malware defenses is about understanding where they come from and what they are capable of. Another important aspect of malware analysis is the ability to collaborate and share threats using the Malware Information Sharing Platform (MISP). One of the initial phases of malware analysis is identification and classification. The most popular source is using VirusTotal to scan and get the results of the malware samples, domain information, and so on. It has a very rich API and a lot of people have written custom apps that leverage the API to perform the automated scans using the API key for identifying the malware type. It generally checks using more than 60 antivirus scanners and tools and provides detailed information.","title":"Automating Lab Setups for Forensics Collection and Malware Analysis"},{"location":"learning/ansible/security_hardening/#virustotal--api-tool-set-up","text":"The following playbook will set up the VirusTotal API tool - name : setting up VirusTotal hosts : malware remote_user : ubuntu become : yes tasks : - name : installing pip apt : name : \"{{ item }}\" with_items : - python-pip - unzip - name : checking if vt already exists stat : path : /usr/local/bin/vt register : vt_status - name : downloading VirusTotal api tool repo unarchive : src : \"https://github.com/doomedraven/VirusTotalApi/archive/master.zip\" dest : /tmp/ remote_src : yes when : vt_status.stat.exists == False - name : installing the dependencies pip : requirements : /tmp/VirusTotalApi-master/requirements.txt when : vt_status.stat.exists == False - name : installing vt command : python /tmp/VirusTotalApi-master/setup.py install when : vt_status.stat.exists == False The playbook execution will download the repository and set up the VirusTotal API tool. The following playbook will find and copy the local malware samples to a remote system and scan them recursively and return the results. Once the scan has been completed, it will remove the samples from the remote system. - name : scanning file in VirusTotal hosts : malware remote_user : ubuntu vars : vt_api_key : XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX #use Ansible-vault vt_api_type : public # public/private vt_intelligence_access : False # True/False files_in_local_system : /tmp/samples/ files_in_remote_system : /tmp/sample-file/ tasks : - name : creating samples directory file : path : \"{{ files_in_remote_system }}\" state : directory - name : copying file to remote system copy : src : \"{{ files_in_local_system }}\" dest : \"{{ files_in_remote_system }}\" directory_mode : yes - name : copying configuration template : src : config.j2 dest : \"{{ files_in_remote_system }}/.vtapi\" - name : running VirusTotal scan command : \"vt -fr {{ files_in_remote_system }}\" args : chdir : \"{{ files_in_remote_system }}\" register : vt_scan - name : removing the samples file : path : \"{{ files_in_remote_system }}\" state : absent - name : VirusTotal scan results debug : msg : \"{{ vt_scan.stdout_lines }}\"","title":"VirusTotal  API tool set up"},{"location":"learning/ansible/security_hardening/#creating-ansible-playbooks-for-collection-and-storage-with-secure-backup-of-forensic-artifacts","text":"Ansible is an apt replacement for all kinds of bash scripts. Typically, for most activities that require analysis, we follow a set pattern: Collect logs from running processes into files with a path we already know Copy the content from these log files periodically to a secure storage locally or accessible remotely over SSH or a network file share Once copied successfully, rotate the logs Since there is a bit of network activity involved, our bash scripts are usually written to be fault tolerant with regard to network connections and become complex very soon. Ansible playbooks can be used to do all of that while being simple to read for everyone.","title":"Creating Ansible playbooks for collection and storage with secure backup of forensic artifacts"},{"location":"learning/ansible/security_hardening/#collecting-log-artifacts-for-incident-response","text":"The key phase in incident response is log analysis. This playbook will collect the logs from all the hosts and store it locally. This allows responders to perform the further analysis. # Reference https://www.Ansible.com/security-automation-with-Ansible - name : Gather log files hosts : servers become : yes tasks : - name : List files to grab find : paths : - /var/log patterns : - '*.log*' recurse : yes register : log_files - name : Grab files fetch : src : \"{{ item.path }}\" dest : \"/tmp/LOGS_{{ Ansible_fqdn }}/\" with_items : \"{{ log_files.files }}\" This playbook execution will collect a list of logs in specified locations in remote hosts using Ansible modules and store them in the local system.","title":"Collecting log artifacts for incident response"},{"location":"learning/ansible/security_hardening/#secure-backups-for-data-collection","text":"When collecting multiple sets of data from servers, it's important to store them securely with encrypted backups. This can be achieved by backing up the data to storage services such as S3. This Ansible playbook allows us to install and copy the collected data to the AWS S3 service with encryption enabled. - name : backing up the log data hosts : localhost gather_facts : false become : yes vars : s3_access_key : XXXXXXX # Use Ansible-vault to encrypt s3_access_secret : XXXXXXX # Use Ansible-vault to encrypt localfolder : /tmp/LOGS/ # Trailing slash is important remotebucket : secretforensicsdatausingAnsible # This should be unique in s3 tasks : - name : installing s3cmd if not installed apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - python-magic - python-dateutil - s3cmd - name : create s3cmd config file template : src : s3cmd.j2 dest : /root/.s3cfg owner : root group : root mode : 0640 - name : make sure \"{{ remotebucket }}\" is avilable command : \"s3cmd mb s3://{{ remotebucket }}/ -c /root/.s3cfg\" - name : running the s3 backup to \"{{ remotebucket }}\" command : \"s3cmd sync {{ localfolder }} --preserve s3://{{ remotebucket }}/ -c /root/.s3cfg\" The Ansible playbook installing s3cmd, creating the new bucket called secretforensicsdatausingAnsible, and copying the local log data to the remote S3 bucket. The configuration file looks like the following for the s3cmd configuration [ default ] access_key = {{ s3_access_key }} secret_key = {{ s3_access_secret }} host_base = s3.amazonaws.com host_bucket = %(bucket)s.s3.amazonaws.com website_endpoint = http://%(bucket)s.s3-website-%(location)s.amazonaws.com/ use_https = True signature_v2 = True We can see that the logs are successfully uploaded into the secretforensicsdatausingAnsible S3 bucket in AWS S3.","title":"Secure backups for data collection"},{"location":"learning/ansible/security_testing/","text":"Automating Web Application Security Testing Using OWASP ZAP \u00b6 The OWASP Zed Attack Proxy (commonly known as ZAP) is one of the most popular web application security testing tools. It has many features that allow it to be used for manual security testing; it also fits nicely into continuous integration/continuous delivery (CI/CD) environments after some tweaking and configuration. More details about the project can be found at https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project . Open Web Application Security Project (OWASP) is a worldwide not-for-profit charitable organization focused on improving the security of software. OWASP ZAP includes many different tools and features in one package. For a pentester tasked with doing the security testing of web applications, the following features are invaluable Feature: Use case Intercepting proxy: This allows us to intercept requests and responses in the browser Active scanner: Automatically run web security scans against targets Passive scanner: Glean information about security issues from pages that get downloaded using spider tools and so on Spiders: Before ZAP can attack an application, it creates a site map of the application by crawling all the possible web pages on it REST API: Allows ZAP to be run in headless mode and to be controlled for running automated scanner, spider, and get the results ZAP is a Java-based software. The typical way of using it will involve the following: Java Runtime Environment (JRE) 7 or more recent installed in the operating system of your choice (macOS, Windows, Linux) Install ZAP using package managers, installers from the official downloads page The best way to achieve that is to use OWASP ZAP as a container. In fact, this is the kind of setup Mozilla uses ZAP in a CI/CD pipeline to verify the baseline security controls at every release. Installing OWASP ZAP \u00b6 We are going to use OWASP ZAP as a container, which requires container runtime in the host operating system. The team behind OWASP ZAP releases ZAP Docker images on a weekly basis via Docker Hub. The approach of pulling Docker images based on tags is popular in modern DevOps environments and it makes sense that we talk about automation with respect to that. Installing Docker runtime \u00b6 # The following playbook will install Docker Community Edition software in Ubuntu 16.04 - name : installing docker on ubuntu hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu apt_repo_data : \"deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable\" apt_gpg_key : https://download.docker.com/linux/ubuntu/gpg tasks : - name : adding docker gpg key apt_key : url : \"{{ apt_gpg_key }}\" state : present - name : add docker repository apt_repository : repo : \"{{ apt_repo_data }}\" state : present - name : installing docker-ce apt : name : docker-ce state : present update_cache : yes - name : install python-pip apt : name : python-pip state : present - name : install docker-py pip : name : \"{{ item }}\" state : present with_items : - docker-py OWASP ZAP Docker container setup \u00b6 The two new modules to deal with Docker containers that we will be using here are docker_image and docker_container. # The following playbook will take some time to complete as it has to download about 1 GB of data from the internet - name : setting up owasp zap container hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly tasks : - name : pulling {{ owasp_zap_image_name }} container docker_image : name : \"{{ owasp_zap_image_name }}\" - name : running owasp zap container docker_container : name : owasp-zap image : \"{{ owasp_zap_image_name }}\" interactive : yes state : started user : zap command : zap.sh -daemon -host 0.0.0.0 -port 8090 -config api.disablekey=true -config api.addrs.addr.name=.* -config api.addrs.addr.regex=true ports : - \"8090:8090\" You can access the ZAP API interface by navigating to http://ZAPSERVERIPADDRESS:8090 A specialized tool for working with Containers - Ansible Container \u00b6 Currently, we are using Docker modules to perform container operations. A new tool, ansible-container, provides an Ansible-centric workflow for building, running, testing, and deploying containers. This allows us to build, push, and run containers using existing playbooks. Dockerfiles are like writing shell scripts, therefore, ansible-container will allow us to codify those Dockerfiles and build them using existing playbooks rather writing complex scripts. The ansible-container supports various orchestration tools, such as Kubernetes and OpenShift. It can also be used to push the build images to private registries such as Google Container Registry and Docker Hub. Running an OWASP ZAP Baseline scan \u00b6 The following playbook runs the Docker Baseline scan against a given website URL. It also stores the output of the Baseline's scan in the host system in HTML, Markdown, and XML formats. - name : Running OWASP ZAP Baseline Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-base-line-scan-dvws tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0770 - name : running owasp zap baseline scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-baseline.py -t {{ website_url }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan command : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\" Explore the parameters of the preceding playbook: website_url is the domain (or) URL that you want to perform the Baseline scan, we can pass this via \u2013extra-vars \"website_url: http://192.168.33.111 \" from the ansible-playbook command reports_location is the path to ZAP host machine where reports get stored Security testing against web applications and websites \u00b6 An active scan may cause the vulnerability to be exploited in the application. Also, this type of scan requires extra configuration, which includes authentication and sensitive functionalities. The following playbook will run the full scan against the DVWS application. Now we can see that the playbook looks almost similar, except the flags sent to command: - name : Running OWASP ZAP Full Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-full-scan-dvws tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0777 - name : running owasp zap full scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-full-scan.py -t {{ website_url }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan raw : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\" Testing web APIs \u00b6 Similar to the ZAP Baseline scan, the fine folks behind ZAP provide a script as part of their live and weekly Docker images. We can use it to run scans against API endpoints defined either by OpenAPI specification or Simple Object Access Protocol (SOAP). The script can understand the API specifications and import all the definitions. Based on this, it runs an active scan against all the URLs found. - name : Running OWASP ZAP API Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-api-scan-dvws api_type : openapi tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0777 - name : running owasp zap api scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-api-scan.py -t {{ website_url }} -f {{ api_type }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan raw : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\" Vulnerability Scanning with Nessus \u00b6 Scanning for vulnerabilities is one of the best understood periodic activities security teams take up on their computers. There are well-documented strategies and best practices for doing regular scanning for vulnerabilities in computers, networks, operating system software, and application software: Basic network scans Credentials patch audit Correlating system information with known vulnerabilities With networked systems, this type of scanning is usually executed from a connected host that has the right kind of permissions to scan for security issues. One of the most popular vulnerability scanning tools is Nessus. Nessus started as a network vulnerability scanning tool, but now incorporates features such as the following: Port scanning Network vulnerability scanning Web application-specific scanning Host-based vulnerability scanning Introduction to Nessus \u00b6 The vulnerability database that Nessus has is its main advantage. While the techniques to understanding which service is running and what version of the software is running the service are known to us, answering the question, \"Does this service have a known vulnerability\" is the important one. Apart from a regularly updated vulnerability database, Nessus also has information on default credentials found in applications, default paths, and locations. All of this fine-tuned in an easy way to use CLI or web-based tool. We will try out the standard activities required for that and see what steps are needed to automate them using Ansible. Installing Nessus using a playbook. Configuring Nessus. Running a scan. Running a scan using AutoNessus. Installing the Nessus REST API Python client. Downloading a report using the API. Installing Nessus for vulnerability assessments \u00b6 - name : installing nessus server hosts : nessus remote_user : \"{{ remote_user_name }}\" gather_facts : no vars : remote_user_name : ubuntu nessus_download_url : \"http://downloads.nessus.org/nessus3dl.php?file=Nessus-6.11.2-ubuntu1110_amd64.deb&licence_accept=yes&t=84ed6ee87f926f3d17a218b2e52b61f0\" tasks : - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : downloading the package and installing apt : deb : \"{{ nessus_download_url }}\" - name : start the nessus daemon service : name : \"nessusd\" enabled : yes state : started Configuring Nessus for vulnerability scanning \u00b6 Perform the following steps to configure Nessus for vulnerability scanning: We have to navigate to https://NESSUSSERVERIP:8834 to confirm and start the service It returns with an SSL error and we need to accept the SSL error and confirm the security exception and continue with the installation Click on Confirm Security Exception and continue to proceed with the installation steps. Click on Continue and provide the details of the user, this user has full administrator access. Then finally, we have to provide the registration code (Activation Code), which can be obtained from registering at https://www.tenable.com/products/nessus-home Now it will install the required plugins. It will take a while to install, and once it is done we can log in to use the application. Now, we have successfully set up the Nessus vulnerability scanner. Basic network scanning \u00b6 Nessus has a wide variety of scans, some of them are free and some of them will be available only in a paid version. So, we can also customize the scanning if required. We can start with a basic network scan to see what's happening in the network. This scan will perform a basic full system scan for the given hosts. We have to mention the scan name and targets. Targets are just the hosts we want. Targets can be given in different formats, such as 192.168.33.1 for a single host, 192.168.33.1-10 for a range of hosts, and also we can upload the target file from our computer. Running a scan using AutoNessus \u00b6 With the AutoNessus script, we can do the following: List scans List scan policies Do actions on scans such as start, stop, pause, and resume The best part of AutoNessus is that since this is a command-line tool, it can easily become part of scheduled tasks and other automation workflows. Setting up AutoNessus \u00b6 The following code is the Ansible playbook snippet to set up AutoNessus and configure it to use Nessus using credentials. - name : installing python-pip apt : name : python-pip update_cache : yes state : present - name : install python requests pip : name : requests - name : setting up autonessus get_url : url : \"https://github.com/redteamsecurity/AutoNessus/raw/master/autoNessus.py\" dest : /usr/bin/autoNessus mode : 0755 - name : updating the credentials replace : path : /usr/bin/autoNessus regexp : \"{{ item.src }}\" replace : \"{{ item.dst }}\" backup : yes no_log : True with_items : - { src : \"token = ''\" , dst : \"token = '{{ nessus_user_token }}'\" } - { src : \"url = 'https://localhost:8834'\" , dst : \"url = '{{ nessus_url }}'\" } - { src : \"username = 'xxxxx'\" , dst : \"username = '{{ nessus_user_name }}'\" } - { src : \"password = 'xxxxx'\" , dst : \"password = '{{ nessus_user_password }}'\" } no_log : True will censor the output in the log console of Ansible output. It will be very useful when we are using secrets and keys inside playbooks. Before running the automated scans using AutoNessus, we have to create them in the Nessus portal with required customization, and we can use these automated playbooks to perform tasks on top of it. Listing current available scans and IDs \u00b6 - name : list current scans and IDs using autoNessus command : \"autoNessus -l\" register : list_scans_output - debug : msg : \"{{ list_scans_output.stdout_lines }}\" Starting a specified scan using scan ID \u00b6 - name : starting nessus scan \"{{ scan_id }}\" using autoNessus command : \"autoNessus -sS {{ scan_id }}\" register : start_scan_output - debug : msg : \"{{ start_scan_output.stdout_lines }}\" - Similarly, we can perform pause, resume, stop, list policies, and so on. Using the AutoNessus program, these playbooks are available. This can be improved by advancing the Nessus API scripts. Storing results \u00b6 The entire report can be exported into multiple formats, such as HTML, CSV, and Nessus. This helps to give more a detailed structure of vulnerabilities found, solutions with risk rating, and other references The output report can be customized based on the audience, if it goes to the technical team, we can list all the vulnerabilities and remediation. For example, if management wants to get the report, we can only get the executive summary of the issues. Reports can be sent by email as well using notification options in Nessus configuration. Installing the Nessus REST API Python client \u00b6 Official API documentation can be obtained by connecting to your Nessus server under 8834/nessus6-api.html. To perform any operations using the Nessus REST API, we have to obtain the API keys from the portal. This can be found in user settings. Please make sure to save these keys Downloading reports using the Nessus REST API \u00b6 - name : working with nessus rest api connection : local hosts : localhost gather_facts : no vars : scan_id : 17 nessus_access_key : 620fe4ffaed47e9fe429ed749207967ecd7a77471105d8 nessus_secret_key : 295414e22dc9a56abc7a89dab713487bd397cf860751a2 nessus_url : https://192.168.33.109:8834 nessus_report_format : html tasks : - name : export the report for given scan \"{{ scan_id }}\" uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export\" method : POST validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" body : \"format={{ nessus_report_format }}&chapters=vuln_by_host;remediations\" register : export_request - debug : msg : \"File id is {{ export_request.json.file }} and scan id is {{ scan_id }}\" - name : check the report status for \"{{ export_request.json.file }}\" uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export/{{ export_request.json.file }}/status\" method : GET validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" register : report_status - debug : msg : \"Report status is {{ report_status.json.status }}\" - name : downloading the report locally uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export/{{ export_request.json.file }}/download\" method : GET validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" return_content : yes dest : \"./{{ scan_id }}_{{ export_request.json.file }}.{{ nessus_report_format }}\" register : report_output - debug : msg : \"Report can be found at ./{{ scan_id }}_{{ export_request.json.file }}.{{ nessus_report_format }}\" Nessus configuration \u00b6 Nessus allows us to create different users with role-based authentication to perform scans and review with different access levels. Summary \u00b6 Security teams and IT teams rely on tools for vulnerability scanning, management, remediation, and continuous security processes. Nessus, by being one of the most popular and useful tools, was an automatic choice for the authors to try and automate. Writing an Ansible Module for Security Testing \u00b6 Ansible primarily works by pushing small bits of code to the nodes it connects to. These codes/programs are what we know as Ansible modules. Typically in the case of a Linux host these are copied over SSH, executed, and then removed from the node. We will look at the following: How to set up the development environment -Writing an Ansible hello world module to understand the basics -Where to seek further help -Defining a security problem statement -Addressing that problem by writing a module of our own Along with that, we will try to understand and attempt to answer the following questions: What are the good use cases for modules? When does it make sense to use roles? How do modules differ from plugins? Getting started with a hello world Ansible module \u00b6 We will pass one argument to our custom module and show if we have success or failure for the module executing based on that. Since all of this is new to us, we will look at the following things: The source code of the hello world module The output of that module for both success and failure The command that we will use to invoke it Setting up the development environment \u00b6 The primary requirement for Ansible 2.4 is Python 2.6 or higher and Python 3.5 or higher. If you have either of them installed, we can follow the simple steps to get the development environment going. From the Ansible Developer Guide: Clone the Ansible repository: $ git clone ansible/ansible.git Change the directory into the repository root directory: $ cd ansible Create a virtual environment: $ python3 -m venv venv (or for Python 2 $ virtualenv venv Note, this requires you to install the virtualenv package: $ pip install virtualenv Activate the virtual environment: $ . venv/bin/activate Install the development requirements: $ pip install -r requirements.txt Run the environment setup script for each new dev shell process: $ . hacking/env-setup This playbook will set up the developer environment by installing and setting up the virtual environment. - name : Setting Developer Environment hosts : dev remote_user : madhu become : yes vars : ansible_code_path : \"/home/madhu/ansible-code\" tasks : - name : installing prerequirements if not installed apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - git - virtualenv - python-pip - name : downloading ansible repo locally git : repo : https://github.com/ansible/ansible.git dest : \"{{ ansible_code_path }}/venv\" - name : creating virtual environment pip : virtualenv : \"{{ ansible_code_path }}\" virtualenv_command : virtualenv requirements : \"{{ ansible_code_path }}/venv/requirements.txt\"","title":"Automating Web Application Security Testing Using OWASP ZAP"},{"location":"learning/ansible/security_testing/#automating-web-application-security-testing-using-owasp-zap","text":"The OWASP Zed Attack Proxy (commonly known as ZAP) is one of the most popular web application security testing tools. It has many features that allow it to be used for manual security testing; it also fits nicely into continuous integration/continuous delivery (CI/CD) environments after some tweaking and configuration. More details about the project can be found at https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project . Open Web Application Security Project (OWASP) is a worldwide not-for-profit charitable organization focused on improving the security of software. OWASP ZAP includes many different tools and features in one package. For a pentester tasked with doing the security testing of web applications, the following features are invaluable Feature: Use case Intercepting proxy: This allows us to intercept requests and responses in the browser Active scanner: Automatically run web security scans against targets Passive scanner: Glean information about security issues from pages that get downloaded using spider tools and so on Spiders: Before ZAP can attack an application, it creates a site map of the application by crawling all the possible web pages on it REST API: Allows ZAP to be run in headless mode and to be controlled for running automated scanner, spider, and get the results ZAP is a Java-based software. The typical way of using it will involve the following: Java Runtime Environment (JRE) 7 or more recent installed in the operating system of your choice (macOS, Windows, Linux) Install ZAP using package managers, installers from the official downloads page The best way to achieve that is to use OWASP ZAP as a container. In fact, this is the kind of setup Mozilla uses ZAP in a CI/CD pipeline to verify the baseline security controls at every release.","title":"Automating Web Application Security Testing Using OWASP ZAP"},{"location":"learning/ansible/security_testing/#installing-owasp-zap","text":"We are going to use OWASP ZAP as a container, which requires container runtime in the host operating system. The team behind OWASP ZAP releases ZAP Docker images on a weekly basis via Docker Hub. The approach of pulling Docker images based on tags is popular in modern DevOps environments and it makes sense that we talk about automation with respect to that.","title":"Installing OWASP ZAP"},{"location":"learning/ansible/security_testing/#installing-docker-runtime","text":"# The following playbook will install Docker Community Edition software in Ubuntu 16.04 - name : installing docker on ubuntu hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu apt_repo_data : \"deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable\" apt_gpg_key : https://download.docker.com/linux/ubuntu/gpg tasks : - name : adding docker gpg key apt_key : url : \"{{ apt_gpg_key }}\" state : present - name : add docker repository apt_repository : repo : \"{{ apt_repo_data }}\" state : present - name : installing docker-ce apt : name : docker-ce state : present update_cache : yes - name : install python-pip apt : name : python-pip state : present - name : install docker-py pip : name : \"{{ item }}\" state : present with_items : - docker-py","title":"Installing Docker runtime"},{"location":"learning/ansible/security_testing/#owasp-zap-docker-container-setup","text":"The two new modules to deal with Docker containers that we will be using here are docker_image and docker_container. # The following playbook will take some time to complete as it has to download about 1 GB of data from the internet - name : setting up owasp zap container hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly tasks : - name : pulling {{ owasp_zap_image_name }} container docker_image : name : \"{{ owasp_zap_image_name }}\" - name : running owasp zap container docker_container : name : owasp-zap image : \"{{ owasp_zap_image_name }}\" interactive : yes state : started user : zap command : zap.sh -daemon -host 0.0.0.0 -port 8090 -config api.disablekey=true -config api.addrs.addr.name=.* -config api.addrs.addr.regex=true ports : - \"8090:8090\" You can access the ZAP API interface by navigating to http://ZAPSERVERIPADDRESS:8090","title":"OWASP ZAP Docker container setup"},{"location":"learning/ansible/security_testing/#a-specialized-tool-for-working-with-containers---ansible-container","text":"Currently, we are using Docker modules to perform container operations. A new tool, ansible-container, provides an Ansible-centric workflow for building, running, testing, and deploying containers. This allows us to build, push, and run containers using existing playbooks. Dockerfiles are like writing shell scripts, therefore, ansible-container will allow us to codify those Dockerfiles and build them using existing playbooks rather writing complex scripts. The ansible-container supports various orchestration tools, such as Kubernetes and OpenShift. It can also be used to push the build images to private registries such as Google Container Registry and Docker Hub.","title":"A specialized tool for working with Containers - Ansible Container"},{"location":"learning/ansible/security_testing/#running-an-owasp-zap-baseline-scan","text":"The following playbook runs the Docker Baseline scan against a given website URL. It also stores the output of the Baseline's scan in the host system in HTML, Markdown, and XML formats. - name : Running OWASP ZAP Baseline Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-base-line-scan-dvws tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0770 - name : running owasp zap baseline scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-baseline.py -t {{ website_url }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan command : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\" Explore the parameters of the preceding playbook: website_url is the domain (or) URL that you want to perform the Baseline scan, we can pass this via \u2013extra-vars \"website_url: http://192.168.33.111 \" from the ansible-playbook command reports_location is the path to ZAP host machine where reports get stored","title":"Running an OWASP ZAP Baseline scan"},{"location":"learning/ansible/security_testing/#security-testing-against-web-applications-and-websites","text":"An active scan may cause the vulnerability to be exploited in the application. Also, this type of scan requires extra configuration, which includes authentication and sensitive functionalities. The following playbook will run the full scan against the DVWS application. Now we can see that the playbook looks almost similar, except the flags sent to command: - name : Running OWASP ZAP Full Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-full-scan-dvws tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0777 - name : running owasp zap full scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-full-scan.py -t {{ website_url }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan raw : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\"","title":"Security testing against web applications and websites"},{"location":"learning/ansible/security_testing/#testing-web-apis","text":"Similar to the ZAP Baseline scan, the fine folks behind ZAP provide a script as part of their live and weekly Docker images. We can use it to run scans against API endpoints defined either by OpenAPI specification or Simple Object Access Protocol (SOAP). The script can understand the API specifications and import all the definitions. Based on this, it runs an active scan against all the URLs found. - name : Running OWASP ZAP API Scan hosts : zap remote_user : \"{{ remote_user_name }}\" gather_facts : no become : yes vars : remote_user_name : ubuntu owasp_zap_image_name : owasp/zap2docker-weekly website_url : {{ website_url }} reports_location : /zapdata/ scan_name : owasp-zap-api-scan-dvws api_type : openapi tasks : - name : adding write permissions to reports directory file : path : \"{{ reports_location }}\" state : directory owner : root group : root recurse : yes mode : 0777 - name : running owasp zap api scan container against \"{{ website_url }}\" docker_container : name : \"{{ scan_name }}\" image : \"{{ owasp_zap_image_name }}\" interactive : yes auto_remove : yes state : started volumes : \"{{ reports_location }}:/zap/wrk:rw\" command : \"zap-api-scan.py -t {{ website_url }} -f {{ api_type }} -r {{ scan_name }}_report.html\" - name : getting raw output of the scan raw : \"docker logs -f {{ scan_name }}\" register : scan_output - debug : msg : \"{{ scan_output }}\"","title":"Testing web APIs"},{"location":"learning/ansible/security_testing/#vulnerability-scanning-with-nessus","text":"Scanning for vulnerabilities is one of the best understood periodic activities security teams take up on their computers. There are well-documented strategies and best practices for doing regular scanning for vulnerabilities in computers, networks, operating system software, and application software: Basic network scans Credentials patch audit Correlating system information with known vulnerabilities With networked systems, this type of scanning is usually executed from a connected host that has the right kind of permissions to scan for security issues. One of the most popular vulnerability scanning tools is Nessus. Nessus started as a network vulnerability scanning tool, but now incorporates features such as the following: Port scanning Network vulnerability scanning Web application-specific scanning Host-based vulnerability scanning","title":"Vulnerability Scanning with Nessus"},{"location":"learning/ansible/security_testing/#introduction-to-nessus","text":"The vulnerability database that Nessus has is its main advantage. While the techniques to understanding which service is running and what version of the software is running the service are known to us, answering the question, \"Does this service have a known vulnerability\" is the important one. Apart from a regularly updated vulnerability database, Nessus also has information on default credentials found in applications, default paths, and locations. All of this fine-tuned in an easy way to use CLI or web-based tool. We will try out the standard activities required for that and see what steps are needed to automate them using Ansible. Installing Nessus using a playbook. Configuring Nessus. Running a scan. Running a scan using AutoNessus. Installing the Nessus REST API Python client. Downloading a report using the API.","title":"Introduction to Nessus"},{"location":"learning/ansible/security_testing/#installing-nessus-for-vulnerability-assessments","text":"- name : installing nessus server hosts : nessus remote_user : \"{{ remote_user_name }}\" gather_facts : no vars : remote_user_name : ubuntu nessus_download_url : \"http://downloads.nessus.org/nessus3dl.php?file=Nessus-6.11.2-ubuntu1110_amd64.deb&licence_accept=yes&t=84ed6ee87f926f3d17a218b2e52b61f0\" tasks : - name : install python 2 raw : test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) - name : downloading the package and installing apt : deb : \"{{ nessus_download_url }}\" - name : start the nessus daemon service : name : \"nessusd\" enabled : yes state : started","title":"Installing Nessus for vulnerability assessments"},{"location":"learning/ansible/security_testing/#configuring-nessus-for-vulnerability-scanning","text":"Perform the following steps to configure Nessus for vulnerability scanning: We have to navigate to https://NESSUSSERVERIP:8834 to confirm and start the service It returns with an SSL error and we need to accept the SSL error and confirm the security exception and continue with the installation Click on Confirm Security Exception and continue to proceed with the installation steps. Click on Continue and provide the details of the user, this user has full administrator access. Then finally, we have to provide the registration code (Activation Code), which can be obtained from registering at https://www.tenable.com/products/nessus-home Now it will install the required plugins. It will take a while to install, and once it is done we can log in to use the application. Now, we have successfully set up the Nessus vulnerability scanner.","title":"Configuring Nessus for vulnerability scanning"},{"location":"learning/ansible/security_testing/#basic-network-scanning","text":"Nessus has a wide variety of scans, some of them are free and some of them will be available only in a paid version. So, we can also customize the scanning if required. We can start with a basic network scan to see what's happening in the network. This scan will perform a basic full system scan for the given hosts. We have to mention the scan name and targets. Targets are just the hosts we want. Targets can be given in different formats, such as 192.168.33.1 for a single host, 192.168.33.1-10 for a range of hosts, and also we can upload the target file from our computer.","title":"Basic network scanning"},{"location":"learning/ansible/security_testing/#running-a-scan-using-autonessus","text":"With the AutoNessus script, we can do the following: List scans List scan policies Do actions on scans such as start, stop, pause, and resume The best part of AutoNessus is that since this is a command-line tool, it can easily become part of scheduled tasks and other automation workflows.","title":"Running a scan using AutoNessus"},{"location":"learning/ansible/security_testing/#setting-up-autonessus","text":"The following code is the Ansible playbook snippet to set up AutoNessus and configure it to use Nessus using credentials. - name : installing python-pip apt : name : python-pip update_cache : yes state : present - name : install python requests pip : name : requests - name : setting up autonessus get_url : url : \"https://github.com/redteamsecurity/AutoNessus/raw/master/autoNessus.py\" dest : /usr/bin/autoNessus mode : 0755 - name : updating the credentials replace : path : /usr/bin/autoNessus regexp : \"{{ item.src }}\" replace : \"{{ item.dst }}\" backup : yes no_log : True with_items : - { src : \"token = ''\" , dst : \"token = '{{ nessus_user_token }}'\" } - { src : \"url = 'https://localhost:8834'\" , dst : \"url = '{{ nessus_url }}'\" } - { src : \"username = 'xxxxx'\" , dst : \"username = '{{ nessus_user_name }}'\" } - { src : \"password = 'xxxxx'\" , dst : \"password = '{{ nessus_user_password }}'\" } no_log : True will censor the output in the log console of Ansible output. It will be very useful when we are using secrets and keys inside playbooks. Before running the automated scans using AutoNessus, we have to create them in the Nessus portal with required customization, and we can use these automated playbooks to perform tasks on top of it.","title":"Setting up AutoNessus"},{"location":"learning/ansible/security_testing/#listing-current-available-scans-and-ids","text":"- name : list current scans and IDs using autoNessus command : \"autoNessus -l\" register : list_scans_output - debug : msg : \"{{ list_scans_output.stdout_lines }}\"","title":"Listing current available scans and IDs"},{"location":"learning/ansible/security_testing/#starting-a-specified-scan-using-scan-id","text":"- name : starting nessus scan \"{{ scan_id }}\" using autoNessus command : \"autoNessus -sS {{ scan_id }}\" register : start_scan_output - debug : msg : \"{{ start_scan_output.stdout_lines }}\" - Similarly, we can perform pause, resume, stop, list policies, and so on. Using the AutoNessus program, these playbooks are available. This can be improved by advancing the Nessus API scripts.","title":"Starting a specified scan using scan ID"},{"location":"learning/ansible/security_testing/#storing-results","text":"The entire report can be exported into multiple formats, such as HTML, CSV, and Nessus. This helps to give more a detailed structure of vulnerabilities found, solutions with risk rating, and other references The output report can be customized based on the audience, if it goes to the technical team, we can list all the vulnerabilities and remediation. For example, if management wants to get the report, we can only get the executive summary of the issues. Reports can be sent by email as well using notification options in Nessus configuration.","title":"Storing results"},{"location":"learning/ansible/security_testing/#installing-the-nessus-rest-api-python-client","text":"Official API documentation can be obtained by connecting to your Nessus server under 8834/nessus6-api.html. To perform any operations using the Nessus REST API, we have to obtain the API keys from the portal. This can be found in user settings. Please make sure to save these keys","title":"Installing the Nessus REST API Python client"},{"location":"learning/ansible/security_testing/#downloading-reports-using-the-nessus-rest-api","text":"- name : working with nessus rest api connection : local hosts : localhost gather_facts : no vars : scan_id : 17 nessus_access_key : 620fe4ffaed47e9fe429ed749207967ecd7a77471105d8 nessus_secret_key : 295414e22dc9a56abc7a89dab713487bd397cf860751a2 nessus_url : https://192.168.33.109:8834 nessus_report_format : html tasks : - name : export the report for given scan \"{{ scan_id }}\" uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export\" method : POST validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" body : \"format={{ nessus_report_format }}&chapters=vuln_by_host;remediations\" register : export_request - debug : msg : \"File id is {{ export_request.json.file }} and scan id is {{ scan_id }}\" - name : check the report status for \"{{ export_request.json.file }}\" uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export/{{ export_request.json.file }}/status\" method : GET validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" register : report_status - debug : msg : \"Report status is {{ report_status.json.status }}\" - name : downloading the report locally uri : url : \"{{ nessus_url }}/scans/{{ scan_id }}/export/{{ export_request.json.file }}/download\" method : GET validate_certs : no headers : X-ApiKeys : \"accessKey={{ nessus_access_key }}; secretKey={{ nessus_secret_key }}\" return_content : yes dest : \"./{{ scan_id }}_{{ export_request.json.file }}.{{ nessus_report_format }}\" register : report_output - debug : msg : \"Report can be found at ./{{ scan_id }}_{{ export_request.json.file }}.{{ nessus_report_format }}\"","title":"Downloading reports using the Nessus REST API"},{"location":"learning/ansible/security_testing/#nessus-configuration","text":"Nessus allows us to create different users with role-based authentication to perform scans and review with different access levels.","title":"Nessus configuration"},{"location":"learning/ansible/security_testing/#summary","text":"Security teams and IT teams rely on tools for vulnerability scanning, management, remediation, and continuous security processes. Nessus, by being one of the most popular and useful tools, was an automatic choice for the authors to try and automate.","title":"Summary"},{"location":"learning/ansible/security_testing/#writing-an-ansible-module-for-security-testing","text":"Ansible primarily works by pushing small bits of code to the nodes it connects to. These codes/programs are what we know as Ansible modules. Typically in the case of a Linux host these are copied over SSH, executed, and then removed from the node. We will look at the following: How to set up the development environment -Writing an Ansible hello world module to understand the basics -Where to seek further help -Defining a security problem statement -Addressing that problem by writing a module of our own Along with that, we will try to understand and attempt to answer the following questions: What are the good use cases for modules? When does it make sense to use roles? How do modules differ from plugins?","title":"Writing an Ansible Module for Security Testing"},{"location":"learning/ansible/security_testing/#getting-started-with-a-hello-world-ansible-module","text":"We will pass one argument to our custom module and show if we have success or failure for the module executing based on that. Since all of this is new to us, we will look at the following things: The source code of the hello world module The output of that module for both success and failure The command that we will use to invoke it","title":"Getting started with a hello world Ansible module"},{"location":"learning/ansible/security_testing/#setting-up-the-development-environment","text":"The primary requirement for Ansible 2.4 is Python 2.6 or higher and Python 3.5 or higher. If you have either of them installed, we can follow the simple steps to get the development environment going. From the Ansible Developer Guide: Clone the Ansible repository: $ git clone ansible/ansible.git Change the directory into the repository root directory: $ cd ansible Create a virtual environment: $ python3 -m venv venv (or for Python 2 $ virtualenv venv Note, this requires you to install the virtualenv package: $ pip install virtualenv Activate the virtual environment: $ . venv/bin/activate Install the development requirements: $ pip install -r requirements.txt Run the environment setup script for each new dev shell process: $ . hacking/env-setup This playbook will set up the developer environment by installing and setting up the virtual environment. - name : Setting Developer Environment hosts : dev remote_user : madhu become : yes vars : ansible_code_path : \"/home/madhu/ansible-code\" tasks : - name : installing prerequirements if not installed apt : name : \"{{ item }}\" state : present update_cache : yes with_items : - git - virtualenv - python-pip - name : downloading ansible repo locally git : repo : https://github.com/ansible/ansible.git dest : \"{{ ansible_code_path }}/venv\" - name : creating virtual environment pip : virtualenv : \"{{ ansible_code_path }}\" virtualenv_command : virtualenv requirements : \"{{ ansible_code_path }}/venv/requirements.txt\"","title":"Setting up the development environment"},{"location":"learning/ansible/troubleshooting/","text":"Troubleshooting \u00b6 It's also important to remember that, due to its nature, in Ansible code, we describe the desired state rather than stating a sequence of steps to obtain the desired state. This difference means that the system is less prone to logical errors. Nevertheless, a bug in a Playbook could mean a potential misconfiguration on all of your machines. This should be taken very seriously. It is even more critical when critical parts of the system are changed, such as SSH daemon or sudo configuration, since the risk is you locking yourself out of the system. Digging into playbook execution problems \u00b6 There are cases where an Ansible execution will interrupt. Many things can cause these situations. The single most frequent cause of problems I've found while executing Ansible playbooks is the network. Since the machine that is issuing the commands and the one that is performing them are usually linked through the network, a problem in the network will immediately show itself as an Ansible execution problem. For instance, if you run the /bin/false command, it will always return 1. To execute this in a playbook so that you can avoid it blocking there, you can write something like the following: - name : Run a command that will return 1 command : /bin/false ignore_errors : yes Be aware that this is a particular case, and often, the best approach is to fix your application so that you're following UNIX standards and return 0 if the application runs appropriately, instead of putting a workaround in your Playbooks. Using host facts to diagnose failures \u00b6 Some execution failures derive from the state of the target machine. The most common problem of this kind is the case where Ansible expects a file or variable to be present, but it's not. Sometimes, it can be enough to print the machine facts to find the problem. - hosts : target_host tasks : - name : Display all variables/facts known for a host debug : var : hostvars[inventory_hostname] This technique will give you a lot of information about the state of the target machine during Ansible execution. Testing with a playbook \u00b6 One of the most complex things in the IT field is not creating software and systems, but debugging them when they have problems. Ansible is not an exception. No matter how good you are at creating Ansible playbooks, sooner or later, you'll find yourself debugging a playbook that is not behaving as you thought it would. The simplest way of performing basic tests is to print out the values of variables during execution. - hosts : localhost tasks : - shell : /usr/bin/uptime register : result - debug : var : result The debug module is the module that allows you to print the value of a variable (by using the var option) or a fixed string (by using the msg option) during Ansible's execution. The debug module also provides the verbosity option. - hosts : localhost tasks : - shell : /usr/bin/uptime register : result - debug : var : result verbosity : 2 We set the minimum required verbosity to 2, and by default, Ansible runs with a verbosity of 0. To see the result of using the debug module with this new playbook ansible-playbook debug2.yaml -vv By putting two -v options in the command line, we will be running Ansible with verbosity of 2. This will not only affect this specific module but all the modules (or Ansible itself) that are set to behave differently at different debug levels. Using check mode \u00b6 Although you might be confident in the code you have written, it still pays to test it before running it for real in a production environment. In such cases, it is a good idea to be able to run your code, but with a safety net in place. This is what check mode is for. - hosts : localhost tasks : - name : Touch a file file : path : /tmp/myfile state : touch ansible-playbook check-mode.yaml --check Ansible check mode is usually called a dry run. The idea is that the run won't change the state of the machine and will only highlight the differences between the current status and the status declared in the playbook. Not all modules support check mode, but all major modules do, and more and more modules are being added at every release. In particular, note that the command and shell modules do not support it because it is impossible for the module to tell what commands will result in a change, and what won't. Therefore, these modules will always return changed when they're run outside of check mode because they assume a change has been made. A similar feature to check mode is the \u2013diff flag. What this flag allows us to do is track what exactly changed during an Ansible execution. ansible-playbook check-mode.yaml --diff The output says changed, which means that something was changed (more specifically, the file was created), and in the output, we can see a diff-like output that tells us that the state moved from absent to touch, which means the file was created. mtime and atime also changed, but this is probably due to how files are created and checked. Solving host connection issues \u00b6 Ansible is often used to manage remote hosts or systems. To do this, Ansible will need to be able to connect to the remote host, and only after that will it be able to issue commands. Sometimes, the problem is that Ansible is unable to connect to the remote host. A typical example of this is when you try to manage a machine that hasn't booted yet. Being able to quickly recognize these kinds of problems and fix them promptly will help you save a lot of time. - hosts : all tasks : - name : Touch a file file : path : /tmp/myfile state : touch We can try to run the remote.yaml playbook against a non-existent FQDN ansible-playbook -i host.example.com, remote.yaml The output will clearly inform us that the SSH service did not reply in time. SSH connections usually fail for one of two reasons: The SSH client is unable to establish a connection with the SSH server The SSH server refuses the credentials provided by the SSH client It's very probable that the IP address or the port is wrong, so the TCP connection isn't feasible. Usually, double-checking the IP and the hostname (if it's a DNS, check that it resolves to the right IP) solves the problem. To investigate this further, you can try performing an SSH connection from the same machine to check if there are problems. ssh host.example.com -vvv The second problem might be a little bit more complex to debug since it can happen for multiple reasons. One of those is that you are trying to connect to the wrong host and you don't have the credentials for that machine. Another common case is that the username is wrong. To debug it, you can take the user@host address that is shown in the error (in my case, fale@host.example.com ) and use the same command you used previously. ssh fale@host.example.com -vvv This should raise the same error that Ansible reported to you, but with much more details. Passing working variables via the CLI \u00b6 One thing that can help during debugging, and definitely helps for code reusability, is passing variables to playbooks via the command line. Every time your application \u2013 either an Ansible playbook or any kind of application \u2013 receives an input from a third party (a human, in this case), it should ensure that the value is reasonable. An example of this would be to check that the variable has been set and therefore is not an empty string. This is a security golden rule, but should also be applied when the user is trusted since the user might mistype the variable name . The application should identify this and protect the whole system by protecting itself. - hosts : localhost tasks : - debug : var : variable Now that we have an Ansible playbook that allows us to see if a variable has been set to what we were expecting, let's run it with variable declared in the execution statement. ansible-playbook printvar.yaml --extra-vars = '{\"variable\": \"Hello, World!\"}' Ansible allows variables to be set in various modes and with different priorities. More specifically, you can set them with the following. Command-line values (lowest priority) Role defaults Inventory files or script group vars Inventory group_vars/all Playbook group_vars/all Inventory group_vars/* Playbook group_vars/* Inventory files or script host vars Inventory host_vars/* Playbook host_vars/* Host facts/cached set_facts Play vars Play vars_prompt Play vars_files Role vars (defined in role/vars/main.yml) Block vars (only for tasks in block) Task vars (only for the task) include_vars set_facts/registered vars Role (and include_role) params include params Extra vars (highest priority) Limiting the host's execution \u00b6 While testing a playbook, it might make sense to test on a restricted number of machines; for instance, just one. - hosts : all tasks : - debug : msg : \"Hello, World!\" [ hosts ] host1.example.com host2.example.com host3.example.com If we just want to run it against host3.example.com, we will need to specify this on the command line. ansible-playbook -i inventory helloworld.yaml --limit = host3.example.com By using the \u2013limit keyword, we can force Ansible to ignore all the hosts that are outside what is specified in the limit parameter. It's possible to specify multiple hosts as a list or with patterns, so both of the following commands will execute the playbook against host2.example.com and host3.example.com ansible-playbook -i inventory helloworld.yaml --limit = host2.example.com,host3.example.com ansible-playbook -i inventory helloworld.yaml --limit = host [ 2 -3 ] .example.com Flushing the code cache \u00b6 Everywhere in IT, caches are used to speed up operations, and Ansible is not an exception. Usually, caches are good, and for this reason, they are heavily used ubiquitously. However, they might create some problems if they cache a value they should not have cached or if they are not flushed, even if the value has changed. Flushing caches in Ansible is very straightforward, and it's enough to run ansible-playbook, which we are already running, with the addition of the \u2013flush-cache option ansible-playbook -i inventory helloworld.yaml --flush-cache Ansible uses Redis to save host variables, as well as execution variables. Sometimes, those variables might be left behind and influence the following executions. When Ansible finds a variable that should be set in the step it just started, Ansible might assume that the step has already been completed, and therefore pick up that old variable as if it has just been created. By using the \u2013flush-cache option, we can avoid this since it will ensure that Ansible flushes the Redis cache during its execution. Checking for bad syntax \u00b6 Defining whether a file has the right syntax or not is fairly easy for a machine, but might be more complex for humans. This does not mean that machines are able to fix the code for you, but they can quickly identify whether a problem is present or not. To use Ansible's built-in syntax checker, we need a playbook with a syntax error. - hosts : all tasks : - debug : msg : \"Hello, World!\" We can use the \u2013syntax-check command ansible-playbook syntaxcheck.yaml --syntax-check Since Ansible knows all the supported options in all the supported modules, it can quickly read your code and validate whether the YAML you provided contains all the required fields and that it does not contain any unsupported fields.","title":"Troubleshooting"},{"location":"learning/ansible/troubleshooting/#troubleshooting","text":"It's also important to remember that, due to its nature, in Ansible code, we describe the desired state rather than stating a sequence of steps to obtain the desired state. This difference means that the system is less prone to logical errors. Nevertheless, a bug in a Playbook could mean a potential misconfiguration on all of your machines. This should be taken very seriously. It is even more critical when critical parts of the system are changed, such as SSH daemon or sudo configuration, since the risk is you locking yourself out of the system.","title":"Troubleshooting"},{"location":"learning/ansible/troubleshooting/#digging-into-playbook-execution-problems","text":"There are cases where an Ansible execution will interrupt. Many things can cause these situations. The single most frequent cause of problems I've found while executing Ansible playbooks is the network. Since the machine that is issuing the commands and the one that is performing them are usually linked through the network, a problem in the network will immediately show itself as an Ansible execution problem. For instance, if you run the /bin/false command, it will always return 1. To execute this in a playbook so that you can avoid it blocking there, you can write something like the following: - name : Run a command that will return 1 command : /bin/false ignore_errors : yes Be aware that this is a particular case, and often, the best approach is to fix your application so that you're following UNIX standards and return 0 if the application runs appropriately, instead of putting a workaround in your Playbooks.","title":"Digging into playbook execution problems"},{"location":"learning/ansible/troubleshooting/#using-host-facts-to-diagnose-failures","text":"Some execution failures derive from the state of the target machine. The most common problem of this kind is the case where Ansible expects a file or variable to be present, but it's not. Sometimes, it can be enough to print the machine facts to find the problem. - hosts : target_host tasks : - name : Display all variables/facts known for a host debug : var : hostvars[inventory_hostname] This technique will give you a lot of information about the state of the target machine during Ansible execution.","title":"Using host facts to diagnose failures"},{"location":"learning/ansible/troubleshooting/#testing-with-a-playbook","text":"One of the most complex things in the IT field is not creating software and systems, but debugging them when they have problems. Ansible is not an exception. No matter how good you are at creating Ansible playbooks, sooner or later, you'll find yourself debugging a playbook that is not behaving as you thought it would. The simplest way of performing basic tests is to print out the values of variables during execution. - hosts : localhost tasks : - shell : /usr/bin/uptime register : result - debug : var : result The debug module is the module that allows you to print the value of a variable (by using the var option) or a fixed string (by using the msg option) during Ansible's execution. The debug module also provides the verbosity option. - hosts : localhost tasks : - shell : /usr/bin/uptime register : result - debug : var : result verbosity : 2 We set the minimum required verbosity to 2, and by default, Ansible runs with a verbosity of 0. To see the result of using the debug module with this new playbook ansible-playbook debug2.yaml -vv By putting two -v options in the command line, we will be running Ansible with verbosity of 2. This will not only affect this specific module but all the modules (or Ansible itself) that are set to behave differently at different debug levels.","title":"Testing with a playbook"},{"location":"learning/ansible/troubleshooting/#using-check-mode","text":"Although you might be confident in the code you have written, it still pays to test it before running it for real in a production environment. In such cases, it is a good idea to be able to run your code, but with a safety net in place. This is what check mode is for. - hosts : localhost tasks : - name : Touch a file file : path : /tmp/myfile state : touch ansible-playbook check-mode.yaml --check Ansible check mode is usually called a dry run. The idea is that the run won't change the state of the machine and will only highlight the differences between the current status and the status declared in the playbook. Not all modules support check mode, but all major modules do, and more and more modules are being added at every release. In particular, note that the command and shell modules do not support it because it is impossible for the module to tell what commands will result in a change, and what won't. Therefore, these modules will always return changed when they're run outside of check mode because they assume a change has been made. A similar feature to check mode is the \u2013diff flag. What this flag allows us to do is track what exactly changed during an Ansible execution. ansible-playbook check-mode.yaml --diff The output says changed, which means that something was changed (more specifically, the file was created), and in the output, we can see a diff-like output that tells us that the state moved from absent to touch, which means the file was created. mtime and atime also changed, but this is probably due to how files are created and checked.","title":"Using check mode"},{"location":"learning/ansible/troubleshooting/#solving-host-connection-issues","text":"Ansible is often used to manage remote hosts or systems. To do this, Ansible will need to be able to connect to the remote host, and only after that will it be able to issue commands. Sometimes, the problem is that Ansible is unable to connect to the remote host. A typical example of this is when you try to manage a machine that hasn't booted yet. Being able to quickly recognize these kinds of problems and fix them promptly will help you save a lot of time. - hosts : all tasks : - name : Touch a file file : path : /tmp/myfile state : touch We can try to run the remote.yaml playbook against a non-existent FQDN ansible-playbook -i host.example.com, remote.yaml The output will clearly inform us that the SSH service did not reply in time. SSH connections usually fail for one of two reasons: The SSH client is unable to establish a connection with the SSH server The SSH server refuses the credentials provided by the SSH client It's very probable that the IP address or the port is wrong, so the TCP connection isn't feasible. Usually, double-checking the IP and the hostname (if it's a DNS, check that it resolves to the right IP) solves the problem. To investigate this further, you can try performing an SSH connection from the same machine to check if there are problems. ssh host.example.com -vvv The second problem might be a little bit more complex to debug since it can happen for multiple reasons. One of those is that you are trying to connect to the wrong host and you don't have the credentials for that machine. Another common case is that the username is wrong. To debug it, you can take the user@host address that is shown in the error (in my case, fale@host.example.com ) and use the same command you used previously. ssh fale@host.example.com -vvv This should raise the same error that Ansible reported to you, but with much more details.","title":"Solving host connection issues"},{"location":"learning/ansible/troubleshooting/#passing-working-variables-via-the-cli","text":"One thing that can help during debugging, and definitely helps for code reusability, is passing variables to playbooks via the command line. Every time your application \u2013 either an Ansible playbook or any kind of application \u2013 receives an input from a third party (a human, in this case), it should ensure that the value is reasonable. An example of this would be to check that the variable has been set and therefore is not an empty string. This is a security golden rule, but should also be applied when the user is trusted since the user might mistype the variable name . The application should identify this and protect the whole system by protecting itself. - hosts : localhost tasks : - debug : var : variable Now that we have an Ansible playbook that allows us to see if a variable has been set to what we were expecting, let's run it with variable declared in the execution statement. ansible-playbook printvar.yaml --extra-vars = '{\"variable\": \"Hello, World!\"}' Ansible allows variables to be set in various modes and with different priorities. More specifically, you can set them with the following. Command-line values (lowest priority) Role defaults Inventory files or script group vars Inventory group_vars/all Playbook group_vars/all Inventory group_vars/* Playbook group_vars/* Inventory files or script host vars Inventory host_vars/* Playbook host_vars/* Host facts/cached set_facts Play vars Play vars_prompt Play vars_files Role vars (defined in role/vars/main.yml) Block vars (only for tasks in block) Task vars (only for the task) include_vars set_facts/registered vars Role (and include_role) params include params Extra vars (highest priority)","title":"Passing working variables via the CLI"},{"location":"learning/ansible/troubleshooting/#limiting-the-hosts-execution","text":"While testing a playbook, it might make sense to test on a restricted number of machines; for instance, just one. - hosts : all tasks : - debug : msg : \"Hello, World!\" [ hosts ] host1.example.com host2.example.com host3.example.com If we just want to run it against host3.example.com, we will need to specify this on the command line. ansible-playbook -i inventory helloworld.yaml --limit = host3.example.com By using the \u2013limit keyword, we can force Ansible to ignore all the hosts that are outside what is specified in the limit parameter. It's possible to specify multiple hosts as a list or with patterns, so both of the following commands will execute the playbook against host2.example.com and host3.example.com ansible-playbook -i inventory helloworld.yaml --limit = host2.example.com,host3.example.com ansible-playbook -i inventory helloworld.yaml --limit = host [ 2 -3 ] .example.com","title":"Limiting the host's execution"},{"location":"learning/ansible/troubleshooting/#flushing-the-code-cache","text":"Everywhere in IT, caches are used to speed up operations, and Ansible is not an exception. Usually, caches are good, and for this reason, they are heavily used ubiquitously. However, they might create some problems if they cache a value they should not have cached or if they are not flushed, even if the value has changed. Flushing caches in Ansible is very straightforward, and it's enough to run ansible-playbook, which we are already running, with the addition of the \u2013flush-cache option ansible-playbook -i inventory helloworld.yaml --flush-cache Ansible uses Redis to save host variables, as well as execution variables. Sometimes, those variables might be left behind and influence the following executions. When Ansible finds a variable that should be set in the step it just started, Ansible might assume that the step has already been completed, and therefore pick up that old variable as if it has just been created. By using the \u2013flush-cache option, we can avoid this since it will ensure that Ansible flushes the Redis cache during its execution.","title":"Flushing the code cache"},{"location":"learning/ansible/troubleshooting/#checking-for-bad-syntax","text":"Defining whether a file has the right syntax or not is fairly easy for a machine, but might be more complex for humans. This does not mean that machines are able to fix the code for you, but they can quickly identify whether a problem is present or not. To use Ansible's built-in syntax checker, we need a playbook with a syntax error. - hosts : all tasks : - debug : msg : \"Hello, World!\" We can use the \u2013syntax-check command ansible-playbook syntaxcheck.yaml --syntax-check Since Ansible knows all the supported options in all the supported modules, it can quickly read your code and validate whether the YAML you provided contains all the required fields and that it does not contain any unsupported fields.","title":"Checking for bad syntax"},{"location":"learning/docker/docker-notes/","text":"Important Links Docker Projects Master list of Docker Resources and Projects Play With Docker , a great resource for web-based docker testing and also has a library of labs built by Docker Captains and others, and supported by Docker Inc. Play With Docker Labs DockerHub Recipes Docker Cloud: CI/CD and Server Ops Docker and Pi Projects Docker Training Content Docker Mastery Bret's Podcast Bret's Youtube Docker Shell Config Docker Devops Containers vs VM ebook Docker Refernce Docker Certificated Associate Dockerfile Reference Dockerfile Best Practice Formatting Docker CLI Output Docker Image Docker Registry Config Docker Registry Garbage Collection Docker Registry as cache Docker Storage Docker secrets Docker CLI to kubectl Software Design Immutable Software 12 factor App 12 Fractured Apps Devops Roadmap Docker Best Practises Docker Best practise inside a code repo Healthcheck in Dockerfile Starting container process caused \"exec: \\\"ping\\\": executable file not found in $PATH \" : unknown apt-get update && apt-get install -y iputils-ping Starting mysql container and running ps causes \"ps: command not found\" apt-get update && apt-get install procps Creating and Using Containers Like a Boss \u00b6 Check Our Docker Install and Config \u00b6 docker version - verified cli can talk to engine docker info - most config values of engine Image vs. Container \u00b6 An Image is the application we want to run A Container is an instance of that image running as a process You can have many containers running off the same image Docker's default image \"registry\" is called Docker Hub docker container run --publish 80 :80 --detach --name webhost nginx docker container run -it # start new container interactively docker container exec -it # run additional command in existing container docker container ls -a docker container logs webhost docker logs <container-id> 2 > & 1 | grep <search> # Push all logs to host terminal - NOTE : When you say docker pull nginx , basically the image is being pulled from library\\nginx Container VS. VM: It's Just a Process \u00b6 docker run --name mongo -d mongo docker container top # process list in one container docker stop mongo docker ps docker start mongo docker container inspect # details of one container config docker container stats # performance stats for all containers The Mighty Hub: Using Docker Hub Registry Images \u00b6 docker pull nginx docker image ls Images and Their Layers: Discover the Image Cache \u00b6 docker history nginx:latest docker image inspect nginx Image Tagging and Pushing to Docker Hub \u00b6 docker pull nginx:latest docker image ls docker image tag nginx bretfisher/nginx docker login cat .docker/config.json docker image push bretfisher/nginx docker image push bretfisher/nginx bretfisher/nginx:testing Getting a Shell Inside Containers: No Need for SSH \u00b6 docker container exec -it mysql -- bash docker container run -it alpine -- bash docker container run -it alpine -- sh Cleaning Docker images \u00b6 Use docker system df to see space usage. docker image prune to clean up just \"dangling\" images. The big one is usually docker image prune -a which will remove all images you're not using. docker volume prune to remove unused volumes docker system prune will clean up everything (Nuke everything that is not used currently). docker system prune -a wipe everything. Docker Networks: Concepts for Private and Public Comms in Containers \u00b6 Each container connected to a private virtual network \"bridge\" Each virtual network routes through NAT firewall on host IP All containers on a virtual network can talk to each other without -p Best practice is to create a new virtual network for each app: network \"my_web_app\" for mysql and php/apache containers network \"my_api\" for mongo and nodejs containers docker container run -p 80 :80 --name webhost -d nginx docker container port webhost docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost Docker Networks: CLI Management of Virtual Networks \u00b6 Show networks docker network ls Inspect a network docker network inspect Create a network docker network create --driver Attach a network to container docker network connect Detach a network from container docker network disconnect docker network ls docker network inspect bridge docker network create my_app_net docker container run -d --name new_nginx --network my_app_net nginx docker network inspect my_app_net docker network connect <new network id> <container id> docker container disconnect <new network id> <container id> Docker Networks: DNS and How Containers Find Each Other \u00b6 Create your apps so frontend/backend sit on same Docker network Their inter-communication never leaves host All externally exposed ports closed by default You must manually expose via -p , which is better default security! Containers shouldn't rely on IP's for inter-communication DNS for friendly names is built-in if you use custom networks docker container run -d --name my_nginx --network my_app_net nginx docker container exec -it my_nginx ping new_nginx docker container exec -it new_nginx ping my_nginx DNS Round Robin Testing \u00b6 docker network create dude docker container run -d --net dude --net-alias search elasticsearch:2 docker container ls docker container run --rm -- net dude alpine nslookup search docker container run --rm --net dude centos curl -s search:9200 Container Lifetime & Persistent Data: Volumes, Volumes, Volumes \u00b6 Persistent Data: Data Volumes \u00b6 Containers are usually immutable and ephemeral \"immutable infrastructure\": only re-deploy containers, never change This is the ideal scenario, but what about databases, or unique data? Docker gives us features to ensure these \"separation of concerns\" This is known as \"persistent data\" Two ways: Volumes and Bind Mounts Volumes : make special location outside of container UFS Bind Mounts : link container path to host path docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD = True -v mysql-db:/var/lib/mysql mysql docker volume ls docker volume inspect mysql-db Persistent Data: Bind Mounting \u00b6 Used for local development Usecase: When you are changing files on laptop which you want to serve in the app It can be run only during docker run as there is no explicit volume command in the dockerfile the volume will be mounted in the working directory of the container docker container run -d --name nginx -p 80 :80 -v $( pwd ) :/usr/share/nginx/html nginx docker container exec -it nginx -- bash cd /usr/share/nginx/html && ls -la docker log streaming docker container logs -f <container name> Database Passwords in Containers \u00b6 When running postgres now, you'll need to either set a password, or tell it to allow any connection (which was the default before this change). -you need to either set a password with the environment variable: POSTGRES_PASSWORD=mypasswd Or tell it to ignore passwords with the environment variable: POSTGRES_HOST_AUTH_METHOD=trust Making It Easier with Docker Compose: The Multi-Container Tool \u00b6 Why: configure relationships between containers Why: save our docker container run settings in easy-to-read file Why: create one-liner developer environment startups YAML-formatted file that describes our solution options for: containers, networks, volumes A CLI tool docker-compose used for local dev/test automation with those YAML files docker-compose.yml is default filename, but any can be used with docker-compose -f Not a production-grade tool but ideal for local development and test Two most common commands are: docker-compose up # setup volumes/networks and start all containers docker-compose down # stop all containers and remove cont/vol/net Trying Out Basic Compose Commands \u00b6 docker-compose up docker-compose up -d # Running compose in bacground docker-compose down docker-compose down -v --rmi local/all # Removes images and volumes # Compose operations docker-compose logs docker-compose ps docker-compose top docker-compose build # Build images or docker-compose up --build Swarm Intro and Creating a 3-Node Swarm Cluster \u00b6 Swarm Mode is a clustering solution built inside Docker Not enabled by default docker swarm init: What Just Happened? \u00b6 Lots of PKI and security automation Root Signing Certificate created for our Swarm Certificate is issued for first Manager node Join tokens are created Raft database created to store root CA, configs and secrets Encrypted by default on disk (1.13+) No need for another key/value system to hold orchestration/secrets Replicates logs amongst Managers via mutual TLS in \"control plane\" Create Your First Service and Scale it Locally \u00b6 docker info # swarm is down by default docker swarm init # start swarm docker node ls docker service create alpine ping 8 .8.8.8 # creates service frosty_newton docker service ls docker service ps frosty_newton docker container ls docker service update frosty_newton --replicas 3 # creates 3 replicas docker service ls docker service rm frosty_newton # deletes the service docker service ls docker container ls Creating a 3-Node Swarm Cluster \u00b6 docker-machine + VirtualBox - Free and runs locally, but requires a machine with 8GB memory docker-machine create node1 docker-machine ssh node1 docker-machine env node1 docker swarm init docker swarm init --advertise-addr node1 docker node ls docker node update --role manager node2 # Update role to existing node docker swarm join-token manager # Shows join token for manager role docker service create --replicas 3 alpine ping 8 .8.8.8 # Creates service with 3 replicas and starts ping process docker service ls docker service ps <service name> docker node ps docker node ps node2 Scaling Out with Overlay Networking \u00b6 # Create Backend network docker network create --driver overlay mydrupal docker network ls docker service create --name psql --netowrk mydrupal -e POSTGRES_PASSWORD = mypass postgres docker service ls docker service ps psql docker container logs psql <container name> # Create Frontend network docker service create --name drupal --network mydrupal -p 80 :80 drupal docker service inspect drupal Scaling Out with Routing Mesh \u00b6 docker service create --name search --replicas 3 -p 9200 :9200 elasticsearch:2 docker service ps search Create a Multi-Service Multi-Node Web App \u00b6 docker network create -d overlay backend docker network create -d overlay frontend docker service create --name vote -p 80 :80 --network frontend \\ -- replica 2 dockersamples/examplevotingapp_vote:before docker service create --name redis --network frontend \\ --replica 1 redis:3.2 docker service create --name worker --network frontend --network backend dockersamples/examplevotingapp_worker docker service create --name db --network backend \\ --mount type = volume,source = db-data,target = /var/lib/postgresql/data postgres:9.4 docker service create --name result --network backend -p 5001 :80 COPY INFO docker service ls docker service logs worker Swarm Stacks and Production Grade Compose \u00b6 Docker adds a new layer of abstraction to Swarm called Stacks Stacks accept Compose files as their declarative definition for services, networks, and volumes We use docker stack deploy rather then docker service create Stacks manages all those objects for us, including overlay network per stack. Adds stack name to start of their name Compose now ignores deploy: , Swarm ignores build: docker stack deploy -c example-voting-app-stack.yml voteapp docker stack ls docker stack services voteapp docker stack ps voteapp Using Secrets in Swarm Services \u00b6 What is a Secret? - Usernames and passwords - TLS certificates and keys - SSH keys - Any data you would prefer not be \"on front page of news\" docker secret create psql_usr psql_usr.txt echo \"myDBpassWORD\" | docker secret create psql_pass - TAB COMPLETION docker secret inspect psql_usr docker service create --name psql --secret psql_user \\ --secret psql_pass -e POSTGRES_PASSWORD_FILE = /run/secrets/psql_pass \\ -e POSTGRES_USER_FILE = /run/secrets/psql_user postgres docker exec -it <container name> bash cat /run/secrets/psql_user Swarm App Lifecycle \u00b6 Full App Lifecycle: Dev, Build and Deploy With a Single Compose Design \u00b6 Single set of Compose files for: - Local docker-compose up development environment - Remote docker-compose up CI environment - Remote docker stack deploy production environment docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d docker-compose -f docker-compose.yml -f docker-compose.prod.yml config Service Updates: Changing Things In Flight \u00b6 Provides rolling replacement of tasks/containers in a service Limits downtime (be careful with \"prevents\" downtime) Will replace containers for most changes Has many, many cli options to control the update Create options will usually change, adding -add or -rm to them Includes rollback and healthcheck options Also has scale & rollback subcommand for quicker access docker service scale web=4 and docker service rollback web Just update the image used to a newer version docker service update --image myapp:1.2.1 <servicename> Adding an environment variable and remove a port docker service update --env-add NODE_ENV=production --publish-rm 8080 Change number of replicas of two services docker service scale web=8 api=6 docker service create -p 8088 :80 --name web nginx:1.13.7 docker service scale web = 5 docker service update --image nginx:1.13.6 web docker service update --publish-rm 8088 --publish-add 9090 :80 docker service update --force web # forces rebalancing of the service without changing anything docker service rm web Healthchecks in Dockerfiles \u00b6 HEALTHCHECK was added in 1.12 Supported in Dockerfile, Compose YAML, docker run, and Swarm Services Docker engine will exec's the command in the container (e.g. curl localhost) It expects exit 0 (OK) or exit 1 (Error) Three container states: starting, healthy, unhealthy Much better then \"is binary still running?\" Options for healthcheck command --interval = DURATION ( default: 30s ) --timeout = DURATION ( default: 30s ) --start-period = DURATION ( default: 0s ) ( 17 .09+ ) --retries = N ( default: 3 ) docker container run --name p2 -d --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres docker service create --name p2 --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres Container Registries: Image Storage and Distribution \u00b6 Run a Private Docker Registry \u00b6 Secure your Registry with TLS Storage cleanup via Garbage Collection Enable Hub caching via \"\u2013registry-mirror\" # Run the registry image docker container run -d -p 5000 :5000 --name registry registry # Re-tag an existing image and push it to your new registry docker pull hello-world docker run hello-world docker tag hello-world 127 .0.0.1:5000/hello-world docker push 127 .0.0.1:5000/hello-world # Remove that image from local cache and pull it from new registry docker image remove hello-world docker image remove 127 .0.0.1:5000/hello-world docker pull 127 .0.0.1:5000/hello-world:latest # Re-create registry using a bind mount and see how it stores data docker container kill registry docker container rm registry docker container run -d -p 5000 :5000 --name registry -v $( pwd ) /registry-data:/var/lib/registry registry Using Docker Registry With Swarm \u00b6 docker node ls docker service create --name registry --publish 5000 :5000 registry docker service ps registry docker pull nginx docker tag nginx 127 .0.0.1:5000/nginx docker push 127 .0.0.1:5000/nginx docker service create --name nginx -p 80 :80 --replicas 5 --detach = false 127 .0.0.1:5000/nginx docker service ps nginx Using Docker in Production \u00b6 Focus on Dockerfiles first. Study ENTRYPOINT of Hub official images. Use it for config of images before CMD is executed. use ENTRYPOINT to set default values for all environments and then overide using ENV values. EntryPoint vs CMD FROM official distros. Make it == start, log all things in stdout/stderr, documented in file, lean and scale. Using SaaS for - Image Registry, Logging, Monitoring, Look at CNCF Landscape Using Layer 7 Reverse Proxy if port 80 and 443 are used by multiple apps Docker Security \u00b6 Docker Security Checklist Docker Engine Security Docker Security Tools Seccomp App Armor Docker Bench CIS Docker checklist Running Docker as non root user # Creating non root user in alpine RUN addgroup -g 1000 node \\ && adduser -u 1000 -G node -s /bin/sh -D node # Creating non root user in stretch RUN groupadd --gid 1000 node \\ && useradd --uid 1000 --gid node --shell /bin/bash --create-home node Sample Dockerfile with USER User Namespaces Shift Left Security Trivy - Image Scanning Sysdig Falco Appamror Profiles Seccomp Profile Docker Context \u00b6 Start a node on play with Docker Copy the IP of the node Set the Docker Context with the Host Name of the node and port 2375 Contexts are created in the home folder of user called .docker/context docker context create --docker \"host=tcp://<Host Name>:2375\" <context-name> docker context ls docker context use <context-name> docker ps # Should show the new context of play with docker # Overriding Context to default in commandline docker -c default ps docker -c <context-name> ps # Looping through all the context and executing ps for c in ` docker context ls -q ` ; do ` docker -c $c ps ` ; done # Creates the image in all context for c in ` docker context ls -q ` ; do ` docker -c $c run hello-world ` ; done Recommendations \u00b6 To change permissions on file system (chown or chmod) use a Entrypoint script. Look up to official images for examples for Entrypoint One App or Website use one container, specially if using an orchestrator like K8s or Docker Swarm. Scaling is also a benefit due to one-one relationship. Changing Docker IP range Use Cloud DB as service instead of in containers Run one process per container Strict Separation of Config from Code. Use Env variables to achieve this. Using Development workflow in Compose Write all the ENV variables at the top of Dockerfile Using Env variables in Dockerfile Override Env variables in Docker Compose file say for Dev testing Using Env variables in Docker Entrypoint to write into Application config files during start up. Secrets and Application specific config goes into specific ENV var blocks. This can be changed. Defaults or data specific to SERVER or LANGUAGE goes to another ENV block and can be kept static. This avoids them being set for each ENV. Encrypting traffic for local development use Lets Encrypt ad store them in .cert folder in Home Directory. Encrypting traffic for production use Lets Encrypt and maybe Traefik as Front proxy. See example using Swarm COPY vs ADD. Use COPY to copy artefacts in the same repo to the image. Use ADD when you want to download something from the Internet or to untar or unzip. You can also replace using wget statements with ADD. Combine multiple RUN into a single statement. Delete packages which are downloaded and installed also in a single command to save image size. No secrets like configs, certificates should be saved in Image. Pass them during runtime. Always have a CMD in the image, even if its inheriting it from BASE image Version apt packages and BASE images Use multistage Dokcer builds to have Dev dependencies and Prod dependencies separate. Have healthchecks in K8s instead of Dockerfile Use DNS RoundRobin for Database inside Compose file so it switches of Virtual IP on the Overlay network and gives direct access from FrontEnd Service to Backend container. Setting resource limits inside Compose file DRY your compose files using templates","title":"Docker"},{"location":"learning/docker/docker-notes/#creating-and-using-containers-like-a-boss","text":"","title":"Creating and Using Containers Like a Boss"},{"location":"learning/docker/docker-notes/#check-our-docker-install-and-config","text":"docker version - verified cli can talk to engine docker info - most config values of engine","title":"Check Our Docker Install and Config"},{"location":"learning/docker/docker-notes/#image-vs-container","text":"An Image is the application we want to run A Container is an instance of that image running as a process You can have many containers running off the same image Docker's default image \"registry\" is called Docker Hub docker container run --publish 80 :80 --detach --name webhost nginx docker container run -it # start new container interactively docker container exec -it # run additional command in existing container docker container ls -a docker container logs webhost docker logs <container-id> 2 > & 1 | grep <search> # Push all logs to host terminal - NOTE : When you say docker pull nginx , basically the image is being pulled from library\\nginx","title":"Image vs. Container"},{"location":"learning/docker/docker-notes/#container-vs-vm-its-just-a-process","text":"docker run --name mongo -d mongo docker container top # process list in one container docker stop mongo docker ps docker start mongo docker container inspect # details of one container config docker container stats # performance stats for all containers","title":"Container VS. VM: It's Just a Process"},{"location":"learning/docker/docker-notes/#the-mighty-hub-using-docker-hub-registry-images","text":"docker pull nginx docker image ls","title":"The Mighty Hub: Using Docker Hub Registry Images"},{"location":"learning/docker/docker-notes/#images-and-their-layers-discover-the-image-cache","text":"docker history nginx:latest docker image inspect nginx","title":"Images and Their Layers: Discover the Image Cache"},{"location":"learning/docker/docker-notes/#image-tagging-and-pushing-to-docker-hub","text":"docker pull nginx:latest docker image ls docker image tag nginx bretfisher/nginx docker login cat .docker/config.json docker image push bretfisher/nginx docker image push bretfisher/nginx bretfisher/nginx:testing","title":"Image Tagging and Pushing to Docker Hub"},{"location":"learning/docker/docker-notes/#getting-a-shell-inside-containers-no-need-for-ssh","text":"docker container exec -it mysql -- bash docker container run -it alpine -- bash docker container run -it alpine -- sh","title":"Getting a Shell Inside Containers: No Need for SSH"},{"location":"learning/docker/docker-notes/#cleaning-docker-images","text":"Use docker system df to see space usage. docker image prune to clean up just \"dangling\" images. The big one is usually docker image prune -a which will remove all images you're not using. docker volume prune to remove unused volumes docker system prune will clean up everything (Nuke everything that is not used currently). docker system prune -a wipe everything.","title":"Cleaning Docker images"},{"location":"learning/docker/docker-notes/#docker-networks-concepts-for-private-and-public-comms-in-containers","text":"Each container connected to a private virtual network \"bridge\" Each virtual network routes through NAT firewall on host IP All containers on a virtual network can talk to each other without -p Best practice is to create a new virtual network for each app: network \"my_web_app\" for mysql and php/apache containers network \"my_api\" for mongo and nodejs containers docker container run -p 80 :80 --name webhost -d nginx docker container port webhost docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost","title":"Docker Networks: Concepts for Private and Public Comms in Containers"},{"location":"learning/docker/docker-notes/#docker-networks-cli-management-of-virtual-networks","text":"Show networks docker network ls Inspect a network docker network inspect Create a network docker network create --driver Attach a network to container docker network connect Detach a network from container docker network disconnect docker network ls docker network inspect bridge docker network create my_app_net docker container run -d --name new_nginx --network my_app_net nginx docker network inspect my_app_net docker network connect <new network id> <container id> docker container disconnect <new network id> <container id>","title":"Docker Networks: CLI Management of Virtual Networks"},{"location":"learning/docker/docker-notes/#docker-networks-dns-and-how-containers-find-each-other","text":"Create your apps so frontend/backend sit on same Docker network Their inter-communication never leaves host All externally exposed ports closed by default You must manually expose via -p , which is better default security! Containers shouldn't rely on IP's for inter-communication DNS for friendly names is built-in if you use custom networks docker container run -d --name my_nginx --network my_app_net nginx docker container exec -it my_nginx ping new_nginx docker container exec -it new_nginx ping my_nginx","title":"Docker Networks: DNS and How Containers Find Each Other"},{"location":"learning/docker/docker-notes/#dns-round-robin-testing","text":"docker network create dude docker container run -d --net dude --net-alias search elasticsearch:2 docker container ls docker container run --rm -- net dude alpine nslookup search docker container run --rm --net dude centos curl -s search:9200","title":"DNS Round Robin Testing"},{"location":"learning/docker/docker-notes/#container-lifetime--persistent-data-volumes-volumes-volumes","text":"","title":"Container Lifetime &amp; Persistent Data: Volumes, Volumes, Volumes"},{"location":"learning/docker/docker-notes/#persistent-data-data-volumes","text":"Containers are usually immutable and ephemeral \"immutable infrastructure\": only re-deploy containers, never change This is the ideal scenario, but what about databases, or unique data? Docker gives us features to ensure these \"separation of concerns\" This is known as \"persistent data\" Two ways: Volumes and Bind Mounts Volumes : make special location outside of container UFS Bind Mounts : link container path to host path docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD = True -v mysql-db:/var/lib/mysql mysql docker volume ls docker volume inspect mysql-db","title":"Persistent Data: Data Volumes"},{"location":"learning/docker/docker-notes/#persistent-data-bind-mounting","text":"Used for local development Usecase: When you are changing files on laptop which you want to serve in the app It can be run only during docker run as there is no explicit volume command in the dockerfile the volume will be mounted in the working directory of the container docker container run -d --name nginx -p 80 :80 -v $( pwd ) :/usr/share/nginx/html nginx docker container exec -it nginx -- bash cd /usr/share/nginx/html && ls -la docker log streaming docker container logs -f <container name>","title":"Persistent Data: Bind Mounting"},{"location":"learning/docker/docker-notes/#database-passwords-in-containers","text":"When running postgres now, you'll need to either set a password, or tell it to allow any connection (which was the default before this change). -you need to either set a password with the environment variable: POSTGRES_PASSWORD=mypasswd Or tell it to ignore passwords with the environment variable: POSTGRES_HOST_AUTH_METHOD=trust","title":"Database Passwords in Containers"},{"location":"learning/docker/docker-notes/#making-it-easier-with-docker-compose-the-multi-container-tool","text":"Why: configure relationships between containers Why: save our docker container run settings in easy-to-read file Why: create one-liner developer environment startups YAML-formatted file that describes our solution options for: containers, networks, volumes A CLI tool docker-compose used for local dev/test automation with those YAML files docker-compose.yml is default filename, but any can be used with docker-compose -f Not a production-grade tool but ideal for local development and test Two most common commands are: docker-compose up # setup volumes/networks and start all containers docker-compose down # stop all containers and remove cont/vol/net","title":"Making It Easier with Docker Compose: The Multi-Container Tool"},{"location":"learning/docker/docker-notes/#trying-out-basic-compose-commands","text":"docker-compose up docker-compose up -d # Running compose in bacground docker-compose down docker-compose down -v --rmi local/all # Removes images and volumes # Compose operations docker-compose logs docker-compose ps docker-compose top docker-compose build # Build images or docker-compose up --build","title":"Trying Out Basic Compose Commands"},{"location":"learning/docker/docker-notes/#swarm-intro-and-creating-a-3-node-swarm-cluster","text":"Swarm Mode is a clustering solution built inside Docker Not enabled by default","title":"Swarm Intro and Creating a 3-Node Swarm Cluster"},{"location":"learning/docker/docker-notes/#docker-swarm-init-what-just-happened","text":"Lots of PKI and security automation Root Signing Certificate created for our Swarm Certificate is issued for first Manager node Join tokens are created Raft database created to store root CA, configs and secrets Encrypted by default on disk (1.13+) No need for another key/value system to hold orchestration/secrets Replicates logs amongst Managers via mutual TLS in \"control plane\"","title":"docker swarm init: What Just Happened?"},{"location":"learning/docker/docker-notes/#create-your-first-service-and-scale-it-locally","text":"docker info # swarm is down by default docker swarm init # start swarm docker node ls docker service create alpine ping 8 .8.8.8 # creates service frosty_newton docker service ls docker service ps frosty_newton docker container ls docker service update frosty_newton --replicas 3 # creates 3 replicas docker service ls docker service rm frosty_newton # deletes the service docker service ls docker container ls","title":"Create Your First Service and Scale it Locally"},{"location":"learning/docker/docker-notes/#creating-a-3-node-swarm-cluster","text":"docker-machine + VirtualBox - Free and runs locally, but requires a machine with 8GB memory docker-machine create node1 docker-machine ssh node1 docker-machine env node1 docker swarm init docker swarm init --advertise-addr node1 docker node ls docker node update --role manager node2 # Update role to existing node docker swarm join-token manager # Shows join token for manager role docker service create --replicas 3 alpine ping 8 .8.8.8 # Creates service with 3 replicas and starts ping process docker service ls docker service ps <service name> docker node ps docker node ps node2","title":"Creating a 3-Node Swarm Cluster"},{"location":"learning/docker/docker-notes/#scaling-out-with-overlay-networking","text":"# Create Backend network docker network create --driver overlay mydrupal docker network ls docker service create --name psql --netowrk mydrupal -e POSTGRES_PASSWORD = mypass postgres docker service ls docker service ps psql docker container logs psql <container name> # Create Frontend network docker service create --name drupal --network mydrupal -p 80 :80 drupal docker service inspect drupal","title":"Scaling Out with Overlay Networking"},{"location":"learning/docker/docker-notes/#scaling-out-with-routing-mesh","text":"docker service create --name search --replicas 3 -p 9200 :9200 elasticsearch:2 docker service ps search","title":"Scaling Out with Routing Mesh"},{"location":"learning/docker/docker-notes/#create-a-multi-service-multi-node-web-app","text":"docker network create -d overlay backend docker network create -d overlay frontend docker service create --name vote -p 80 :80 --network frontend \\ -- replica 2 dockersamples/examplevotingapp_vote:before docker service create --name redis --network frontend \\ --replica 1 redis:3.2 docker service create --name worker --network frontend --network backend dockersamples/examplevotingapp_worker docker service create --name db --network backend \\ --mount type = volume,source = db-data,target = /var/lib/postgresql/data postgres:9.4 docker service create --name result --network backend -p 5001 :80 COPY INFO docker service ls docker service logs worker","title":"Create a Multi-Service Multi-Node Web App"},{"location":"learning/docker/docker-notes/#swarm-stacks-and-production-grade-compose","text":"Docker adds a new layer of abstraction to Swarm called Stacks Stacks accept Compose files as their declarative definition for services, networks, and volumes We use docker stack deploy rather then docker service create Stacks manages all those objects for us, including overlay network per stack. Adds stack name to start of their name Compose now ignores deploy: , Swarm ignores build: docker stack deploy -c example-voting-app-stack.yml voteapp docker stack ls docker stack services voteapp docker stack ps voteapp","title":"Swarm Stacks and Production Grade Compose"},{"location":"learning/docker/docker-notes/#using-secrets-in-swarm-services","text":"What is a Secret? - Usernames and passwords - TLS certificates and keys - SSH keys - Any data you would prefer not be \"on front page of news\" docker secret create psql_usr psql_usr.txt echo \"myDBpassWORD\" | docker secret create psql_pass - TAB COMPLETION docker secret inspect psql_usr docker service create --name psql --secret psql_user \\ --secret psql_pass -e POSTGRES_PASSWORD_FILE = /run/secrets/psql_pass \\ -e POSTGRES_USER_FILE = /run/secrets/psql_user postgres docker exec -it <container name> bash cat /run/secrets/psql_user","title":"Using Secrets in Swarm Services"},{"location":"learning/docker/docker-notes/#swarm-app-lifecycle","text":"","title":"Swarm App Lifecycle"},{"location":"learning/docker/docker-notes/#full-app-lifecycle-dev-build-and-deploy-with-a-single-compose-design","text":"Single set of Compose files for: - Local docker-compose up development environment - Remote docker-compose up CI environment - Remote docker stack deploy production environment docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d docker-compose -f docker-compose.yml -f docker-compose.prod.yml config","title":"Full App Lifecycle: Dev, Build and Deploy With a Single Compose Design"},{"location":"learning/docker/docker-notes/#service-updates-changing-things-in-flight","text":"Provides rolling replacement of tasks/containers in a service Limits downtime (be careful with \"prevents\" downtime) Will replace containers for most changes Has many, many cli options to control the update Create options will usually change, adding -add or -rm to them Includes rollback and healthcheck options Also has scale & rollback subcommand for quicker access docker service scale web=4 and docker service rollback web Just update the image used to a newer version docker service update --image myapp:1.2.1 <servicename> Adding an environment variable and remove a port docker service update --env-add NODE_ENV=production --publish-rm 8080 Change number of replicas of two services docker service scale web=8 api=6 docker service create -p 8088 :80 --name web nginx:1.13.7 docker service scale web = 5 docker service update --image nginx:1.13.6 web docker service update --publish-rm 8088 --publish-add 9090 :80 docker service update --force web # forces rebalancing of the service without changing anything docker service rm web","title":"Service Updates: Changing Things In Flight"},{"location":"learning/docker/docker-notes/#healthchecks-in-dockerfiles","text":"HEALTHCHECK was added in 1.12 Supported in Dockerfile, Compose YAML, docker run, and Swarm Services Docker engine will exec's the command in the container (e.g. curl localhost) It expects exit 0 (OK) or exit 1 (Error) Three container states: starting, healthy, unhealthy Much better then \"is binary still running?\" Options for healthcheck command --interval = DURATION ( default: 30s ) --timeout = DURATION ( default: 30s ) --start-period = DURATION ( default: 0s ) ( 17 .09+ ) --retries = N ( default: 3 ) docker container run --name p2 -d --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres docker service create --name p2 --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres","title":"Healthchecks in Dockerfiles"},{"location":"learning/docker/docker-notes/#container-registries-image-storage-and-distribution","text":"","title":"Container Registries: Image Storage and Distribution"},{"location":"learning/docker/docker-notes/#run-a-private-docker-registry","text":"Secure your Registry with TLS Storage cleanup via Garbage Collection Enable Hub caching via \"\u2013registry-mirror\" # Run the registry image docker container run -d -p 5000 :5000 --name registry registry # Re-tag an existing image and push it to your new registry docker pull hello-world docker run hello-world docker tag hello-world 127 .0.0.1:5000/hello-world docker push 127 .0.0.1:5000/hello-world # Remove that image from local cache and pull it from new registry docker image remove hello-world docker image remove 127 .0.0.1:5000/hello-world docker pull 127 .0.0.1:5000/hello-world:latest # Re-create registry using a bind mount and see how it stores data docker container kill registry docker container rm registry docker container run -d -p 5000 :5000 --name registry -v $( pwd ) /registry-data:/var/lib/registry registry","title":"Run a Private Docker Registry"},{"location":"learning/docker/docker-notes/#using-docker-registry-with-swarm","text":"docker node ls docker service create --name registry --publish 5000 :5000 registry docker service ps registry docker pull nginx docker tag nginx 127 .0.0.1:5000/nginx docker push 127 .0.0.1:5000/nginx docker service create --name nginx -p 80 :80 --replicas 5 --detach = false 127 .0.0.1:5000/nginx docker service ps nginx","title":"Using Docker Registry With Swarm"},{"location":"learning/docker/docker-notes/#using-docker-in-production","text":"Focus on Dockerfiles first. Study ENTRYPOINT of Hub official images. Use it for config of images before CMD is executed. use ENTRYPOINT to set default values for all environments and then overide using ENV values. EntryPoint vs CMD FROM official distros. Make it == start, log all things in stdout/stderr, documented in file, lean and scale. Using SaaS for - Image Registry, Logging, Monitoring, Look at CNCF Landscape Using Layer 7 Reverse Proxy if port 80 and 443 are used by multiple apps","title":"Using Docker in Production"},{"location":"learning/docker/docker-notes/#docker-security","text":"Docker Security Checklist Docker Engine Security Docker Security Tools Seccomp App Armor Docker Bench CIS Docker checklist Running Docker as non root user # Creating non root user in alpine RUN addgroup -g 1000 node \\ && adduser -u 1000 -G node -s /bin/sh -D node # Creating non root user in stretch RUN groupadd --gid 1000 node \\ && useradd --uid 1000 --gid node --shell /bin/bash --create-home node Sample Dockerfile with USER User Namespaces Shift Left Security Trivy - Image Scanning Sysdig Falco Appamror Profiles Seccomp Profile","title":"Docker Security"},{"location":"learning/docker/docker-notes/#docker-context","text":"Start a node on play with Docker Copy the IP of the node Set the Docker Context with the Host Name of the node and port 2375 Contexts are created in the home folder of user called .docker/context docker context create --docker \"host=tcp://<Host Name>:2375\" <context-name> docker context ls docker context use <context-name> docker ps # Should show the new context of play with docker # Overriding Context to default in commandline docker -c default ps docker -c <context-name> ps # Looping through all the context and executing ps for c in ` docker context ls -q ` ; do ` docker -c $c ps ` ; done # Creates the image in all context for c in ` docker context ls -q ` ; do ` docker -c $c run hello-world ` ; done","title":"Docker Context"},{"location":"learning/docker/docker-notes/#recommendations","text":"To change permissions on file system (chown or chmod) use a Entrypoint script. Look up to official images for examples for Entrypoint One App or Website use one container, specially if using an orchestrator like K8s or Docker Swarm. Scaling is also a benefit due to one-one relationship. Changing Docker IP range Use Cloud DB as service instead of in containers Run one process per container Strict Separation of Config from Code. Use Env variables to achieve this. Using Development workflow in Compose Write all the ENV variables at the top of Dockerfile Using Env variables in Dockerfile Override Env variables in Docker Compose file say for Dev testing Using Env variables in Docker Entrypoint to write into Application config files during start up. Secrets and Application specific config goes into specific ENV var blocks. This can be changed. Defaults or data specific to SERVER or LANGUAGE goes to another ENV block and can be kept static. This avoids them being set for each ENV. Encrypting traffic for local development use Lets Encrypt ad store them in .cert folder in Home Directory. Encrypting traffic for production use Lets Encrypt and maybe Traefik as Front proxy. See example using Swarm COPY vs ADD. Use COPY to copy artefacts in the same repo to the image. Use ADD when you want to download something from the Internet or to untar or unzip. You can also replace using wget statements with ADD. Combine multiple RUN into a single statement. Delete packages which are downloaded and installed also in a single command to save image size. No secrets like configs, certificates should be saved in Image. Pass them during runtime. Always have a CMD in the image, even if its inheriting it from BASE image Version apt packages and BASE images Use multistage Dokcer builds to have Dev dependencies and Prod dependencies separate. Have healthchecks in K8s instead of Dockerfile Use DNS RoundRobin for Database inside Compose file so it switches of Virtual IP on the Overlay network and gives direct access from FrontEnd Service to Backend container. Setting resource limits inside Compose file DRY your compose files using templates","title":"Recommendations"},{"location":"learning/k8s/helm-templating/","text":"Go Template Dry run values of a template helm template RELEASE .<Chart path> # Will render the template along with the default values Template Macros : Used for reusing code in the helm templates and is written in _helpers.tpl","title":"Helm templating"},{"location":"learning/k8s/k8s-notes/","text":"Tailing logs from multiple containers on laptop K8s Tutorials K8s DNS Kubectl Usage Convention K8s API Reference Operator Hub Awesome Operator List Creating EKS using Terraform Kubernetes Prompt Why Kubernetes \u00b6 Orchestration: Next logical step in journey to faster DevOps First, understand why you may need orchestration Not every solution needs orchestration Servers + Change Rate = Benefit of orchestration K8s Learning Resources \u00b6 Play with k8s Katacoda Install Kubernetes \u00b6 Linux - Microk8s Install SNAP first using apt-get sudo snap install microk8s --classic --channel = 1 .17/stable # Install specific k8s version microk8s.enable dns # Enbale DNS microk8s.status # Check status Windows - Minikube minikube start --kubernetes-version = '1.17.4' # Install specific k8s version minikube ip # IP of the machine minikube status # Check status minikube stop # Stop minkube service Kubernetes Container Abstractions \u00b6 Pod: one or more containers running together on one Node. Basic unit of deployment. Containers are always in pods Controller: For creating/updating pods and other objects. Many types of Controllers inc. Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob, etc. Service: network endpoint to connect to a pod Namespace: Filtered group of objects in cluster - Secrets, ConfigMaps, and more. Our First Pod With Kubectl run \u00b6 Two ways to deploy Pods (containers): Via commands, or via YAML Object hieracrhy - Pods -> ReplicaSet -> Deployment kubectl run my-nginx --image nginx # Creates a single pod kubectl run nginx-pod --generator = run-pod/v1 -- image nginx # Another way to create pod kubectl get pods # list the pod kubectl create deployment nginx --image nginx # Creates a deployment kubectl deployment deployment nginx # Deletes a deployment kubectl create deployment nginx --image nginx --dry-run --port 80 -- expose # Using Dry run option Scaling ReplicaSets \u00b6 kubectl create deployment my-apache --image httpd kubectl scale deploy/my-apache --replicas 2 # Scale up by 2 kubectl scale deployment my-apache --replicas 2 # Scale up by 2 kubectl get all Inspecting Kubernetes Objects \u00b6 kubectl get deploy,pods # Get multiple resources in one line kubectl get pods -o wide # Get all pods, in wide format (gives more info) kubectl get pods --show-labels # Get all pods and show labels kubectl logs deployment/my-apache kubectl logs deployment/my-apache --follow --tail 1 # Show the last line kubectl logs -l run = my-apache # Show logs using label kubectl describe pod/my-apache-<pod id> # Shows the pod configuration including events kubectl get pods -w # Watches the pods in real time kubectl delete pod/my-apache-<pod id> # Deletes a single instance Exposing Kubernetes Ports \u00b6 A service is a stable address for pod(s) If we want to connect to pod(s), we need a service CoreDNS allows us to resolve services by name There are different types of services ClusterIP NodePort LoadBalancer ExternalName ClusterIP and NodePort services are always available in Kubernetes kubectl expose creates a service for existing pods Basic Service Types \u00b6 ClusterIP (default) Single, internal virtual IP allocated Only reachable from within cluster (nodes and pods) Pods can reach service on apps port number NodePort High port allocated on each node Port is open on every node\u2019s IP Anyone can connect (if they can reach node) Other pods need to be updated to this port LoadBalancer Controls a LB endpoint external to the cluster Only available when infra provider gives you a LB (AWS ELB, etc) Creates NodePort+ClusterIP services, tells LB to send to NodePort ExternalName Adds CNAME DNS record to CoreDNS only Not used for Pods, but for giving pods a DNS name to use for something outside Kubernetes # To show how to reach a ClusterIP deployment which is only accessible from the cluster in a Laptop kubectl create deployment httpenv --image = bretfisher/httpenv # simple http server kubectl scale deployment/httpenv --replicas = 5 kubectl expose deployment/httpenv --port 8888 # Create a ClusterIP service (default) kubectl get service # Shows services # Uses Generator option and launches the pod and gives BASH terminal kubectl run --generator run-pod/v1 tmp-shell --rm -it --image bretfisher/netshoot -- bash # Launch another pod to run curl curl httpenv:8888 curl [ ip of service ] :8888 # Creating a NodePort and LoadBalancer Service \u00b6 Nodeport Port Range: 30000 to 32767 Did you know that a NodePort service also creates a ClusterIP? These three service types are additive, each one creates the ones above it: ClusterIP NodePort LoadBalancer If you're on Docker Desktop, it provides a built-in LoadBalancer that publishes the \u2013port on localhost If you're on kubeadm, minikube, or microk8s No built-in LB You can still run the command, it'll just stay at LoadBalancer recieves the packet on 8888, then transfers it to the Nodeport of the Node and then to the ClusterIP of the service. \"pending\" (but its NodePort works) kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort kubectl get services curl localhost:<Node Port> # Get this from svc output kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer kubectl get services curl localhost:8888 # Pod Port kubectl delete service/httpenv service/httpenv-np kubectl delete service/httpenv-lb deployment/httpenv Kubernetes Services DNS \u00b6 Internal DNS is provided by CoreDNS Services also have a FQDN curl <hostname>.<namespace>.svc.cluster.local curl <hostname> kubectl get namespaces curl <hostname>.<namespace>.svc.cluster.local Kubernetes Management Techniques \u00b6 Run, Expose and Create Generators \u00b6 These commands use helper templates called \"generators\" Every resource in Kubernetes has a specification or \"spec\" You can output those templates with \u2013dry-run -o yaml kubectl create deployment sample \u2013image nginx \u2013dry-run -o yaml You can use those YAML defaults as a starting point Generators are \"opinionated defaults\" Generator Examples \u00b6 \u2022 Using dry-run with yaml output we can see the generators kubectl create deployment test \u2013image nginx \u2013dry-run -o yaml kubectl create job test \u2013image nginx \u2013dry-run -o yaml kubectl expose deployment/test \u2013port 80 \u2013dry-run -o yaml - You need the deployment to exist before this works Imperative vs. Declarative \u00b6 Imperative : Focus on how a program operates Declarative : Focus on what a program should accomplish - Example: \"I'd like a cup of coffee\" Imperative : I boil water, scoop out 42 grams of medium-fine grounds, pour over 700 grams of water, etc. Declarative : \"Barista, I'd like a a cup of coffee\". (Barista is the engine that works through the steps, including retrying to make a cup, and is only finished when I have a cup) Kubernetes Imperative \u00b6 Examples: kubectl run, kubectl create deployment, kubectl update We start with a state we know (no deployment exists) We ask kubectl run to create a deployment Different commands are required to change that deployment Different commands are required per object Imperative is easier when you know the state Imperative is easier to get started Imperative is easier for humans at the CLI Imperative is NOT easy to automate Kubernetes Declarative \u00b6 Example: kubectl apply -f my-resources.yaml We don't know the current state We only know what we want the end result to be (yaml contents) Same command each time (tiny exception for delete) Resources can be all in a file, or many files (apply a whole dir) Requires understanding the YAML keys and values More work than kubectl run for just starting a pod The easiest way to automate The eventual path to GitOps happiness Three Management Approaches \u00b6 Imperative commands: run, expose, scale, edit, create deployment Best for dev/learning/personal projects Easy to learn, hardest to manage over time Imperative Commands Imperative objects: create -f file.yml, replace -f file.yml, delete\u2026 Good for prod of small environments, single file per command Store your changes in git-based yaml files Hard to automate Imperative Config File Declarative objects: apply -f file.yml or dir, diff Best for prod, easier to automate Harder to understand and predict changes Declarative Config File Recommendations \u00b6 Most Important Rule : Don't mix the three approaches Recommendations: Learn the Imperative CLI for easy control of local and test setups Move to apply -f file.yml and apply -f directory for prod Store yaml in git, git commit each change before you apply This trains you for later doing GitOps (where git commits are automatically applied to clusters) Moving to Declarative Kubernetes YAML \u00b6 Using kubectl apply \u00b6 create/update resources in a file kubectl apply -f myfile.yaml create/update a whole directory of yaml kubectl apply -f myyaml/ create/update from a URL kubectl apply -f https://bret.run/pod.yml Be careful, lets look at it first (browser or curl) # Using Shell curl -L https://bret.run/pod # Using Windows CMD Win PoSH? start https://bret.run/pod.yml Kubernetes Configuration YAML \u00b6 Kubernetes configuration file (YAML or JSON) Each file contains one or more manifests Each manifest describes an API object (deployment, job, secret) Each manifest needs four parts (root key:values in the file) apiVersion: kind: metadata: spec: Building Your YAML Files \u00b6 kind : We can get a list of resources the cluster supports kubectl api-resources Notice some resources have multiple API's (old vs. new) apiVersion : We can get the API versions the cluster supports kubectl api-versions metadata : only name is required spec : Where all the action is at! Building Your YAML spec - explain Command \u00b6 We can get all the keys each kind supports kubectl explain services \u2013recursive Oh boy! Let's slow down kubectl explain services.spec We can walk through the spec this way kubectl explain services.spec.type spec: can have sub spec: of other resources kubectl explain deployment.spec.template.spec.volumes.nfs.server Use kubectl api-versions or kubectl api-resources along with kubectl explain as documentation on explain could be old We can also use docs kubernetes.io/docs/reference/#api-reference Dry Runs With Apply YAML \u00b6 dry-run a create (client side only) kubectl apply -f app.yml \u2013dry-run dry-run a create/update on server kubectl apply -f app.yml \u2013server-dry-run see a diff visually kubectl diff -f app.yml Difference between dry-run and diff Labels and Label Selectors \u00b6 Labels goes under metadata: in your YAML Simple list of key: value for identifying your resource later by selecting, grouping, or filtering for it Common examples include tier: frontend, app: api, env: prod, customer: acme.co Not meant to hold complex, large, or non- identifying info, which is what annotations are for filter a get command kubectl get pods -l app=nginx apply only matching labels kubectl apply -f myfile.yaml -l app=nginx Label Recommendation Label Selectors (Use case for Labels) \u00b6 The \"glue\" telling Services and Deployments which pods are theirs Many resources use Label Selectors to \"link\" resource dependencies You'll see these match up in the Service and Deployment YAML Using Label selectors Use Labels and Selectors to control which pods go to which nodes Assigning Pods to Nodes Taints and Tolerations also control node placement Taints and Tolerations Your Next Steps, and The Future of Kubernetes \u00b6 Storage in Kubernetes \u00b6 Storage and stateful workloads are harder in all systems Containers make it both harder and easier than before StatefulSets is a new resource type, making Pods more sticky Recommendation : avoid stateful workloads for first few deployments until you're good at the basics Use db-as-a-service whenever you can Volumes in Kubernetes \u00b6 Creating and connecting Volumes: 2 types Volumes Tied to lifecycle of a Pod All containers in a single Pod can share them PersistentVolumes Created at the cluster level, outlives a Pod Separates storage config from Pod using it Multiple Pods can share them CSI plugins are the new way to connect to storage Ingress \u00b6 None of our Service types work at OSI Layer 7 (HTTP) How do we route outside connections based on hostname or URL? Example Usecase: app1.com and app2.com are 2 different deployments in the cluster and both listen on port 443. You will need Ingress to understand the DNS and route traffic to those apps Ingress Controllers (optional) do this with 3 rd party proxies Nginx is popular, but Traefik, HAProxy, F5, Envoy, Istio, etc. Recommendation: Check out Traefik Implementation is specific to Controller chosen Why Controller - To configure LB which is outside the cluster CRD's and The Operator Pattern \u00b6 You can add 3 rd party Resources and Controllers This extends Kubernetes API and CLI A pattern is starting to emerge of using these together Operator : automate deployment and management of complex apps e.g. Databases, monitoring tools, backups, and custom ingresses Higher Deployment Abstractions \u00b6 All our kubectl commands just talk to the Kubernetes API Kubernetes has limited built-in templating, versioning, tracking, and management of your apps Helm is the most popular Compose on Kubernetes comes with Docker Desktop Remember these are optional, and your distro may have a preference Most distros support Helm Templating YAML \u00b6 Many of the deployment tools have templating options You'll need a solution as the number of environments/apps grow Helm was the first \"winner\" in this space, but can be complex Official Kustomize feature works out-of-the-box (as of 1.14) docker app and compose-on-kubernetes are Docker's way Kubernetes Dashboard \u00b6 Default GUI for \"upstream\" Kubernetes Clouds don't have it by default Let's you view resources and upload YAML Safety first! Namespaces and Context \u00b6 Namespaces limit scope, aka \"virtual clusters\" Not related to Docker/Linux namespaces Won't need them in small clusters There are some built-in, to hide system stuff from kubectl \"users\" kubectl get namespaces kubectl get all --all-namespaces - Context changes kubectl cluster and namespace - See ~/.kube/config file kubectl config get-contexts # Selectively show output of Kube config kubectl config get-contexts -o name kubectl config set*","title":"Kubernetes"},{"location":"learning/k8s/k8s-notes/#why-kubernetes","text":"Orchestration: Next logical step in journey to faster DevOps First, understand why you may need orchestration Not every solution needs orchestration Servers + Change Rate = Benefit of orchestration","title":"Why Kubernetes"},{"location":"learning/k8s/k8s-notes/#k8s-learning-resources","text":"Play with k8s Katacoda","title":"K8s Learning Resources"},{"location":"learning/k8s/k8s-notes/#install-kubernetes","text":"Linux - Microk8s Install SNAP first using apt-get sudo snap install microk8s --classic --channel = 1 .17/stable # Install specific k8s version microk8s.enable dns # Enbale DNS microk8s.status # Check status Windows - Minikube minikube start --kubernetes-version = '1.17.4' # Install specific k8s version minikube ip # IP of the machine minikube status # Check status minikube stop # Stop minkube service","title":"Install Kubernetes"},{"location":"learning/k8s/k8s-notes/#kubernetes-container-abstractions","text":"Pod: one or more containers running together on one Node. Basic unit of deployment. Containers are always in pods Controller: For creating/updating pods and other objects. Many types of Controllers inc. Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob, etc. Service: network endpoint to connect to a pod Namespace: Filtered group of objects in cluster - Secrets, ConfigMaps, and more.","title":"Kubernetes Container Abstractions"},{"location":"learning/k8s/k8s-notes/#our-first-pod-with-kubectl-run","text":"Two ways to deploy Pods (containers): Via commands, or via YAML Object hieracrhy - Pods -> ReplicaSet -> Deployment kubectl run my-nginx --image nginx # Creates a single pod kubectl run nginx-pod --generator = run-pod/v1 -- image nginx # Another way to create pod kubectl get pods # list the pod kubectl create deployment nginx --image nginx # Creates a deployment kubectl deployment deployment nginx # Deletes a deployment kubectl create deployment nginx --image nginx --dry-run --port 80 -- expose # Using Dry run option","title":"Our First Pod With Kubectl run"},{"location":"learning/k8s/k8s-notes/#scaling-replicasets","text":"kubectl create deployment my-apache --image httpd kubectl scale deploy/my-apache --replicas 2 # Scale up by 2 kubectl scale deployment my-apache --replicas 2 # Scale up by 2 kubectl get all","title":"Scaling ReplicaSets"},{"location":"learning/k8s/k8s-notes/#inspecting-kubernetes-objects","text":"kubectl get deploy,pods # Get multiple resources in one line kubectl get pods -o wide # Get all pods, in wide format (gives more info) kubectl get pods --show-labels # Get all pods and show labels kubectl logs deployment/my-apache kubectl logs deployment/my-apache --follow --tail 1 # Show the last line kubectl logs -l run = my-apache # Show logs using label kubectl describe pod/my-apache-<pod id> # Shows the pod configuration including events kubectl get pods -w # Watches the pods in real time kubectl delete pod/my-apache-<pod id> # Deletes a single instance","title":"Inspecting Kubernetes Objects"},{"location":"learning/k8s/k8s-notes/#exposing-kubernetes-ports","text":"A service is a stable address for pod(s) If we want to connect to pod(s), we need a service CoreDNS allows us to resolve services by name There are different types of services ClusterIP NodePort LoadBalancer ExternalName ClusterIP and NodePort services are always available in Kubernetes kubectl expose creates a service for existing pods","title":"Exposing Kubernetes Ports"},{"location":"learning/k8s/k8s-notes/#basic-service-types","text":"ClusterIP (default) Single, internal virtual IP allocated Only reachable from within cluster (nodes and pods) Pods can reach service on apps port number NodePort High port allocated on each node Port is open on every node\u2019s IP Anyone can connect (if they can reach node) Other pods need to be updated to this port LoadBalancer Controls a LB endpoint external to the cluster Only available when infra provider gives you a LB (AWS ELB, etc) Creates NodePort+ClusterIP services, tells LB to send to NodePort ExternalName Adds CNAME DNS record to CoreDNS only Not used for Pods, but for giving pods a DNS name to use for something outside Kubernetes # To show how to reach a ClusterIP deployment which is only accessible from the cluster in a Laptop kubectl create deployment httpenv --image = bretfisher/httpenv # simple http server kubectl scale deployment/httpenv --replicas = 5 kubectl expose deployment/httpenv --port 8888 # Create a ClusterIP service (default) kubectl get service # Shows services # Uses Generator option and launches the pod and gives BASH terminal kubectl run --generator run-pod/v1 tmp-shell --rm -it --image bretfisher/netshoot -- bash # Launch another pod to run curl curl httpenv:8888 curl [ ip of service ] :8888 #","title":"Basic Service Types"},{"location":"learning/k8s/k8s-notes/#creating-a-nodeport-and-loadbalancer-service","text":"Nodeport Port Range: 30000 to 32767 Did you know that a NodePort service also creates a ClusterIP? These three service types are additive, each one creates the ones above it: ClusterIP NodePort LoadBalancer If you're on Docker Desktop, it provides a built-in LoadBalancer that publishes the \u2013port on localhost If you're on kubeadm, minikube, or microk8s No built-in LB You can still run the command, it'll just stay at LoadBalancer recieves the packet on 8888, then transfers it to the Nodeport of the Node and then to the ClusterIP of the service. \"pending\" (but its NodePort works) kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort kubectl get services curl localhost:<Node Port> # Get this from svc output kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer kubectl get services curl localhost:8888 # Pod Port kubectl delete service/httpenv service/httpenv-np kubectl delete service/httpenv-lb deployment/httpenv","title":"Creating a NodePort and LoadBalancer Service"},{"location":"learning/k8s/k8s-notes/#kubernetes-services-dns","text":"Internal DNS is provided by CoreDNS Services also have a FQDN curl <hostname>.<namespace>.svc.cluster.local curl <hostname> kubectl get namespaces curl <hostname>.<namespace>.svc.cluster.local","title":"Kubernetes Services DNS"},{"location":"learning/k8s/k8s-notes/#kubernetes-management-techniques","text":"","title":"Kubernetes Management Techniques"},{"location":"learning/k8s/k8s-notes/#run-expose-and-create-generators","text":"These commands use helper templates called \"generators\" Every resource in Kubernetes has a specification or \"spec\" You can output those templates with \u2013dry-run -o yaml kubectl create deployment sample \u2013image nginx \u2013dry-run -o yaml You can use those YAML defaults as a starting point Generators are \"opinionated defaults\"","title":"Run, Expose and Create Generators"},{"location":"learning/k8s/k8s-notes/#generator-examples","text":"\u2022 Using dry-run with yaml output we can see the generators kubectl create deployment test \u2013image nginx \u2013dry-run -o yaml kubectl create job test \u2013image nginx \u2013dry-run -o yaml kubectl expose deployment/test \u2013port 80 \u2013dry-run -o yaml - You need the deployment to exist before this works","title":"Generator Examples"},{"location":"learning/k8s/k8s-notes/#imperative-vs-declarative","text":"Imperative : Focus on how a program operates Declarative : Focus on what a program should accomplish - Example: \"I'd like a cup of coffee\" Imperative : I boil water, scoop out 42 grams of medium-fine grounds, pour over 700 grams of water, etc. Declarative : \"Barista, I'd like a a cup of coffee\". (Barista is the engine that works through the steps, including retrying to make a cup, and is only finished when I have a cup)","title":"Imperative vs. Declarative"},{"location":"learning/k8s/k8s-notes/#kubernetes-imperative","text":"Examples: kubectl run, kubectl create deployment, kubectl update We start with a state we know (no deployment exists) We ask kubectl run to create a deployment Different commands are required to change that deployment Different commands are required per object Imperative is easier when you know the state Imperative is easier to get started Imperative is easier for humans at the CLI Imperative is NOT easy to automate","title":"Kubernetes Imperative"},{"location":"learning/k8s/k8s-notes/#kubernetes-declarative","text":"Example: kubectl apply -f my-resources.yaml We don't know the current state We only know what we want the end result to be (yaml contents) Same command each time (tiny exception for delete) Resources can be all in a file, or many files (apply a whole dir) Requires understanding the YAML keys and values More work than kubectl run for just starting a pod The easiest way to automate The eventual path to GitOps happiness","title":"Kubernetes Declarative"},{"location":"learning/k8s/k8s-notes/#three-management-approaches","text":"Imperative commands: run, expose, scale, edit, create deployment Best for dev/learning/personal projects Easy to learn, hardest to manage over time Imperative Commands Imperative objects: create -f file.yml, replace -f file.yml, delete\u2026 Good for prod of small environments, single file per command Store your changes in git-based yaml files Hard to automate Imperative Config File Declarative objects: apply -f file.yml or dir, diff Best for prod, easier to automate Harder to understand and predict changes Declarative Config File","title":"Three Management Approaches"},{"location":"learning/k8s/k8s-notes/#recommendations","text":"Most Important Rule : Don't mix the three approaches Recommendations: Learn the Imperative CLI for easy control of local and test setups Move to apply -f file.yml and apply -f directory for prod Store yaml in git, git commit each change before you apply This trains you for later doing GitOps (where git commits are automatically applied to clusters)","title":"Recommendations"},{"location":"learning/k8s/k8s-notes/#moving-to-declarative-kubernetes-yaml","text":"","title":"Moving to Declarative Kubernetes YAML"},{"location":"learning/k8s/k8s-notes/#using-kubectl-apply","text":"create/update resources in a file kubectl apply -f myfile.yaml create/update a whole directory of yaml kubectl apply -f myyaml/ create/update from a URL kubectl apply -f https://bret.run/pod.yml Be careful, lets look at it first (browser or curl) # Using Shell curl -L https://bret.run/pod # Using Windows CMD Win PoSH? start https://bret.run/pod.yml","title":"Using kubectl apply"},{"location":"learning/k8s/k8s-notes/#kubernetes-configuration-yaml","text":"Kubernetes configuration file (YAML or JSON) Each file contains one or more manifests Each manifest describes an API object (deployment, job, secret) Each manifest needs four parts (root key:values in the file) apiVersion: kind: metadata: spec:","title":"Kubernetes Configuration YAML"},{"location":"learning/k8s/k8s-notes/#building-your-yaml-files","text":"kind : We can get a list of resources the cluster supports kubectl api-resources Notice some resources have multiple API's (old vs. new) apiVersion : We can get the API versions the cluster supports kubectl api-versions metadata : only name is required spec : Where all the action is at!","title":"Building Your YAML Files"},{"location":"learning/k8s/k8s-notes/#building-your-yaml-spec---explain-command","text":"We can get all the keys each kind supports kubectl explain services \u2013recursive Oh boy! Let's slow down kubectl explain services.spec We can walk through the spec this way kubectl explain services.spec.type spec: can have sub spec: of other resources kubectl explain deployment.spec.template.spec.volumes.nfs.server Use kubectl api-versions or kubectl api-resources along with kubectl explain as documentation on explain could be old We can also use docs kubernetes.io/docs/reference/#api-reference","title":"Building Your YAML spec - explain Command"},{"location":"learning/k8s/k8s-notes/#dry-runs-with-apply-yaml","text":"dry-run a create (client side only) kubectl apply -f app.yml \u2013dry-run dry-run a create/update on server kubectl apply -f app.yml \u2013server-dry-run see a diff visually kubectl diff -f app.yml Difference between dry-run and diff","title":"Dry Runs With Apply YAML"},{"location":"learning/k8s/k8s-notes/#labels-and-label-selectors","text":"Labels goes under metadata: in your YAML Simple list of key: value for identifying your resource later by selecting, grouping, or filtering for it Common examples include tier: frontend, app: api, env: prod, customer: acme.co Not meant to hold complex, large, or non- identifying info, which is what annotations are for filter a get command kubectl get pods -l app=nginx apply only matching labels kubectl apply -f myfile.yaml -l app=nginx Label Recommendation","title":"Labels and Label Selectors"},{"location":"learning/k8s/k8s-notes/#label-selectors-use-case-for-labels","text":"The \"glue\" telling Services and Deployments which pods are theirs Many resources use Label Selectors to \"link\" resource dependencies You'll see these match up in the Service and Deployment YAML Using Label selectors Use Labels and Selectors to control which pods go to which nodes Assigning Pods to Nodes Taints and Tolerations also control node placement Taints and Tolerations","title":"Label Selectors (Use case for Labels)"},{"location":"learning/k8s/k8s-notes/#your-next-steps-and-the-future-of-kubernetes","text":"","title":"Your Next Steps, and The Future of Kubernetes"},{"location":"learning/k8s/k8s-notes/#storage-in-kubernetes","text":"Storage and stateful workloads are harder in all systems Containers make it both harder and easier than before StatefulSets is a new resource type, making Pods more sticky Recommendation : avoid stateful workloads for first few deployments until you're good at the basics Use db-as-a-service whenever you can","title":"Storage in Kubernetes"},{"location":"learning/k8s/k8s-notes/#volumes-in-kubernetes","text":"Creating and connecting Volumes: 2 types Volumes Tied to lifecycle of a Pod All containers in a single Pod can share them PersistentVolumes Created at the cluster level, outlives a Pod Separates storage config from Pod using it Multiple Pods can share them CSI plugins are the new way to connect to storage","title":"Volumes in Kubernetes"},{"location":"learning/k8s/k8s-notes/#ingress","text":"None of our Service types work at OSI Layer 7 (HTTP) How do we route outside connections based on hostname or URL? Example Usecase: app1.com and app2.com are 2 different deployments in the cluster and both listen on port 443. You will need Ingress to understand the DNS and route traffic to those apps Ingress Controllers (optional) do this with 3 rd party proxies Nginx is popular, but Traefik, HAProxy, F5, Envoy, Istio, etc. Recommendation: Check out Traefik Implementation is specific to Controller chosen Why Controller - To configure LB which is outside the cluster","title":"Ingress"},{"location":"learning/k8s/k8s-notes/#crds-and-the-operator-pattern","text":"You can add 3 rd party Resources and Controllers This extends Kubernetes API and CLI A pattern is starting to emerge of using these together Operator : automate deployment and management of complex apps e.g. Databases, monitoring tools, backups, and custom ingresses","title":"CRD's and The Operator Pattern"},{"location":"learning/k8s/k8s-notes/#higher-deployment-abstractions","text":"All our kubectl commands just talk to the Kubernetes API Kubernetes has limited built-in templating, versioning, tracking, and management of your apps Helm is the most popular Compose on Kubernetes comes with Docker Desktop Remember these are optional, and your distro may have a preference Most distros support Helm","title":"Higher Deployment Abstractions"},{"location":"learning/k8s/k8s-notes/#templating-yaml","text":"Many of the deployment tools have templating options You'll need a solution as the number of environments/apps grow Helm was the first \"winner\" in this space, but can be complex Official Kustomize feature works out-of-the-box (as of 1.14) docker app and compose-on-kubernetes are Docker's way","title":"Templating YAML"},{"location":"learning/k8s/k8s-notes/#kubernetes-dashboard","text":"Default GUI for \"upstream\" Kubernetes Clouds don't have it by default Let's you view resources and upload YAML Safety first!","title":"Kubernetes Dashboard"},{"location":"learning/k8s/k8s-notes/#namespaces-and-context","text":"Namespaces limit scope, aka \"virtual clusters\" Not related to Docker/Linux namespaces Won't need them in small clusters There are some built-in, to hide system stuff from kubectl \"users\" kubectl get namespaces kubectl get all --all-namespaces - Context changes kubectl cluster and namespace - See ~/.kube/config file kubectl config get-contexts # Selectively show output of Kube config kubectl config get-contexts -o name kubectl config set*","title":"Namespaces and Context"},{"location":"learning/linux/admin/","text":"Testing SSH Connection \u00b6 # Before testing copy the user student's public key in the server using ssh-copy-id ssh -l student -p 22 192 .168.0.11 # Use a non root user and specify the SSH port","title":"Admin"},{"location":"learning/linux/admin/#testing-ssh-connection","text":"# Before testing copy the user student's public key in the server using ssh-copy-id ssh -l student -p 22 192 .168.0.11 # Use a non root user and specify the SSH port","title":"Testing SSH Connection"},{"location":"learning/linux/bash/","text":"Important \u00b6 Use Lowercase for user variables. UPPERCASE reserved for env variables set by the OS or Shell startup variables Run man test to find all testing conditions. For example to test if input is a file (-f) or a directory (-d) Use [[ test ]] as it is a best practise while testing conditions Arithmatic operations have to be enclosed by (()) Example: a=5,b=6,c=$((a+b)) kill -l shows all kill signals that are available BASH \u00b6 echo $0 # Shows the default shell that is configured cat /etc/shells # Shows all the configured shells for the user cat /etc/passwd # Shows the configured default shell for the user # Script execution echo $PATH # Shows all the paths where the shell will look for executables mkdir -p scripts # Create a custom script directory to store common task scripts export PATH = '${PATH}:${HOME}/scripts' # Appends custom scripts directory to PATH variable SHEBANG \u00b6 Shebang tells the shell which interpretor to use. Shebang is NOT a comment. #! tells shell its a shebang For python, it would be python interpretor, for shell scripts it would be bash. which bash # Shows the path where bash is installed for the user which -a bash # Shows all the paths for bash. Its a link ls -li /bin/bash /usr/bin/bash # Output shows the same INODE number, so it verfies that its the same bash file that is linked # Inside the script file scripts/my_first_script.sh #!/bin/bash # path of bash found from `which -a bash` # For Python #!/usr/bin/python # path of which python # Execution # 1. Execute the script using full path # 2. Execute the script from inside the directory # 3. Execute the script using bash even if 'x' permission is not present on the file chmod -x my_first_script.sh # Example: bash my_first_script.sh # This way you can override the 'Permission denied' error that comes from Shebang directive # Give executable permission chmod +x my_first_script.sh # source command # Scripts can be executed using . or source notation. # When . is used, the script is executed in a sub-shell # When source is used, the script is executed in the same or current shell ./my_first_script.sh # 1st Method of execution . my_first_script.sh # 2nd Method of execution source my_first_script.sh # 3rd Method of execution Variables \u00b6 # Create Variables # 1. Direct assignment os = Linux # Without any spaces between = distro = \"MX Linux\" # String variable # No floating point variable can be defined in bash, only integers # No special characters in variable names or starting name with number # Display Variables echo $os # Display only var value echo \"I'm learning $distro \" # Display inside string using \"\" echo 'Im learning $distro' # Using single quotes will print $distro as is and not expand the variable value echo \"Im learning \\$os using $distro \" # Using a single \\ will escape the special character $ and print as string # Display all variables in the env set # Shows all set variables env # Shows all env variables printenv # Shows all env variables # Remove variables unset os # Remember there is no $ # Read-Only Variables (constant) # Use declare syntax with -r option declare -r logdir = \"/var/log\" ls $logdir logdir = abc # It will throw error \"readonly variable\" unset logdir # It will throw error \"readonly variable\" User Input \u00b6 read name # It will wait for user to input name and press enter echo $name # Will show the value input by user # Show helpful message read -p \"Enter IP Address: \" ip # This will display the message and wait for user input # Hide the input user value read -s -p \"Enter password: \" pass # -s will not echo the input value that is typed Special Variables and Positional Arguments \u00b6 # Example ./script.sh \ufb01lename1 dir1 10 .0.0.1 # Where $0 is the name of the script itself ( script.sh ) . $1 is the \ufb01rst positional argument ( \ufb01lename1 ) $2 is the second positional argument ( dir1 ) $3 is the last argument of the script ( 10 .0.0.1 ) $# is the number of the positional arguments ( i.e total arguments passed ) \" $* \" is a string representation of all positional arguments: $1 , $2 , $3 .... $? is the most recent foreground command exit status If, Elif and Else Statements \u00b6 # Structure if [[ some_condition_is_true ]] then //execute this code elif [[ some_other_condition_is_true ]] then //execute_this_code else //execute_this_code \ufb01 ### TESTING CONDITIONS => man test ### ### For numbers (integers) ### # -eq equal to # -ne not equal to # -lt less than # -le less than or equal to # -gt greater than # -ge greater than or equal to # For files: # -s file exists and is not empty # -f file exists and is not a directory # -d directory exists # -x file is executable by the user # -w file is writable by the user # -r file is readable by the user # For Strings # = the equality operator for strings if using single square brackets [ ] # == the equality operator for strings if using double square brackets [[ ]] # != the inequality operator for strings # -n $str str is nonzero length # -z $str str is zero length # && => the logical and operator # || => the logical or operator Command Substitution \u00b6 Run the shell command and store the output in a variable. 2 Ways to perform command substitution Using command within back-ticks. Using $(command) now = \"`date`\" # Output of date command is stored in variable now # Note: as output of date is a string, better to enclose the command inside double quotes echo $now # Shows the value users = \" $( who ) \" # Enclosing who command inside double quotes as its output is string echo $users # Piping multiple commands output = \" $( ps -ef | grep bash ) \" echo $output # Getting Current Date and Time man date # Check the current formatting options now = $( date +%F_%H%M ) # DD/MM/YY_HHMM # To use this in a backup scenario sudo tar -cvzf etc- $( date +%F_%H%M ) .tar.gz /etc/ # Adds current date and time to backup file Short form Comparison \u00b6 # Check for regular file [[ -f \"/tmp/text.txt\" ]] && echo \"file found\" || echo \"file not found\" # Check for file exists [[ -e \"/tmp/text.txt\" ]] && echo \"file exists\" || echo \"file not found\" String Comparison \u00b6 Use single = when using [] Use double == when using [[]] Remember to use \"\" to enclose the variables to avoid String spilts # Substring comparison str1 = \"Hello, Linux is a wonderful OS\" if [[ \" $str1 \" == * \"Linux\" * ]] then echo \"Found\" fi # String Length comparison str1 = \"Hello, Linux is a wonderful OS\" if [[ -z \" $str1 \" ]] then echo \"String length is zero\" fi if [[ -n \" $str1 \" ]] then echo \"String length is non zero\" fi # Elseif i = 10 if [[ $i -lt 10 ]] then echo \"i is less than 10.\" elif [[ $i -eq 10 ]] then echo \"i is 10\" else echo \"i is greater than or equal to 10.\" fi ## For Loop The list can be a series of strings separated by spaces, a range of numbers, output of a command, an array, and so on. # Structure for item in LIST do COMMANDS done # Example with strings list for os in Ubuntu Kali CentOS Slackware \"Mx Linux\" do echo \"os is $os \" done # Range for num in { 3 ..7 } do echo \"num is $num \" done # Using Step increment in range for num in { 10 ..100..5 } # This will start the seq from 10 and increment counter by 5 till 100 do echo \"num is $num \" done # Iterating over files in current directory and displaying contents for item in ./* # . is current dir and * is all files do echo \"Displaying contents of $item \" sleep 1 echo \"cat $item \" echo \"########\" done While Loop \u00b6 The set of commands are executed as long as the given condition evaluates to true. # Structure while CONDITION do COMMANDS done # Increment i = 0 while [[ $i -lt 10 ]] do echo \" $i \" let i = i+1 # OR this can also be written as ((i++)) done # Infinite loop while true do echo \"Hello\" done # Process monitoring using Infinite Loop while : # This is another way to represent true do output = \" $( pgrep -l $1 ) \" # If process is not running, output will be 0 if [[ -n \" $output \" ]] then echo \"Process \\\" $1 \\\" is running\" else echo \"Process \\\" $1 \\\" is NOT running\" fi sleep 3 done Case Statements \u00b6 # Structure case EXPRESSION in PATTERN_1 ) STATEMENTS ;; PATTERN_2 ) STATEMENTS ;; PATTERN_N ) STATEMENTS ;; * ) STATEMENTS ;; esac # Sample case wih different combinations #!/bin/bash echo -n \"Enter your favorite pet: \" read PET case $PET in dog ) echo \"Your favorite pet is a dog\" ;; cat | Cat ) echo \"You like cats\" ;; fish | \"African Turtle\" ) echo \"Fish or Turtles are great\" ;; * ) echo \"Your pets are unknown\" ;; esac # Another example #!/bin/bash if [[ $# -ne 2 ]] then echo \"Run the script with 2 args: SIGNAL and PID.\" exit fi case $1 in 1 ) echo \"Sending the SIGHUP signal to $2 \" kill -SIGHUP $2 ;; 2 ) echo \"Sending the SIGINT signal to $2 \" kill -SIGINT $2 ;; 15 ) echo \"Sending the SIGTERM signal to $2 \" kill -SIGTERM $2 ;; * ) echo \"Signal number $1 will not be delivered\" ;; esac # Now start the sleep process in the background sleep 1000 & pgrep sleep # Check the PID for sleep ./signal.sh 1501 1 # Send SIGHUP to sleep process Functions \u00b6 function print_something () { echo \"I'm a simple function\" } # 2nd type of declaring function without the keyword display_something () { echo \"Print here!\" } # Calling the functions print_something display_something - Processing arguments is done using $1, $2 and so on. Its not done using the paranthesis. - Here $1 is the frst argument of the function and not the script. # Passing Arguments and processing return code create_files () { echo \"Creating file $1 \" touch $1 chmod 400 $1 echo \"Creating file $2 \" touch $2 chmod 400 $2 return 10 } create_files aa.txt bb.txt echo $? # Prints the return code sent by the function # Processing return values function lines_in_files () { grep -c \" $1 \" \" $2 \" } # Calling function n = $( lines_in_files \"usb\" \"/var/log/dmesg\" ) echo $n Variable scopes \u00b6 Variable scope is global and is visible inside the functions Varibales if changed inside the function, change is done globally Use local variables inside function which is only inside function body #!/bin/bash var1 = \"AA\" var2 = \"BB\" function funct1 () { echo \"Inside funct1, var1= $var1 var2= $var2 \" } funct1 # Using Global and Local functions #!/bin/bash var1 = \"AA\" var2 = \"BB\" function funct1 () { var1 = \"XX\" local var2 = \"YY\" echo \"Inside funct1, var1= $var1 var2= $var2 \" } funct1 echo \"After calling funct1, var1= $var1 var2= $var2 \" Menus in Bash \u00b6 ITEM is a user de\ufb01ned variable and the LIST is a series of strings, numbers or output of commands. REPLY is the number that is selected by the user. Menus is repeated till break command is executed or Ctrl+C is pressed. Default Prompt is #? and can be changed by overriding PS3 prompt variable # Structure select ITEM in LIST do COMMANDS done # Override Prompt PS3 = \"Choose your country: \" select COUNTRY in Germany France USA \"United Kingdom\" do echo \"COUNTRY is $COUNTRY \" echo \"REPLY is $REPLY \" done # Adding case to select PS3 = \"Choose your country: \" select COUNTRY in Germany France USA \"United Kingdom\" Quit do case $REPLY in 1 ) echo \"You speak French\" ;; 2 ) echo \"You speak German\" ;; 3 ) echo \"You speak American English\" ;; 4 ) echo \"You speak UK English\" ;; 5 ) echo \"Quitting!!\" break ;; * ) echo \"Invalid Option $REPLY \" ;; esac done","title":"Bash"},{"location":"learning/linux/bash/#important","text":"Use Lowercase for user variables. UPPERCASE reserved for env variables set by the OS or Shell startup variables Run man test to find all testing conditions. For example to test if input is a file (-f) or a directory (-d) Use [[ test ]] as it is a best practise while testing conditions Arithmatic operations have to be enclosed by (()) Example: a=5,b=6,c=$((a+b)) kill -l shows all kill signals that are available","title":"Important"},{"location":"learning/linux/bash/#bash","text":"echo $0 # Shows the default shell that is configured cat /etc/shells # Shows all the configured shells for the user cat /etc/passwd # Shows the configured default shell for the user # Script execution echo $PATH # Shows all the paths where the shell will look for executables mkdir -p scripts # Create a custom script directory to store common task scripts export PATH = '${PATH}:${HOME}/scripts' # Appends custom scripts directory to PATH variable","title":"BASH"},{"location":"learning/linux/bash/#shebang","text":"Shebang tells the shell which interpretor to use. Shebang is NOT a comment. #! tells shell its a shebang For python, it would be python interpretor, for shell scripts it would be bash. which bash # Shows the path where bash is installed for the user which -a bash # Shows all the paths for bash. Its a link ls -li /bin/bash /usr/bin/bash # Output shows the same INODE number, so it verfies that its the same bash file that is linked # Inside the script file scripts/my_first_script.sh #!/bin/bash # path of bash found from `which -a bash` # For Python #!/usr/bin/python # path of which python # Execution # 1. Execute the script using full path # 2. Execute the script from inside the directory # 3. Execute the script using bash even if 'x' permission is not present on the file chmod -x my_first_script.sh # Example: bash my_first_script.sh # This way you can override the 'Permission denied' error that comes from Shebang directive # Give executable permission chmod +x my_first_script.sh # source command # Scripts can be executed using . or source notation. # When . is used, the script is executed in a sub-shell # When source is used, the script is executed in the same or current shell ./my_first_script.sh # 1st Method of execution . my_first_script.sh # 2nd Method of execution source my_first_script.sh # 3rd Method of execution","title":"SHEBANG"},{"location":"learning/linux/bash/#variables","text":"# Create Variables # 1. Direct assignment os = Linux # Without any spaces between = distro = \"MX Linux\" # String variable # No floating point variable can be defined in bash, only integers # No special characters in variable names or starting name with number # Display Variables echo $os # Display only var value echo \"I'm learning $distro \" # Display inside string using \"\" echo 'Im learning $distro' # Using single quotes will print $distro as is and not expand the variable value echo \"Im learning \\$os using $distro \" # Using a single \\ will escape the special character $ and print as string # Display all variables in the env set # Shows all set variables env # Shows all env variables printenv # Shows all env variables # Remove variables unset os # Remember there is no $ # Read-Only Variables (constant) # Use declare syntax with -r option declare -r logdir = \"/var/log\" ls $logdir logdir = abc # It will throw error \"readonly variable\" unset logdir # It will throw error \"readonly variable\"","title":"Variables"},{"location":"learning/linux/bash/#user-input","text":"read name # It will wait for user to input name and press enter echo $name # Will show the value input by user # Show helpful message read -p \"Enter IP Address: \" ip # This will display the message and wait for user input # Hide the input user value read -s -p \"Enter password: \" pass # -s will not echo the input value that is typed","title":"User Input"},{"location":"learning/linux/bash/#special-variables-and-positional-arguments","text":"# Example ./script.sh \ufb01lename1 dir1 10 .0.0.1 # Where $0 is the name of the script itself ( script.sh ) . $1 is the \ufb01rst positional argument ( \ufb01lename1 ) $2 is the second positional argument ( dir1 ) $3 is the last argument of the script ( 10 .0.0.1 ) $# is the number of the positional arguments ( i.e total arguments passed ) \" $* \" is a string representation of all positional arguments: $1 , $2 , $3 .... $? is the most recent foreground command exit status","title":"Special Variables and Positional Arguments"},{"location":"learning/linux/bash/#if-elif-and-else-statements","text":"# Structure if [[ some_condition_is_true ]] then //execute this code elif [[ some_other_condition_is_true ]] then //execute_this_code else //execute_this_code \ufb01 ### TESTING CONDITIONS => man test ### ### For numbers (integers) ### # -eq equal to # -ne not equal to # -lt less than # -le less than or equal to # -gt greater than # -ge greater than or equal to # For files: # -s file exists and is not empty # -f file exists and is not a directory # -d directory exists # -x file is executable by the user # -w file is writable by the user # -r file is readable by the user # For Strings # = the equality operator for strings if using single square brackets [ ] # == the equality operator for strings if using double square brackets [[ ]] # != the inequality operator for strings # -n $str str is nonzero length # -z $str str is zero length # && => the logical and operator # || => the logical or operator","title":"If, Elif and Else Statements"},{"location":"learning/linux/bash/#command-substitution","text":"Run the shell command and store the output in a variable. 2 Ways to perform command substitution Using command within back-ticks. Using $(command) now = \"`date`\" # Output of date command is stored in variable now # Note: as output of date is a string, better to enclose the command inside double quotes echo $now # Shows the value users = \" $( who ) \" # Enclosing who command inside double quotes as its output is string echo $users # Piping multiple commands output = \" $( ps -ef | grep bash ) \" echo $output # Getting Current Date and Time man date # Check the current formatting options now = $( date +%F_%H%M ) # DD/MM/YY_HHMM # To use this in a backup scenario sudo tar -cvzf etc- $( date +%F_%H%M ) .tar.gz /etc/ # Adds current date and time to backup file","title":"Command Substitution"},{"location":"learning/linux/bash/#short-form-comparison","text":"# Check for regular file [[ -f \"/tmp/text.txt\" ]] && echo \"file found\" || echo \"file not found\" # Check for file exists [[ -e \"/tmp/text.txt\" ]] && echo \"file exists\" || echo \"file not found\"","title":"Short form Comparison"},{"location":"learning/linux/bash/#string-comparison","text":"Use single = when using [] Use double == when using [[]] Remember to use \"\" to enclose the variables to avoid String spilts # Substring comparison str1 = \"Hello, Linux is a wonderful OS\" if [[ \" $str1 \" == * \"Linux\" * ]] then echo \"Found\" fi # String Length comparison str1 = \"Hello, Linux is a wonderful OS\" if [[ -z \" $str1 \" ]] then echo \"String length is zero\" fi if [[ -n \" $str1 \" ]] then echo \"String length is non zero\" fi # Elseif i = 10 if [[ $i -lt 10 ]] then echo \"i is less than 10.\" elif [[ $i -eq 10 ]] then echo \"i is 10\" else echo \"i is greater than or equal to 10.\" fi ## For Loop The list can be a series of strings separated by spaces, a range of numbers, output of a command, an array, and so on. # Structure for item in LIST do COMMANDS done # Example with strings list for os in Ubuntu Kali CentOS Slackware \"Mx Linux\" do echo \"os is $os \" done # Range for num in { 3 ..7 } do echo \"num is $num \" done # Using Step increment in range for num in { 10 ..100..5 } # This will start the seq from 10 and increment counter by 5 till 100 do echo \"num is $num \" done # Iterating over files in current directory and displaying contents for item in ./* # . is current dir and * is all files do echo \"Displaying contents of $item \" sleep 1 echo \"cat $item \" echo \"########\" done","title":"String Comparison"},{"location":"learning/linux/bash/#while-loop","text":"The set of commands are executed as long as the given condition evaluates to true. # Structure while CONDITION do COMMANDS done # Increment i = 0 while [[ $i -lt 10 ]] do echo \" $i \" let i = i+1 # OR this can also be written as ((i++)) done # Infinite loop while true do echo \"Hello\" done # Process monitoring using Infinite Loop while : # This is another way to represent true do output = \" $( pgrep -l $1 ) \" # If process is not running, output will be 0 if [[ -n \" $output \" ]] then echo \"Process \\\" $1 \\\" is running\" else echo \"Process \\\" $1 \\\" is NOT running\" fi sleep 3 done","title":"While Loop"},{"location":"learning/linux/bash/#case-statements","text":"# Structure case EXPRESSION in PATTERN_1 ) STATEMENTS ;; PATTERN_2 ) STATEMENTS ;; PATTERN_N ) STATEMENTS ;; * ) STATEMENTS ;; esac # Sample case wih different combinations #!/bin/bash echo -n \"Enter your favorite pet: \" read PET case $PET in dog ) echo \"Your favorite pet is a dog\" ;; cat | Cat ) echo \"You like cats\" ;; fish | \"African Turtle\" ) echo \"Fish or Turtles are great\" ;; * ) echo \"Your pets are unknown\" ;; esac # Another example #!/bin/bash if [[ $# -ne 2 ]] then echo \"Run the script with 2 args: SIGNAL and PID.\" exit fi case $1 in 1 ) echo \"Sending the SIGHUP signal to $2 \" kill -SIGHUP $2 ;; 2 ) echo \"Sending the SIGINT signal to $2 \" kill -SIGINT $2 ;; 15 ) echo \"Sending the SIGTERM signal to $2 \" kill -SIGTERM $2 ;; * ) echo \"Signal number $1 will not be delivered\" ;; esac # Now start the sleep process in the background sleep 1000 & pgrep sleep # Check the PID for sleep ./signal.sh 1501 1 # Send SIGHUP to sleep process","title":"Case Statements"},{"location":"learning/linux/bash/#functions","text":"function print_something () { echo \"I'm a simple function\" } # 2nd type of declaring function without the keyword display_something () { echo \"Print here!\" } # Calling the functions print_something display_something - Processing arguments is done using $1, $2 and so on. Its not done using the paranthesis. - Here $1 is the frst argument of the function and not the script. # Passing Arguments and processing return code create_files () { echo \"Creating file $1 \" touch $1 chmod 400 $1 echo \"Creating file $2 \" touch $2 chmod 400 $2 return 10 } create_files aa.txt bb.txt echo $? # Prints the return code sent by the function # Processing return values function lines_in_files () { grep -c \" $1 \" \" $2 \" } # Calling function n = $( lines_in_files \"usb\" \"/var/log/dmesg\" ) echo $n","title":"Functions"},{"location":"learning/linux/bash/#variable-scopes","text":"Variable scope is global and is visible inside the functions Varibales if changed inside the function, change is done globally Use local variables inside function which is only inside function body #!/bin/bash var1 = \"AA\" var2 = \"BB\" function funct1 () { echo \"Inside funct1, var1= $var1 var2= $var2 \" } funct1 # Using Global and Local functions #!/bin/bash var1 = \"AA\" var2 = \"BB\" function funct1 () { var1 = \"XX\" local var2 = \"YY\" echo \"Inside funct1, var1= $var1 var2= $var2 \" } funct1 echo \"After calling funct1, var1= $var1 var2= $var2 \"","title":"Variable scopes"},{"location":"learning/linux/bash/#menus-in-bash","text":"ITEM is a user de\ufb01ned variable and the LIST is a series of strings, numbers or output of commands. REPLY is the number that is selected by the user. Menus is repeated till break command is executed or Ctrl+C is pressed. Default Prompt is #? and can be changed by overriding PS3 prompt variable # Structure select ITEM in LIST do COMMANDS done # Override Prompt PS3 = \"Choose your country: \" select COUNTRY in Germany France USA \"United Kingdom\" do echo \"COUNTRY is $COUNTRY \" echo \"REPLY is $REPLY \" done # Adding case to select PS3 = \"Choose your country: \" select COUNTRY in Germany France USA \"United Kingdom\" Quit do case $REPLY in 1 ) echo \"You speak French\" ;; 2 ) echo \"You speak German\" ;; 3 ) echo \"You speak American English\" ;; 4 ) echo \"You speak UK English\" ;; 5 ) echo \"Quitting!!\" break ;; * ) echo \"Invalid Option $REPLY \" ;; esac done","title":"Menus in Bash"},{"location":"learning/linux/dns/","text":"Testing DNS # Install Bind DNS server apt install bind9 bind9utils bind9-doc -y systemctl status bind9 # Installs and starts the named service # named is the actual daemon or service that is executed by bind9 # Change the DNS server to only serve IPv4 adresses # Update /etc/default/named file and add below config OPTIONS = \"-u bind -4\" # -4 for IPV4 and -6 for IPV6 systemctl reload-or-restart bind9 # restart the service systemctl status bind9 # Testing DNS service # Login to the DNS server using ssh and execute dig -t a @localhost google.com # localhost will use the existing Bind9 service to fetch the DNS answers # a will give the IP address as answers dig -t a @1.1.1.1 google.com # 1.1.1.1 is the public Cloudflare DNS server dig -t a @localhost parrotlinux.org # Multiple Nameservers are shown as athorative DNS servers dig -t ns @localhost cisco.com # It will give the name servers for cisco dig -t a @localhost ns1.cisco.com # It will give the IP address of the nameserver ns1 of cisco # dig function dns -> dig +nocmd $1 ANY +multiline +noall +answer Debug Network calls \u00b6 sudo ss -plnt | grep ':53' # Check which process is listenng on port 53 SSL \u00b6 # Connect SSL on the server to test SSL openssl s_client -connect 'localhost:443' -showcerts --servername 'vm.domain.com' -CAfile '/etc/tls.pem' # Connect SSL by resolving DNS using curl on localhost curl --resolve vm.domain.com:443:127.0.0.1 https://localhost/ping -key /etc/tls.key","title":"Dns"},{"location":"learning/linux/dns/#debug-network-calls","text":"sudo ss -plnt | grep ':53' # Check which process is listenng on port 53","title":"Debug Network calls"},{"location":"learning/linux/dns/#ssl","text":"# Connect SSL on the server to test SSL openssl s_client -connect 'localhost:443' -showcerts --servername 'vm.domain.com' -CAfile '/etc/tls.pem' # Connect SSL by resolving DNS using curl on localhost curl --resolve vm.domain.com:443:127.0.0.1 https://localhost/ping -key /etc/tls.key","title":"SSL"},{"location":"learning/linux/firewall/","text":"netstat nmap Testing Firewall using tcpdump conntrack - Cnnection Tracking basics Linux Firewall Fundamentals \u00b6 Firewalls control network access. Linux firewall = Netfilter + IPTables Netfilter is a kernel framework. IPTables is a packet selection system. Use the iptables command to control the firewall. Default Tables \u00b6 Filter - Most commonly used table. NAT - Network Address Translation. Mangle - Alter packets. Raw - Used to disable connection tracking. Security - Used by SELinux. Default Chains \u00b6 INPUT OUTPUT FORWARD PREROUTING POSTROUTING Rules \u00b6 Rules = Match + Target Match on: Protocol, Source/Dest IP or network, Source/Dest Port, Network Interface Example: protocol: TCP, source IP: 1.2.3.4, dest port: 80 Targets \u00b6 Built-in targets: ACCEPT DROP REJECT LOG RETURN iptables / ip6tables \u00b6 Command line interface to IPTables/netfilter. List / View iptables \u00b6 iptables -L # Display the filter table. iptables -t nat -L # Display the nat table. iptables -nL # Display using numeric output. iptables -vL # Display using verbose output. iptables --line-numbers -L # Use line nums. iptables -vnL # Common combination Chain Policy / Default Target \u00b6 Set the default TARGET for CHAIN: iptables -P CHAIN TARGET Example: iptables -P INPUT DROP # Appending Rules iptables -A CHAIN RULE-SPECIFICATION iptables [ -t TABLE ] -A CHAIN RULE-SPECIFICATION # Inserting Rules iptables -I CHAIN [ RULENUM ] RULE-SPECIFICATION # Deleting Rules iptables -D CHAIN RULE-SPECIFICATION iptables -D CHAIN RULENUM # Flushing rules or deleting tables iptables [ -t table ] -F [ chain ] Rule Specification Options \u00b6 -s 10 .11.12.13 # Source IP, network, or name -d 216 .58.192.0/19 # Destination IP, network, or name -p tcp / udp / icmp # Protocol -p tcp --dport 80 # Destination Port -p tcp --sport 8080 # Source Port -p icmp --icmp-type echo-reply # ICMP packet type, gives pong response -p icmp --icmp-type echo-request # Gives ping response # Rating limiting -m limit --limit rate [ /second/minute/hour/day ] # Match until a limit is reached. -m limit --limit-burst # --limit default is 3/hour and --limit-burst default is 5 -m limit --limit 5 /m --limit-burst 10 # /s = second, /m = minutes -m limit ! --limit 5 /s # ! = invert the match Target / Jump \u00b6 To specify a jump point or target: -j TARGET_OR_CHAIN -j ACCEPT # Built-in target. -j DROP # Built-in target. -j LOGNDROP # Custom chain. Creating and Deleting a Chain \u00b6 Create CHAIN: iptables [-t table] -N CHAIN Delete CHAIN: iptables [-t table] -X CHAIN Saving Rules \u00b6 # Debian / Ubuntu: apt-get install iptables-persistent netfilter-persistent save - Chain Traversal Flowchart Netfilter/iptable Front-Ends \u00b6 Uses iptables command on the back-end Firewalld - CentOS/RHEL UFW - Uncomplicated FireWall (Ubuntu) GUFW - Graphical interface to UFW system-configure-firewall - CentOS/RHEL tcpdump for Testing Firewall \u00b6 ########################## ## Sniffing traffic using tcpdump ########################## # listing all interfaces ifconfig -a # start sniffing on an interface tcpdump -i eth0 # sniffing only packets to or form an ip address, domain (dns lookup) or network tcpdump -i eth0 host 8 .8.8.8 tcpdump -i eth0 dst medium.com -n # -n -> do not convert addresses to names tcpdump -i eth0 net 192 .168.0.0/24 # sniffing only packets to or from a specific port tcpdump -i eth0 port 443 -vv -n # -vv -> verbose # using the `or` operator tcpdump -i eth0 port 80 or port 443 # sniffing only packets to a specific port tcpdump -i eth0 dst port 53 -vv -n # sniffing only packets from a specific port tcpdump -i eth0 dst port 22 -vv -n # -A outputs ascii strings and -X outputs both in ascii and hexadecimal tcpdump -i eth0 port 80 -A -n tcpdump -i eth0 port 80 -X -n # writing captured packets to file tcpdump port 80 -w web.pcap # reading from a file tcpdump -r web.pcap IP Tables Examples \u00b6 # Allow anyone to connect to webserver, but only internal ips to connect via SSH # Block all other traffic iptables -L # List the existing rules, no rules # Accept all incoming TCP traffic on port 80 iptables -A INPUT -p tcp --dport 80 -j ACCEPT # OR iptables -A INPUT -s 0 /0 -p tcp --dport 80 -j ACCEPT iptables -nL # List the current new rule # Accept all incoming SSH traffic on port 22 originating from the internal network iptables -A INPUT -p tcp --dport 22 -s 10 .0.0.0/24 -j ACCEPT # Drop all packets which dont match above 2 rules iptables -A INPUT -j DROP # Testing the filters nc -v 10 .0.0.8 80 # Net cat using an internal IP on port 80 nc -v 10 .0.0.8 22 # Test the SSH connection # Deleting existing rule iptables -D INPUT 1 # Deletes the Web server traffic on port 80 nc -w 2 -v 10 .0.0.8 80 # -w specifies timeout of 2 seconds # Reject the packets instead of dropping them iptables -D INPUT 2 # Deletes the drop rule iptables -A INPUT -J REJECT # Now rejects the packets nc -w 2 -v 10 .0.0.8 80 # Connection refused # Flush all existing rules iptables -F iptables -A INPUT -p tcp --dport 22 -s 10 .0.0.0/24 -j ACCEPT # Create a new custom table iptables -N LOGNDROP # Create a rule to log all incoming traffic other than internal network SSH connection iptables -A LOGNDROP -p tcp -m limit --limit 5 /min -j LOG --log-prefix \"iptables BLOCK \" iptables -A INPUT -j LOGNDROP # Accept and log before dropping the connection other than internal network # In another terminal, tail the syslog file tail -f /var/log/syslog # Other terminal nc -w 2 -v 10 .0.0.8 80 # This will log and drop the connection # Persist the iptables changes after reboot netfilter- persistent save Rule Specification Example \u00b6 # To drop all traffic from given source ip iptables -A INPUT -s 216 .58.219.174 -j DROP # List the filter table, as the above rule didnt specify table name iptables -nL # Chain INPUT (policy ACCEPT) # target prot opt source destination # DROP all -- 216.58.219.174 0.0.0.0/0 # Accept SSH connection from only one network, drop all other SSH connections iptables -A INPUT -s 10 .0.0.0/24 -p tcp --dport 22 -j ACCEPT iptables -A INPUT -p tcp --dport 22 -j DROP # Accept incoming HTTPS traffic from source or any destination iptables -A INPUT -s 8 .8.8.8 -p tcp --dport 443 -j ACCEPT # Accept incoming traffic from google iptables -A INPUT -s 0 /0 -p tcp --dport 443 -j ACCEPT # Incoming traffic from anywhere # Drop outgoing traffic to ubuntu.com iptables -A OUTPUT -d www.ubuntu.com -j DROP iptables -vnL # List shows ubuntu domain name has been resolved to IP address for ubuntu iptables -A OUTPUT -p tcp --dport 443 -d www.ubuntu.com -j DROP # Drop only HTTPS traffic # Use an application Firewall like SQUID to block big websites like google.com or facebook.com # instead of using IPTables # Using IP Ranges to filter traffic # This range filter helps to keep the IPTables managable and readable iptables -A INPUT -p tcp --dport 25 -m iprange --src-range 10 .0.0.10-10.0.0.19 -j DROP # This drops SMTP tcp traffic coming from the 10 ip addresses on port 25 # Dropping outgoing traffic destined for type MULTICAST to avoid joining the server to any group iptables -A OUTPUT -m addrtype --dst-type MULTICAST -j DROP # Using multiple ports for filter iptables -A OUTPUT -p tcp -m multiport --dports 80 ,443 -j ACCEPT # Allows HTTP and HTTPS outgoing traffic # Notice `dports` is plural # Filter traffic for protocols that dont use ports like ICMP cat /etc/protocols # List the protocols and ports # Dropping all traffic except TCP and UDP iptables -P INPUT -j DROP # Policy to drop all traffic by default if no match found iptables -P OUTPUT -j DROP iptables -A INPUT -i lo -j ACCEPT # Allow loopback interface traffic iptables -A OUTPUT -i lo -j ACCEPT iptables -A INPUT -p tcp -j ACCEPT # Allow tcp traffic iptables -A OUTPUT -p tcp -j ACCEPT iptables -A INPUT -p udp -j ACCEPT # Allow udp traffic iptables -A OUTPUT -p udp -j ACCEPT ping 8 .8.8.8 # This will be blocked as its ICMP dig www.google.com # This will work as DNS is UDP traffic # dropping incoming GRE traffic iptables -A INPUT -p gre -j DROP # allowing outgoing ICMP traffic iptables -A OUTPUT -p icmp -j DROP # Filter traffic based on interface (LAN, WLAN) # dropping ssh traffic that's coming on eth0 interface (suppose it's external) iptables -A INPUT -p tcp --dport 22 -i eth0 -j DROP # allowing ssh traffic that's coming on eth1 interface (suppose it's internal) iptables -A INPUT -p tcp --dport 22 -i eth1 -j ACCEPT # allowing outgoing https traffic via eth1 iptables -A OUTPUT -p tcp --dport 443 -o eth1 -j ACCEPT # Filter traffic based out of Negation # This is easy to whitelist one IP # This will drop all HTTPS traffic except that coming from 0.3 IP address iptables -A INPUT ! -s 192 .168.0.3 -p tcp --dport 443 -j DROP # dropping all incoming ssh traffic accepting packets from 100.0.0.1 (management station) iptables -A INPUT -p tcp --dport 22 ! -s 100 .0.0.1 -j DROP # dropping all outgoing https traffic excepting to www.linux.com iptables -A OUTPUT -p tcp --dport 443 ! -d www.linux.com -j DROP # dropping all communication excepting that with the default gateway (mac is b4:6d:83:77:85:f4) iptables -A INPUT -m mac ! --mac-source b4:6d:83:77:85:f4 -j DROP # The DNS Server of your LAN is set to 8.8.8.8. You don't want to allow the users of the LAN to change the DNS server. iptables -A FORWARD -p udp --dport 53 ! -d 8 .8.8.8 -j DROP # Filter traffic based on tcp flags # dropping all incoming tcp packets that have syn set iptables -A INPUT -p tcp --syn -j DROP # logging outgoing traffic that has syn and ack set iptables -A OUTPUT -p tcp --tcp-flags syn,ack,rst,fin syn,ack -j LOG # Allow establishing incoming ssh (tcp/22) connections only from the LAN. # The internal interface is called eth0 and the external interface is called eth1. iptables -A FORWARD -p tcp --dport 22 --syn -i eth0 -j ACCEPT iptables -A FORWARD -p tcp --dport 22 --syn -i eth1 -j DROP # Drop all linux router outgoing packets of type tcp (port 80 and 443) to www.linuxquestions.org iptables -A FORWARD -p tcp --dport 80 -d www.linuxquestions.org -j DROP iptables -A FORWARD -p tcp --dport 443 -d www.linuxquestions.org -j DROP # Connection Tracking # connection tracking = Stateful firewall = better than stateless and is secure # connection tracking can be used even if the protocol is itself stateless (ex: ICMP, UDP) # Netfilter is a Stateful firewall ################## # Example of a Stateful Firewall for a Desktop ################## #!/bin/bash iptables -f iptables -A INPUT -i lo -j ACCEPT iptables -A OUTPUT i lo -j ACCEPT iptables -A INPUT -m state --state INVALID -j DROP # All invalid state connections are blocked iptables -A OUTPUT -m state --state INVALID -j DROP iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # All valid state connections are allowed iptables -A OUTPUT -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT # Allow new connections iptables -P INPUT -j DROP # Drop any other connections to desktop iptables -P OUTPUT -j DROP ################## # Test the Stateful firewall # Open Browser and test a https website # SSH connection to and from the Desktop # Ping and SSH from another machine to Desktop --> This will be dropped as NEW connection is dropped in INPUT # Selectively allow SSH action from the network, add it before INVALID iptables -A INPUT -p tcp --dport 22 -m state --state NEW -s 192 .168.0.20 -j ACCEPT # OR iptables -A INPUT -p tcp --dport 22 --syn -s 192 .168.0.20 -j ACCEPT ################## # Filter using Date and Time date # Check the current date and timezone of the server # Permit SSH From Mon to Fri between 10 AM to 4 PM iptables -A INPUT -p tcp --dport 22 -m time --timestart 10 :00 --timestop 16 :00 -j ACCEPT iptables -A INPUT -p tcp --dport 22 -j DROP # Permit access to website after working hours iptables -A INPUT -p tcp --dport 443 -d www.ubuntu.com -m time --timestart 18 :00 --timestop 8 :00 -j ACCEPT iptables -A INPUT -p tcp --dport 443 -d www.ubuntu.com -j DROP # DoS protection by Connlimit module iptables -A INPUT -p tcp --dport 25 --syn -m connlimit --connlimit-above 5 -j REJECT --reject-with tcp-rst # This rejects and drops conections coming the same IP after 5 TCP connections and resets the connections. # DoS protection by implementing Rate limiting # default limit-burst is 5 and is the allowed packets to be matched from same IP in a single burst, # after this the \"limit\" value is the next allowed value once burst is exhausted # Limit ICMP packets from one IP, max 7 packets per sec and then apply rate limit of 1 per sec iptables -A INPUT -p icmp --icmp-type echo-request -m limit 1 /sec --limit-burst 7 -j ACCEPT iptables -A INPUT -p icmp --icmp-type echo-request -j DROP # Testing the icmp rule # Run tcdump on the server terminal tcpdump icmp -n # This shows only icmp traffic and IP addresses # In the second server, ping the icmp server with 10 request per sec ping -i 0 .1 192 .168.0.10 # Interval # Output on ping server, after 7 packets, rate drops to 1 packet per sec # Allow only 5 new incoming connections per second to port 443 (https) iptables -A INPUT -p tcp --dport 443 --syn -m limit --limit 5 /sec -j ACCEPT iptables -A INPUT -p tcp --dport 443 --syn -j DROP # Dynamic Blacklist Database Creation - Using recent match # Recent match options: # --name: creates a list in which the source IP address will be added and checked. # --set: adds the source IP address to the list. # The list with blacklisted IP addresses is found in: /proc/net/xt_recent/LIST_NAME # Example to blacklist IP for 60 sec, when port 25 is accessed between 8AM to 10 PM iptables -A INPUT -m recent --name hackers --update --seconds 60 -j DROP # Above rule will update the hackers list with recent timestamp when a new request comes from same IP # when a packet is coming, it will be checked against this rule and # if its source ip belongs to the hacker list, the packet will be dropped # last seen time is updated with another 60 seconds (the source ip address stays in the list for another 60 seconds) iptables -A INPUT -p tcp --dport 25 -m time --timestart 8 :00 --timestop 22 :00 \\ -m recent -name hackers --set -j DROP # when the 1st matched packet arrives (tcp/25 between 8:00-10:00 UTC), a list named hacker is created, # the source ip address of the packet is added to that list and the packet is dropped #Testing Backlist process # Second server, open 2 terminals # Terminal 1, run ping, Terminal 2 start NMAP to scan port 25. # As soon as NMAP is run, ping output is stopped as server 2 is added to blacklist nmap -p 25 192 .168.0.10 # Display the blacklist on server 1 cat /proc/net/xt_recent/hackers # After 60 secs after ping output is stopped, running ping again starts working, till nmap is not run again # Filter Output using Quotas # --quota - Data in bytes # Allow one server to download 1GB from website iptables -A OUTPUT -d 80 .0.0.1 -p tcp --sport 80 -m quota --quota 1000000000 -j ACCEPT iptables -A OUTPUT -d 80 .0.0.1 -p tcp --sport 80 -j DROP # Allow serverto accept only 10Mb from another server iptables -A INPUT -s 192 .168.0.20 -p tcp --dport 80 -m quota --quota 10000000 -j ACCEPT iptables -A INPUT -s 192 .168.0.20 -p tcp --dport 80 -j DROP # Testing the quota rule # in Server 2, check for a file with size more than 20Mb ls -lSh file.tar # Gives the size of the file scp file.tar root@192.168.0.10:~ # Copy the file to server 1 home directory # On server1, the transfer stops after 10Mb iptables -vnL # Shows only 10Mb is copied ls ~ # Shows partial file # ipset # \"ipset\" is an extension to iptables that allows us to create a firewall that match entire \"sets\" # of addresses at once. # IP sets are stored in indexed data structures instead of chains which are stored & traversed linearly. sudo apt install ipset # Install ipset ipset -N myset hash:ip # Creates a new set containing IP data ipset -A myset 1 .1.1.1 # Adding data to the set ipset -A myset 2 .2.2.2 ipset -A myset 8 .8.8.8 # Add a rule to drop packets from myset in iptables iptables -A INPUT -m set --match-set myset src -j DROP # Test the rule ping 8 .8.8.8 # Will fail # Adding a network to ipset ipset -N china hast:net -exist # Creates a new set or ignores if it already exist ipset -A china 1 .0.0.0/8 -exist # Adds entry to set or ignores if alreadt exist ipset -L # List the sets ipset -L china # Displays china set ipset -D china 1 .0.0.0/8 # Deletes the entry from china set ipset -F china # Deletes all entries from china set ipset -F # Flushes all entries from all sets ipset -X china # Deletes the set china # Delete will not work, if the set is referenced in an iptable. Delete the reference, then delete the set. # Dynamic Blacklist Database Creation - Using ipset ipset -N auto_blocked hast:ip -exist iptables -I INPUT -p tcp --dport 80 -j SET --add-set auto_blocked src # Adding IP to blocked set iptables -I INPUT -m set --match-set auto_blocked src -j DROP # Adding rule to block the IP set #Testing Backlist process # Second server, open 2 terminals # Terminal 1, run ping, Terminal 2 start tenet to send request to port 80. # As soon as telnet is run, ping output is stopped as server 2 is added to blacklist telnet 192 .168.0.20 80 # Block all IPs and Networks From File # a file called bad_hosts.txt exists in the same directory with the script and # contains IPs and Networks, one per line like: # 11.0.0.16 # 8.8.8.8 # 1.2.3.4 # 192.0.0.0/16 #!/bin/bash # File that contains the IPs and Nets to block FILE = \"bad_hosts.txt\" # Creating a new set ipset -N bad_hosts iphash -exist # Flushing the set if it exists ipset -F bad_hosts echo \"Adding IPs from $FILE to bad_hosts set:\" for ip in ` cat $FILE ` do ipset -A bad_hosts $ip echo -n \" $ip \" done # Adding the iptables rule that references the set and drops all ips and nets echo -e -n \"\\nDropping with iptables... \" iptables -I INPUT -m set --match-set bad_hosts src -j DROP echo \"Done\" # Blocking Countries #!/bin/bash # Check if the file exists (in the current directory) and if yes, remove it if [ -f \"cn-aggregated.zone\" ] then rm cn-aggregated.zone fi # Download the aggregate zone file for China wget http://www.ipdeny.com/ipblocks/data/aggregated/cn-aggregated.zone # Check if there was an error if [ $? -eq 0 ] then echo \"Download Finished!\" else echo \"Download Failed! Exiting ...\" exit 1 fi # Creating a new set called china of type hash:net (nethash) ipset -N china hash:net -exist # Flushing the set ipset -F china # Iterate over the Networks from the file and add them to the set echo \"Adding Networks to set...\" for i in ` cat cn-aggregated.zone ` do ipset -A china $i done # Adding a rule that references the set and drops based on source IP address echo -n \"Blocking CN with iptables ... \" iptables -I INPUT -m set --match-set china src -j DROP echo \"Done\" Target Example \u00b6 # netstat apt install net-tools # Installs netstat netstat -tupan # Shows 't'cp, 'u'dp 'p'rotocols and 'a'll the services that are using the ports # nmap apt install nmap # Installs nmap nmap -sS 192 .168.0.20 # Performs tcp sync scan. Scans only 1000 ports nmap -p- 192 .168.0.20 # Performs all ports scan (0-65535) nmap -p 80 ,50005 -sV 192 .168.0.20 # Specify ports to scan and display the service that is running nmap -sP 192 .168.0.0/24 # Ping scanning (entire Network) nmap -sS 192 .168.0.0/24 --exclude 192 .168.0.10 # Excluding an IP # PRO-TIP: Always output the data into a file nmap -oN output.txt 192 .168.0.1 # Saving the scanning report to a file nmap -A -T aggressive cloudflare.com # -A OS and service detection with faster execution nmap 10 .211.55.6 -O # Server's operating system nmap 10 .211.55.6 -sC -o /tmp/output.txt # Runs a set list of default scripts against your target. nmap 10 .211.55.6 -sU \u2013top-ports 250 # scan the 250 most common UDP ports # Reject # PRO-TIP: It is efficient to REJECT than DROP targets. iptables -j REJECT --help # Shows reject-with types # Reject SSH traffic and test it iptables -A INPUT -p tcp --dport 22 -s 192 .168.0.10 -j REJECT --reject-with tcp-reset # start tcpdump to trace the output on server 1 tcpdump host 192 .168.0.10 -n # Shows incoming traffic from ssh client # In client server, run nmap to scan port 22 nmap -p 22 192 .168.0.20 # reject all incoming ICMP ping packets that are not coming from 10.0.0.1 (management station) iptables -A INPUT ! -s 10 .0.0.1 -p icmp --icmp-type echo-request -j REJECT --reject-with admin-prohib # Log # Logging detailed info about the packet contents. # Will be shown in dmesg command to show the kernel logs that are in memory. Will be lost at server restart. tail -f /var/log/kern.log # Shows the kernel logs that are persisted even after restart. # logging the first packet (tcp syn set) of any incoming ssh connection # use the prefix: ###ssh: iptables -A INPUT -p tcp --syn --dport 22 -j LOG --log-prefix = \"##ssh:\" --log-level info # filtering logs by prefix dmesg | grep \"##ssh:\" # logging only 10 incoming ICMP ping packets per minute iptables -A INPUT -p icmp --icmp-type echo-request -m limit --limit 10 /minute -j LOG --log-prefix = \"ping probe:\" # Tee # Allows you to mirror incoming traffic on one server to another server # mirror all TCP traffic that arrives at 10.0.0.10 to 10.0.0.1. iptables -A INPUT -p tcp -j TEE --gateway 10 .0.0.1 # Testing # Open tcpdump on server 1, From third server ping server 2. tcpdump will show packets coming on server 2 # Redirect # Allows you to transparently proxy a service on a server # Redirect incoming TCP traffic to port 80 to port 8080 on the same host where a Proxy is running. iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 8080 NAT and Port Forwarding Examples \u00b6 # SNAT replaces the private IP address from the packet with the public IP address # of the router external interface. # SNAT helps private servers to access the internet via a router IP. # Example to make the linux server as a Router #!/bin/bash # flush the nat table of all chains iptables -t nat -F # enable routing process echo \"1\" > /proc/sys/net/ipv4/ip_forward # define rules that match traffic that should be NATed # we perform NAT for the entire subnetwork # enp0s3 is the external interface # 80.0.0.1 is the public & static ip address iptables -t nat -A POSTROUTING -s 10 .0.0.0/24 -o enp0s3 -j SNAT --to-source 80 .0.0.1 # OR # if the public IP address is dynamic we use MASQUERADE instead of SNAT iptables -t nat -A POSTROUTING -s 10 .0.0.0/24 -o enp0s3 -j MASQUERADE # it's not mandatory to perform NAT for entire subnet. We could perform NAT only for some protocols # Example: we perform NAT only for TCP iptables -t nat -A POSTROUTING -s 10 .0.0.0/24 -p tcp -o enp0s3 -j SNAT --to-source 80 .0.0.1 # filtering is done on FORWARD CHAIN # Port Forwarding # DNAT permits connections from the Internet to servers with private IP addresses inside LAN. # The client connects to the public IP address of the DNAT Router which in turn # redirects tra\ufb03c to the private server. # The server with the private IP address stays invisible. ### PORT FORWARDING (DNAT) ### #!/bin/bash # flushing nat filter of PREROUTING chain iptables -t nat -F PREROUTING # all the packets coming to the public IP address of the router and port 80 # will be port forwarded to 192.168.0.20 and port 80 iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination 192 .168.0.20 ## VARIANTS # 1.redirect port 8080 on router to port 80 on the private web-server # Internet clients connect to the public IP address of the router and port 8080 and the packets are # redirected to the private server with 192.168.0.20 and port 80 iptables -t nat -A PREROUTING -p tcp --dport 8080 -j DNAT --to-destination 192 .168.0.20:80 #2. Load-Balancing # On all 5 private servers (192.168.0.20-192.168.0.24)run the same service (e.g. HTTPS) # The router will pick-up a random private IP from the range and then translate and send (port-forward) the packet to that IP iptables -t nat -A PREROUTING -p tcp --dport 8080 -j DNAT --to-destination 192 .168.0.20-192.168.0.24 ##** LOAD BALANCE NAT TRAFFIC OVER 2 INTERNET CONNECTIONS WITH DYNAMIC IP ADDRESSES **## #!/bin/bash # Traffic that goes over the first ISP connection # web: 80 443 # email: 25 465 143 993 110 995 # ssh: 22 ISP1 = \"22 25 80 110 143 443 465 993 995\" # flushing nat table and POSTROUTING chain iptables -t nat -F POSTROUTING # enable routing echo \"1\" > /proc/sys/net/ipv4/ip_forward for port in $ISP1 do iptables -t nat -A POSTROUTING -p tcp --dport $port -o eth1 -j MASQUERADE done # Traffic not NATed goes over the 2nd ISP connection iptables -t nat -A POSTROUTING -o eth2 -j MASQUERADE Custom Defined Chain Examples \u00b6 # creating a user-defined chain iptables -N TCP_TRAFFIC # add rules to the chain iptables -A TCP_TRAFFIC -p tcp -j ACCEPT # jump to the chain from the input chain iptables -A INPUT -p tcp -j TCP_TRAFFIC # flush all rules in the chain iptables -F TCP_TRAFFIC # flush all rules otherwise custom chain can't be deleted iptables -F # delete the chain iptables -X TCP_TRAFFIC Chain Traversal Order \u00b6 # Incoming Packet Traversal # Packet comes from the Network and is destined for our host, which is the final destination PREROUTING --> INPUT # Packet comes from the Network and must be routed by the Linux machine PREROUTING --> FORWARD --> POSTROUTING # Outgoing Packet Traversal # Packet is generated by an Application on the host LOCAL APPLICATION --> OUTPUT --> POSTROUTING TCP Wrappers \u00b6 Host-based networking ACL system. Controls access to \u201cwrapped\u201d services. A wrapped service is compiled with libwrap support. Check if a service supports wrappers use ldd . ldd /usr/sbin/sshd | grep libwrap # Prints required shared libraries. Can control access by IP address / networks. Can control access by hostname. Transparent to the client and service. Used with xinetd which is a super daemon as it can manage a lot of smaller services, secures access to your server. Centralized management for multiple network services. Runtime configuration Configuring TCP Wrappers \u00b6 /etc/hosts.allow is checked first. If a match is found, access is granted . /etc/hosts.deny is checked next. If a match is found, access is denied . Access Rules \u00b6 The rule format for hosts.allow and hosts.deny are the same. One rule per line Format: SERVICE(S) : CLIENT(S) [: ACTION(S) ] # Deny All configuration # /etc/hosts.deny: ALL : ALL # Deny all connections # /etc/hosts.allow: # Explicitly list allowed connections here. sshd : 10 .11.12.13 # Allow only ssh connection from this ip # Additional configuration possibilities sshd, imapd : 10 .11.12.13 # Multiple services allowed ALL : 10 .11.12.13 # Wild Card configuration sshd : 10 .11.12.13, 10 .5.6.7 # Multiple IPs sshd : jumpbox.example.com # Domain name sshd : .admin.example.com # Subdomain - server2.admin.example.com or webdev.admin.example.com sshd : jumpbox*.example.com # jumpbox4admins.example.com sshd : jumpbox0?.example.com # jumpbox03.example.com sshd : 10 .11.12. # Subnet range sshd : 10 . sshd : 10 .11.0.0/255.255.0.0 sshd : /etc/hosts.sshd # Allow all hosts in this file imapd : ALL # Wildcard in clients sshd : ALL EXCEPT .hackers.net # Conditions, except IP in hacker.net domains sshd : 10 .11.12.13 : severity emerg # Actions - Log emergency messages for that ssh connection coming from IP sshd : 10 .11.12.13 : severity local0.alert # Raise local alert # Raise custom message # /etc/hosts.deny: sshd : .hackers.net : spawn /usr/bin/wall \u201cAttack in progress.\u201d # Use expressions in message, %a logs client ip sshd : .hackers.net : spawn /usr/bin/wall \u201cAttack from %a.\u201d ## Possible expression expansions # %a (%A) The client (server) host address. # %c Client information. # %d The daemon process name. # %h (%H) The client (server) host name or address. # %n (%N) The client (server) host name. # %p The daemon process id. # %s Server information. # %u The client user name (or \"unknown\"). # %% Expands to a single `% \u0301 character.","title":"Linux_Firewall"},{"location":"learning/linux/firewall/#linux-firewall-fundamentals","text":"Firewalls control network access. Linux firewall = Netfilter + IPTables Netfilter is a kernel framework. IPTables is a packet selection system. Use the iptables command to control the firewall.","title":"Linux Firewall Fundamentals"},{"location":"learning/linux/firewall/#default-tables","text":"Filter - Most commonly used table. NAT - Network Address Translation. Mangle - Alter packets. Raw - Used to disable connection tracking. Security - Used by SELinux.","title":"Default Tables"},{"location":"learning/linux/firewall/#default-chains","text":"INPUT OUTPUT FORWARD PREROUTING POSTROUTING","title":"Default Chains"},{"location":"learning/linux/firewall/#rules","text":"Rules = Match + Target Match on: Protocol, Source/Dest IP or network, Source/Dest Port, Network Interface Example: protocol: TCP, source IP: 1.2.3.4, dest port: 80","title":"Rules"},{"location":"learning/linux/firewall/#targets","text":"Built-in targets: ACCEPT DROP REJECT LOG RETURN","title":"Targets"},{"location":"learning/linux/firewall/#iptables--ip6tables","text":"Command line interface to IPTables/netfilter.","title":"iptables / ip6tables"},{"location":"learning/linux/firewall/#list--view-iptables","text":"iptables -L # Display the filter table. iptables -t nat -L # Display the nat table. iptables -nL # Display using numeric output. iptables -vL # Display using verbose output. iptables --line-numbers -L # Use line nums. iptables -vnL # Common combination","title":"List / View iptables"},{"location":"learning/linux/firewall/#chain-policy--default-target","text":"Set the default TARGET for CHAIN: iptables -P CHAIN TARGET Example: iptables -P INPUT DROP # Appending Rules iptables -A CHAIN RULE-SPECIFICATION iptables [ -t TABLE ] -A CHAIN RULE-SPECIFICATION # Inserting Rules iptables -I CHAIN [ RULENUM ] RULE-SPECIFICATION # Deleting Rules iptables -D CHAIN RULE-SPECIFICATION iptables -D CHAIN RULENUM # Flushing rules or deleting tables iptables [ -t table ] -F [ chain ]","title":"Chain Policy / Default Target"},{"location":"learning/linux/firewall/#rule-specification-options","text":"-s 10 .11.12.13 # Source IP, network, or name -d 216 .58.192.0/19 # Destination IP, network, or name -p tcp / udp / icmp # Protocol -p tcp --dport 80 # Destination Port -p tcp --sport 8080 # Source Port -p icmp --icmp-type echo-reply # ICMP packet type, gives pong response -p icmp --icmp-type echo-request # Gives ping response # Rating limiting -m limit --limit rate [ /second/minute/hour/day ] # Match until a limit is reached. -m limit --limit-burst # --limit default is 3/hour and --limit-burst default is 5 -m limit --limit 5 /m --limit-burst 10 # /s = second, /m = minutes -m limit ! --limit 5 /s # ! = invert the match","title":"Rule Specification Options"},{"location":"learning/linux/firewall/#target--jump","text":"To specify a jump point or target: -j TARGET_OR_CHAIN -j ACCEPT # Built-in target. -j DROP # Built-in target. -j LOGNDROP # Custom chain.","title":"Target / Jump"},{"location":"learning/linux/firewall/#creating-and-deleting-a-chain","text":"Create CHAIN: iptables [-t table] -N CHAIN Delete CHAIN: iptables [-t table] -X CHAIN","title":"Creating and Deleting a Chain"},{"location":"learning/linux/firewall/#saving-rules","text":"# Debian / Ubuntu: apt-get install iptables-persistent netfilter-persistent save - Chain Traversal Flowchart","title":"Saving Rules"},{"location":"learning/linux/firewall/#netfilteriptable-front-ends","text":"Uses iptables command on the back-end Firewalld - CentOS/RHEL UFW - Uncomplicated FireWall (Ubuntu) GUFW - Graphical interface to UFW system-configure-firewall - CentOS/RHEL","title":"Netfilter/iptable Front-Ends"},{"location":"learning/linux/firewall/#tcpdump-for-testing-firewall","text":"########################## ## Sniffing traffic using tcpdump ########################## # listing all interfaces ifconfig -a # start sniffing on an interface tcpdump -i eth0 # sniffing only packets to or form an ip address, domain (dns lookup) or network tcpdump -i eth0 host 8 .8.8.8 tcpdump -i eth0 dst medium.com -n # -n -> do not convert addresses to names tcpdump -i eth0 net 192 .168.0.0/24 # sniffing only packets to or from a specific port tcpdump -i eth0 port 443 -vv -n # -vv -> verbose # using the `or` operator tcpdump -i eth0 port 80 or port 443 # sniffing only packets to a specific port tcpdump -i eth0 dst port 53 -vv -n # sniffing only packets from a specific port tcpdump -i eth0 dst port 22 -vv -n # -A outputs ascii strings and -X outputs both in ascii and hexadecimal tcpdump -i eth0 port 80 -A -n tcpdump -i eth0 port 80 -X -n # writing captured packets to file tcpdump port 80 -w web.pcap # reading from a file tcpdump -r web.pcap","title":"tcpdump for Testing Firewall"},{"location":"learning/linux/firewall/#ip-tables-examples","text":"# Allow anyone to connect to webserver, but only internal ips to connect via SSH # Block all other traffic iptables -L # List the existing rules, no rules # Accept all incoming TCP traffic on port 80 iptables -A INPUT -p tcp --dport 80 -j ACCEPT # OR iptables -A INPUT -s 0 /0 -p tcp --dport 80 -j ACCEPT iptables -nL # List the current new rule # Accept all incoming SSH traffic on port 22 originating from the internal network iptables -A INPUT -p tcp --dport 22 -s 10 .0.0.0/24 -j ACCEPT # Drop all packets which dont match above 2 rules iptables -A INPUT -j DROP # Testing the filters nc -v 10 .0.0.8 80 # Net cat using an internal IP on port 80 nc -v 10 .0.0.8 22 # Test the SSH connection # Deleting existing rule iptables -D INPUT 1 # Deletes the Web server traffic on port 80 nc -w 2 -v 10 .0.0.8 80 # -w specifies timeout of 2 seconds # Reject the packets instead of dropping them iptables -D INPUT 2 # Deletes the drop rule iptables -A INPUT -J REJECT # Now rejects the packets nc -w 2 -v 10 .0.0.8 80 # Connection refused # Flush all existing rules iptables -F iptables -A INPUT -p tcp --dport 22 -s 10 .0.0.0/24 -j ACCEPT # Create a new custom table iptables -N LOGNDROP # Create a rule to log all incoming traffic other than internal network SSH connection iptables -A LOGNDROP -p tcp -m limit --limit 5 /min -j LOG --log-prefix \"iptables BLOCK \" iptables -A INPUT -j LOGNDROP # Accept and log before dropping the connection other than internal network # In another terminal, tail the syslog file tail -f /var/log/syslog # Other terminal nc -w 2 -v 10 .0.0.8 80 # This will log and drop the connection # Persist the iptables changes after reboot netfilter- persistent save","title":"IP Tables Examples"},{"location":"learning/linux/firewall/#rule-specification-example","text":"# To drop all traffic from given source ip iptables -A INPUT -s 216 .58.219.174 -j DROP # List the filter table, as the above rule didnt specify table name iptables -nL # Chain INPUT (policy ACCEPT) # target prot opt source destination # DROP all -- 216.58.219.174 0.0.0.0/0 # Accept SSH connection from only one network, drop all other SSH connections iptables -A INPUT -s 10 .0.0.0/24 -p tcp --dport 22 -j ACCEPT iptables -A INPUT -p tcp --dport 22 -j DROP # Accept incoming HTTPS traffic from source or any destination iptables -A INPUT -s 8 .8.8.8 -p tcp --dport 443 -j ACCEPT # Accept incoming traffic from google iptables -A INPUT -s 0 /0 -p tcp --dport 443 -j ACCEPT # Incoming traffic from anywhere # Drop outgoing traffic to ubuntu.com iptables -A OUTPUT -d www.ubuntu.com -j DROP iptables -vnL # List shows ubuntu domain name has been resolved to IP address for ubuntu iptables -A OUTPUT -p tcp --dport 443 -d www.ubuntu.com -j DROP # Drop only HTTPS traffic # Use an application Firewall like SQUID to block big websites like google.com or facebook.com # instead of using IPTables # Using IP Ranges to filter traffic # This range filter helps to keep the IPTables managable and readable iptables -A INPUT -p tcp --dport 25 -m iprange --src-range 10 .0.0.10-10.0.0.19 -j DROP # This drops SMTP tcp traffic coming from the 10 ip addresses on port 25 # Dropping outgoing traffic destined for type MULTICAST to avoid joining the server to any group iptables -A OUTPUT -m addrtype --dst-type MULTICAST -j DROP # Using multiple ports for filter iptables -A OUTPUT -p tcp -m multiport --dports 80 ,443 -j ACCEPT # Allows HTTP and HTTPS outgoing traffic # Notice `dports` is plural # Filter traffic for protocols that dont use ports like ICMP cat /etc/protocols # List the protocols and ports # Dropping all traffic except TCP and UDP iptables -P INPUT -j DROP # Policy to drop all traffic by default if no match found iptables -P OUTPUT -j DROP iptables -A INPUT -i lo -j ACCEPT # Allow loopback interface traffic iptables -A OUTPUT -i lo -j ACCEPT iptables -A INPUT -p tcp -j ACCEPT # Allow tcp traffic iptables -A OUTPUT -p tcp -j ACCEPT iptables -A INPUT -p udp -j ACCEPT # Allow udp traffic iptables -A OUTPUT -p udp -j ACCEPT ping 8 .8.8.8 # This will be blocked as its ICMP dig www.google.com # This will work as DNS is UDP traffic # dropping incoming GRE traffic iptables -A INPUT -p gre -j DROP # allowing outgoing ICMP traffic iptables -A OUTPUT -p icmp -j DROP # Filter traffic based on interface (LAN, WLAN) # dropping ssh traffic that's coming on eth0 interface (suppose it's external) iptables -A INPUT -p tcp --dport 22 -i eth0 -j DROP # allowing ssh traffic that's coming on eth1 interface (suppose it's internal) iptables -A INPUT -p tcp --dport 22 -i eth1 -j ACCEPT # allowing outgoing https traffic via eth1 iptables -A OUTPUT -p tcp --dport 443 -o eth1 -j ACCEPT # Filter traffic based out of Negation # This is easy to whitelist one IP # This will drop all HTTPS traffic except that coming from 0.3 IP address iptables -A INPUT ! -s 192 .168.0.3 -p tcp --dport 443 -j DROP # dropping all incoming ssh traffic accepting packets from 100.0.0.1 (management station) iptables -A INPUT -p tcp --dport 22 ! -s 100 .0.0.1 -j DROP # dropping all outgoing https traffic excepting to www.linux.com iptables -A OUTPUT -p tcp --dport 443 ! -d www.linux.com -j DROP # dropping all communication excepting that with the default gateway (mac is b4:6d:83:77:85:f4) iptables -A INPUT -m mac ! --mac-source b4:6d:83:77:85:f4 -j DROP # The DNS Server of your LAN is set to 8.8.8.8. You don't want to allow the users of the LAN to change the DNS server. iptables -A FORWARD -p udp --dport 53 ! -d 8 .8.8.8 -j DROP # Filter traffic based on tcp flags # dropping all incoming tcp packets that have syn set iptables -A INPUT -p tcp --syn -j DROP # logging outgoing traffic that has syn and ack set iptables -A OUTPUT -p tcp --tcp-flags syn,ack,rst,fin syn,ack -j LOG # Allow establishing incoming ssh (tcp/22) connections only from the LAN. # The internal interface is called eth0 and the external interface is called eth1. iptables -A FORWARD -p tcp --dport 22 --syn -i eth0 -j ACCEPT iptables -A FORWARD -p tcp --dport 22 --syn -i eth1 -j DROP # Drop all linux router outgoing packets of type tcp (port 80 and 443) to www.linuxquestions.org iptables -A FORWARD -p tcp --dport 80 -d www.linuxquestions.org -j DROP iptables -A FORWARD -p tcp --dport 443 -d www.linuxquestions.org -j DROP # Connection Tracking # connection tracking = Stateful firewall = better than stateless and is secure # connection tracking can be used even if the protocol is itself stateless (ex: ICMP, UDP) # Netfilter is a Stateful firewall ################## # Example of a Stateful Firewall for a Desktop ################## #!/bin/bash iptables -f iptables -A INPUT -i lo -j ACCEPT iptables -A OUTPUT i lo -j ACCEPT iptables -A INPUT -m state --state INVALID -j DROP # All invalid state connections are blocked iptables -A OUTPUT -m state --state INVALID -j DROP iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # All valid state connections are allowed iptables -A OUTPUT -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT # Allow new connections iptables -P INPUT -j DROP # Drop any other connections to desktop iptables -P OUTPUT -j DROP ################## # Test the Stateful firewall # Open Browser and test a https website # SSH connection to and from the Desktop # Ping and SSH from another machine to Desktop --> This will be dropped as NEW connection is dropped in INPUT # Selectively allow SSH action from the network, add it before INVALID iptables -A INPUT -p tcp --dport 22 -m state --state NEW -s 192 .168.0.20 -j ACCEPT # OR iptables -A INPUT -p tcp --dport 22 --syn -s 192 .168.0.20 -j ACCEPT ################## # Filter using Date and Time date # Check the current date and timezone of the server # Permit SSH From Mon to Fri between 10 AM to 4 PM iptables -A INPUT -p tcp --dport 22 -m time --timestart 10 :00 --timestop 16 :00 -j ACCEPT iptables -A INPUT -p tcp --dport 22 -j DROP # Permit access to website after working hours iptables -A INPUT -p tcp --dport 443 -d www.ubuntu.com -m time --timestart 18 :00 --timestop 8 :00 -j ACCEPT iptables -A INPUT -p tcp --dport 443 -d www.ubuntu.com -j DROP # DoS protection by Connlimit module iptables -A INPUT -p tcp --dport 25 --syn -m connlimit --connlimit-above 5 -j REJECT --reject-with tcp-rst # This rejects and drops conections coming the same IP after 5 TCP connections and resets the connections. # DoS protection by implementing Rate limiting # default limit-burst is 5 and is the allowed packets to be matched from same IP in a single burst, # after this the \"limit\" value is the next allowed value once burst is exhausted # Limit ICMP packets from one IP, max 7 packets per sec and then apply rate limit of 1 per sec iptables -A INPUT -p icmp --icmp-type echo-request -m limit 1 /sec --limit-burst 7 -j ACCEPT iptables -A INPUT -p icmp --icmp-type echo-request -j DROP # Testing the icmp rule # Run tcdump on the server terminal tcpdump icmp -n # This shows only icmp traffic and IP addresses # In the second server, ping the icmp server with 10 request per sec ping -i 0 .1 192 .168.0.10 # Interval # Output on ping server, after 7 packets, rate drops to 1 packet per sec # Allow only 5 new incoming connections per second to port 443 (https) iptables -A INPUT -p tcp --dport 443 --syn -m limit --limit 5 /sec -j ACCEPT iptables -A INPUT -p tcp --dport 443 --syn -j DROP # Dynamic Blacklist Database Creation - Using recent match # Recent match options: # --name: creates a list in which the source IP address will be added and checked. # --set: adds the source IP address to the list. # The list with blacklisted IP addresses is found in: /proc/net/xt_recent/LIST_NAME # Example to blacklist IP for 60 sec, when port 25 is accessed between 8AM to 10 PM iptables -A INPUT -m recent --name hackers --update --seconds 60 -j DROP # Above rule will update the hackers list with recent timestamp when a new request comes from same IP # when a packet is coming, it will be checked against this rule and # if its source ip belongs to the hacker list, the packet will be dropped # last seen time is updated with another 60 seconds (the source ip address stays in the list for another 60 seconds) iptables -A INPUT -p tcp --dport 25 -m time --timestart 8 :00 --timestop 22 :00 \\ -m recent -name hackers --set -j DROP # when the 1st matched packet arrives (tcp/25 between 8:00-10:00 UTC), a list named hacker is created, # the source ip address of the packet is added to that list and the packet is dropped #Testing Backlist process # Second server, open 2 terminals # Terminal 1, run ping, Terminal 2 start NMAP to scan port 25. # As soon as NMAP is run, ping output is stopped as server 2 is added to blacklist nmap -p 25 192 .168.0.10 # Display the blacklist on server 1 cat /proc/net/xt_recent/hackers # After 60 secs after ping output is stopped, running ping again starts working, till nmap is not run again # Filter Output using Quotas # --quota - Data in bytes # Allow one server to download 1GB from website iptables -A OUTPUT -d 80 .0.0.1 -p tcp --sport 80 -m quota --quota 1000000000 -j ACCEPT iptables -A OUTPUT -d 80 .0.0.1 -p tcp --sport 80 -j DROP # Allow serverto accept only 10Mb from another server iptables -A INPUT -s 192 .168.0.20 -p tcp --dport 80 -m quota --quota 10000000 -j ACCEPT iptables -A INPUT -s 192 .168.0.20 -p tcp --dport 80 -j DROP # Testing the quota rule # in Server 2, check for a file with size more than 20Mb ls -lSh file.tar # Gives the size of the file scp file.tar root@192.168.0.10:~ # Copy the file to server 1 home directory # On server1, the transfer stops after 10Mb iptables -vnL # Shows only 10Mb is copied ls ~ # Shows partial file # ipset # \"ipset\" is an extension to iptables that allows us to create a firewall that match entire \"sets\" # of addresses at once. # IP sets are stored in indexed data structures instead of chains which are stored & traversed linearly. sudo apt install ipset # Install ipset ipset -N myset hash:ip # Creates a new set containing IP data ipset -A myset 1 .1.1.1 # Adding data to the set ipset -A myset 2 .2.2.2 ipset -A myset 8 .8.8.8 # Add a rule to drop packets from myset in iptables iptables -A INPUT -m set --match-set myset src -j DROP # Test the rule ping 8 .8.8.8 # Will fail # Adding a network to ipset ipset -N china hast:net -exist # Creates a new set or ignores if it already exist ipset -A china 1 .0.0.0/8 -exist # Adds entry to set or ignores if alreadt exist ipset -L # List the sets ipset -L china # Displays china set ipset -D china 1 .0.0.0/8 # Deletes the entry from china set ipset -F china # Deletes all entries from china set ipset -F # Flushes all entries from all sets ipset -X china # Deletes the set china # Delete will not work, if the set is referenced in an iptable. Delete the reference, then delete the set. # Dynamic Blacklist Database Creation - Using ipset ipset -N auto_blocked hast:ip -exist iptables -I INPUT -p tcp --dport 80 -j SET --add-set auto_blocked src # Adding IP to blocked set iptables -I INPUT -m set --match-set auto_blocked src -j DROP # Adding rule to block the IP set #Testing Backlist process # Second server, open 2 terminals # Terminal 1, run ping, Terminal 2 start tenet to send request to port 80. # As soon as telnet is run, ping output is stopped as server 2 is added to blacklist telnet 192 .168.0.20 80 # Block all IPs and Networks From File # a file called bad_hosts.txt exists in the same directory with the script and # contains IPs and Networks, one per line like: # 11.0.0.16 # 8.8.8.8 # 1.2.3.4 # 192.0.0.0/16 #!/bin/bash # File that contains the IPs and Nets to block FILE = \"bad_hosts.txt\" # Creating a new set ipset -N bad_hosts iphash -exist # Flushing the set if it exists ipset -F bad_hosts echo \"Adding IPs from $FILE to bad_hosts set:\" for ip in ` cat $FILE ` do ipset -A bad_hosts $ip echo -n \" $ip \" done # Adding the iptables rule that references the set and drops all ips and nets echo -e -n \"\\nDropping with iptables... \" iptables -I INPUT -m set --match-set bad_hosts src -j DROP echo \"Done\" # Blocking Countries #!/bin/bash # Check if the file exists (in the current directory) and if yes, remove it if [ -f \"cn-aggregated.zone\" ] then rm cn-aggregated.zone fi # Download the aggregate zone file for China wget http://www.ipdeny.com/ipblocks/data/aggregated/cn-aggregated.zone # Check if there was an error if [ $? -eq 0 ] then echo \"Download Finished!\" else echo \"Download Failed! Exiting ...\" exit 1 fi # Creating a new set called china of type hash:net (nethash) ipset -N china hash:net -exist # Flushing the set ipset -F china # Iterate over the Networks from the file and add them to the set echo \"Adding Networks to set...\" for i in ` cat cn-aggregated.zone ` do ipset -A china $i done # Adding a rule that references the set and drops based on source IP address echo -n \"Blocking CN with iptables ... \" iptables -I INPUT -m set --match-set china src -j DROP echo \"Done\"","title":"Rule Specification Example"},{"location":"learning/linux/firewall/#target-example","text":"# netstat apt install net-tools # Installs netstat netstat -tupan # Shows 't'cp, 'u'dp 'p'rotocols and 'a'll the services that are using the ports # nmap apt install nmap # Installs nmap nmap -sS 192 .168.0.20 # Performs tcp sync scan. Scans only 1000 ports nmap -p- 192 .168.0.20 # Performs all ports scan (0-65535) nmap -p 80 ,50005 -sV 192 .168.0.20 # Specify ports to scan and display the service that is running nmap -sP 192 .168.0.0/24 # Ping scanning (entire Network) nmap -sS 192 .168.0.0/24 --exclude 192 .168.0.10 # Excluding an IP # PRO-TIP: Always output the data into a file nmap -oN output.txt 192 .168.0.1 # Saving the scanning report to a file nmap -A -T aggressive cloudflare.com # -A OS and service detection with faster execution nmap 10 .211.55.6 -O # Server's operating system nmap 10 .211.55.6 -sC -o /tmp/output.txt # Runs a set list of default scripts against your target. nmap 10 .211.55.6 -sU \u2013top-ports 250 # scan the 250 most common UDP ports # Reject # PRO-TIP: It is efficient to REJECT than DROP targets. iptables -j REJECT --help # Shows reject-with types # Reject SSH traffic and test it iptables -A INPUT -p tcp --dport 22 -s 192 .168.0.10 -j REJECT --reject-with tcp-reset # start tcpdump to trace the output on server 1 tcpdump host 192 .168.0.10 -n # Shows incoming traffic from ssh client # In client server, run nmap to scan port 22 nmap -p 22 192 .168.0.20 # reject all incoming ICMP ping packets that are not coming from 10.0.0.1 (management station) iptables -A INPUT ! -s 10 .0.0.1 -p icmp --icmp-type echo-request -j REJECT --reject-with admin-prohib # Log # Logging detailed info about the packet contents. # Will be shown in dmesg command to show the kernel logs that are in memory. Will be lost at server restart. tail -f /var/log/kern.log # Shows the kernel logs that are persisted even after restart. # logging the first packet (tcp syn set) of any incoming ssh connection # use the prefix: ###ssh: iptables -A INPUT -p tcp --syn --dport 22 -j LOG --log-prefix = \"##ssh:\" --log-level info # filtering logs by prefix dmesg | grep \"##ssh:\" # logging only 10 incoming ICMP ping packets per minute iptables -A INPUT -p icmp --icmp-type echo-request -m limit --limit 10 /minute -j LOG --log-prefix = \"ping probe:\" # Tee # Allows you to mirror incoming traffic on one server to another server # mirror all TCP traffic that arrives at 10.0.0.10 to 10.0.0.1. iptables -A INPUT -p tcp -j TEE --gateway 10 .0.0.1 # Testing # Open tcpdump on server 1, From third server ping server 2. tcpdump will show packets coming on server 2 # Redirect # Allows you to transparently proxy a service on a server # Redirect incoming TCP traffic to port 80 to port 8080 on the same host where a Proxy is running. iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 8080","title":"Target Example"},{"location":"learning/linux/firewall/#nat-and-port-forwarding-examples","text":"# SNAT replaces the private IP address from the packet with the public IP address # of the router external interface. # SNAT helps private servers to access the internet via a router IP. # Example to make the linux server as a Router #!/bin/bash # flush the nat table of all chains iptables -t nat -F # enable routing process echo \"1\" > /proc/sys/net/ipv4/ip_forward # define rules that match traffic that should be NATed # we perform NAT for the entire subnetwork # enp0s3 is the external interface # 80.0.0.1 is the public & static ip address iptables -t nat -A POSTROUTING -s 10 .0.0.0/24 -o enp0s3 -j SNAT --to-source 80 .0.0.1 # OR # if the public IP address is dynamic we use MASQUERADE instead of SNAT iptables -t nat -A POSTROUTING -s 10 .0.0.0/24 -o enp0s3 -j MASQUERADE # it's not mandatory to perform NAT for entire subnet. We could perform NAT only for some protocols # Example: we perform NAT only for TCP iptables -t nat -A POSTROUTING -s 10 .0.0.0/24 -p tcp -o enp0s3 -j SNAT --to-source 80 .0.0.1 # filtering is done on FORWARD CHAIN # Port Forwarding # DNAT permits connections from the Internet to servers with private IP addresses inside LAN. # The client connects to the public IP address of the DNAT Router which in turn # redirects tra\ufb03c to the private server. # The server with the private IP address stays invisible. ### PORT FORWARDING (DNAT) ### #!/bin/bash # flushing nat filter of PREROUTING chain iptables -t nat -F PREROUTING # all the packets coming to the public IP address of the router and port 80 # will be port forwarded to 192.168.0.20 and port 80 iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination 192 .168.0.20 ## VARIANTS # 1.redirect port 8080 on router to port 80 on the private web-server # Internet clients connect to the public IP address of the router and port 8080 and the packets are # redirected to the private server with 192.168.0.20 and port 80 iptables -t nat -A PREROUTING -p tcp --dport 8080 -j DNAT --to-destination 192 .168.0.20:80 #2. Load-Balancing # On all 5 private servers (192.168.0.20-192.168.0.24)run the same service (e.g. HTTPS) # The router will pick-up a random private IP from the range and then translate and send (port-forward) the packet to that IP iptables -t nat -A PREROUTING -p tcp --dport 8080 -j DNAT --to-destination 192 .168.0.20-192.168.0.24 ##** LOAD BALANCE NAT TRAFFIC OVER 2 INTERNET CONNECTIONS WITH DYNAMIC IP ADDRESSES **## #!/bin/bash # Traffic that goes over the first ISP connection # web: 80 443 # email: 25 465 143 993 110 995 # ssh: 22 ISP1 = \"22 25 80 110 143 443 465 993 995\" # flushing nat table and POSTROUTING chain iptables -t nat -F POSTROUTING # enable routing echo \"1\" > /proc/sys/net/ipv4/ip_forward for port in $ISP1 do iptables -t nat -A POSTROUTING -p tcp --dport $port -o eth1 -j MASQUERADE done # Traffic not NATed goes over the 2nd ISP connection iptables -t nat -A POSTROUTING -o eth2 -j MASQUERADE","title":"NAT and Port Forwarding Examples"},{"location":"learning/linux/firewall/#custom-defined-chain-examples","text":"# creating a user-defined chain iptables -N TCP_TRAFFIC # add rules to the chain iptables -A TCP_TRAFFIC -p tcp -j ACCEPT # jump to the chain from the input chain iptables -A INPUT -p tcp -j TCP_TRAFFIC # flush all rules in the chain iptables -F TCP_TRAFFIC # flush all rules otherwise custom chain can't be deleted iptables -F # delete the chain iptables -X TCP_TRAFFIC","title":"Custom Defined Chain Examples"},{"location":"learning/linux/firewall/#chain-traversal-order","text":"# Incoming Packet Traversal # Packet comes from the Network and is destined for our host, which is the final destination PREROUTING --> INPUT # Packet comes from the Network and must be routed by the Linux machine PREROUTING --> FORWARD --> POSTROUTING # Outgoing Packet Traversal # Packet is generated by an Application on the host LOCAL APPLICATION --> OUTPUT --> POSTROUTING","title":"Chain Traversal Order"},{"location":"learning/linux/firewall/#tcp-wrappers","text":"Host-based networking ACL system. Controls access to \u201cwrapped\u201d services. A wrapped service is compiled with libwrap support. Check if a service supports wrappers use ldd . ldd /usr/sbin/sshd | grep libwrap # Prints required shared libraries. Can control access by IP address / networks. Can control access by hostname. Transparent to the client and service. Used with xinetd which is a super daemon as it can manage a lot of smaller services, secures access to your server. Centralized management for multiple network services. Runtime configuration","title":"TCP Wrappers"},{"location":"learning/linux/firewall/#configuring-tcp-wrappers","text":"/etc/hosts.allow is checked first. If a match is found, access is granted . /etc/hosts.deny is checked next. If a match is found, access is denied .","title":"Configuring TCP Wrappers"},{"location":"learning/linux/firewall/#access-rules","text":"The rule format for hosts.allow and hosts.deny are the same. One rule per line Format: SERVICE(S) : CLIENT(S) [: ACTION(S) ] # Deny All configuration # /etc/hosts.deny: ALL : ALL # Deny all connections # /etc/hosts.allow: # Explicitly list allowed connections here. sshd : 10 .11.12.13 # Allow only ssh connection from this ip # Additional configuration possibilities sshd, imapd : 10 .11.12.13 # Multiple services allowed ALL : 10 .11.12.13 # Wild Card configuration sshd : 10 .11.12.13, 10 .5.6.7 # Multiple IPs sshd : jumpbox.example.com # Domain name sshd : .admin.example.com # Subdomain - server2.admin.example.com or webdev.admin.example.com sshd : jumpbox*.example.com # jumpbox4admins.example.com sshd : jumpbox0?.example.com # jumpbox03.example.com sshd : 10 .11.12. # Subnet range sshd : 10 . sshd : 10 .11.0.0/255.255.0.0 sshd : /etc/hosts.sshd # Allow all hosts in this file imapd : ALL # Wildcard in clients sshd : ALL EXCEPT .hackers.net # Conditions, except IP in hacker.net domains sshd : 10 .11.12.13 : severity emerg # Actions - Log emergency messages for that ssh connection coming from IP sshd : 10 .11.12.13 : severity local0.alert # Raise local alert # Raise custom message # /etc/hosts.deny: sshd : .hackers.net : spawn /usr/bin/wall \u201cAttack in progress.\u201d # Use expressions in message, %a logs client ip sshd : .hackers.net : spawn /usr/bin/wall \u201cAttack from %a.\u201d ## Possible expression expansions # %a (%A) The client (server) host address. # %c Client information. # %d The daemon process name. # %h (%H) The client (server) host name or address. # %n (%N) The client (server) host name. # %p The daemon process id. # %s Server information. # %u The client user name (or \"unknown\"). # %% Expands to a single `% \u0301 character.","title":"Access Rules"},{"location":"learning/linux/hardware/","text":"lshw # Displays detailed hardware information lshw -json | less # Dsiplays the same information in json output lshw -short # Displays the summary of the same information lscpu # Shows CPU information lshw -C cpu # Shows CPU information lscpu | grep -i mhz # Shows only speed lscpu -J # Shows in JSON format # dmidecode will only show the current memory that is installed dmidecode -t memory # Shows memory module information dmidecode -t memory | grep -i size # Shows memory module size dmidecode -t memory | grep -i max # Shows max memory module size that can be installed in the machine # Check for free memory top free -m # Display the PCI Bus information and the interfaces used lspci lspic | grep -i vga # Display USB Controllers and the devices connected lsusb # Getting Harddisk information lshw -C disk lshw -C disk -short # Summary lsblk # Shows partitions fdisk -l # Shows detailed block information fdisk -l /dev/sda hdparm -t --direct /dev/sda # Shows HDD performance by benchmarking # Wireless Device iw list # Battery Status apt install acpi acpi -bi # Shows battery # Getting information from filesystem cat /proc/cpuinfo cat /proc/meminfo cat /proc/partitions uname -r # Shows kernel information","title":"Hardware"},{"location":"learning/linux/linux/","text":"Introduction \u00b6 Understanding Linux Filesystem Templates folder CronTab Guru Basic commands \u00b6 Important Commands Terminal Ctrl+Alt+T # Open the terminal Ctrl+D # Close the terminal exit # Close the terminal Ctrl + L # Clear the screen, but will keep the current command Ctrl + Shift + # Increases font size of the terminal Utility cal # Calendar current month cal -3 # Current -1, Current , Current +1 month cal 5 1967 # Format is (Month and Year). Gives May 1967 date # Current date in BST (default) date -u # Current date in UTC date --date \u201c30 days\u201d # Gives current date + 30 days (future date) date --date \u201c30 days ago\u201d # Gives current date \u2013 30 days (past date) which echo # Shows where the command is stored in PATH hostname -I # Gives IP address echo $? # Gives the output 0/1 value stored after a command is run wc \u2013l file1 # Gives line count in file1 wc file1 # Give word count in file1 History history # List all the commands executed !! # Run the previous command !50 # Run the command that is on line 50 of history output history \u2013c ; history \u2013w ; # Clears history and writes back to the file Ctrl + r # reverse searches for your input. Press Esc to edit the matched command man Using the Manual There are 8 sections in the manual. Important are 1, 5 and 8 sections man -k <search term> # Search the manual for pages matching <search term>. man -k tmux # example of searching for tmux in the manual pages man -k \"list directory contents\" # Double quote seraches complete words man 1 tmux # Opens section 1 of tmux manual page, 1 is default and can be ignored man ls # Shows section 1 of ls command help cd # Shows the help pages if man pages are not present Redirection of Streams echo \"Hello\" 1 > output.txt # Standard output is redirected to output.txt echo \"Hello\" > output.txt # Standard output is default echo \"World\" 1 >> output.txt # Standard output is appended to output.txt echo \"Error\" 2 > error.txt # Standard error is redirected to error.txt cat -k bla 2 >> error.txt # Program error is redirected and appended to error.txt echo \"Hello World\" 1 >> output.txt 2 >> error.txt # Use both std output and error cat 0 < input.txt # Standard input is read from a file and sent to cat command cat < input.txt # Standard input is default cat 0 < input.txt 1 >> output.txt 2 >> error.txt # Use all 3 data streams cat -k bla 1 >> output.txt 2 > & 1 # Redirect Standard error to standard output stream and write to file Redirection to Terminals tty # Current terminal connected to Linux, gives path cat < input.txt > /dev/pts/1 # In another terminal, Standard input is read from a file and sent to tty 1 terminal Ctrl + Alt + F1 / chvt 1 # Goes to physical terminal with no graphics. Similarly you can change to 2 to 6 tty terminals. Ctrl + Alt + F7 / chvt 7 # Comes back to Graphical terminal Piping date | cut --delimiter \" \" --fields 1 # Output of date is input to cut command - Tee command - Used to store intermediate output in a file and then stream passed horizontally through the pipeline - tee command takes a snapshot of the standard output and then passes it along date > date.txt | cut --delimiter \" \" --fields 1 # Output will not work and date will only be stored in file and not passed to cut command date | tee date.txt | cut --delimiter \" \" --fields 1 # Output of date is first stored in file, then passed to cut command for display to Standard Output date | tee date.txt | cut --delimiter \" \" --fields 1 | tee today.txt cat file1.txt file2.txt | tee unsorted.txt | sort -r > reversed.txt # Output chaining and storing intermediate data in files - XARGS command (Powerful pipeline command) - Allows piped data into command line arguments - date | echo # Output of date is passed to echo, but echo doesn't accept standard input, only commandline arguments date | xargs echo # xargs will convert standard output into command line arguments date | cut --delimiter \" \" --fields 1 | xargs echo # Prints the day of the week Alias Used to store reusable scripts in .bash_aliases file can be used in scripts alias # Shows all the alias setup for the user # Store an alias in the `.bash_aliases` file alias calmagic = 'xargs cal -A 1 -B 1 > /home/leslie/calOutput.txt' # In the terminal use if in a pipe command, STDOUT will be stored in a file echo \"12 2021\" | calmagic File System Navigation # File Listing pwd # Prints absolute path of current working directory(CWD) ls \u2013l # Long list of CWD ls \u2013a # Shows all files including hidden ls -F # Shows directories as ending with / along with other files stat <filename> # Detailed file information file <filename> # File type ls -ld # Detailed folder information # Change Directories cd - # Helps to switch directories. Like a Toggle (Alt + Tab) in windows cd OR cd ~ # User Home directory from anywhere cd .. # Back to parent directory of CWD Wildcards Wildcards and How to use - The star wildcard has the broadest meaning of any of the wildcards, as it can represent zero characters, all single characters or any string. - The question mark (?) is used as a wildcard character in shell commands to represent exactly one character, which can be any single character. - The square wildcard can represent any of the characters enclosed in the brackets. ls *.txt # Matches all txt files ls ???.txt # Matches all 3 letter txt files ls file [ 123 ] .txt # Matches all files ending with 1 to 3 ls file [ A-Z ] .txt # Matches all files ending with A to Z ls file [ 0 -9 ][ A-Z ] .txt # Matches all files ending with 0A to 9Z File and Folders # Create Operations touch file1 # Creates a new file1 echo \"Hello\" > hello.txt # Creates and writes using redirection # -p is parent directory which is data and inside that 2 directories called sales & mkt is created mkdir \u2013p /data/ { sales,mkt } # Brace exapansion will allow to create folders. Sequence can be expressed as .. mkdir -p /tmp/ { jan,feb,mar } _ { 2020 ..2023 } # Brace expansion for files, it will create 10 files inside each folder touch { jan,feb,mar } _ { 2020 ..2023 } /file { 1 ..10 } # Delete Operations rm file1 # Deletes file1 rm *.txt # Deletes all txt files # Deletes all files and folders inside the main folder and the main folder as well # CAUTION: Use the recursive option with care rm -r /tmp/ { jan,feb,mar } _ { 2020 ..2023 } / # Deletes only empty directories rmdir /tmp/ # Skips folders which have files # Copy Operations cp /data/sales/file1 /data/mkt/file1 cp /data/sales/* . # Copy all files to CWD cp -r /data/sales /data/backup # Copy sales folder to backup folder # Move and Rename Operations mv file1 file2 # Rename file in the same folder mv /data/mkt/ /data/hr # Rename folder, Note the slash after first folder mv /data/sales/* /tmp/backup/ # Move files to new location mv /data/mkt/ /tmp/newFolder # Move and rename the folder Nano - Editing M Key can be Alt or Cmd depending on keyboard layout Enable spell checking on nano by editing /etc/nanorc and uncomment set speller in the file. Ctrl + O # Write data out to file Ctrl + R # Copy contents of one file into another Ctrl + K # Cuts entire line, also used as a delete Alt + 6 # Copy entire line Ctrl + U # Paste the line Ctrl + T # Spell check the file Alt + U # Undo changes Alt + E # Redo changes # File operations in vi > filename # Empties an existing file :x # Saves file changes instead of :wq Search Files find ~/projects # Find matches of files and folders from projects and below find . # Find from CWD and below find . -maxdepth 1 # Find from CWD and one level below find . -type f # Find files only find . -type d # Find folder only find . -maxdepth 1 -type d # Find folder only and one level below find . -name \"*.txt\" # Find files ending with matching patterns find . -maxdepth 3 -iname \"*.TXT\" # Find files with case insensitive matching patterns find . -type f -size +100k # Find files greater than 100 Kb # Find files greater than 100 Kb AND less than 5 Mb and count them find . -type f -size +100k -size -5M | wc -l # Find files less than 100 Kb OR greater than 5 Mb and count them find . -type f -size -100k -o -size +5M | wc -l - Find and Execute commands # Find and copy files to backup folder. `\\;` denotes end of exec command find . -type f -size +100k -size +5M -exec cp {} ~/Desktop/backup \\; ### # Find file called needle.txt inside haystack folder ### # Create 100 folders and inside each folder 100 files mkdir -p haystack/folder { 1 ..100 } touch haystack/folder { 1 ..100 } /file { 1 ..100 } # Create file in one random folder touch haystack/folder $( shuf -i 1 -100 -n 1 ) /needle.txt ### # Finding the file using name find haystack/ -type f -name \"needle.txt\" # Move the file to haystack folder find haystack/ -type f -name \"needle.txt\" -exec mv {} ~/tmp/haystack \\; ### View/Read File Contents cat cat file1 file2 > file3 # Concatenate 2 files and write into file3 cat \u2013vet file3 # displays special characters in the file e.g. EOL as $. Useful if sh files are created in windows tac - Flips the file contents vertically tac file3 # Reads the file in reverse rev - Reverses the contents of each line rev file3 # Reads the line in reverse less - Allows to page through big files less file3 # Shows one page at a time. Use Arrow keys to scroll # Output of find piped to less command for scrolling find . -type f -name \"*.txt\" | less head - Shows limited lines from top of output cat file3 | head -n 3 # Shows first 3 lines tail - Shows limited lines from bottom of output cat file3 | tail -n 3 # Shows last 3 lines tail \u2013f /var/log/messages # follows the file and continuously shows the 10 lines Sort sort words.txt > sorted.txt # Sorts in Asc order and redirects to sorted.txt sort -r word.txt > reverse.txt # Sorts in Des order sort -n numbers.txt # Sorts in Asc numeric order based on digit placement sort -nr numbers.txt # Sorts in Des numeric order based on digit placement sort -u numbers0-9.txt # Sorts and shows only unique values - Sorting data in tabular format # Sort on the basis of file size (5th column and its numeric) ls -l /etc | head -n 20 | sort -k 5n # Reverse (r) the output showing largest files first ls -l /etc | head -n 20 | sort -k 5nr # Sort on the basis of largest file size in human readable format ls -lh /etc | head -n 20 | sort -k 5hr # Sort on the basis of month ls -lh /etc | head -n 20 | sort -k 6M Search data - grep grep is case-sensitive search command # grep <search term> file-name grep e words.txt # Shows matching lines as STDOUT grep -c e words.txt # Counts the matching lines # Search in case insensitive manner grep -i gadsby gadsby_manuscript.txt # Search strings using quotes grep -ci \"our boys\" gadsby_manuscript.txt # Invert the search grep -v \"our boys\" gadsby_manuscript.txt # Searches for server and not servers. \\b is the word boundary grep \u2018 \\b server \\b \u2019/etc/ntp.conf # Searches for server beginning in the line. \\b is the word boundary grep \u2018^server \\b \u2019/etc/ntp.conf Filter data using grep ls -lF / | grep opt # Shows details for opt folder only ls -F /etc | grep -v / # Shows only files in etc folder - Remove Commented and Blank Lines # Empty lines can be shown as ^$. \u2013v reverses our search and \u2013e allows more than one expression. O/p is sent to std o/p grep \u2013ve \u2018^#\u2019 \u2013ve\u2019^$\u2019 /etc/ntp.conf # -v ^# says I don\u2019t want to see lines starting with #. ^$ says I don\u2019t want to see lines that begin with EOL marker Archival and Compression Two step process: Create the tar ball, then compress the tar Compression tool comparison # Create the tar ball tar -cvf backup.tar file [ 1 -3 ] .txt # Create, Verbose, Files to archive tar -tf backup.tar # Test for tar file without unzipping # Compress the tar ball ## 3 compression tools - gzip -> bzip2 -> xz (Compression and time increases from left to right) gzip backup.tar # Compresses the tar ball and adds .gz extension to tar ball gunzip backup.tar.gz # Decompress the gzip ### bzip2 backup.tar # Smaller file size than gzip and adds .bz2 extension to tar ball bunzip2 backup.tar.bz2 # Decompress the bzip. Best used for larger file sizes # Open the tar ball contents tar -xvf backup.tar # Extract, Verbose, Files to unarchive ### Create tar and compress in single command # Adding the z option for gzip and renaming the tar as .gz tar -cvzf backup.tar.gz file [ 1 -3 ] .txt tar -xvzf backup.tar.gz # Adding the j option for bzip2 and renaming the tar as .bz2 tar -cvjf backup.tar.bz2 file [ 1 -3 ] .txt tar -xvjf backup.tar.bz2 ### BASH #!/bin/bash # First line in the script \"SHEBANG\" tells type of script ### # To create an executable script, create a `bin` folder in your home. # Move all utility shell scripts to bin. Also remove .sh file extenstions # Make the file as executable `chmod +x data_backup` # Add the `~/bin` to the PATH variable # Edit `.bashrc` with PATH=\"$PATH:$HOME/bin\" # Now all scripts in bin folder are executable from command line ### # Set and unset variables export $VARIABLE # Sets the variable unset VARIABLE # Removes the variable, NOTE \u2013 No $ in variable Cron Scheduling crontab -e < select editor> # Opens the template crontab # Multiple options for each column of crontab using comma. # SPACE is used to delimit the columns of crontab # */<value> can divide the time intervals ### # min hours \"day of month\" month \"day of week (0-6)\" ### * * * * * bash ~/data_backup.sh Package Management apt-cache search docx # Searches apt for programs that can work with MS Word apt-cache show <package> | less # Gives software information # Apt cache information resides in /var/lib/apt/lists sudo apt-get update # Updates the apt lists sudo apt-get upgrade # Upgraded to the latest software versions from the list sudo apt-get install <pkg-name> # Install package sudo apt-get purge <pkg-name> # Remove & uninstall package. Recommended approach sudo apt-get autoremove # Removes any installed package dependecies # Package compressed archives are stored in `/var/cache/apt/archives` sudo apt-get clean # Removes all package compressed acrhives sudo apt-get autoclean # Removes only package compressed acrhives that cannot be downloaded - Source Code for apps OS uname # Shows kernal uname -o # Shows OS uname -m # Shows computer architecture x86_64 (64 bit), x86 (32 bit) lsb_release -a # Distro version Misc fdisk -l # Gives device wise memory details free / free -m # Gives amount of free memory lsblk # Lists all partitions swapon \u2013s # List all swap files ps # Process id of the current bash shell shutdown \u2013h now / poweroff / init 0 # Power downs the system restart / init 6 / reboot # Restarts the system shutdown \u2013r + 1 \u201cWe are restarting\u201d # Restarts the system give all logged in users 1 min to shut down all process su - # Login to root id / id bob # Shows the current user and group id sudo -i # Interactive shell for password of the current user, to get elevated access ssh localhost # ssh connection to same server. Type exit or Ctrl + D to logout of ssh. who / w # Gives the list of terminals that are connected and who has logged on to the server Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Tips \u00b6 Alt + F2 Gives the run command terminal and then type gnome-system-monitor is like task Manger in windows. Gives graphical overview of the system and can kill processes. Putting & after any commands runs it in the background. Run jobs to see all the background running jobs. Type fg to bring the background running jobs to the foreground. Ctrl + C to cancel the job then. !<anycharacter> will search for the last command in history starting with that character !?etc executes the last command that contains etc Filesystem \u00b6 Creating Partitions \u00b6 fdisk or gdisk utility to partition. If Mountpoint is /, that is primary partition. Creating Filesystems \u00b6 mkfs.ext4 \u2013b 4096 /dev/sdb1 # Creates 4MB block size file system mkfs.xfs \u2013b size = 64k /dev/sdb2 # Creates 64k block size file system. Xfs is specialized filesystem Mounting Data \u00b6 mkdir \u2013p /data/ { sales,mkt } mount /dev/sdb1 /data/sales # Mounts device to data/sales directory mount /dev/sdb2 /data/mkt Unmounting Data \u00b6 umount /dev/sdb1 or umount /dev/sdb { 1 ,2 } # Unmounts both the devices Virtual Memory or Swap Filesystem \u00b6 They are temporary space requirements Virtual memory in Linux can be a Disk Partition or Swap file. Use gdisk to create swap filesystem. Option L and then hex code 8200. To make the swap filesystem permanent, make an entry in /etc/fstab file, so changes are persistent even after system reboot. partprobe /dev/sdb # Sync saved partition in memory. Or it requires system reboot mkswap /dev/sdb3 # Select the right swap device to create the filesystem swapon /dev/sdb3 # Mount the filesystem Troubleshooting Linux filesystem \u00b6 df \u2013hT # list all filesystem with space details du \u2013hs /etc # gives diskusage of etc directory with memory dumpe2fs /dev/sdb1 | less # human readable details for the device dd if = /dev/sda of = /data/sales/file count = 1 bs = 512 # takes data backup of sda to sales/file of the first 512 bytes dd if = /data/sales/file of = /dev/sda # copies the data back in case of recovery tar \u2013cvf /data/sales/etc.tar /etc # backs up etc directory by creating a tar file umount /dev/sdb1 # unmounts sales directory tune2fs \u2013L \u201cDATA\u201d /dev/sdb1 # adding label to the file system debugfs /dev/sdb1 # enters debug of sdb1 directory. Type quit to exit File Permissions \u00b6 # Format for file permission : User-Group-Others # Symbolic Notation (Default permission) RWX \u2013 RW - R # Octal Notation 7 - 6 - 4 # So RWX is 111 i.e. 7, RW is 110 i.e. 6 and R is 100 i.e. 4 umask 2 # sets default permission to all the files in the directory chmod 777 file1 # Changes permission for a file1 chmod u = rwx,g = rw,o = rw file 2 # Verbose way to set permissions chmod +rx file3 # Sets read & write for User, group and others ls \u2013ld /data # Shows permission for a single directory chgrp users /data # Adds users group to the directory - Even if user does not have write access to a file, he has delete / add file access to a directory. - chmod o+t /data .Users can delete only their files and not other\u2019s. Root will not be able to delete files in this directory. - This permission is sent on the /tmp directory by default at installation. So only user\u2019s own file can be deleted, not of others. Links (Hard and Soft Links) \u00b6 Soft links are also called as Symbolic Links or symlinks . Here one file will be a pointer to the other file. If file has more than one name, it\u2019s called hard link. To find the number of sub directories , use stat dirname . Links number -2 is the total number of sub directories. Each directory has a minimum of 2 links, hence subtract 2. ln file2 file5 # Creates hard link between file2 and file5. # Shows the inode number which is same i.e. the same metadata is present for both. Cat on both the files shows the same data content ls \u2013li file2 file5 ln \u2013s file3 file4 # Creates a symlink between file 3 and file5. Cat on both the files shows the same data content ls \u2013li file3 file4 # Shows the symlink, but they are different files. Inode number is different. readlink file5 # shows where the link is Applying Quotas \u00b6 Quotas can be applied to Space/inodes, Group, User or File System. repquota \u2013auv # Give quota report per user space usage along with limits quotaon /dev/sdb1 # Checks quota limit # enable quotas and edit the hard and soft limits. Soft limit can be exceeded for 7 days, after which it is enforced. edquota \u2013u <username> # enables quota via command line. Soft limit is 21000 is 21MB, hard limit is 26MB setquota \u2013u <username> 21000 26000 0 0 /dev/sdb1 Directory Listing and Alias \u00b6 ls \u2013F /dir1 # shows directory with a / and symlink as @ at the end of the name ls \u2013-color = auto /dir1 # shows the same file types in color alias ls = \u2019ls \u2013-color = auto\u2019 # creates an alias for ls with color ls \u2013lh file1 # list in human readable format ls \u2013lt /etc # shows long listing with time modified in descending order ls \u2013ltr /etc | less # shows reverse listing, q to quit Synchronize Directories \u00b6 mkdir /backup rsync \u2013av /home/ /backup/ # archive home dir to backup dir. / after home and backup is important rsync \u2013av --delete /home/ /backup/ # sync deletions of data as well, otherwise rsync ignores it by default rsync \u2013ave ssh # sync data between servers using e option Process Management \u00b6 Monitor Process \u00b6 which ps # shows the installation directory for ps uptime # shows the uptime of the system along with the load average in the range of 1 min, 5 mins and 15 mins # Rule of Thumb for uptime --> Load average for single core value should be less than 1, for dual core less than 2 etc. which uptime # shows the installation directory for uptime cat /proc/uptime # shows uptime and idle time cat /proc/loadavg # shows load avg for 1,5 and 15 mins, active process running/total process, last process id that was issued Jobs \u00b6 sleep 180 # sleeps for 180 secs in foreground. Ctrl + Z to pause the job. Run bg to put the sleep command in background. jobs # shows running jobs fg 1 # puts the sleep command in foreground Managing Processes \u00b6 ps to display processes and kill to send signals. pgrep, pkill and killall are great shortcuts. The default kill signal is -15 which can also be written as \u2013term or \u2013sigterm . To really kill it is -9, -kill or \u2013sigkill . echo $$ # Shows current process ps \u2013l # long listing with the process ps \u2013ef # shows all the processes for all users ps \u2013eaf | grep processname pgrep nginx # shows process ids for nginx sleep 900 & pkill sleep # searches for sleep process and kills it killall sleep # searches for all running sleep process and kills it kill \u2013l # shows the multiple kill signals available kill -9 <process id> # forcefully terminates the process, also use kill \u2013kill <process id> top \u2192 kill, renice, sort and display processes Running top, you can toggle between the information displayed at the top lines. l \u2013 on/off load, t \u2013 on/off tasks, m \u2013 on/off memory Sorting of top is on %CPU, f \u2013 shows current fields being shown on output of top. Select the new field to sort and type s Type r for renice and put in the process id. Esc and Enter to quit the shell Type k for kill and put in the process id. Esc and Enter to quit the shell q to quit out of top top # shows all running processes, q to quit top \u2013n 1 # shows the running processes for 1 capture and quits top \u2013n 2 \u2013d 3 # shows 2 captures with a delay of 3 seconds and quits Editors \u00b6 Vi \u00b6 : # Last line mode q, q! # quit the file x, wq, wq! # save and exit the file i, I # insert from cursor position, I for inserting from start of the line a, A # append after the cursor, A for append from last character in the line o, O # insert line below the cursor, O for above the current cursor position dd # delete the line u # undo the changes Line Navigation \u00b6 <Linenumber>G # e.g. 7G, takes cursor to 7th line in the file G # only G takes cursor to end of file w , b # w takes cursor to next word, b takes cursor to one word before ^ , $ # ^takes cursor to start of line, $ to end of the line vi +127 /etc/file1 # opens the file and takes cursor to 127th line vi +/Document /etc/file1.conf # opens the file and takes cursor to first occurrence of \u201cDocument\u201d set number / set nonumber # from last line mode, it will show and stop line number display syntax on # highlighting on, e.g. xml highlighting etc. Read and Write \u00b6 r /etc/hosts # Open an existing file, use : and then you can get content from hosts file into current file w newfile # :, it will copy entire file contents into newfile in the same directory 3 ,7w newfile # it will copy line 3 to 7 into newfile Search and Replace \u00b6 %s/Hi/Hello # Open an existing file, use : and you can search Hi and Replace with Hello. %s signifies entire document search /Hello # searches for Hello in the document. Type n to get next occurrence, N will take cursor in reverse 1 ,20s/Hi/Hello # searches for 1st 20 lines for Hi and replaces with Hello 14 ,20s/^/ / # from 14th to 20th line, it will add 3 spaces from the start of the line, just like Tab BASH Scripting \u00b6 Understanding Variables \u00b6 Local variables \u2192 accessible only to the current shell, FRUIT=\u2019apple\u2019, echo $FRUIT Global variables \u2192 you need to set and then export it to make it global. export FRUIT=\u2019apple\u2019 Simple Script \u00b6 vi hello.sh #!/bin/bash # Path to the interpreter echo \u201cHello World\u201d exit 0 # return code, :wq chmod +x hello.sh hello.sh # execute the script as it\u2019s in the home directory /user/bin Getting user input \u00b6 vi hello.sh #!/bin/bash echo \u2013e \u201cEnter your name: \\c \u201d # -e is the escape sequence, -c is for the prompt read INPUT_NAME # read the input data into a variable echo \u201cHello $INPUT_NAME \u201d exit 0 User Input types \u00b6 $1 $2 # $1 is the 1st input parameter, 2nd Parameter and so on. $0 # is the script name itself $# # count of input parameters $* # is collection of all the arguments Multiple inputs using positional parameters \u00b6 vi hello.sh #!/bin/bash echo \u201cHello $1 $2 \u201d # $1 is the 1st input parameter, $0 is the script name itself, $2 is the 2nd input parameter and so on exit 0 Code Snippets \u00b6 Gedit \u2192 Gnome Editor \u2192 Add the Snippet Plugin (Applications \u2192 Accessories \u2192 gedit. Preferences in gedit tab \u2192 Plugins enable Snippet Plugin and restart gedit) Conditional Statement - IF \u00b6 if [[condition]] \u2192 testing for string condition if ((condition)) \u2192 testing for numeric condition e.g. if (( $# < 1 )) \u2192 if count of input parameter vi hello.sh #!/bin/bash if (( $# 1 )) then echo \u201cUsage: $0 <name>\u201d exit 1 fi echo \u201cHello $1 $2 \u201d exit 0 Case Statement \u00b6 vi hello.sh #!/bin/bash if [[ ! \u2013d $1 ]] # if the 1st argument is not a directory then echo \u201cUsage: $0 <directory>\u201d exit 1 fi case $2 in \u201cdirectory\u201d ) find $1 \u2013maxdepth 1 \u2013type d ;; # break \u201clink\u201d ) find $1 \u2013maxdepth 1 \u2013type l ;; # break * ) # default statement echo \u201cUsage: $0 <directory> directory | link\u201d ;; esac exit 0 For \u00b6 vi hello.sh #!/bin/bash for u in $* # $* is collection of arguments, u is temporary variable do # do block useradd $u # access to temp variable is via $ echo Password1 | passwd \u2013stdin $u # use the passwd command and get the user input from keyboard passwd \u2013e $u # expire the password, so they can change it at first login done echo \u201cFinished\u201d # at time of execution ./hello.sh fred mary john vi listsize.sh #!/bin/bash for file in $( ls ) # for each file, in the output of ls do [[ ! \u2013f ]] && continue # not a file then continue to next # use the stats to get statistics of the file, to get the last accessed date and then format the date LA = $( stat \u2013c %x $file | cut \u2013d \u201c \u201d \u2013f1 ) echo \u201c $file is $( du \u2013b $file ) bytes and was last accessed on $LA \u201d # use du to get file size done While \u00b6 vi loop.sh #!/bin/bash -x # -x is for debug mode COUNT = 10 while (( COUNT > 0 )) do echo \u2013e \u201c $COUNT \\c \u201d # \\c will suppress the line feed (enter) sleep 1 (( COUNT -- )) # round brackets to avoid using $ symbol done - Use the until when you want to stop the loop when the condition becomes true. User Management \u00b6 Managing Users: User Lifecycle ==> useradd, usermod, userdel Local databases ==> /etc/passwd, /etc/shadow (encrypted) passwd (to set the password) pwconv (move pass to encrypted) pwunconv (move back to unencrypted) # /etc/passwd file structure # It has 7 filed separated by : Login Name, Optional encrypted password or \u201cx\u201d, Numerical UID, Numerical GID, Username or comment, User home directory, Optional command interpreter # /etc/shadow file structure where the actual passwords are stored # It has 8 filed separated by : Login Name, encrypted password ( if it begins with ! the account is locked ) , Date of last password change, Minimum password age, Maximum password age, Password warning period, password inactivity, account expiry date # /etc/login.defs The password ageing defaults can be configured with this file useradd \u2013D # shows the default settings for a user that is added cat /etc/default/useradd # shows where the defaults are set useradd bob # only adds the user, no home directory is created. Once the user logs in, it will get created tail -3 /etc/passwd # shows that bob is added useradd \u2013m bob # also creates the home directory useradd -m -d /home/bob -s /bin/bash # Creating user and home dir in one command tail -3 /etc/shadow # shows the password for the user passwd bob # add the password for bob passwd \u2013l bob # locks the account passwd \u2013u bob # unlocks the account passwd --status bob # Shows the status of account bob, 2nd column (L - Locked, NP - No Password, P valid password) usermod bob \u2013c \u201cBob Smith\u201d # adding additional details for the user userdel \u2013r bob # removes the user and home directory Group Management \u00b6 Group Lifecycle \u2192 groupadd, groupmod, groupdel Local databases \u2192 /etc/group, /etc/gshadow (encrypted) gpasswd (to set password) newgrp (switch to new groups) # /etc/group structure # It has 4 fields: Group Name, Password, Numerical GID, User list that is comma separated # /etc/gshadow structure # It has 4 field: Group Name, Encrypted password, Admin list that is comma separated, # this can be managed used \u2013A cmd # Members, this can be managed using the \u2013M cmd Private groups are enabled by default. The user added is also added to the same group. If this is disabled, users will belong to the groups users. Use useradd \u2013N to overwrite private groups. This can be enabled or disabled by setting USERGROUPS_ENAB in /etc/login.defs useradd \u2013m \u2013g users jim # -g is Primary Group, -G is secondary groups. Secondary groups are more traditional groups id jim usermod \u2013G sudo,adm jim # added jim to secondary groups sudo, adm useradd \u2013N \u2013m sally # adds sally to the default group gpasswd \u2013M jim,sally sudo # adds 2 users to sudo group groupadd sales gpasswd sales # sets the new password for sales newgrp sales # add the user to the sales group temporarily. If the user logs out, he is removed from the group Automate System Tasks \u00b6 Regular Tasks \u2192 cron (more than once a day but misses job if turned off), anacron (run jobs missed on startup but jobs can run just once a day) Once Off \u2192 at (runs at specified time and date), batch (runs when load average drops below 0.8) System Cron Jobs \u00b6 # /etc/crontab, /etc/cron.d # cron files # /etc/cron.<time> # where time is hourly, daily, weekly and monthly, contains scripts that need to be executed # Adding a system cron job cd /etc/cron.d vi daily-backup # add a new file 30 20 * * 1 -5 root /root/back.sh # run back.sh from Mon to Fri at 20:30 # Adding a user cron job crontab \u2013e # edit the user crontab file */10 10 1 1 1 tail /etc/passwd # Runs once on 1st day if it\u2019s a Mon of Jan, at 10 am for every 10 mins crontab \u2013l # list all cron jobs crontab \u2013r # remove the cron job # anacron: /etc/anacrontab structure # It has 4 fields Period in days or macro ( Daily, Monthly ) , Delay ( minutes after system startup for job to run ) , Job Identifier ( used to name timestamp file indicating when job was last run ) Command ( that needs to be executed ) @weekly 120 weekly-backup ls /etc // weekly, 120 mins after startup it will run weekly-backup Batch \u00b6 at and batch commands at noon tomorrow # Enter the command line, Ctrl + D to save at> ls /etc # enter the command that needs to be executed atq # shows the jobs queue atrm # remove the job batch # Enter the command line, Ctrl + D to save at> ls /etc > /root/file1 # redirect the o/p to file1. It will run if the system load avg is less than 0.8. Security for Cron \u00b6 Everyone is allowed to run their own cron and at jobs, unless you add entries to /etc/cron.allow or /etc/at.allow. No one is denied unless you add entries to /etc/cron.deny or /etc/at.deny Networking Fundamentals \u00b6 Network Time Protocol (NTP) \u00b6 Configuring Network Time Protocol (NTP) \u00b6 # vi /etc/ntp.conf # prefixing i with date creates a backup of the original file. Removes commented and blank lines sed \u2013i. $( date +%F ) \u2018/^#d ; /^$/d\u2019 /etc/ntp.conf Implementing the configuration file changes \u00b6 vi /etc/ntp.conf # Add lines other can default just below the driftfile command statsdir /var/log/ntpstats # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server instead of on the internet debian servers Save the file and restart the service \u00b6 service ntp restart # sudo if no access # check if the ntpstats directory is accessible to the ntp service # The user should be ntp and it should have write access ls \u2013ld /var/log/ntpstats/ - Date \u2192 Current system date and time. This is the time in memory. - HwClock \u2192 Hardware date and time set by the BIOS. hwclock \u2013r # shows the hardware clock hwclock \u2013-systohc # sets the hardware clock from system clock Hwclock \u2013-hctosys # sets the system clock from hardware clock NTP Tools \u00b6 ntpdate (once off adjustment) ntpq (query the ntp server) ntpq \u2013p (shows peers) ntpstat (Shows status but not on debian. Try ntpdc \u2013c sysinfo) ntpq \u2013c \u201cassociations\u201d \u2192 shows associations # Configuring NTP on centos --> Install ntp ntpdate 192 .168.0.3 # one off update with a local machine in the network # vi /etc/ntp.conf # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server systemctl start ntpd # save and restart systemctl enable ntpd # enable to start at system startup Managing System Log Daemons \u00b6 Rocket-Fast System for Log Processing (rsyslogd) \u00b6 rsyslogd \u2013v # vi /etc/rsyslog.conf # Adding a simple log rule # For any log event greater than or equal to info make a log entry in local5 log. Local5 could be a simple application local5.info /var/log/local5 systemctl restart rsyslog.service # restart the service # to test this in working using command line logger \u2013p local5.info \u201cScript started\u201d # p is priority, if you see /var/log/local5 file, the log would be present /var/log/ folder structure \u00b6 messages (Nearly everything is logged here) secure (su and sudo events amongst others) dmesg (kernel ring buffer messages) Logrotate \u00b6 ls /etc/cron.daily # has the logrotate script which will rotate log files cd /etc/logrotate.d/ # folder where all apps rotation policy is set cp syslog local5 # copy existing app conf for local5 app # vi local5 # make edits to point to /var/log/local5 file /var/log/local5 { weekly # period for rotation size +10 # size of the file for rotation compress # use compression for the rotated log file rotate 4 # keep 4 weeks of logs before overwriting } # manually running the rotate logrotate /etc/logrotate.conf # on execution, all files mentioned will be interrogated and log backup will be created Journalctl \u00b6 Responsible for viewing and log management. Need to be a member of adm group to read this. By default journal is memory resident i.e. it will be lost on restart journalctl # view the journal journalctl \u2013n 10 # shows the last 10 entries journalctl \u2013n 10 \u2013p err # shows the last 10 entries with priority errors mkdir /var/log/journal # to make the journal data persistent systemctl restart system-journald systemctl status system-journald # shows that the journal data is persistent usermod \u2013a \u2013G adm username # adding user to adm group, -a is append chgrp \u2013R adm /var/log/journal # recursively give adm group access jornalctl \u2013-disk-usage # shows disk usage journalctl \u2013-verify # verify the journal integrity SSH \u00b6 Remote access using SSH \u00b6 Server Configuration /etc/ssh/sshd_config The public key of the server is used to authenticate to the client. The public key of the server is stored in /etc/ssh/ssh_host_rsa_key.pub It is down to the client to check the public key using: StrictHostkeyChecking Server public keys are stored centrally in /etc/ssh/ssh_known_host or locally under ~/.ssh/known_hosts SSH Server Configuration \u00b6 netstat \u2013antl # shows the open tcp ports of the server grep ssh /etc/services # shows the services using ssh lsof \u2013i # Also shows open ports # vi /etc/ssh/sshd_config # Uncomment AddressFamily line and change as below AddressFamily inet # Now ssh will only listen on IPv6 systemctl restart sshd # vi /etc/ssh/sshd_config # Uncomment below lines and modify LoginGraceTime 1m # To avoid denial of service attacks and freeing up your service quickly PermitRootLogin no # 2 level authentication, first as normal user and then root SyslogFacility AUTHPRIV ClientAliveInterval 300 ClientAliveCountMax 0 MaxSessions 10 systemctl restart sshd Client Configuration and Authentication \u00b6 Client Configuration /etc/ssh/ssh_config Generate Private and Public keypair using ssh_keygen Use ssh-copy-id to copy to host we want to authenticate with. To provide Single Sign On using ssh-agent Client/User public keys are stored in ~/.ssh/authorized_keys using ssh-copy-id. To connect to server using ssh \u00b6 cd # home directory ls \u2013a # to show all hidden files ssh pi@192.168.0.97 # ssh using user and ip address. Add the password of the user to authenticate cd .ssh cat known_hosts # shows the client ip and public keys exit or logout or Ctrl + D # to end the ssh session To generate keypairs \u00b6 cd .ssh # On the client home directory ssh-keygen \u2013t rsa # generate key pair. Private key is encrypted using a passphrase # This will copy the generated public key to the target server. To which user\u2019s directory at the server we will # connect as. Give the password of the server\u2019s account. ssh-copy-id \u2013i id_rsa.pub pi@192.168.0.97 ssh pi@192.168.0.97 # now connect to the server using passphrase of the private key # From another terminal say tty we can now add the private key once and don\u2019t need to authenticate to the target server ssh-agent bash # fire up another bash terminal ssh-add .ssh/id_rsa # add the private key from the home directory. Enter the passphrase ssh \u2013l or ssh \u2013L # list all identities added ssh pi@192.168.0.97 SSH Tunnels \u00b6 ssh \u2013f \u2013N \u2013L 80 :localhost:80 user@s1.com # -f = execute in background, -N = We are not running any commands on remote host # -L = listening on port 80, we are listening on localhost and forwarding to port 80 on the remote host # On the remote host it has to listen on ssh called s1.com. We connect as user called user. # Example # Webservice on 192.168.0.3 # on a different machine, login as standard user cd .ssh ssh \u2013f \u2013N \u2013L 9000 :localhost:80 andrew@192.168.0.3 # we are listening on port 9000 on the localhost and forwarding traffic to port 80 om 192.168.0.3 netstat \u2013antlp # we can see that localhost:9000 is listening on ssh # On the client machine open the browser and type in http://127.0.0.1:9000 we will see the webservice data kill <process id of ssh> # shutdown the ssh tunneling process after finishing the work Configuring Network Protocols in Linux \u00b6 /etc/services \u00b6 Network services are identified by a port address Common services and associated port address is listed in /etc/services netstat \u2013alt will list services listening via TCP this resolves address to name in /etc/services # To verify the above we can use strace to map the netstat data with the services that are running strace netstat \u2013alt 2 > & 1 | grep /etc/services grep http /etc/services # service and port mapping dig command \u00b6 which dig # get the path of dig rpm \u2013qf /usr/bin/dig # dig is not installed by default. Hence needs to be installed. dig \u2013t AAAA ipv6.bbc.co.uk # shows the IPv6 address Interface Configuration Files \u00b6 ifconfig # shows ip address details ifconfig eth0 192 .168.0.99 netmask 255 .255.255.0 up # sets ip address for Ethernet card ip address show # same as ifconfig # These settings are lost upon restart unless they are written to configuration files. /etc/sysconfig/network-scripts/ # centos /etc/network/interfaces # debian To make the IP address static \u00b6 cd /etc/sysconfig/network-scripts/ vi ifcfg-ens32 # open the config file # Replace and add the lines BOOTPROTO = \u201dstatic\u201d # change from dhcp IPADDR = 192 .168.0.240 # select a static ip address NETMASK = 255 .255.255.0 # add a class c subnet GATEWAY = 192 .168.0.1 # add the gateway address DNS1 = 8 .8.8.8 # add google as the DNS server # Restart the services systemctl restart network.service Networking Tools \u00b6 nmap \u00b6 nmap localhost # shows all open ports used by localhost nmap 192 .168.0.3 # shows open ports at remote host nmap \u2013v 192 .168.0.3 # verbose mode nmap \u2013iL ip.txt # input file containing all ip address to be scanned nmap -p 80 ,443 192 .168.0.3 -P0 # Scan 2 ports without using ping. This is done if iptables is blocking ping netstat \u00b6 netstat \u2013a # shows all connections netstat \u2013at # shows all tcp connections netstat \u2013alt # shows all listening tcp connections netstat \u2013altpe # shows all user and process ids and listening tcp connections netstat \u2013s # statistics netstat \u2013i # shows interfaces netstat \u2013g # multicast groups netstat \u2013nr # network route tables # Default checking netstat -tupan | grep ssh # Shows the pots for ssh Show Sockets (ss) \u00b6 ss \u2013t \u2013a # shows all tcp connections ss \u2013o state established \u2018 ( dport = :ssh or sport = :ssh ) \u2019 # shows all ssh connections ss \u2013x src /tmp/.X11-unix/* # shows X11 connections using socket files lsof \u00b6 lsof \u2013i -4 # list all ipv4 connections lsof \u2013i :23, 24 # list all port 22 and 23 connections lsof \u2013p 1385 # list process id 1385 connections Testing Network Connectivity \u00b6 ping www.centos.org # test network connectivity ping \u2013c3 www.centos.org # sends only 3 pings traceroute www.centos.org # describes the route to destination from source tracepath www.centos.org # shows maximum transmission unit size Host Name Resolution Tools \u00b6 hostname # Confirms the hostname cat /etc/hostname # shows the current host name hostname myComputer # changes the hostname to myComputer. You need to login as root to change this dig www.centos.org # resolves to ip address dig \u2013t MX centos.org # mail servers associated with centos Managing Interfaces \u00b6 ip a s # Shows ip addresses ip n s # Shows Neighbor shows looking at ARP cache ip r s # Shows root table ip ma s # Shows Multicast groups ip l # Shows network cards ifdown eth0 # Brings down interface ifup eth0 # Brings up interface Securing Access to your Server \u00b6 # Temporary disables logins for users other than root. Just the existence of this file will prevent user logins and is controlled via PAM(Pluggable Authentication Modules) /etc/nologin # Create a blank file, but if the user tries to login via ssh username@localhost, the connection will be immediately closed. Only root can access the server then. touch /etc/nologin rm /etc/nologin # Now users can login. This can be used as a temporary measure cd /etc/pam.d/ # Config files for authentication modules grep nologin * # Shows instances where nologin file exists last # shows last user activity present in /var/log/wtmp file. lastlog # List the last login time for each user lastlog | grep \u2013v Never # Reverse the grep search to check for all user logins ulimit \u00b6 Puts restrictions on system resources ulimit \u2013a # Shows system limitations that can be applied ulimit \u2013u 8000 # -u is for avoiding fork bombs and is defaulted to 4096. A std user can set a new value to 8000. # It will remain for his profile till system restart. cd /etc/security # To set the limits which can be persisted even after restart cat limits.conf # Shows soft (can be changed by processes) and hard (can\u2019t be changed by processes) limits cd limits.d/ # ls to see the files inside this directory. Edit the file and add an entry for the user account with soft or hard limits and save the file. Avoiding fork bombs \u00b6 They are a potential Denial of Service Attack ulimit \u2013u ##### Do not run on Production Machines. Test only in laptop ##### ps \u2013u username | wc -l # Shows the number of processes running under the user and gives the count # On the command line create a function called foo foo (){ echo hello } # Execute the function by just calling it and pressing enter foo # Similarly to execute a fork bomb, instead of foo, call it : : (){ : | : & # Here the function calls itself and pipes the output to itself } ; : # End the function with a semicolon and then call the function : # BEWARE: Do this only at your own risk. Ensure ulimit is set to protect the resources on the server # xinetd \u00b6 Called the super daemon as it can manage a lot of smaller services, secures access to your server /etc/xinetd.d /etc/xinetd.conf tftp server (Trivial file transfer protocol) # Sample configuration service tftp { socket_type = dgram # data gram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /tftpboot # root directory of the server disable = no } Implementing TFTP using xinetd.d service \u00b6 ls /etc/xinetd.d/ # blank directory yum install tftp-server tftp # install client and server, also xinetd as it\u2019s a dependency vi /etc/xinetd.d/tftp # after installation, delete disable line from the configuration to enable tftp # if the server directory doent exist, create it mkdir \u2013p /var/lib/tftpboot systemctl enable xinetd systemctl start xinetd netstat \u2013aulpe | grep tftp # shows the port # As a root user, create a temp file inside var/lib/tftpboot directory with hello text vi var/lib/tftpboot/file1 # Logout and login as standard user. Use TFTP to transfer the file to standard user tftp 127 .0.0.1 # Press enter get file1 # Get the file1 created by root using tftp quit # At the same file location do a cat to see the contents of the file cat file1 TCP Wrappers \u00b6 Alternative to firewalling on the server # To check if service supports TCP wrappers. Once we can determine this, we can use hosts.allow or hosts.deny ldd </path to service name> | grep libwrap # To set this up for a service, 2 entries are made # This in /etc/hosts.allow file to allow access to 0.3 IP which is raspberry pi. tftpd is the name of the binary in .tftpd : 192 .168.0.3 # This in /etc/hosts.deny file to deny access to all other IP except for 0.3 IP in .tftpd : ALL # If the client appears in both files then allow takes precedence and access is granted ldd /usr/sbin/xinetd | grep libwrap tftp 192 .168.0.240 # login from raspberry pi and access the remote server, disable your firewall before trying this. Delegating Admin rights using sudoers \u00b6 id # check if the user is already an admin and part of wheel group in centos cd /etc grep wheel sudoers # check the current setup visudo # to edit sudoers file %wheel ALL =( root ) ALL # Uncomment the line for %wheel, change (ALL) to (root), so only root can change the sudoers. Save and exit Data Encryption \u00b6 Using GPG to encrypt data between users \u00b6 password = $( mkpassword \u2013m sha-512 Password1 ) # encrypt the password using sha and mkpassword and put in a variable echo $password for u in marta ivan ; do # take an input from marta and ivan for users sudo useradd \u2013m \u2013p $password $u # add the user and set the encrypted password done # Run the command and create 2 users ivan and marta # Install GPG if not present dkpg \u2013S $( which gpg ) # Login as ivan and generate private and public keys for gpg encryption su - ivan gpg --gen-key # Take the default settings gpg --list-key # List the keys gpg --export \u2013a <email from gpg gen-key step> > /tmp/ivankey # export the public key and place in tmp folder for marta to access exit su - marta vi secret.txt # create a plain txt file for encryption chmod 400 secret.txt # make the file writeable by marta only gpg --import /tmp/ivankey # import ivan public key gpg \u2013e \u2013r <ivan mailid> secret.txt # add the recipient and encrypt the file mv secret.txt.gpg /tmp/ # move the encrypted file to tmp and exit su \u2013 ivan gpg \u2013d /tmp/secret.txt.gpg # decrypt the file and enter the passphrase for the private key Implementing LUKS for Full Disk Encryption \u00b6 example Laptops, USB sudo apt-get install cryptsetup # Install LUKS tools lsblk # list the blocks sudo fdisk /dev/sdb # partition the sdb drive and create a new partition called sdb1 sudo cryptsetup \u2013y luksFormat /dev/sdb1 # Allows to store encrypted data in this partition. Give a passphrase sudo cryptsetup luksDump /dev/sdb1 # Shows details of the encrypted partition sudo cryptsetup luksOpen /dev/sdb1 private_data # Assign a mapper called private_data ls /dev/mapper # Shows the new mapper setup and that is a device sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted EFS for data encryption \u00b6 sudo apt-get install ecryptfs-utils # Install EFS tools su \u2013 ivan # Login as std user ecryptfs-setup-private # Create a private key with passphrase. logout and login back as ivan ls # Private directory is created by EFS echo hello > Private/data # Write to private directory cat Private/data # ivan can see the data ecryptfs-umount-private # unmount the private directory ls # Now ivan cannot see the data cd /.Private/ # Go to the ivan\u2019s hidden Private folder with a dot ls # You can see the encrypted data file ecryptfs-mount-private # Mount the directory again Compiling software from source \u00b6 Install gcc compiler sudo apt-get install gcc make # go to gnu.org/software/Downloads -> coreutils -> file ending with .xz # tar -xvJf coreutils-8.28.tar.xz # J is for xz, j is for bzip2 # cd into src folder and select ls.c file # Add a line in main function printf(\"Hello World\\n\"); # After changes, change directory one level up and run the configure script bash configure # This will check for any configuration changes to the src files and update the make file # Excute the make command make # After binary code is compiled, it needs to be updated in OS sudo make install # Close the terminal and restart. New software is working ;)","title":"Linux"},{"location":"learning/linux/linux/#introduction","text":"Understanding Linux Filesystem Templates folder CronTab Guru","title":"Introduction"},{"location":"learning/linux/linux/#basic-commands","text":"Important Commands Terminal Ctrl+Alt+T # Open the terminal Ctrl+D # Close the terminal exit # Close the terminal Ctrl + L # Clear the screen, but will keep the current command Ctrl + Shift + # Increases font size of the terminal Utility cal # Calendar current month cal -3 # Current -1, Current , Current +1 month cal 5 1967 # Format is (Month and Year). Gives May 1967 date # Current date in BST (default) date -u # Current date in UTC date --date \u201c30 days\u201d # Gives current date + 30 days (future date) date --date \u201c30 days ago\u201d # Gives current date \u2013 30 days (past date) which echo # Shows where the command is stored in PATH hostname -I # Gives IP address echo $? # Gives the output 0/1 value stored after a command is run wc \u2013l file1 # Gives line count in file1 wc file1 # Give word count in file1 History history # List all the commands executed !! # Run the previous command !50 # Run the command that is on line 50 of history output history \u2013c ; history \u2013w ; # Clears history and writes back to the file Ctrl + r # reverse searches for your input. Press Esc to edit the matched command man Using the Manual There are 8 sections in the manual. Important are 1, 5 and 8 sections man -k <search term> # Search the manual for pages matching <search term>. man -k tmux # example of searching for tmux in the manual pages man -k \"list directory contents\" # Double quote seraches complete words man 1 tmux # Opens section 1 of tmux manual page, 1 is default and can be ignored man ls # Shows section 1 of ls command help cd # Shows the help pages if man pages are not present Redirection of Streams echo \"Hello\" 1 > output.txt # Standard output is redirected to output.txt echo \"Hello\" > output.txt # Standard output is default echo \"World\" 1 >> output.txt # Standard output is appended to output.txt echo \"Error\" 2 > error.txt # Standard error is redirected to error.txt cat -k bla 2 >> error.txt # Program error is redirected and appended to error.txt echo \"Hello World\" 1 >> output.txt 2 >> error.txt # Use both std output and error cat 0 < input.txt # Standard input is read from a file and sent to cat command cat < input.txt # Standard input is default cat 0 < input.txt 1 >> output.txt 2 >> error.txt # Use all 3 data streams cat -k bla 1 >> output.txt 2 > & 1 # Redirect Standard error to standard output stream and write to file Redirection to Terminals tty # Current terminal connected to Linux, gives path cat < input.txt > /dev/pts/1 # In another terminal, Standard input is read from a file and sent to tty 1 terminal Ctrl + Alt + F1 / chvt 1 # Goes to physical terminal with no graphics. Similarly you can change to 2 to 6 tty terminals. Ctrl + Alt + F7 / chvt 7 # Comes back to Graphical terminal Piping date | cut --delimiter \" \" --fields 1 # Output of date is input to cut command - Tee command - Used to store intermediate output in a file and then stream passed horizontally through the pipeline - tee command takes a snapshot of the standard output and then passes it along date > date.txt | cut --delimiter \" \" --fields 1 # Output will not work and date will only be stored in file and not passed to cut command date | tee date.txt | cut --delimiter \" \" --fields 1 # Output of date is first stored in file, then passed to cut command for display to Standard Output date | tee date.txt | cut --delimiter \" \" --fields 1 | tee today.txt cat file1.txt file2.txt | tee unsorted.txt | sort -r > reversed.txt # Output chaining and storing intermediate data in files - XARGS command (Powerful pipeline command) - Allows piped data into command line arguments - date | echo # Output of date is passed to echo, but echo doesn't accept standard input, only commandline arguments date | xargs echo # xargs will convert standard output into command line arguments date | cut --delimiter \" \" --fields 1 | xargs echo # Prints the day of the week Alias Used to store reusable scripts in .bash_aliases file can be used in scripts alias # Shows all the alias setup for the user # Store an alias in the `.bash_aliases` file alias calmagic = 'xargs cal -A 1 -B 1 > /home/leslie/calOutput.txt' # In the terminal use if in a pipe command, STDOUT will be stored in a file echo \"12 2021\" | calmagic File System Navigation # File Listing pwd # Prints absolute path of current working directory(CWD) ls \u2013l # Long list of CWD ls \u2013a # Shows all files including hidden ls -F # Shows directories as ending with / along with other files stat <filename> # Detailed file information file <filename> # File type ls -ld # Detailed folder information # Change Directories cd - # Helps to switch directories. Like a Toggle (Alt + Tab) in windows cd OR cd ~ # User Home directory from anywhere cd .. # Back to parent directory of CWD Wildcards Wildcards and How to use - The star wildcard has the broadest meaning of any of the wildcards, as it can represent zero characters, all single characters or any string. - The question mark (?) is used as a wildcard character in shell commands to represent exactly one character, which can be any single character. - The square wildcard can represent any of the characters enclosed in the brackets. ls *.txt # Matches all txt files ls ???.txt # Matches all 3 letter txt files ls file [ 123 ] .txt # Matches all files ending with 1 to 3 ls file [ A-Z ] .txt # Matches all files ending with A to Z ls file [ 0 -9 ][ A-Z ] .txt # Matches all files ending with 0A to 9Z File and Folders # Create Operations touch file1 # Creates a new file1 echo \"Hello\" > hello.txt # Creates and writes using redirection # -p is parent directory which is data and inside that 2 directories called sales & mkt is created mkdir \u2013p /data/ { sales,mkt } # Brace exapansion will allow to create folders. Sequence can be expressed as .. mkdir -p /tmp/ { jan,feb,mar } _ { 2020 ..2023 } # Brace expansion for files, it will create 10 files inside each folder touch { jan,feb,mar } _ { 2020 ..2023 } /file { 1 ..10 } # Delete Operations rm file1 # Deletes file1 rm *.txt # Deletes all txt files # Deletes all files and folders inside the main folder and the main folder as well # CAUTION: Use the recursive option with care rm -r /tmp/ { jan,feb,mar } _ { 2020 ..2023 } / # Deletes only empty directories rmdir /tmp/ # Skips folders which have files # Copy Operations cp /data/sales/file1 /data/mkt/file1 cp /data/sales/* . # Copy all files to CWD cp -r /data/sales /data/backup # Copy sales folder to backup folder # Move and Rename Operations mv file1 file2 # Rename file in the same folder mv /data/mkt/ /data/hr # Rename folder, Note the slash after first folder mv /data/sales/* /tmp/backup/ # Move files to new location mv /data/mkt/ /tmp/newFolder # Move and rename the folder Nano - Editing M Key can be Alt or Cmd depending on keyboard layout Enable spell checking on nano by editing /etc/nanorc and uncomment set speller in the file. Ctrl + O # Write data out to file Ctrl + R # Copy contents of one file into another Ctrl + K # Cuts entire line, also used as a delete Alt + 6 # Copy entire line Ctrl + U # Paste the line Ctrl + T # Spell check the file Alt + U # Undo changes Alt + E # Redo changes # File operations in vi > filename # Empties an existing file :x # Saves file changes instead of :wq Search Files find ~/projects # Find matches of files and folders from projects and below find . # Find from CWD and below find . -maxdepth 1 # Find from CWD and one level below find . -type f # Find files only find . -type d # Find folder only find . -maxdepth 1 -type d # Find folder only and one level below find . -name \"*.txt\" # Find files ending with matching patterns find . -maxdepth 3 -iname \"*.TXT\" # Find files with case insensitive matching patterns find . -type f -size +100k # Find files greater than 100 Kb # Find files greater than 100 Kb AND less than 5 Mb and count them find . -type f -size +100k -size -5M | wc -l # Find files less than 100 Kb OR greater than 5 Mb and count them find . -type f -size -100k -o -size +5M | wc -l - Find and Execute commands # Find and copy files to backup folder. `\\;` denotes end of exec command find . -type f -size +100k -size +5M -exec cp {} ~/Desktop/backup \\; ### # Find file called needle.txt inside haystack folder ### # Create 100 folders and inside each folder 100 files mkdir -p haystack/folder { 1 ..100 } touch haystack/folder { 1 ..100 } /file { 1 ..100 } # Create file in one random folder touch haystack/folder $( shuf -i 1 -100 -n 1 ) /needle.txt ### # Finding the file using name find haystack/ -type f -name \"needle.txt\" # Move the file to haystack folder find haystack/ -type f -name \"needle.txt\" -exec mv {} ~/tmp/haystack \\; ### View/Read File Contents cat cat file1 file2 > file3 # Concatenate 2 files and write into file3 cat \u2013vet file3 # displays special characters in the file e.g. EOL as $. Useful if sh files are created in windows tac - Flips the file contents vertically tac file3 # Reads the file in reverse rev - Reverses the contents of each line rev file3 # Reads the line in reverse less - Allows to page through big files less file3 # Shows one page at a time. Use Arrow keys to scroll # Output of find piped to less command for scrolling find . -type f -name \"*.txt\" | less head - Shows limited lines from top of output cat file3 | head -n 3 # Shows first 3 lines tail - Shows limited lines from bottom of output cat file3 | tail -n 3 # Shows last 3 lines tail \u2013f /var/log/messages # follows the file and continuously shows the 10 lines Sort sort words.txt > sorted.txt # Sorts in Asc order and redirects to sorted.txt sort -r word.txt > reverse.txt # Sorts in Des order sort -n numbers.txt # Sorts in Asc numeric order based on digit placement sort -nr numbers.txt # Sorts in Des numeric order based on digit placement sort -u numbers0-9.txt # Sorts and shows only unique values - Sorting data in tabular format # Sort on the basis of file size (5th column and its numeric) ls -l /etc | head -n 20 | sort -k 5n # Reverse (r) the output showing largest files first ls -l /etc | head -n 20 | sort -k 5nr # Sort on the basis of largest file size in human readable format ls -lh /etc | head -n 20 | sort -k 5hr # Sort on the basis of month ls -lh /etc | head -n 20 | sort -k 6M Search data - grep grep is case-sensitive search command # grep <search term> file-name grep e words.txt # Shows matching lines as STDOUT grep -c e words.txt # Counts the matching lines # Search in case insensitive manner grep -i gadsby gadsby_manuscript.txt # Search strings using quotes grep -ci \"our boys\" gadsby_manuscript.txt # Invert the search grep -v \"our boys\" gadsby_manuscript.txt # Searches for server and not servers. \\b is the word boundary grep \u2018 \\b server \\b \u2019/etc/ntp.conf # Searches for server beginning in the line. \\b is the word boundary grep \u2018^server \\b \u2019/etc/ntp.conf Filter data using grep ls -lF / | grep opt # Shows details for opt folder only ls -F /etc | grep -v / # Shows only files in etc folder - Remove Commented and Blank Lines # Empty lines can be shown as ^$. \u2013v reverses our search and \u2013e allows more than one expression. O/p is sent to std o/p grep \u2013ve \u2018^#\u2019 \u2013ve\u2019^$\u2019 /etc/ntp.conf # -v ^# says I don\u2019t want to see lines starting with #. ^$ says I don\u2019t want to see lines that begin with EOL marker Archival and Compression Two step process: Create the tar ball, then compress the tar Compression tool comparison # Create the tar ball tar -cvf backup.tar file [ 1 -3 ] .txt # Create, Verbose, Files to archive tar -tf backup.tar # Test for tar file without unzipping # Compress the tar ball ## 3 compression tools - gzip -> bzip2 -> xz (Compression and time increases from left to right) gzip backup.tar # Compresses the tar ball and adds .gz extension to tar ball gunzip backup.tar.gz # Decompress the gzip ### bzip2 backup.tar # Smaller file size than gzip and adds .bz2 extension to tar ball bunzip2 backup.tar.bz2 # Decompress the bzip. Best used for larger file sizes # Open the tar ball contents tar -xvf backup.tar # Extract, Verbose, Files to unarchive ### Create tar and compress in single command # Adding the z option for gzip and renaming the tar as .gz tar -cvzf backup.tar.gz file [ 1 -3 ] .txt tar -xvzf backup.tar.gz # Adding the j option for bzip2 and renaming the tar as .bz2 tar -cvjf backup.tar.bz2 file [ 1 -3 ] .txt tar -xvjf backup.tar.bz2 ### BASH #!/bin/bash # First line in the script \"SHEBANG\" tells type of script ### # To create an executable script, create a `bin` folder in your home. # Move all utility shell scripts to bin. Also remove .sh file extenstions # Make the file as executable `chmod +x data_backup` # Add the `~/bin` to the PATH variable # Edit `.bashrc` with PATH=\"$PATH:$HOME/bin\" # Now all scripts in bin folder are executable from command line ### # Set and unset variables export $VARIABLE # Sets the variable unset VARIABLE # Removes the variable, NOTE \u2013 No $ in variable Cron Scheduling crontab -e < select editor> # Opens the template crontab # Multiple options for each column of crontab using comma. # SPACE is used to delimit the columns of crontab # */<value> can divide the time intervals ### # min hours \"day of month\" month \"day of week (0-6)\" ### * * * * * bash ~/data_backup.sh Package Management apt-cache search docx # Searches apt for programs that can work with MS Word apt-cache show <package> | less # Gives software information # Apt cache information resides in /var/lib/apt/lists sudo apt-get update # Updates the apt lists sudo apt-get upgrade # Upgraded to the latest software versions from the list sudo apt-get install <pkg-name> # Install package sudo apt-get purge <pkg-name> # Remove & uninstall package. Recommended approach sudo apt-get autoremove # Removes any installed package dependecies # Package compressed archives are stored in `/var/cache/apt/archives` sudo apt-get clean # Removes all package compressed acrhives sudo apt-get autoclean # Removes only package compressed acrhives that cannot be downloaded - Source Code for apps OS uname # Shows kernal uname -o # Shows OS uname -m # Shows computer architecture x86_64 (64 bit), x86 (32 bit) lsb_release -a # Distro version Misc fdisk -l # Gives device wise memory details free / free -m # Gives amount of free memory lsblk # Lists all partitions swapon \u2013s # List all swap files ps # Process id of the current bash shell shutdown \u2013h now / poweroff / init 0 # Power downs the system restart / init 6 / reboot # Restarts the system shutdown \u2013r + 1 \u201cWe are restarting\u201d # Restarts the system give all logged in users 1 min to shut down all process su - # Login to root id / id bob # Shows the current user and group id sudo -i # Interactive shell for password of the current user, to get elevated access ssh localhost # ssh connection to same server. Type exit or Ctrl + D to logout of ssh. who / w # Gives the list of terminals that are connected and who has logged on to the server Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal","title":"Basic commands"},{"location":"learning/linux/linux/#tips","text":"Alt + F2 Gives the run command terminal and then type gnome-system-monitor is like task Manger in windows. Gives graphical overview of the system and can kill processes. Putting & after any commands runs it in the background. Run jobs to see all the background running jobs. Type fg to bring the background running jobs to the foreground. Ctrl + C to cancel the job then. !<anycharacter> will search for the last command in history starting with that character !?etc executes the last command that contains etc","title":"Tips"},{"location":"learning/linux/linux/#filesystem","text":"","title":"Filesystem"},{"location":"learning/linux/linux/#creating-partitions","text":"fdisk or gdisk utility to partition. If Mountpoint is /, that is primary partition.","title":"Creating Partitions"},{"location":"learning/linux/linux/#creating-filesystems","text":"mkfs.ext4 \u2013b 4096 /dev/sdb1 # Creates 4MB block size file system mkfs.xfs \u2013b size = 64k /dev/sdb2 # Creates 64k block size file system. Xfs is specialized filesystem","title":"Creating Filesystems"},{"location":"learning/linux/linux/#mounting-data","text":"mkdir \u2013p /data/ { sales,mkt } mount /dev/sdb1 /data/sales # Mounts device to data/sales directory mount /dev/sdb2 /data/mkt","title":"Mounting Data"},{"location":"learning/linux/linux/#unmounting-data","text":"umount /dev/sdb1 or umount /dev/sdb { 1 ,2 } # Unmounts both the devices","title":"Unmounting Data"},{"location":"learning/linux/linux/#virtual-memory-or-swap-filesystem","text":"They are temporary space requirements Virtual memory in Linux can be a Disk Partition or Swap file. Use gdisk to create swap filesystem. Option L and then hex code 8200. To make the swap filesystem permanent, make an entry in /etc/fstab file, so changes are persistent even after system reboot. partprobe /dev/sdb # Sync saved partition in memory. Or it requires system reboot mkswap /dev/sdb3 # Select the right swap device to create the filesystem swapon /dev/sdb3 # Mount the filesystem","title":"Virtual Memory or Swap Filesystem"},{"location":"learning/linux/linux/#troubleshooting-linux-filesystem","text":"df \u2013hT # list all filesystem with space details du \u2013hs /etc # gives diskusage of etc directory with memory dumpe2fs /dev/sdb1 | less # human readable details for the device dd if = /dev/sda of = /data/sales/file count = 1 bs = 512 # takes data backup of sda to sales/file of the first 512 bytes dd if = /data/sales/file of = /dev/sda # copies the data back in case of recovery tar \u2013cvf /data/sales/etc.tar /etc # backs up etc directory by creating a tar file umount /dev/sdb1 # unmounts sales directory tune2fs \u2013L \u201cDATA\u201d /dev/sdb1 # adding label to the file system debugfs /dev/sdb1 # enters debug of sdb1 directory. Type quit to exit","title":"Troubleshooting Linux filesystem"},{"location":"learning/linux/linux/#file-permissions","text":"# Format for file permission : User-Group-Others # Symbolic Notation (Default permission) RWX \u2013 RW - R # Octal Notation 7 - 6 - 4 # So RWX is 111 i.e. 7, RW is 110 i.e. 6 and R is 100 i.e. 4 umask 2 # sets default permission to all the files in the directory chmod 777 file1 # Changes permission for a file1 chmod u = rwx,g = rw,o = rw file 2 # Verbose way to set permissions chmod +rx file3 # Sets read & write for User, group and others ls \u2013ld /data # Shows permission for a single directory chgrp users /data # Adds users group to the directory - Even if user does not have write access to a file, he has delete / add file access to a directory. - chmod o+t /data .Users can delete only their files and not other\u2019s. Root will not be able to delete files in this directory. - This permission is sent on the /tmp directory by default at installation. So only user\u2019s own file can be deleted, not of others.","title":"File Permissions"},{"location":"learning/linux/linux/#links-hard-and-soft-links","text":"Soft links are also called as Symbolic Links or symlinks . Here one file will be a pointer to the other file. If file has more than one name, it\u2019s called hard link. To find the number of sub directories , use stat dirname . Links number -2 is the total number of sub directories. Each directory has a minimum of 2 links, hence subtract 2. ln file2 file5 # Creates hard link between file2 and file5. # Shows the inode number which is same i.e. the same metadata is present for both. Cat on both the files shows the same data content ls \u2013li file2 file5 ln \u2013s file3 file4 # Creates a symlink between file 3 and file5. Cat on both the files shows the same data content ls \u2013li file3 file4 # Shows the symlink, but they are different files. Inode number is different. readlink file5 # shows where the link is","title":"Links (Hard and Soft Links)"},{"location":"learning/linux/linux/#applying-quotas","text":"Quotas can be applied to Space/inodes, Group, User or File System. repquota \u2013auv # Give quota report per user space usage along with limits quotaon /dev/sdb1 # Checks quota limit # enable quotas and edit the hard and soft limits. Soft limit can be exceeded for 7 days, after which it is enforced. edquota \u2013u <username> # enables quota via command line. Soft limit is 21000 is 21MB, hard limit is 26MB setquota \u2013u <username> 21000 26000 0 0 /dev/sdb1","title":"Applying Quotas"},{"location":"learning/linux/linux/#directory-listing-and-alias","text":"ls \u2013F /dir1 # shows directory with a / and symlink as @ at the end of the name ls \u2013-color = auto /dir1 # shows the same file types in color alias ls = \u2019ls \u2013-color = auto\u2019 # creates an alias for ls with color ls \u2013lh file1 # list in human readable format ls \u2013lt /etc # shows long listing with time modified in descending order ls \u2013ltr /etc | less # shows reverse listing, q to quit","title":"Directory Listing and Alias"},{"location":"learning/linux/linux/#synchronize-directories","text":"mkdir /backup rsync \u2013av /home/ /backup/ # archive home dir to backup dir. / after home and backup is important rsync \u2013av --delete /home/ /backup/ # sync deletions of data as well, otherwise rsync ignores it by default rsync \u2013ave ssh # sync data between servers using e option","title":"Synchronize Directories"},{"location":"learning/linux/linux/#process-management","text":"","title":"Process Management"},{"location":"learning/linux/linux/#monitor-process","text":"which ps # shows the installation directory for ps uptime # shows the uptime of the system along with the load average in the range of 1 min, 5 mins and 15 mins # Rule of Thumb for uptime --> Load average for single core value should be less than 1, for dual core less than 2 etc. which uptime # shows the installation directory for uptime cat /proc/uptime # shows uptime and idle time cat /proc/loadavg # shows load avg for 1,5 and 15 mins, active process running/total process, last process id that was issued","title":"Monitor Process"},{"location":"learning/linux/linux/#jobs","text":"sleep 180 # sleeps for 180 secs in foreground. Ctrl + Z to pause the job. Run bg to put the sleep command in background. jobs # shows running jobs fg 1 # puts the sleep command in foreground","title":"Jobs"},{"location":"learning/linux/linux/#managing-processes","text":"ps to display processes and kill to send signals. pgrep, pkill and killall are great shortcuts. The default kill signal is -15 which can also be written as \u2013term or \u2013sigterm . To really kill it is -9, -kill or \u2013sigkill . echo $$ # Shows current process ps \u2013l # long listing with the process ps \u2013ef # shows all the processes for all users ps \u2013eaf | grep processname pgrep nginx # shows process ids for nginx sleep 900 & pkill sleep # searches for sleep process and kills it killall sleep # searches for all running sleep process and kills it kill \u2013l # shows the multiple kill signals available kill -9 <process id> # forcefully terminates the process, also use kill \u2013kill <process id> top \u2192 kill, renice, sort and display processes Running top, you can toggle between the information displayed at the top lines. l \u2013 on/off load, t \u2013 on/off tasks, m \u2013 on/off memory Sorting of top is on %CPU, f \u2013 shows current fields being shown on output of top. Select the new field to sort and type s Type r for renice and put in the process id. Esc and Enter to quit the shell Type k for kill and put in the process id. Esc and Enter to quit the shell q to quit out of top top # shows all running processes, q to quit top \u2013n 1 # shows the running processes for 1 capture and quits top \u2013n 2 \u2013d 3 # shows 2 captures with a delay of 3 seconds and quits","title":"Managing Processes"},{"location":"learning/linux/linux/#editors","text":"","title":"Editors"},{"location":"learning/linux/linux/#vi","text":": # Last line mode q, q! # quit the file x, wq, wq! # save and exit the file i, I # insert from cursor position, I for inserting from start of the line a, A # append after the cursor, A for append from last character in the line o, O # insert line below the cursor, O for above the current cursor position dd # delete the line u # undo the changes","title":"Vi"},{"location":"learning/linux/linux/#line-navigation","text":"<Linenumber>G # e.g. 7G, takes cursor to 7th line in the file G # only G takes cursor to end of file w , b # w takes cursor to next word, b takes cursor to one word before ^ , $ # ^takes cursor to start of line, $ to end of the line vi +127 /etc/file1 # opens the file and takes cursor to 127th line vi +/Document /etc/file1.conf # opens the file and takes cursor to first occurrence of \u201cDocument\u201d set number / set nonumber # from last line mode, it will show and stop line number display syntax on # highlighting on, e.g. xml highlighting etc.","title":"Line Navigation"},{"location":"learning/linux/linux/#read-and-write","text":"r /etc/hosts # Open an existing file, use : and then you can get content from hosts file into current file w newfile # :, it will copy entire file contents into newfile in the same directory 3 ,7w newfile # it will copy line 3 to 7 into newfile","title":"Read and Write"},{"location":"learning/linux/linux/#search-and-replace","text":"%s/Hi/Hello # Open an existing file, use : and you can search Hi and Replace with Hello. %s signifies entire document search /Hello # searches for Hello in the document. Type n to get next occurrence, N will take cursor in reverse 1 ,20s/Hi/Hello # searches for 1st 20 lines for Hi and replaces with Hello 14 ,20s/^/ / # from 14th to 20th line, it will add 3 spaces from the start of the line, just like Tab","title":"Search and Replace"},{"location":"learning/linux/linux/#bash-scripting","text":"","title":"BASH Scripting"},{"location":"learning/linux/linux/#understanding-variables","text":"Local variables \u2192 accessible only to the current shell, FRUIT=\u2019apple\u2019, echo $FRUIT Global variables \u2192 you need to set and then export it to make it global. export FRUIT=\u2019apple\u2019","title":"Understanding Variables"},{"location":"learning/linux/linux/#simple-script","text":"vi hello.sh #!/bin/bash # Path to the interpreter echo \u201cHello World\u201d exit 0 # return code, :wq chmod +x hello.sh hello.sh # execute the script as it\u2019s in the home directory /user/bin","title":"Simple Script"},{"location":"learning/linux/linux/#getting-user-input","text":"vi hello.sh #!/bin/bash echo \u2013e \u201cEnter your name: \\c \u201d # -e is the escape sequence, -c is for the prompt read INPUT_NAME # read the input data into a variable echo \u201cHello $INPUT_NAME \u201d exit 0","title":"Getting user input"},{"location":"learning/linux/linux/#user-input-types","text":"$1 $2 # $1 is the 1st input parameter, 2nd Parameter and so on. $0 # is the script name itself $# # count of input parameters $* # is collection of all the arguments","title":"User Input types"},{"location":"learning/linux/linux/#multiple-inputs-using-positional-parameters","text":"vi hello.sh #!/bin/bash echo \u201cHello $1 $2 \u201d # $1 is the 1st input parameter, $0 is the script name itself, $2 is the 2nd input parameter and so on exit 0","title":"Multiple inputs using positional parameters"},{"location":"learning/linux/linux/#code-snippets","text":"Gedit \u2192 Gnome Editor \u2192 Add the Snippet Plugin (Applications \u2192 Accessories \u2192 gedit. Preferences in gedit tab \u2192 Plugins enable Snippet Plugin and restart gedit)","title":"Code Snippets"},{"location":"learning/linux/linux/#conditional-statement---if","text":"if [[condition]] \u2192 testing for string condition if ((condition)) \u2192 testing for numeric condition e.g. if (( $# < 1 )) \u2192 if count of input parameter vi hello.sh #!/bin/bash if (( $# 1 )) then echo \u201cUsage: $0 <name>\u201d exit 1 fi echo \u201cHello $1 $2 \u201d exit 0","title":"Conditional Statement - IF"},{"location":"learning/linux/linux/#case-statement","text":"vi hello.sh #!/bin/bash if [[ ! \u2013d $1 ]] # if the 1st argument is not a directory then echo \u201cUsage: $0 <directory>\u201d exit 1 fi case $2 in \u201cdirectory\u201d ) find $1 \u2013maxdepth 1 \u2013type d ;; # break \u201clink\u201d ) find $1 \u2013maxdepth 1 \u2013type l ;; # break * ) # default statement echo \u201cUsage: $0 <directory> directory | link\u201d ;; esac exit 0","title":"Case Statement"},{"location":"learning/linux/linux/#for","text":"vi hello.sh #!/bin/bash for u in $* # $* is collection of arguments, u is temporary variable do # do block useradd $u # access to temp variable is via $ echo Password1 | passwd \u2013stdin $u # use the passwd command and get the user input from keyboard passwd \u2013e $u # expire the password, so they can change it at first login done echo \u201cFinished\u201d # at time of execution ./hello.sh fred mary john vi listsize.sh #!/bin/bash for file in $( ls ) # for each file, in the output of ls do [[ ! \u2013f ]] && continue # not a file then continue to next # use the stats to get statistics of the file, to get the last accessed date and then format the date LA = $( stat \u2013c %x $file | cut \u2013d \u201c \u201d \u2013f1 ) echo \u201c $file is $( du \u2013b $file ) bytes and was last accessed on $LA \u201d # use du to get file size done","title":"For"},{"location":"learning/linux/linux/#while","text":"vi loop.sh #!/bin/bash -x # -x is for debug mode COUNT = 10 while (( COUNT > 0 )) do echo \u2013e \u201c $COUNT \\c \u201d # \\c will suppress the line feed (enter) sleep 1 (( COUNT -- )) # round brackets to avoid using $ symbol done - Use the until when you want to stop the loop when the condition becomes true.","title":"While"},{"location":"learning/linux/linux/#user-management","text":"Managing Users: User Lifecycle ==> useradd, usermod, userdel Local databases ==> /etc/passwd, /etc/shadow (encrypted) passwd (to set the password) pwconv (move pass to encrypted) pwunconv (move back to unencrypted) # /etc/passwd file structure # It has 7 filed separated by : Login Name, Optional encrypted password or \u201cx\u201d, Numerical UID, Numerical GID, Username or comment, User home directory, Optional command interpreter # /etc/shadow file structure where the actual passwords are stored # It has 8 filed separated by : Login Name, encrypted password ( if it begins with ! the account is locked ) , Date of last password change, Minimum password age, Maximum password age, Password warning period, password inactivity, account expiry date # /etc/login.defs The password ageing defaults can be configured with this file useradd \u2013D # shows the default settings for a user that is added cat /etc/default/useradd # shows where the defaults are set useradd bob # only adds the user, no home directory is created. Once the user logs in, it will get created tail -3 /etc/passwd # shows that bob is added useradd \u2013m bob # also creates the home directory useradd -m -d /home/bob -s /bin/bash # Creating user and home dir in one command tail -3 /etc/shadow # shows the password for the user passwd bob # add the password for bob passwd \u2013l bob # locks the account passwd \u2013u bob # unlocks the account passwd --status bob # Shows the status of account bob, 2nd column (L - Locked, NP - No Password, P valid password) usermod bob \u2013c \u201cBob Smith\u201d # adding additional details for the user userdel \u2013r bob # removes the user and home directory","title":"User Management"},{"location":"learning/linux/linux/#group-management","text":"Group Lifecycle \u2192 groupadd, groupmod, groupdel Local databases \u2192 /etc/group, /etc/gshadow (encrypted) gpasswd (to set password) newgrp (switch to new groups) # /etc/group structure # It has 4 fields: Group Name, Password, Numerical GID, User list that is comma separated # /etc/gshadow structure # It has 4 field: Group Name, Encrypted password, Admin list that is comma separated, # this can be managed used \u2013A cmd # Members, this can be managed using the \u2013M cmd Private groups are enabled by default. The user added is also added to the same group. If this is disabled, users will belong to the groups users. Use useradd \u2013N to overwrite private groups. This can be enabled or disabled by setting USERGROUPS_ENAB in /etc/login.defs useradd \u2013m \u2013g users jim # -g is Primary Group, -G is secondary groups. Secondary groups are more traditional groups id jim usermod \u2013G sudo,adm jim # added jim to secondary groups sudo, adm useradd \u2013N \u2013m sally # adds sally to the default group gpasswd \u2013M jim,sally sudo # adds 2 users to sudo group groupadd sales gpasswd sales # sets the new password for sales newgrp sales # add the user to the sales group temporarily. If the user logs out, he is removed from the group","title":"Group Management"},{"location":"learning/linux/linux/#automate-system-tasks","text":"Regular Tasks \u2192 cron (more than once a day but misses job if turned off), anacron (run jobs missed on startup but jobs can run just once a day) Once Off \u2192 at (runs at specified time and date), batch (runs when load average drops below 0.8)","title":"Automate System Tasks"},{"location":"learning/linux/linux/#system-cron-jobs","text":"# /etc/crontab, /etc/cron.d # cron files # /etc/cron.<time> # where time is hourly, daily, weekly and monthly, contains scripts that need to be executed # Adding a system cron job cd /etc/cron.d vi daily-backup # add a new file 30 20 * * 1 -5 root /root/back.sh # run back.sh from Mon to Fri at 20:30 # Adding a user cron job crontab \u2013e # edit the user crontab file */10 10 1 1 1 tail /etc/passwd # Runs once on 1st day if it\u2019s a Mon of Jan, at 10 am for every 10 mins crontab \u2013l # list all cron jobs crontab \u2013r # remove the cron job # anacron: /etc/anacrontab structure # It has 4 fields Period in days or macro ( Daily, Monthly ) , Delay ( minutes after system startup for job to run ) , Job Identifier ( used to name timestamp file indicating when job was last run ) Command ( that needs to be executed ) @weekly 120 weekly-backup ls /etc // weekly, 120 mins after startup it will run weekly-backup","title":"System Cron Jobs"},{"location":"learning/linux/linux/#batch","text":"at and batch commands at noon tomorrow # Enter the command line, Ctrl + D to save at> ls /etc # enter the command that needs to be executed atq # shows the jobs queue atrm # remove the job batch # Enter the command line, Ctrl + D to save at> ls /etc > /root/file1 # redirect the o/p to file1. It will run if the system load avg is less than 0.8.","title":"Batch"},{"location":"learning/linux/linux/#security-for-cron","text":"Everyone is allowed to run their own cron and at jobs, unless you add entries to /etc/cron.allow or /etc/at.allow. No one is denied unless you add entries to /etc/cron.deny or /etc/at.deny","title":"Security for Cron"},{"location":"learning/linux/linux/#networking-fundamentals","text":"","title":"Networking Fundamentals"},{"location":"learning/linux/linux/#network-time-protocol-ntp","text":"","title":"Network Time Protocol (NTP)"},{"location":"learning/linux/linux/#configuring-network-time-protocol-ntp","text":"# vi /etc/ntp.conf # prefixing i with date creates a backup of the original file. Removes commented and blank lines sed \u2013i. $( date +%F ) \u2018/^#d ; /^$/d\u2019 /etc/ntp.conf","title":"Configuring Network Time Protocol (NTP)"},{"location":"learning/linux/linux/#implementing-the-configuration-file-changes","text":"vi /etc/ntp.conf # Add lines other can default just below the driftfile command statsdir /var/log/ntpstats # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server instead of on the internet debian servers","title":"Implementing the configuration file changes"},{"location":"learning/linux/linux/#save-the-file-and-restart-the-service","text":"service ntp restart # sudo if no access # check if the ntpstats directory is accessible to the ntp service # The user should be ntp and it should have write access ls \u2013ld /var/log/ntpstats/ - Date \u2192 Current system date and time. This is the time in memory. - HwClock \u2192 Hardware date and time set by the BIOS. hwclock \u2013r # shows the hardware clock hwclock \u2013-systohc # sets the hardware clock from system clock Hwclock \u2013-hctosys # sets the system clock from hardware clock","title":"Save the file and restart the service"},{"location":"learning/linux/linux/#ntp-tools","text":"ntpdate (once off adjustment) ntpq (query the ntp server) ntpq \u2013p (shows peers) ntpstat (Shows status but not on debian. Try ntpdc \u2013c sysinfo) ntpq \u2013c \u201cassociations\u201d \u2192 shows associations # Configuring NTP on centos --> Install ntp ntpdate 192 .168.0.3 # one off update with a local machine in the network # vi /etc/ntp.conf # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server systemctl start ntpd # save and restart systemctl enable ntpd # enable to start at system startup","title":"NTP Tools"},{"location":"learning/linux/linux/#managing-system-log-daemons","text":"","title":"Managing System Log Daemons"},{"location":"learning/linux/linux/#rocket-fast-system-for-log-processing-rsyslogd","text":"rsyslogd \u2013v # vi /etc/rsyslog.conf # Adding a simple log rule # For any log event greater than or equal to info make a log entry in local5 log. Local5 could be a simple application local5.info /var/log/local5 systemctl restart rsyslog.service # restart the service # to test this in working using command line logger \u2013p local5.info \u201cScript started\u201d # p is priority, if you see /var/log/local5 file, the log would be present","title":"Rocket-Fast System for Log Processing (rsyslogd)"},{"location":"learning/linux/linux/#varlog-folder-structure","text":"messages (Nearly everything is logged here) secure (su and sudo events amongst others) dmesg (kernel ring buffer messages)","title":"/var/log/ folder structure"},{"location":"learning/linux/linux/#logrotate","text":"ls /etc/cron.daily # has the logrotate script which will rotate log files cd /etc/logrotate.d/ # folder where all apps rotation policy is set cp syslog local5 # copy existing app conf for local5 app # vi local5 # make edits to point to /var/log/local5 file /var/log/local5 { weekly # period for rotation size +10 # size of the file for rotation compress # use compression for the rotated log file rotate 4 # keep 4 weeks of logs before overwriting } # manually running the rotate logrotate /etc/logrotate.conf # on execution, all files mentioned will be interrogated and log backup will be created","title":"Logrotate"},{"location":"learning/linux/linux/#journalctl","text":"Responsible for viewing and log management. Need to be a member of adm group to read this. By default journal is memory resident i.e. it will be lost on restart journalctl # view the journal journalctl \u2013n 10 # shows the last 10 entries journalctl \u2013n 10 \u2013p err # shows the last 10 entries with priority errors mkdir /var/log/journal # to make the journal data persistent systemctl restart system-journald systemctl status system-journald # shows that the journal data is persistent usermod \u2013a \u2013G adm username # adding user to adm group, -a is append chgrp \u2013R adm /var/log/journal # recursively give adm group access jornalctl \u2013-disk-usage # shows disk usage journalctl \u2013-verify # verify the journal integrity","title":"Journalctl"},{"location":"learning/linux/linux/#ssh","text":"","title":"SSH"},{"location":"learning/linux/linux/#remote-access-using-ssh","text":"Server Configuration /etc/ssh/sshd_config The public key of the server is used to authenticate to the client. The public key of the server is stored in /etc/ssh/ssh_host_rsa_key.pub It is down to the client to check the public key using: StrictHostkeyChecking Server public keys are stored centrally in /etc/ssh/ssh_known_host or locally under ~/.ssh/known_hosts","title":"Remote access using SSH"},{"location":"learning/linux/linux/#ssh-server-configuration","text":"netstat \u2013antl # shows the open tcp ports of the server grep ssh /etc/services # shows the services using ssh lsof \u2013i # Also shows open ports # vi /etc/ssh/sshd_config # Uncomment AddressFamily line and change as below AddressFamily inet # Now ssh will only listen on IPv6 systemctl restart sshd # vi /etc/ssh/sshd_config # Uncomment below lines and modify LoginGraceTime 1m # To avoid denial of service attacks and freeing up your service quickly PermitRootLogin no # 2 level authentication, first as normal user and then root SyslogFacility AUTHPRIV ClientAliveInterval 300 ClientAliveCountMax 0 MaxSessions 10 systemctl restart sshd","title":"SSH Server Configuration"},{"location":"learning/linux/linux/#client-configuration-and-authentication","text":"Client Configuration /etc/ssh/ssh_config Generate Private and Public keypair using ssh_keygen Use ssh-copy-id to copy to host we want to authenticate with. To provide Single Sign On using ssh-agent Client/User public keys are stored in ~/.ssh/authorized_keys using ssh-copy-id.","title":"Client Configuration and Authentication"},{"location":"learning/linux/linux/#to-connect-to-server-using-ssh","text":"cd # home directory ls \u2013a # to show all hidden files ssh pi@192.168.0.97 # ssh using user and ip address. Add the password of the user to authenticate cd .ssh cat known_hosts # shows the client ip and public keys exit or logout or Ctrl + D # to end the ssh session","title":"To connect to server using ssh"},{"location":"learning/linux/linux/#to-generate-keypairs","text":"cd .ssh # On the client home directory ssh-keygen \u2013t rsa # generate key pair. Private key is encrypted using a passphrase # This will copy the generated public key to the target server. To which user\u2019s directory at the server we will # connect as. Give the password of the server\u2019s account. ssh-copy-id \u2013i id_rsa.pub pi@192.168.0.97 ssh pi@192.168.0.97 # now connect to the server using passphrase of the private key # From another terminal say tty we can now add the private key once and don\u2019t need to authenticate to the target server ssh-agent bash # fire up another bash terminal ssh-add .ssh/id_rsa # add the private key from the home directory. Enter the passphrase ssh \u2013l or ssh \u2013L # list all identities added ssh pi@192.168.0.97","title":"To generate keypairs"},{"location":"learning/linux/linux/#ssh-tunnels","text":"ssh \u2013f \u2013N \u2013L 80 :localhost:80 user@s1.com # -f = execute in background, -N = We are not running any commands on remote host # -L = listening on port 80, we are listening on localhost and forwarding to port 80 on the remote host # On the remote host it has to listen on ssh called s1.com. We connect as user called user. # Example # Webservice on 192.168.0.3 # on a different machine, login as standard user cd .ssh ssh \u2013f \u2013N \u2013L 9000 :localhost:80 andrew@192.168.0.3 # we are listening on port 9000 on the localhost and forwarding traffic to port 80 om 192.168.0.3 netstat \u2013antlp # we can see that localhost:9000 is listening on ssh # On the client machine open the browser and type in http://127.0.0.1:9000 we will see the webservice data kill <process id of ssh> # shutdown the ssh tunneling process after finishing the work","title":"SSH Tunnels"},{"location":"learning/linux/linux/#configuring-network-protocols-in-linux","text":"","title":"Configuring Network Protocols in Linux"},{"location":"learning/linux/linux/#etcservices","text":"Network services are identified by a port address Common services and associated port address is listed in /etc/services netstat \u2013alt will list services listening via TCP this resolves address to name in /etc/services # To verify the above we can use strace to map the netstat data with the services that are running strace netstat \u2013alt 2 > & 1 | grep /etc/services grep http /etc/services # service and port mapping","title":"/etc/services"},{"location":"learning/linux/linux/#dig-command","text":"which dig # get the path of dig rpm \u2013qf /usr/bin/dig # dig is not installed by default. Hence needs to be installed. dig \u2013t AAAA ipv6.bbc.co.uk # shows the IPv6 address","title":"dig command"},{"location":"learning/linux/linux/#interface-configuration-files","text":"ifconfig # shows ip address details ifconfig eth0 192 .168.0.99 netmask 255 .255.255.0 up # sets ip address for Ethernet card ip address show # same as ifconfig # These settings are lost upon restart unless they are written to configuration files. /etc/sysconfig/network-scripts/ # centos /etc/network/interfaces # debian","title":"Interface Configuration Files"},{"location":"learning/linux/linux/#to-make-the-ip-address-static","text":"cd /etc/sysconfig/network-scripts/ vi ifcfg-ens32 # open the config file # Replace and add the lines BOOTPROTO = \u201dstatic\u201d # change from dhcp IPADDR = 192 .168.0.240 # select a static ip address NETMASK = 255 .255.255.0 # add a class c subnet GATEWAY = 192 .168.0.1 # add the gateway address DNS1 = 8 .8.8.8 # add google as the DNS server # Restart the services systemctl restart network.service","title":"To make the IP address static"},{"location":"learning/linux/linux/#networking-tools","text":"","title":"Networking Tools"},{"location":"learning/linux/linux/#nmap","text":"nmap localhost # shows all open ports used by localhost nmap 192 .168.0.3 # shows open ports at remote host nmap \u2013v 192 .168.0.3 # verbose mode nmap \u2013iL ip.txt # input file containing all ip address to be scanned nmap -p 80 ,443 192 .168.0.3 -P0 # Scan 2 ports without using ping. This is done if iptables is blocking ping","title":"nmap"},{"location":"learning/linux/linux/#netstat","text":"netstat \u2013a # shows all connections netstat \u2013at # shows all tcp connections netstat \u2013alt # shows all listening tcp connections netstat \u2013altpe # shows all user and process ids and listening tcp connections netstat \u2013s # statistics netstat \u2013i # shows interfaces netstat \u2013g # multicast groups netstat \u2013nr # network route tables # Default checking netstat -tupan | grep ssh # Shows the pots for ssh","title":"netstat"},{"location":"learning/linux/linux/#show-sockets-ss","text":"ss \u2013t \u2013a # shows all tcp connections ss \u2013o state established \u2018 ( dport = :ssh or sport = :ssh ) \u2019 # shows all ssh connections ss \u2013x src /tmp/.X11-unix/* # shows X11 connections using socket files","title":"Show Sockets (ss)"},{"location":"learning/linux/linux/#lsof","text":"lsof \u2013i -4 # list all ipv4 connections lsof \u2013i :23, 24 # list all port 22 and 23 connections lsof \u2013p 1385 # list process id 1385 connections","title":"lsof"},{"location":"learning/linux/linux/#testing-network-connectivity","text":"ping www.centos.org # test network connectivity ping \u2013c3 www.centos.org # sends only 3 pings traceroute www.centos.org # describes the route to destination from source tracepath www.centos.org # shows maximum transmission unit size","title":"Testing Network Connectivity"},{"location":"learning/linux/linux/#host-name-resolution-tools","text":"hostname # Confirms the hostname cat /etc/hostname # shows the current host name hostname myComputer # changes the hostname to myComputer. You need to login as root to change this dig www.centos.org # resolves to ip address dig \u2013t MX centos.org # mail servers associated with centos","title":"Host Name Resolution Tools"},{"location":"learning/linux/linux/#managing-interfaces","text":"ip a s # Shows ip addresses ip n s # Shows Neighbor shows looking at ARP cache ip r s # Shows root table ip ma s # Shows Multicast groups ip l # Shows network cards ifdown eth0 # Brings down interface ifup eth0 # Brings up interface","title":"Managing Interfaces"},{"location":"learning/linux/linux/#securing-access-to-your-server","text":"# Temporary disables logins for users other than root. Just the existence of this file will prevent user logins and is controlled via PAM(Pluggable Authentication Modules) /etc/nologin # Create a blank file, but if the user tries to login via ssh username@localhost, the connection will be immediately closed. Only root can access the server then. touch /etc/nologin rm /etc/nologin # Now users can login. This can be used as a temporary measure cd /etc/pam.d/ # Config files for authentication modules grep nologin * # Shows instances where nologin file exists last # shows last user activity present in /var/log/wtmp file. lastlog # List the last login time for each user lastlog | grep \u2013v Never # Reverse the grep search to check for all user logins","title":"Securing Access to your Server"},{"location":"learning/linux/linux/#ulimit","text":"Puts restrictions on system resources ulimit \u2013a # Shows system limitations that can be applied ulimit \u2013u 8000 # -u is for avoiding fork bombs and is defaulted to 4096. A std user can set a new value to 8000. # It will remain for his profile till system restart. cd /etc/security # To set the limits which can be persisted even after restart cat limits.conf # Shows soft (can be changed by processes) and hard (can\u2019t be changed by processes) limits cd limits.d/ # ls to see the files inside this directory. Edit the file and add an entry for the user account with soft or hard limits and save the file.","title":"ulimit"},{"location":"learning/linux/linux/#avoiding-fork-bombs","text":"They are a potential Denial of Service Attack ulimit \u2013u ##### Do not run on Production Machines. Test only in laptop ##### ps \u2013u username | wc -l # Shows the number of processes running under the user and gives the count # On the command line create a function called foo foo (){ echo hello } # Execute the function by just calling it and pressing enter foo # Similarly to execute a fork bomb, instead of foo, call it : : (){ : | : & # Here the function calls itself and pipes the output to itself } ; : # End the function with a semicolon and then call the function : # BEWARE: Do this only at your own risk. Ensure ulimit is set to protect the resources on the server #","title":"Avoiding fork bombs"},{"location":"learning/linux/linux/#xinetd","text":"Called the super daemon as it can manage a lot of smaller services, secures access to your server /etc/xinetd.d /etc/xinetd.conf tftp server (Trivial file transfer protocol) # Sample configuration service tftp { socket_type = dgram # data gram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /tftpboot # root directory of the server disable = no }","title":"xinetd"},{"location":"learning/linux/linux/#implementing-tftp-using-xinetdd-service","text":"ls /etc/xinetd.d/ # blank directory yum install tftp-server tftp # install client and server, also xinetd as it\u2019s a dependency vi /etc/xinetd.d/tftp # after installation, delete disable line from the configuration to enable tftp # if the server directory doent exist, create it mkdir \u2013p /var/lib/tftpboot systemctl enable xinetd systemctl start xinetd netstat \u2013aulpe | grep tftp # shows the port # As a root user, create a temp file inside var/lib/tftpboot directory with hello text vi var/lib/tftpboot/file1 # Logout and login as standard user. Use TFTP to transfer the file to standard user tftp 127 .0.0.1 # Press enter get file1 # Get the file1 created by root using tftp quit # At the same file location do a cat to see the contents of the file cat file1","title":"Implementing TFTP using xinetd.d service"},{"location":"learning/linux/linux/#tcp-wrappers","text":"Alternative to firewalling on the server # To check if service supports TCP wrappers. Once we can determine this, we can use hosts.allow or hosts.deny ldd </path to service name> | grep libwrap # To set this up for a service, 2 entries are made # This in /etc/hosts.allow file to allow access to 0.3 IP which is raspberry pi. tftpd is the name of the binary in .tftpd : 192 .168.0.3 # This in /etc/hosts.deny file to deny access to all other IP except for 0.3 IP in .tftpd : ALL # If the client appears in both files then allow takes precedence and access is granted ldd /usr/sbin/xinetd | grep libwrap tftp 192 .168.0.240 # login from raspberry pi and access the remote server, disable your firewall before trying this.","title":"TCP Wrappers"},{"location":"learning/linux/linux/#delegating-admin-rights-using-sudoers","text":"id # check if the user is already an admin and part of wheel group in centos cd /etc grep wheel sudoers # check the current setup visudo # to edit sudoers file %wheel ALL =( root ) ALL # Uncomment the line for %wheel, change (ALL) to (root), so only root can change the sudoers. Save and exit","title":"Delegating Admin rights using sudoers"},{"location":"learning/linux/linux/#data-encryption","text":"","title":"Data Encryption"},{"location":"learning/linux/linux/#using-gpg-to-encrypt-data-between-users","text":"password = $( mkpassword \u2013m sha-512 Password1 ) # encrypt the password using sha and mkpassword and put in a variable echo $password for u in marta ivan ; do # take an input from marta and ivan for users sudo useradd \u2013m \u2013p $password $u # add the user and set the encrypted password done # Run the command and create 2 users ivan and marta # Install GPG if not present dkpg \u2013S $( which gpg ) # Login as ivan and generate private and public keys for gpg encryption su - ivan gpg --gen-key # Take the default settings gpg --list-key # List the keys gpg --export \u2013a <email from gpg gen-key step> > /tmp/ivankey # export the public key and place in tmp folder for marta to access exit su - marta vi secret.txt # create a plain txt file for encryption chmod 400 secret.txt # make the file writeable by marta only gpg --import /tmp/ivankey # import ivan public key gpg \u2013e \u2013r <ivan mailid> secret.txt # add the recipient and encrypt the file mv secret.txt.gpg /tmp/ # move the encrypted file to tmp and exit su \u2013 ivan gpg \u2013d /tmp/secret.txt.gpg # decrypt the file and enter the passphrase for the private key","title":"Using GPG to encrypt data between users"},{"location":"learning/linux/linux/#implementing-luks-for-full-disk-encryption","text":"example Laptops, USB sudo apt-get install cryptsetup # Install LUKS tools lsblk # list the blocks sudo fdisk /dev/sdb # partition the sdb drive and create a new partition called sdb1 sudo cryptsetup \u2013y luksFormat /dev/sdb1 # Allows to store encrypted data in this partition. Give a passphrase sudo cryptsetup luksDump /dev/sdb1 # Shows details of the encrypted partition sudo cryptsetup luksOpen /dev/sdb1 private_data # Assign a mapper called private_data ls /dev/mapper # Shows the new mapper setup and that is a device sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted","title":"Implementing LUKS for Full Disk Encryption"},{"location":"learning/linux/linux/#efs-for-data-encryption","text":"sudo apt-get install ecryptfs-utils # Install EFS tools su \u2013 ivan # Login as std user ecryptfs-setup-private # Create a private key with passphrase. logout and login back as ivan ls # Private directory is created by EFS echo hello > Private/data # Write to private directory cat Private/data # ivan can see the data ecryptfs-umount-private # unmount the private directory ls # Now ivan cannot see the data cd /.Private/ # Go to the ivan\u2019s hidden Private folder with a dot ls # You can see the encrypted data file ecryptfs-mount-private # Mount the directory again","title":"EFS for data encryption"},{"location":"learning/linux/linux/#compiling-software-from-source","text":"Install gcc compiler sudo apt-get install gcc make # go to gnu.org/software/Downloads -> coreutils -> file ending with .xz # tar -xvJf coreutils-8.28.tar.xz # J is for xz, j is for bzip2 # cd into src folder and select ls.c file # Add a line in main function printf(\"Hello World\\n\"); # After changes, change directory one level up and run the configure script bash configure # This will check for any configuration changes to the src files and update the make file # Excute the make command make # After binary code is compiled, it needs to be updated in OS sudo make install # Close the terminal and restart. New software is working ;)","title":"Compiling software from source"},{"location":"learning/linux/router/","text":"Requirement - The Core Router\u2014First Line of Defense \u00b6 Linux as SOHO Router. SOHO stands for Small Offices and Home Offices, and usually refers to situations where there exists just one computer at home to a few computers in a small office. If you have an old computer that you are about to throw away, you can easily install Linux on it and make it your own SOHO router having the advantage of higher flexibility at zero cost. A SOHO router has a WAN port that is an Ethernet port where the provider connection must be plugged in. The Provider's CPE (Customer Premises Equipment) can be of any type (xDSL modem, wireless bridge, cable modem, fiber optic media converter) that can provide an Ethernet connection. SOHO routers usually have four to eight Ethernet ports for the LAN. This is basically a small four-to-eight-port switch that's built in the SOHO router. Some SOHO routers also have a wireless access point chipset that is bridged to the built-in switch. Our computer that will run Linux and act as SOHO router must have two Ethernet cards, one for the WAN function of a SOHO router into which the provider's CPE is plugged, and one for the LAN. If you want the LAN to be wired and wireless, the Ethernet interface for the local network will be plugged into an access point with a built-in switch. However it is, everything is basically a LAN (wired, wireless, bridged, or switched); so, from the firewall point of view, it doesn't really matter what we use at the first and second layers of the OSI model (access points, hubs, switches). The provider usually assigns us a public IP address that can be either statically assigned or dynamically assigned using DHCP or PPPoE. From a security point of view, the core router is the first line of defense. The first line of defense should stop all attacks coming from outside. However, if the firewall at the core router fails, then the firewall on each server is responsible for the security of that server. If a user gains unauthorized access due to a bug in one of the services running on one server, the core router firewall or the firewall on that server are useless. In this case, the attacker has access to a server directly connected to the other servers in the server farm, and so the first line of defense has failed. The INPUT chain for the core router is the simplest of all the firewalls so far. We need to set the policy to DROP, allow SSH, ICMP, and DNS, and disallow SYN packets\u2014that's all! In the FORWARD chain, we will do most of the operations we already did on the servers. Defining the Security Policy \u00b6 Before building up any firewalls, we have to decide what the firewall must do by creating a security policy that can be from a document (recommended) to a piece of paper, or some thoughts in our heads. The desktop can access anything. Also, we want to be able to log in on the desktop computer from outside using VNC (Virtual Network Computing). The Linux router must run SSH so that we can log in to it from the internal network and from the office. One file server for Storage requirement. One DNS server and a Transparent proxy server. One Secrets server for secret management and encryption as a service. Application servers to host applications. We need to be able to access both Linux servers in the network using SSH from the desktop computer as well as from some public IP addresses we have at home. All servers must be accessed only from computers from home. Deny access from people outside the home network to see their file shares. Use a transparent proxy for them to deny access to .pif and .scr files. Browse the Web, but not to download .pif, .scr, .exe, .zip, and .rar files. Access HTTPS port 443 TCP. We will need access to SSH on both Linux servers we have from a list of allowed hosts from our network and from outside (e.g. network administrators' home IP addresses). First, we decide on a port (22 is default for SSH), let's say 61146 TCP for running SSH on both our hosts. Next, we need to build the list of IP addresses that are authorized to access the SSH ports. Let's say those are 1.1.2.0/26, 1.1.3.192, 1.1.9.21, 1.1.19.61. The simplest way to build the rules for SSH is to create a chain called MANAGEMENT. On our Linux router, we need to apply the rules in the MANAGEMENT chain for incoming packets with the destination port 61146 TCP. There's one thing that we did here that might compromise security (although it's very unlikely)\u2014a little hole in the firewall. I'm talking about -p udp --sport 53 -j ACCEPT . Starting at one point, an unpatched service on UDP might give an attacker the possibility to pass our firewalls by using port 53 as source port. #!/bin/bash #define the prefix for the network (where we are) # Every time we add a site we modify the prefix with the network that we use at that location. PREFIX = 192 .168.3 IPT = \"/sbin/iptables\" ############# Begin the NAT table opperations ###### #Flush all the rules in the nat table $IPT -t nat -F #load some modules for NAT to work better /sbin/modprobe ip_nat_ftp /sbin/modprobe ip_nat_irc #SNAT sales and accounting to port 53 UDP (DNS) $IPT -t nat -A POSTROUTING -o eth0 -s 192 .168.1.0/24 -p udp --dport 53 -j SNAT --to 1 .1.2.96-1.1.2.254 #Transparent Proxy for sales and accounting $IPT -t nat -A PREROUTING -s 192 .168.1.0/24 -p tcp --dport 80 -j REDIRECT --to-port 3128 #SNAT Sales and accounting for HTTPS $IPT -t nat -A POSTROUTING -o eth0 -s 192 .168.1.0/24 -p tcp --dport 443 -j SNAT --to 1 .1.2.96-1.1.2.254 #Drop everything else from sales and accouting to the internet $IPT -t nat -A POSTROUTING -o eth0 -s 192 .168.1.0/24 -j DROP #Transparent Proxy for management $IPT -t nat -A PREROUTING -s 1 .1.2.64/27 -p tcp --dport 80 -j REDIRECT --to-port 3128 #MASQ internal departments $IPT -t nat -A POSTROUTING -s 192 .168.1.0/24 -o eth0 -j MASQUERADE ############# End the NAT table operations ###### ########## INPUT Chain begin ######## #Flush netfilter table $IPT -F #policy Drop $IPT -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT DROP # Dont accept IPv6 traffic ip6tables -P INPUT DROP ip6tables -P FORWARD DROP ip6tables -P OUTPUT DROP #Accept all on the loopback interface $IPT -A INPUT -i lo -j ACCEPT #Accept icmp $IPT -A INPUT -p icmp -j ACCEPT #Allow admins SSH access $IPT -A INPUT -s 1 .2.3.16/28 -p tcp --dport 22 -j ACCEPT #Allow the core router to receive DNS packets $IPT -A INPUT -p udp --sport 53 -j ACCEPT #Allow non syn packets (connections initiated by this machine) $IPT -A INPUT -p tcp ! --syn -j ACCEPT ########## INPUT Chain end ######## ########## FORWARD Chain begin ######## ########## policy ACCEPT - default #Deny Access to switches and wireless equipment $IPT -A FORWARD -s ! 1 .2.3.16/28 -d 192 .168.100.0/24 -j DROP #Allow SSH access to everything from admins $IPT -A FORWARD -s ! 1 .2.3.16/28 -p tcp --dport 22 -j ACCEPT #Allow established tcp packets out eth1 and eth2 $IPT -A FORWARD -o eth1 -p tcp ! --syn -j ACCEPT $IPT -A FORWARD -o eth2 -p tcp ! --syn -j ACCEPT #Create a chain for the intranet server #Intranet server $IPT -N INTRANET $IPT -A FORWARD -d 1 .2.3.10 -j INTRANET ###### INTRANET Chain #DROP web packets for the intranet application $IPT -A INTRANET -p tcp --dport 80 -j DROP #DROP SSH (Packets from ADMINS don't pass through here) $IPT -A INTRANET -p tcp --dport 22 -j DROP #Drop Samba and ms-ds for packets on eth2 $IPT -A FORWARD -o eth2 -p tcp --dport 137 :139 -j DROP $IPT -A FORWARD -o eth2 -p udp --dport 137 :139 -j DROP $IPT -A FORWARD -o eth2 -p tcp --dport 445 -j DROP #wireless server - simple, we don't need a chain $IPT -A FORWARD -d 1 .2.3.130 -p udp --sport 53 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.130 -p ICMP -j ACCEPT $IPT -A FORWARD -d 1 .2.3.130 -j DROP #AAA server $IPT -A FORWARD -d 1 .2.3.1 -s ! 1 .2.3.131 -p udp --dport 1812 :1814 -j DROP $IPT -A FORWARD -d 1 .2.3.1 -p tcp --dport 23 -j DROP #SQL server $IPT -A FORWARD -d 1 .2.3.2 -s ! 1 .2.3.10 -p tcp --dport 5432 -j DROP $IPT -A FORWARD -d 1 .2.3.2 -p ICMP -j ACCEPT $IPT -A FORWARD -d 1 .2.3.2 -j DROP #MAIL server $IPT -A FORWARD -d 1 .2.3.3 -p tcp --dport 25 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.3 -p tcp --dport 110 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.3 -p udp --sport 53 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.3 -p ICMP -j ACCEPT $IPT -A FORWARD -d 1 .2.3.3 -j DROP #WEB server $IPT -A FORWARD -d 1 .2.3.4 -p tcp --dport 50000 :52000 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.4 -p tcp --dport 80 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.4 -p tcp --dport 21 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.4 -p ICMP -j ACCEPT $IPT -A FORWARD -d 1 .2.3.4 -j DROP #Access Server $IPT -A FORWARD -d 1 .2.3.131 -s 1 .2.3.1 -p udp --sport 1812 :1814 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.131 -s 1 .2.3.1 -p udp --sport 53 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.131 -s 1 .2.3.16/28 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.131 -p ICMP -j ACCEPT $IPT -A FORWARD -d 1 .2.3.131 -j DROP #delete MANAGEMENT chain if exists $IPT -X MANAGEMENT #create MANAGEMENT chain $IPT -N MANAGEMENT #add authorized IPs to the MANAGEMENT chain, drop all the others $IPT -A MANAGEMENT -s 1 .1.2.0/26 -j ACCEPT $IPT -A MANAGEMENT -s 1 .1.3.192 -j ACCEPT $IPT -A MANAGEMENT -s 1 .1.9.21 -j ACCEPT $IPT -A MANAGEMENT -s 1 .1.19.61 -j ACCEPT $IPT -A MANAGEMENT -s 0 /0 -j DROP #Jump incoming packets for port 61146 TCP to the MANAGEMENT chain $IPT -A INPUT -p tcp --dport 61146 -j MANAGEMENT #Jump packets destined to 1.1.2.2 port 61146 TCP to the MANAGEMENT #chain $IPT -A FORWARD -d 1 .1.2.2 -p tcp --dport 61146 -j MANAGEMENT #Allow users to connect to openvpn $IPT -A INPUT -p tcp --dport 6669 -j ACCEPT #Allow internal departments SAMBA connections $IPT -A INPUT -s 192 .168.1.0/24 -p tcp --dport 137 :139 -j ACCEPT #Allow admins SAMBA connections $IPT -A INPUT -s 1 .2.3.16/28 -p tcp --dport 137 :139 -j ACCEPT #deny access to the intranet web server $IPT -A INPUT -i eth0 -p tcp --dport 80 -j DROP #filter the PostgreSQL port $IPT -A INPUT -p tcp --dport 5432 -j DROP #drop incoming TCP SYN packets $IPT -A INPUT -i eth0 -p tcp --syn -j DROP #allow http, pop3, smtp for the web and mail server $IPT -A FORWARD -d 1 .1.2.2 -p tcp -m multiport --dport 80 ,25,110 -j ACCEPT #drop all other tcp traffic for the web and mail server $IPT -A FORWARD -d 1 .1.2.2 -p tcp --syn -j DROP #deny SSH access except admins $IPT -A INPUT -p tcp --dport 22 -s ! 192 .168.1.0/29 -j DROP #NAT all to the internet. Don't nat to network at HQ $IPT -t nat -A POSTROUTING -s $PREFIX .0/24 -d ! 192 .168.1.0/24 -j MASQUERADE It is very important for us to secure the database server, and this is not even very difficult. $IPT -A FORWARD -d 1.2.3.2 -s ! 1.2.3.10 -p tcp --dport 5432 -j DROP This will drop packets from other sources than 1.2.3.10 (the intranet server) to the database server on port 5432/TCP. However, on the database server we have a chain named SQL in which we allow connections from 1.2.3.10 and 1.2.3.1 (the AAA server). The reason why we didn't add a rule in the core router for packets with the source 1.2.3.1 to pass to 1.2.3.2 is that those servers are in the same subnet (directly connected) and packets from one to another don't pass through the core router. Firewall script for the database server #!/bin/bash IPT = \"/sbin/iptables\" #flush rules $IPT -F #INPUT chain policy DROP $IPT -P INPUT DROP #Accept packets on loopback $IPT -A INPUT -i lo -j ACCEPT #Accept SSH from Admins $IPT -A INPUT -s 1 .2.3.16/28 -p tcp --dport 22 -j ACCEPT #Create SQL chain and jump packets for 5432/tcp to it $IPT -N SQL $IPT -A INPUT -p udp --dport 5432 -j SQL #Allow SQL connections from AAA and intranet servers $IPT -A SQL -s 1 .2.3.1 -j ACCEPT $IPT -A SQL -s 1 .2.3.10 -j ACCEPT $IPT -A SQL -j DROP #Allow outgoing TCP connections $IPT -A INPUT -s 1 .2.3.1 -p udp --sport 53 -j ACCEPT $IPT -A INPUT -p tcp ! --syn -j ACCEPT - Firewall script for the Web server #!/bin/bash IPT = \"/sbin/iptables\" #flush rules $IPT -F #INPUT chain policy DROP $IPT -P INPUT DROP #Accept packets on loopback $IPT -A INPUT -i lo -j ACCEPT #Accept SSH from Admins $IPT -A INPUT -s 1 .2.3.16/28 -p tcp --dport 22 -j ACCEPT #Accept FTP connections $IPT -A INPUT -p tcp --dport 21 -j ACCEPT #Active connections initiate the requests #So we only have to accept passive connections $IPT -A INPUT -p tcp --dport 50000 :52000 -j ACCEPT #Accept web connections $IPT -A INPUT -p tcp --dport 80 -j ACCEPT #Allow outgoing TCP connections $IPT -A INPUT -s 1 .2.3.1 -p udp --sport 53 -j ACCEPT $IPT -A INPUT -p tcp ! --syn -j ACCEPT Hardening DWRT Rules # click on security tab and click all boxes that say limit....+ARP spoofing # turn on SPI firewall # on services disable telnet and ssh if you don't use them # disable traffic daemon # add those lines to commands and save in firewall # To block some of most TCP-based DDoS attcks. # This rule blocks all packets that are not a SYN packet and don't belong to an established TCP connection. iptables -t mangle -I PREROUTING -m conntrack --ctstate INVALID -j DROP # This blocks all packets that aren't new (don't belong to an established connection) and don't use SYN flag. This rule is similar to the above rule but I found that it catches some packets that other ones doesn't. iptables -t mangle -I PREROUTING -p tcp ! --syn -m conntrack --ctstate NEW -j DROP # The next rule blocks new packets (only SYN packets can be new packets) that use a TCP MSS value that is not common. iptables -t mangle -I PREROUTING -p tcp -m conntrack --ctstate NEW -m tcpmss ! --mss 536 :65535 -j DROP # The below ruleset blocks packets that use bogus TCP flags, ie. TCP flags that legitimate packets wouldn\u2019t use. iptables -t mangle -A PREROUTING -p tcp --tcp-flags FIN,SYN FIN,SYN -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags SYN,RST SYN,RST -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags FIN,RST FIN,RST -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags FIN,ACK FIN -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags ACK,URG URG -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags ACK,PSH PSH -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags ALL NONE -j DROP # These rules block spoofed packets originating from private (local) subnets. On your public network interface you usually don\u2019t want to receive packets from private source IPs. iptables -t mangle -A PREROUTING -s 224 .0.0.0/3 -j DROP iptables -t mangle -A PREROUTING -s 169 .254.0.0/16 -j DROP iptables -t mangle -A PREROUTING -s 172 .16.0.0/12 -j DROP iptables -t mangle -A PREROUTING -s 192 .0.2.0/24 -j DROP iptables -t mangle -A PREROUTING -s 192 .168.0.0/16 -j DROP iptables -t mangle -A PREROUTING -s 10 .0.0.0/8 -j DROP iptables -t mangle -A PREROUTING -s 0 .0.0.0/8 -j DROP iptables -t mangle -A PREROUTING -s 240 .0.0.0/5 -j DROP iptables -t mangle -A PREROUTING -s 127 .0.0.0/8 ! -i lo -j DROP # This drops all ICMP packets. ICMP is only used to ping a host to find out if it\u2019s still alive. Because it\u2019s usually not needed and only represents another vulnerability that attackers can exploit, we block all ICMP packets to mitigate Ping of Death (ping flood), ICMP flood and ICMP fragmentation flood. iptables -t mangle -A PREROUTING -p icmp -j DROP # This rule blocks fragmented packets. Normally you don\u2019t need those and blocking fragments will mitigate UDP fragmentation flood. But most of the time UDP fragmentation floods use a high amount of bandwidth that is likely to exhaust the capacity of your network card, which makes this rule optional and probably not the most useful one. iptables -t mangle -A PREROUTING -f -j DROP iptables -I INPUT -f -j DROP iptables -I INPUT -p tcp --tcp-flags ALL ALL -j DROP iptables -I INPUT -p tcp --tcp-flags ALL NONE -j DROP # This iptables rule helps against connection attacks. It rejects connections from hosts that have more than 80 established connections. If you face any issues you should raise the limit as this could cause troubles with legitimate clients that establish a large number of TCP connections. iptables -A INPUT -p tcp -m connlimit --connlimit-above 80 -j REJECT --reject-with tcp-reset # Limits the new TCP connections that a client can establish per second. This can be useful against connection attacks, but not so much against SYN floods because the usually use an endless amount of different spoofed source IPs. iptables -A INPUT -p tcp -m conntrack --ctstate NEW -m limit --limit 60 /s --limit-burst 20 -j ACCEPT iptables -A INPUT -p tcp -m conntrack --ctstate NEW -j DROP ### SSH brute-force protection ### /sbin/iptables -A INPUT -p tcp --dport ssh -m conntrack --ctstate NEW -m recent --set /sbin/iptables -A INPUT -p tcp --dport ssh -m conntrack --ctstate NEW -m recent --update --seconds 60 --hitcount 10 -j DROP ### Protection against port scanning ### /sbin/iptables -N port-scanning /sbin/iptables -A port-scanning -p tcp --tcp-flags SYN,ACK,FIN,RST RST -m limit --limit 1 /s --limit-burst 2 -j RETURN /sbin/iptables -A port-scanning -j DROP iptables -I INPUT -s ` nvram get lan_ipaddr ` / ` nvram get lan_netmask ` -d ` nvram get wan_ipaddr ` -j DROP iptables -I FORWARD -f -j DROP iptables -I FORWARD -p tcp --dport 25 -j DROP iptables -I FORWARD -p udp --dport 25 -j DROP iptables -I FORWARD -p tcp -o ` get_wanface ` --dport 25 -j REJECT iptables -I FORWARD -p tcp --dport 137 -j DROP iptables -I FORWARD -p udp --dport 137 -j DROP iptables -I FORWARD -p udp --dport 138 -j DROP iptables -I FORWARD -p tcp --dport 139 -j DROP iptables -I FORWARD -p udp --dport 139 -j DROP iptables -I FORWARD -p tcp --dport 445 -j DROP iptables -I FORWARD -p udp --dport 445 -j DROP iptables -I FORWARD -p tcp --dport 31337 -j DROP iptables -I FORWARD -p udp --dport 31337 -j DROP iptables -I FORWARD -p tcp --tcp-flags ALL ALL -j DROP iptables -I FORWARD -p tcp --tcp-flags ALL NONE -j DROP iptables -A FORWARD -m recent --name portscan --rcheck --seconds 8640 -j DROP SSH Testing Firewall Firewall Script for Home Server \u00b6 # # Example fast and scalable firewall configuration with iptables # Please only implement if you fully understand the functionality # because is very easy to lockout yourself from your computer if # the script isn't adapted to your specific situation. # *filter :INPUT ACCEPT [ 0 :0 ] :FORWARD ACCEPT [ 0 :0 ] :OUTPUT ACCEPT [ 0 :0 ] :Always - [ 0 :0 ] :Allow - [ 0 :0 ] :Bogus - [ 0 :0 ] :Enemies - [ 0 :0 ] :Friends - [ 0 :0 ] -A INPUT -j Bogus -A INPUT -j Always -A INPUT -j Enemies -A INPUT -j Allow -A FORWARD -j Bogus -A FORWARD -j Always -A FORWARD -j Enemies -A FORWARD -j Allow -A Bogus -p tcp -m tcp --tcp-flags SYN,FIN SYN,FIN -j DROP -A Bogus -p tcp -m tcp --tcp-flags SYN,RST SYN,RST -j DROP -A Bogus -s 169 .254.0.0/16 -j DROP -A Bogus -s 172 .16.0.0/12 -j DROP -A Bogus -s 192 .0.2.0/24 -j DROP -A Bogus -s 192 .168.0.0/16 -j DROP -A Bogus -s 10 .0.0.0/8 -j DROP -A Bogus -s 127 .0.0.0/8 -i ! lo -j DROP -A Always -p udp --dport 123 -j ACCEPT -A Always -m state --state ESTABLISHED,RELATED -j ACCEPT -A Always -i lo -j ACCEPT -A Enemies -m recent --name psc --update --seconds 60 -j DROP -A Enemies -i ! lo -m tcp -p tcp --dport 1433 -m recent --name psc --set -j DROP -A Enemies -i ! lo -m tcp -p tcp --dport 3306 -m recent --name psc --set -j DROP -A Enemies -i ! lo -m tcp -p tcp --dport 8086 -m recent --name psc --set -j DROP -A Enemies -i ! lo -m tcp -p tcp --dport 10000 -m recent --name psc --set -j DROP -A Enemies -s 99 .99.99.99 -j DROP -A Friends -s 123 .123.123.123 -j ACCEPT -A Friends -s 111 .111.111.0/24 -j ACCEPT -A Friends -j DROP -A Allow -p icmp --icmp-type echo-request -j Friends -A Allow -p icmp --icmp-type any -m limit --limit 1 /second -j ACCEPT -A Allow -p icmp --icmp-type any -j DROP -A Allow -p tcp -m state --state NEW -m tcp --dport 22 -j Friends -A Allow -p tcp -m state --state NEW -m tcp --dport 25 -j ACCEPT -A Allow -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT -A Allow -p tcp -m state --state NEW -m tcp --dport 443 -j ACCEPT -A Allow -j DROP COMMIT - The first block gives us the names of the chains that we will use, and the initial values of the packet and bytes counters. With these settings we reset the counters each time the firewall configuration is loaded. The default action for the three internal chains INPUT, FORWARD and OUTPUT is set to ACCEPT. This means that the packet is accepted by each of these chains, except when a rule in the chain decides otherwise. The five user defined chains do not have a default action. The firewall logic will therefore continue processing other rules when the rules inside those chains don\u2019t give a definite answer what to do with a packet. - The next two blocks add four filter rules to the INPUT and FORWARD chain. Each rule causes an unconditional jump to a user defined chain. The sequence in which these commands are listed in the iptables file is important, because this will also be the sequence in which the rules will be processed in the kernel. - The Bogus group contains a handful of rules which define invalid packets. This includes packets with both the SYN and FIN flags or the SYN and RST flags, packets from specific ranges of invalid IP addresses and packets with IP addresses which belong to the virtual loop-back adapter (127.x.x.x) , but which are not from a local source. These are all packets crafted by hackers or attackers to confuse or intrude your system. - The second group Always contains high-priority packets which should always be accepted without much delay. These packets might originate from an attacker, and even be part of a DDOS attack, but we accept that situation. The processing of NTP packets has such a low overhead that even when packets are coming in at a very high speed, it won\u2019t take too much CPU resources. There are also no states preserved as with the TCP protocol which could cause buffer overflows. The only thing which might happen is saturation of the network, but that would happen with a DDOS attack independent of us accepting or dropping the incoming packets. - The next rule is equally important. It tells us to accept all packets of a connection which has previously been accepted as OK. Many firewall rule sets miss this particular rule, but from a performance point of view it is a very important one. Most TCP/IP protocols exchange a lot of packets over one single connection. If we effectively only test the first packet of that stream and then mark the stream as either accepted or dropped, there is just little overhead involved when processing accepted network connections. - The third rule accepts all traffic to the loop-back adapter. What many firewall rule sets seem to forget is that also traffic to and from the loop-back adapter is screened by the firewall. This can cause severe performance problems if the loop-back adapter is for example used for connections between an Apache web server and an SQL database. - In the next block we define our friends. As you probably have seen, the Friends chain is not called directly from the INPUT or OUTPUT chain. It is called by other rules which we will define later to accept access to specific ports from some IP addresses but not from others. The Friends rule set is called to check the source IP address against a number of possible matches. The first rule checks if the IP address equals 123.123.123.123. If this is the case, the packet is accepted without further processing of firewall rules. It then checks against a block of IP addresses ranging from 111.111.111.0 to 111.111.111.255. If the IP address are in this range, the packet is accepted. The third rule drops all packets which do not match any of the previous IP address matches. It is obvious that the IP addresses mentioned here are just examples. You should replace them by addresses where friendly traffic to your server is normally coming from, for example the IP address of your home ADSL connection or IP addresses from other servers in your network. You are allowed to add extra rules before the -A Friends -j DROP line. - Enemies can be identified in two ways. Either by their source IP address, or by their behavior. Unfortunately the firewall in the kernel operates at a low level in the networking hierarchy and many behavioral actions are only visible at the application level, for example password dictionary attacks on SSH servers or spam mails. But some behavior can be detected at the lowest networking level and one of this is port scanning. Port scanning is trying to connect to a number of TCP/IP ports of the attacked computer to search for open connections with SQL servers, control panels or other network aware applications. My experience has shown that many port scanners are looking for unprotected connections to MySQL and MSSQL servers. Because my servers only accept SQL requests from local applications, every request from the outside world is by definition an enemy. In the IP firewall I have setup rules which are triggered if requests are coming in from the network for port 1433, port 3306 and some other well known ports. Every IP address from which such a request originates is put in quarantine for a period of 60 seconds. This is done by the \u2013name psc \u2013set and checking with every subsequent packet of the last set action for that IP address was less than 60 seconds ago. The Linux kernel will maintain a list of port-scan IPs which can be accessed at the location /proc/net/ipt_recent/psc. I have used a 60 seconds grace time because in some occasions I might be testing something and accidentally look like a port scanner according to the firewall. In that case I will only lockout myself for a period of one minute. But it is also possible to increase this value to for example 15 minutes or one hour. Other enemies are enemies which are known by IP address or IP address range. I have added the address 99.99.99.99 as an example, but this could also be ranges like 111.111.111.0/24. Adding and removing enemies from the firewall \u00b6 Friends come and go, but enemies accumulate . Many enemies are temporary by nature. These are computers infected by malware, or servers with configuration flaws like open mail or web proxies. Once these problems are fixed, an enemy can turn in a friend again. #!/bin/sh iptables -L -n -v | grep -q $1 RETVAL = $? if [ $RETVAL -eq 0 ] ; then echo \"IP address already in blocklist: $1 \" exit 0 fi echo \"Adding enemy $1 \" iptables -A Enemies -s $1 -j DROP - This scripts first makes a dump of the current firewall rules and checks if the IP address or IP range matches an existing rule. If this is the case a warning is shown and the script does nothing. If no matching rule has been found, the IP address or IP range is added to the list of enemies. By checking the whole rules list against the new IP address, we have some basic protection against adding one of our own IPs to the firewall because this script will also fail if it finds the IP address or range in the Friends table. But there is no check of individual IPs against IP ranges so you still have to be careful about what you do. #!/bin/sh echo \"Deleting enemy $1 \" iptables -D Enemies -s $1 -j DROP - This scripts deletes a rule from the Enemies table which matches the IP address or IP range. Install DD-WRT \u00b6 First method: Copy the factory-to-ddwrt file in the firware upgrade section of http://192.168.1.1/ router IP. Connect the Laptop to LAN Port 1 of the router. Wait for the proces to complete. Second method: In case the process is interrupted or to flash back to TP-LINK firware back. Override custom TP-LINK software from your router, use TFTP service on a Network LAN server. You need to set up a TFTP Server on your computer with IP 192.168.0.66 . You can do this by linking the MAC address of the computer with the 192.168.0.66 IP and restart the WI-FI connection. Verify if the computer has the correct IP. Once verified, copy the DD-WRT software to /tftpboot directory on the computer. Rename the file from tl-wr841nd-webflash.bin to wr841nv10_tp_recovery.bin . Start checking the tftp server logs tail -f /var/log/syslog to check the request Shutdown the router and connect the WAN port to main router. Hold down the Reset button on the back of the router and switch it on again, release the reset button when there pop-up a flashing window. Wait for the router to connect to the TFTP service and download the recovery.bin file. This will kickstart the router installation process. Wait for 10 mins for the process to complete. Now using the LAN port, connect the router to the computer and access the IP assigned by the main router to start DD-WRT configuration. Network Configuration \u00b6 One of your routers will be connected to your ISP. This router needs to be assigned the internal LAN IP 192.168.0.1, and hand out DHCP addresses on something like 192.168.0.100 through 192.168.0.200. Call this router Router A. Connect a cable from one of the LAN ports on router A to the WAN port on router B. Set Router B's WAN IP address to 192.168.0.201, and its internal LAN IP to 192.168.1.1. Tell it to hand out DHCP addresses on something like 192.168.1.100 through 192.168.1.200. Set the DNS settings on this router's DHCP to what you want for your kids. Use Static WAN IP on Router B to set the IP 192.168.0.201. Router A should work normally. If you have things you need accessible via the Internet, make sure it's connected to Router A and setup your port forwarding on Router A like you would on any normal router. Router B will \"get internet\" through Router A. Router B will be double NATed. This means it's very difficult for machines from the Internet to connect to anything behind router B. To get Internet passed through to Router B servers in the second network, Static Route on Router A. # Network Destination: 192.168.1.0 # Subnet Mask: 255.255.255.0 # Default Gateway: 192.168.0.201 # Interface: LAN/WLAN # Description: Internal Network Setup Router B configuration . Backup Initial settings, incase we mess up the configurations later on. [Downloads/tp-wr841n/dd-wrt /DD-WRT_TP-Link_TL-WR841ND_v10-Config-Backup.bin] ssh root@192.168.1.1 -i ~/.ssh/ansible-user https://wiki.dd-wrt.com/wiki/index.php/Separate_LAN_and_WLAN WLAN Separate From LAN, With Independent Dhcp, Etc https://wiki.dd-wrt.com/wiki/index.php/Linking_Subnets_with_Static_Routes Install DD-WRT in Virtualbox \u00b6 Get the public DD-WRT image Convert DD-WRT x86 Image to VDI cd /home/leslie/Downloads/tp-wr841n vboxmanage convertdd dd-wrt_public_vga.image dd-wrt.vdi # resize the vdi to 256MB vboxmanage modifyhd --resize 256 dd-wrt.vdi Launch Virtualbox and Create a New VM by selecting Machine > New # Name: dd-wrt # Machine Folder: /home/leslie/machines # Type: Linux # Version: Other Linux (32-bit) # Memory Size: 256 MB # Hard disk: Do not add a virtual hard disk # Click Create # Copy the dd-wrt.vdi file from downloads into /home/leslie/machines/dd-wrt # Select the VM and Click Settings # Select Storage # Click Add Storage Attachment > Add Hard Disk > Choose existing disk # Click Add and browse to /home/leslie/machines/dd-wrt/dd-wrt.vdi # Click OK # Select Network # Set Attached to: Bridge Adapter (Keeping it NAT for testing) # Make sure the DD-WRT VM is selected and click Start > Normal # Wait for the text to stop scrolling and press Enter # Login with root/admin # Set the IP address to something in your current subnet so you can reach it # ifconfig br0 192.168.56.1 (Not required) Launch your web browser of choice and access the DD-WRT web UI with the IP address in the previous step Setup Virtual Network Lab \u00b6 Configure Virtual Box # Open Virtual Box and select Tool --> Menu --> Network --> Properties # Create new Network vboxnet1 # Change the Default Address from 192.168.56.1 to 192.168.57.2 # x.x.x.2 is so that we can have an Edge Router which will have DHCP enabled and will give out IP. # So if the VMs have 99.x number range, we know the Virtual Edge router has given the address and not Virtual Box DHCP. # Disable DHCP in the next tab for the above reason. # Save the interface twice, so the changes are stored and not re-enabled. Install Open-WRT in Virtualbox \u00b6 Setup Router in Virtualbox # Download the Stable x86 - 64 bit - combined ext4 (as that has the fewest limitations) https://downloads.openwrt.org/releases/22.03.2/targets/x86/64/ # Extract the file in a directory openwrt # Execute the below command VBoxManage convertfromraw --format VDI openwrt-22.03.2-x86-64-generic-ext4-combined.img openwrt.vdi # resize the vdi to 512MB VBoxManage modifymedium openwrt.vdi --resize 512 # Copy the directory to /home/leslie/machines - Launch Virtualbox and Create a New VM by selecting Machine > New # Name: OpenWRT Edge Router # Machine Folder: /home/leslie/machines -> Default # Type: Linux # Version: Linux 2.6/3.x/4.x (64 Bit) --> The latest Linux 64bit # Memory Size: 512 MB # Hard disk: Do not add a virtual hard disk # Click Create # Copy the openwrt.vdi file from downloads into /home/leslie/machines/openwrt # Add the HDD to the VirtualBox Media --> Tools -- Hard Disks --> Add -- Browse to /home/leslie/machines/openwrt and select openwrt.vdi # Select the VM and Click Settings # Select Storage # Click Add Storage Attachment > Add Hard Disk > Choose existing disk # Note: Add this under Controller: SATA # Click Add openwrt.vdi # Click OK - Configure Edge Router # On the VirtualBox interface, open Settings --> Network # Adapter1 --> Host-Only Adapter and vboxnet1 # This will allow it to act as LAN and allow access to the internal network. # So my Laptop IP will be assigned on this interface, allowing access to my original WAN # Adapter2 --> Bridge Adapter and select the Wireless Network 'wslp01'. # This will connect to the Internet. - Start the Openwrt VM. - Adjust the window size \u2192 View \u2192 Adjust Window Size - Configure Openwrt IP uci show network # This will show lan.ipaddr=192.168.1.1 # Change the Network LAN IP Address to match vboxnet1, but to 57.1 so it can act as Edge Router uci set network.lan.ipaddr = 192 .168.57.2 uci show network # Confirm the change uci commit # Commit the change reboot # Reboot - Testing the network connectivity ping 192 .168.57.2 # Ping a IP of the Host ping www.google.com # Ping a internet address # If both works, we should be able to open the Router's page on the browser and access luci (Router web UI) # Open http://192.168.57.1 # Login and verify the Network --> Interfaces. WAN is the Main Address and LAN is x.x.57.1 and both are up. # Stop the openwrt server halt - Shutdown the machine and take a snaphot. - OpenWRT \u2192 Machine \u2192 Tools \u2192 Snapshots \u2192 Take - LAN and WAN working after Initial Setup Configure Internal Router # Click Machine and Clone the Edge Router and Rename to OpenWRT Internal Router # Path: /home/leslie/machines/openwrt # MAC Address Policy: Generate new MAC Address for all Network Adapters # Clone Type: Full clone # Snapshots: Everything # Click Clone # Remove the WAN Adpater as its not required to access Internet # Settings --> Network --> Disable Adpater 2 # Start the machine # Configure server to move from Fixed IP to DHCP (which it will get from Edge Router) uci set network.lan.proto = dhcp uci show network # Confirm the change uci commit # Commit the change reboot # Reboot Start the Edge Router to confirm if Internal Router gets the IP address correctly In the Browser status page, we can see internal router has an address assigned. Rename the Openwrt server to give a definte name Settings - Change the Hostname - Openwrt-Edge Similarly of the Internal router - Openwrt-Internal Reboot both machines to confirm the hostname changes. Take a snapshot of the Internal Router and delete the orignal Snapshot 1 as its not required any more. Adding more machines to the LAN, ensure only Adapter1 is added as HostOnly. This would mean, servers are on the LAN and go through the Internal router and then through the Edge router to access the Internet. Setup the Servers \u00b6 Add the route to reach the Gateway for the Ethernet interface ifconfig # Identify the Ethernet interface route -n # Check the default route, n shows the IP address resolution traceroute google.com # Add the route for the eth0 interface netstat -r Testing \u00b6 Combining tcpdump with Wireshark is a powerful combination, particularly when you wish to dig into full application layer sessions as the decoders can assemble the full stream. # On Server 1 - Setup netcat listner (which acts as Server) nc -nlvp 22 # On Server 2 - Which needs to be tested (which acts as Client) nc 192 .168.57.11 22 # This will initiate a TCP connection and start a chat session # To kill the netcat session - Ctrl + d # OR # Server 1 sudo tcpdump -i enp0s8 # All traffic can be seen sudo tcpdump -i enp0s8 -v port 22 # Filter only SSH traffic # Server 2 sudo nmap -p 22 192 .168.57.11 # OR Check all ports sudo nmap -v -Pn 192 .168.57.11 # Check which service has opened which ports netstat -anp # Do a network scan to see if any other machine can be accessed sudo nmap -Pn -sV 192 .168.0.0/16 # When such a scan is run, no alert or monitoring is generated logging the time of this activity # Capture UDP traffic sudo tcpdump -i eth0 udp # DHCP Example # Monitoring DHCP request and reply. DHCP requests are seen on port 67 and the reply is on 68. Using the verbose parameter -v we get to see the protocol options and other details. sudo tcpdump -v -n port 67 or 68 # host filter will capture traffic going to (destination) and from (source) the IP address. # In this example, an network traffic from 10.10.1.1 can be captured on the localhost. # Run the below command in terminal 1. Start a new terminal and ping 10.10.1.1, you will see the output in terminal 1. sudo tcpdump -i eth0 host 10 .10.1.1 # capture only packets going one way using src or dst. # This will capture traffic only in one direction sudo tcpdump -i eth0 dst 10 .10.1.20 # Another example is to use domain name sudo tcpdump -i eth0 dst medium.com -n # -n will not convert the IP address to domain name # In another terminal curl medium.com or open a brower and type medium.com to test traffic # To capture all network traffic in the subnet use net option sudo tcpdump -i eth0 net 192 .168.0.0/24 # Capture HTTPS traffic using port option sudo tcpdump -i eth0 port 443 -vv -n # using the `or` operator sudo tcpdump -i eth0 port 80 or port 443 -vv -n # Capture all ICMP packets sudo tcpdump -n icmp # Capture ICP traffic to and from the DNS # To test this, in another terminal, ping 8.8.8.8 which is ICMP traffic sudo tcpdump -i eth0 -n icmp and host 8 .8.8.8 # Filter on the icmp type to select on icmp packets that are not standard ping packets. sudo tcpdump 'icmp[icmptype] != icmp-echo and icmp[icmptype] != icmp-echoreply' # See the NTP query and response. sudo tcpdump dst port 123 # Detect Port Scan in Network Traffic tcpdump -nn # Capture DNS Request and Response # Outbound DNS request to Google public DNS and the A record (ip address) response can be seen in this capture. sudo tcpdump -i wlp58s0 -s0 port 53 -vv -n # Capture DNS request going through port 53 sudo tcpdump -i eth0 dst port 53 -vv -n # Capture with tcpdump and view in Wireshark # This tip is a favorite, pipe the raw tcpdump output right into wireshark on your local machine. Don't forget the not port 22 so you are not capturing your SSH traffic. ssh root@remotesystem 'tcpdump -s0 -c 1000 -nn -w - not port 22' | wireshark -k -i - # Another tip is to use count -c on the remote tcpdump to allow the capture to finish otherwise hitting ctrl-c will not only kill tcpdump but also Wireshark and your capture. # Top Hosts by Packets sudo tcpdump -nnn -t -c 200 | cut -f 1 ,2,3,4 -d '.' | sort | uniq -c | sort -nr | head -n 20 Firewall \u00b6 # My goal was to migrate my existing linux box with an ethernet interface and a wifi card which was acting as a crude AP, but keep the same functionality: firewalling, separate subnets for wired and wireless, separate dhcp, etc. # I wanted my wifi on a separate subnet from my LAN, with its own DHCP scope. In these examples, lan is 192.168.7.0/24, and wifi is 192.168.8.0/24 ## wan: vlan1 ## lan: br0 - 192.168.7.1 ## wifi: eth1 - 192.168.8.1 ## permit incoming connections from WLAN iptables -I INPUT 2 -i eth1 -m state --state NEW -j logaccept ## fixup forwarding table ## the lan2wan target didn't work for me, replace it with straight accept iptables -R FORWARD 5 -i br0 -o vlan1 -j ACCEPT ## permit WLAN -> WAN iptables -I FORWARD 7 -i eth1 -o vlan1 -j ACCEPT ## disallow WLAN -> LAN iptables -I FORWARD 7 -i eth1 -o br0 -m state --state NEW -j DROP ## disallow LAN -> WLAN iptables -I FORWARD -i br0 -o eth1 -m state --state NEW -j DROP ## disallow WLAN -> WAN subnet iptables -I FORWARD -i eth1 -d ` nvram get wan_ipaddr ` / ` nvram get wan_netmask ` -m state --state NEW -j DROP ## disallow WLAN -> direct router access iptables -I INPUT -i eth1 -m state --state NEW -j DROP ## Allow WLAN -> DHCP on the router iptables -I INPUT -i eth1 -p udp --dport 67 -j ACCEPT ## Allow WLAN -> DNS on the router iptables -I INPUT -i eth1 -p udp --dport 53 -j ACCEPT iptables -I INPUT -i eth1 -p tcp --dport 53 -j ACCEPT # This is for allowing netwrok access between 2 networks # Allow the entire 192.168.0.0/16 block to be forwarded through the router iptables -I FORWARD -s 192 .168.0.0/16 -j ACCEPT # OR # Allow Router2 to forward traffic from Router1 and Router3's subnets iptables -I FORWARD -s 192 .168.1.0/24 -j ACCEPT iptables -I FORWARD -s 192 .168.3.0/24 -j ACCEPT # Allow Router3 to forward traffic from Router1 and Router2's subnets iptables -I FORWARD -s 192 .168.1.0/24 -j ACCEPT iptables -I FORWARD -s 192 .168.2.0/24 -j ACCEPT","title":"Requirement - The Core Router\u2014First Line of Defense"},{"location":"learning/linux/router/#requirement---the-core-routerfirst-line-of-defense","text":"Linux as SOHO Router. SOHO stands for Small Offices and Home Offices, and usually refers to situations where there exists just one computer at home to a few computers in a small office. If you have an old computer that you are about to throw away, you can easily install Linux on it and make it your own SOHO router having the advantage of higher flexibility at zero cost. A SOHO router has a WAN port that is an Ethernet port where the provider connection must be plugged in. The Provider's CPE (Customer Premises Equipment) can be of any type (xDSL modem, wireless bridge, cable modem, fiber optic media converter) that can provide an Ethernet connection. SOHO routers usually have four to eight Ethernet ports for the LAN. This is basically a small four-to-eight-port switch that's built in the SOHO router. Some SOHO routers also have a wireless access point chipset that is bridged to the built-in switch. Our computer that will run Linux and act as SOHO router must have two Ethernet cards, one for the WAN function of a SOHO router into which the provider's CPE is plugged, and one for the LAN. If you want the LAN to be wired and wireless, the Ethernet interface for the local network will be plugged into an access point with a built-in switch. However it is, everything is basically a LAN (wired, wireless, bridged, or switched); so, from the firewall point of view, it doesn't really matter what we use at the first and second layers of the OSI model (access points, hubs, switches). The provider usually assigns us a public IP address that can be either statically assigned or dynamically assigned using DHCP or PPPoE. From a security point of view, the core router is the first line of defense. The first line of defense should stop all attacks coming from outside. However, if the firewall at the core router fails, then the firewall on each server is responsible for the security of that server. If a user gains unauthorized access due to a bug in one of the services running on one server, the core router firewall or the firewall on that server are useless. In this case, the attacker has access to a server directly connected to the other servers in the server farm, and so the first line of defense has failed. The INPUT chain for the core router is the simplest of all the firewalls so far. We need to set the policy to DROP, allow SSH, ICMP, and DNS, and disallow SYN packets\u2014that's all! In the FORWARD chain, we will do most of the operations we already did on the servers.","title":"Requirement - The Core Router\u2014First Line of Defense"},{"location":"learning/linux/router/#defining-the-security-policy","text":"Before building up any firewalls, we have to decide what the firewall must do by creating a security policy that can be from a document (recommended) to a piece of paper, or some thoughts in our heads. The desktop can access anything. Also, we want to be able to log in on the desktop computer from outside using VNC (Virtual Network Computing). The Linux router must run SSH so that we can log in to it from the internal network and from the office. One file server for Storage requirement. One DNS server and a Transparent proxy server. One Secrets server for secret management and encryption as a service. Application servers to host applications. We need to be able to access both Linux servers in the network using SSH from the desktop computer as well as from some public IP addresses we have at home. All servers must be accessed only from computers from home. Deny access from people outside the home network to see their file shares. Use a transparent proxy for them to deny access to .pif and .scr files. Browse the Web, but not to download .pif, .scr, .exe, .zip, and .rar files. Access HTTPS port 443 TCP. We will need access to SSH on both Linux servers we have from a list of allowed hosts from our network and from outside (e.g. network administrators' home IP addresses). First, we decide on a port (22 is default for SSH), let's say 61146 TCP for running SSH on both our hosts. Next, we need to build the list of IP addresses that are authorized to access the SSH ports. Let's say those are 1.1.2.0/26, 1.1.3.192, 1.1.9.21, 1.1.19.61. The simplest way to build the rules for SSH is to create a chain called MANAGEMENT. On our Linux router, we need to apply the rules in the MANAGEMENT chain for incoming packets with the destination port 61146 TCP. There's one thing that we did here that might compromise security (although it's very unlikely)\u2014a little hole in the firewall. I'm talking about -p udp --sport 53 -j ACCEPT . Starting at one point, an unpatched service on UDP might give an attacker the possibility to pass our firewalls by using port 53 as source port. #!/bin/bash #define the prefix for the network (where we are) # Every time we add a site we modify the prefix with the network that we use at that location. PREFIX = 192 .168.3 IPT = \"/sbin/iptables\" ############# Begin the NAT table opperations ###### #Flush all the rules in the nat table $IPT -t nat -F #load some modules for NAT to work better /sbin/modprobe ip_nat_ftp /sbin/modprobe ip_nat_irc #SNAT sales and accounting to port 53 UDP (DNS) $IPT -t nat -A POSTROUTING -o eth0 -s 192 .168.1.0/24 -p udp --dport 53 -j SNAT --to 1 .1.2.96-1.1.2.254 #Transparent Proxy for sales and accounting $IPT -t nat -A PREROUTING -s 192 .168.1.0/24 -p tcp --dport 80 -j REDIRECT --to-port 3128 #SNAT Sales and accounting for HTTPS $IPT -t nat -A POSTROUTING -o eth0 -s 192 .168.1.0/24 -p tcp --dport 443 -j SNAT --to 1 .1.2.96-1.1.2.254 #Drop everything else from sales and accouting to the internet $IPT -t nat -A POSTROUTING -o eth0 -s 192 .168.1.0/24 -j DROP #Transparent Proxy for management $IPT -t nat -A PREROUTING -s 1 .1.2.64/27 -p tcp --dport 80 -j REDIRECT --to-port 3128 #MASQ internal departments $IPT -t nat -A POSTROUTING -s 192 .168.1.0/24 -o eth0 -j MASQUERADE ############# End the NAT table operations ###### ########## INPUT Chain begin ######## #Flush netfilter table $IPT -F #policy Drop $IPT -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT DROP # Dont accept IPv6 traffic ip6tables -P INPUT DROP ip6tables -P FORWARD DROP ip6tables -P OUTPUT DROP #Accept all on the loopback interface $IPT -A INPUT -i lo -j ACCEPT #Accept icmp $IPT -A INPUT -p icmp -j ACCEPT #Allow admins SSH access $IPT -A INPUT -s 1 .2.3.16/28 -p tcp --dport 22 -j ACCEPT #Allow the core router to receive DNS packets $IPT -A INPUT -p udp --sport 53 -j ACCEPT #Allow non syn packets (connections initiated by this machine) $IPT -A INPUT -p tcp ! --syn -j ACCEPT ########## INPUT Chain end ######## ########## FORWARD Chain begin ######## ########## policy ACCEPT - default #Deny Access to switches and wireless equipment $IPT -A FORWARD -s ! 1 .2.3.16/28 -d 192 .168.100.0/24 -j DROP #Allow SSH access to everything from admins $IPT -A FORWARD -s ! 1 .2.3.16/28 -p tcp --dport 22 -j ACCEPT #Allow established tcp packets out eth1 and eth2 $IPT -A FORWARD -o eth1 -p tcp ! --syn -j ACCEPT $IPT -A FORWARD -o eth2 -p tcp ! --syn -j ACCEPT #Create a chain for the intranet server #Intranet server $IPT -N INTRANET $IPT -A FORWARD -d 1 .2.3.10 -j INTRANET ###### INTRANET Chain #DROP web packets for the intranet application $IPT -A INTRANET -p tcp --dport 80 -j DROP #DROP SSH (Packets from ADMINS don't pass through here) $IPT -A INTRANET -p tcp --dport 22 -j DROP #Drop Samba and ms-ds for packets on eth2 $IPT -A FORWARD -o eth2 -p tcp --dport 137 :139 -j DROP $IPT -A FORWARD -o eth2 -p udp --dport 137 :139 -j DROP $IPT -A FORWARD -o eth2 -p tcp --dport 445 -j DROP #wireless server - simple, we don't need a chain $IPT -A FORWARD -d 1 .2.3.130 -p udp --sport 53 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.130 -p ICMP -j ACCEPT $IPT -A FORWARD -d 1 .2.3.130 -j DROP #AAA server $IPT -A FORWARD -d 1 .2.3.1 -s ! 1 .2.3.131 -p udp --dport 1812 :1814 -j DROP $IPT -A FORWARD -d 1 .2.3.1 -p tcp --dport 23 -j DROP #SQL server $IPT -A FORWARD -d 1 .2.3.2 -s ! 1 .2.3.10 -p tcp --dport 5432 -j DROP $IPT -A FORWARD -d 1 .2.3.2 -p ICMP -j ACCEPT $IPT -A FORWARD -d 1 .2.3.2 -j DROP #MAIL server $IPT -A FORWARD -d 1 .2.3.3 -p tcp --dport 25 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.3 -p tcp --dport 110 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.3 -p udp --sport 53 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.3 -p ICMP -j ACCEPT $IPT -A FORWARD -d 1 .2.3.3 -j DROP #WEB server $IPT -A FORWARD -d 1 .2.3.4 -p tcp --dport 50000 :52000 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.4 -p tcp --dport 80 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.4 -p tcp --dport 21 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.4 -p ICMP -j ACCEPT $IPT -A FORWARD -d 1 .2.3.4 -j DROP #Access Server $IPT -A FORWARD -d 1 .2.3.131 -s 1 .2.3.1 -p udp --sport 1812 :1814 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.131 -s 1 .2.3.1 -p udp --sport 53 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.131 -s 1 .2.3.16/28 -j ACCEPT $IPT -A FORWARD -d 1 .2.3.131 -p ICMP -j ACCEPT $IPT -A FORWARD -d 1 .2.3.131 -j DROP #delete MANAGEMENT chain if exists $IPT -X MANAGEMENT #create MANAGEMENT chain $IPT -N MANAGEMENT #add authorized IPs to the MANAGEMENT chain, drop all the others $IPT -A MANAGEMENT -s 1 .1.2.0/26 -j ACCEPT $IPT -A MANAGEMENT -s 1 .1.3.192 -j ACCEPT $IPT -A MANAGEMENT -s 1 .1.9.21 -j ACCEPT $IPT -A MANAGEMENT -s 1 .1.19.61 -j ACCEPT $IPT -A MANAGEMENT -s 0 /0 -j DROP #Jump incoming packets for port 61146 TCP to the MANAGEMENT chain $IPT -A INPUT -p tcp --dport 61146 -j MANAGEMENT #Jump packets destined to 1.1.2.2 port 61146 TCP to the MANAGEMENT #chain $IPT -A FORWARD -d 1 .1.2.2 -p tcp --dport 61146 -j MANAGEMENT #Allow users to connect to openvpn $IPT -A INPUT -p tcp --dport 6669 -j ACCEPT #Allow internal departments SAMBA connections $IPT -A INPUT -s 192 .168.1.0/24 -p tcp --dport 137 :139 -j ACCEPT #Allow admins SAMBA connections $IPT -A INPUT -s 1 .2.3.16/28 -p tcp --dport 137 :139 -j ACCEPT #deny access to the intranet web server $IPT -A INPUT -i eth0 -p tcp --dport 80 -j DROP #filter the PostgreSQL port $IPT -A INPUT -p tcp --dport 5432 -j DROP #drop incoming TCP SYN packets $IPT -A INPUT -i eth0 -p tcp --syn -j DROP #allow http, pop3, smtp for the web and mail server $IPT -A FORWARD -d 1 .1.2.2 -p tcp -m multiport --dport 80 ,25,110 -j ACCEPT #drop all other tcp traffic for the web and mail server $IPT -A FORWARD -d 1 .1.2.2 -p tcp --syn -j DROP #deny SSH access except admins $IPT -A INPUT -p tcp --dport 22 -s ! 192 .168.1.0/29 -j DROP #NAT all to the internet. Don't nat to network at HQ $IPT -t nat -A POSTROUTING -s $PREFIX .0/24 -d ! 192 .168.1.0/24 -j MASQUERADE It is very important for us to secure the database server, and this is not even very difficult. $IPT -A FORWARD -d 1.2.3.2 -s ! 1.2.3.10 -p tcp --dport 5432 -j DROP This will drop packets from other sources than 1.2.3.10 (the intranet server) to the database server on port 5432/TCP. However, on the database server we have a chain named SQL in which we allow connections from 1.2.3.10 and 1.2.3.1 (the AAA server). The reason why we didn't add a rule in the core router for packets with the source 1.2.3.1 to pass to 1.2.3.2 is that those servers are in the same subnet (directly connected) and packets from one to another don't pass through the core router. Firewall script for the database server #!/bin/bash IPT = \"/sbin/iptables\" #flush rules $IPT -F #INPUT chain policy DROP $IPT -P INPUT DROP #Accept packets on loopback $IPT -A INPUT -i lo -j ACCEPT #Accept SSH from Admins $IPT -A INPUT -s 1 .2.3.16/28 -p tcp --dport 22 -j ACCEPT #Create SQL chain and jump packets for 5432/tcp to it $IPT -N SQL $IPT -A INPUT -p udp --dport 5432 -j SQL #Allow SQL connections from AAA and intranet servers $IPT -A SQL -s 1 .2.3.1 -j ACCEPT $IPT -A SQL -s 1 .2.3.10 -j ACCEPT $IPT -A SQL -j DROP #Allow outgoing TCP connections $IPT -A INPUT -s 1 .2.3.1 -p udp --sport 53 -j ACCEPT $IPT -A INPUT -p tcp ! --syn -j ACCEPT - Firewall script for the Web server #!/bin/bash IPT = \"/sbin/iptables\" #flush rules $IPT -F #INPUT chain policy DROP $IPT -P INPUT DROP #Accept packets on loopback $IPT -A INPUT -i lo -j ACCEPT #Accept SSH from Admins $IPT -A INPUT -s 1 .2.3.16/28 -p tcp --dport 22 -j ACCEPT #Accept FTP connections $IPT -A INPUT -p tcp --dport 21 -j ACCEPT #Active connections initiate the requests #So we only have to accept passive connections $IPT -A INPUT -p tcp --dport 50000 :52000 -j ACCEPT #Accept web connections $IPT -A INPUT -p tcp --dport 80 -j ACCEPT #Allow outgoing TCP connections $IPT -A INPUT -s 1 .2.3.1 -p udp --sport 53 -j ACCEPT $IPT -A INPUT -p tcp ! --syn -j ACCEPT Hardening DWRT Rules # click on security tab and click all boxes that say limit....+ARP spoofing # turn on SPI firewall # on services disable telnet and ssh if you don't use them # disable traffic daemon # add those lines to commands and save in firewall # To block some of most TCP-based DDoS attcks. # This rule blocks all packets that are not a SYN packet and don't belong to an established TCP connection. iptables -t mangle -I PREROUTING -m conntrack --ctstate INVALID -j DROP # This blocks all packets that aren't new (don't belong to an established connection) and don't use SYN flag. This rule is similar to the above rule but I found that it catches some packets that other ones doesn't. iptables -t mangle -I PREROUTING -p tcp ! --syn -m conntrack --ctstate NEW -j DROP # The next rule blocks new packets (only SYN packets can be new packets) that use a TCP MSS value that is not common. iptables -t mangle -I PREROUTING -p tcp -m conntrack --ctstate NEW -m tcpmss ! --mss 536 :65535 -j DROP # The below ruleset blocks packets that use bogus TCP flags, ie. TCP flags that legitimate packets wouldn\u2019t use. iptables -t mangle -A PREROUTING -p tcp --tcp-flags FIN,SYN FIN,SYN -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags SYN,RST SYN,RST -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags FIN,RST FIN,RST -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags FIN,ACK FIN -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags ACK,URG URG -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags ACK,PSH PSH -j DROP iptables -t mangle -A PREROUTING -p tcp --tcp-flags ALL NONE -j DROP # These rules block spoofed packets originating from private (local) subnets. On your public network interface you usually don\u2019t want to receive packets from private source IPs. iptables -t mangle -A PREROUTING -s 224 .0.0.0/3 -j DROP iptables -t mangle -A PREROUTING -s 169 .254.0.0/16 -j DROP iptables -t mangle -A PREROUTING -s 172 .16.0.0/12 -j DROP iptables -t mangle -A PREROUTING -s 192 .0.2.0/24 -j DROP iptables -t mangle -A PREROUTING -s 192 .168.0.0/16 -j DROP iptables -t mangle -A PREROUTING -s 10 .0.0.0/8 -j DROP iptables -t mangle -A PREROUTING -s 0 .0.0.0/8 -j DROP iptables -t mangle -A PREROUTING -s 240 .0.0.0/5 -j DROP iptables -t mangle -A PREROUTING -s 127 .0.0.0/8 ! -i lo -j DROP # This drops all ICMP packets. ICMP is only used to ping a host to find out if it\u2019s still alive. Because it\u2019s usually not needed and only represents another vulnerability that attackers can exploit, we block all ICMP packets to mitigate Ping of Death (ping flood), ICMP flood and ICMP fragmentation flood. iptables -t mangle -A PREROUTING -p icmp -j DROP # This rule blocks fragmented packets. Normally you don\u2019t need those and blocking fragments will mitigate UDP fragmentation flood. But most of the time UDP fragmentation floods use a high amount of bandwidth that is likely to exhaust the capacity of your network card, which makes this rule optional and probably not the most useful one. iptables -t mangle -A PREROUTING -f -j DROP iptables -I INPUT -f -j DROP iptables -I INPUT -p tcp --tcp-flags ALL ALL -j DROP iptables -I INPUT -p tcp --tcp-flags ALL NONE -j DROP # This iptables rule helps against connection attacks. It rejects connections from hosts that have more than 80 established connections. If you face any issues you should raise the limit as this could cause troubles with legitimate clients that establish a large number of TCP connections. iptables -A INPUT -p tcp -m connlimit --connlimit-above 80 -j REJECT --reject-with tcp-reset # Limits the new TCP connections that a client can establish per second. This can be useful against connection attacks, but not so much against SYN floods because the usually use an endless amount of different spoofed source IPs. iptables -A INPUT -p tcp -m conntrack --ctstate NEW -m limit --limit 60 /s --limit-burst 20 -j ACCEPT iptables -A INPUT -p tcp -m conntrack --ctstate NEW -j DROP ### SSH brute-force protection ### /sbin/iptables -A INPUT -p tcp --dport ssh -m conntrack --ctstate NEW -m recent --set /sbin/iptables -A INPUT -p tcp --dport ssh -m conntrack --ctstate NEW -m recent --update --seconds 60 --hitcount 10 -j DROP ### Protection against port scanning ### /sbin/iptables -N port-scanning /sbin/iptables -A port-scanning -p tcp --tcp-flags SYN,ACK,FIN,RST RST -m limit --limit 1 /s --limit-burst 2 -j RETURN /sbin/iptables -A port-scanning -j DROP iptables -I INPUT -s ` nvram get lan_ipaddr ` / ` nvram get lan_netmask ` -d ` nvram get wan_ipaddr ` -j DROP iptables -I FORWARD -f -j DROP iptables -I FORWARD -p tcp --dport 25 -j DROP iptables -I FORWARD -p udp --dport 25 -j DROP iptables -I FORWARD -p tcp -o ` get_wanface ` --dport 25 -j REJECT iptables -I FORWARD -p tcp --dport 137 -j DROP iptables -I FORWARD -p udp --dport 137 -j DROP iptables -I FORWARD -p udp --dport 138 -j DROP iptables -I FORWARD -p tcp --dport 139 -j DROP iptables -I FORWARD -p udp --dport 139 -j DROP iptables -I FORWARD -p tcp --dport 445 -j DROP iptables -I FORWARD -p udp --dport 445 -j DROP iptables -I FORWARD -p tcp --dport 31337 -j DROP iptables -I FORWARD -p udp --dport 31337 -j DROP iptables -I FORWARD -p tcp --tcp-flags ALL ALL -j DROP iptables -I FORWARD -p tcp --tcp-flags ALL NONE -j DROP iptables -A FORWARD -m recent --name portscan --rcheck --seconds 8640 -j DROP SSH Testing Firewall","title":"Defining the Security Policy"},{"location":"learning/linux/router/#firewall-script-for-home-server","text":"# # Example fast and scalable firewall configuration with iptables # Please only implement if you fully understand the functionality # because is very easy to lockout yourself from your computer if # the script isn't adapted to your specific situation. # *filter :INPUT ACCEPT [ 0 :0 ] :FORWARD ACCEPT [ 0 :0 ] :OUTPUT ACCEPT [ 0 :0 ] :Always - [ 0 :0 ] :Allow - [ 0 :0 ] :Bogus - [ 0 :0 ] :Enemies - [ 0 :0 ] :Friends - [ 0 :0 ] -A INPUT -j Bogus -A INPUT -j Always -A INPUT -j Enemies -A INPUT -j Allow -A FORWARD -j Bogus -A FORWARD -j Always -A FORWARD -j Enemies -A FORWARD -j Allow -A Bogus -p tcp -m tcp --tcp-flags SYN,FIN SYN,FIN -j DROP -A Bogus -p tcp -m tcp --tcp-flags SYN,RST SYN,RST -j DROP -A Bogus -s 169 .254.0.0/16 -j DROP -A Bogus -s 172 .16.0.0/12 -j DROP -A Bogus -s 192 .0.2.0/24 -j DROP -A Bogus -s 192 .168.0.0/16 -j DROP -A Bogus -s 10 .0.0.0/8 -j DROP -A Bogus -s 127 .0.0.0/8 -i ! lo -j DROP -A Always -p udp --dport 123 -j ACCEPT -A Always -m state --state ESTABLISHED,RELATED -j ACCEPT -A Always -i lo -j ACCEPT -A Enemies -m recent --name psc --update --seconds 60 -j DROP -A Enemies -i ! lo -m tcp -p tcp --dport 1433 -m recent --name psc --set -j DROP -A Enemies -i ! lo -m tcp -p tcp --dport 3306 -m recent --name psc --set -j DROP -A Enemies -i ! lo -m tcp -p tcp --dport 8086 -m recent --name psc --set -j DROP -A Enemies -i ! lo -m tcp -p tcp --dport 10000 -m recent --name psc --set -j DROP -A Enemies -s 99 .99.99.99 -j DROP -A Friends -s 123 .123.123.123 -j ACCEPT -A Friends -s 111 .111.111.0/24 -j ACCEPT -A Friends -j DROP -A Allow -p icmp --icmp-type echo-request -j Friends -A Allow -p icmp --icmp-type any -m limit --limit 1 /second -j ACCEPT -A Allow -p icmp --icmp-type any -j DROP -A Allow -p tcp -m state --state NEW -m tcp --dport 22 -j Friends -A Allow -p tcp -m state --state NEW -m tcp --dport 25 -j ACCEPT -A Allow -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT -A Allow -p tcp -m state --state NEW -m tcp --dport 443 -j ACCEPT -A Allow -j DROP COMMIT - The first block gives us the names of the chains that we will use, and the initial values of the packet and bytes counters. With these settings we reset the counters each time the firewall configuration is loaded. The default action for the three internal chains INPUT, FORWARD and OUTPUT is set to ACCEPT. This means that the packet is accepted by each of these chains, except when a rule in the chain decides otherwise. The five user defined chains do not have a default action. The firewall logic will therefore continue processing other rules when the rules inside those chains don\u2019t give a definite answer what to do with a packet. - The next two blocks add four filter rules to the INPUT and FORWARD chain. Each rule causes an unconditional jump to a user defined chain. The sequence in which these commands are listed in the iptables file is important, because this will also be the sequence in which the rules will be processed in the kernel. - The Bogus group contains a handful of rules which define invalid packets. This includes packets with both the SYN and FIN flags or the SYN and RST flags, packets from specific ranges of invalid IP addresses and packets with IP addresses which belong to the virtual loop-back adapter (127.x.x.x) , but which are not from a local source. These are all packets crafted by hackers or attackers to confuse or intrude your system. - The second group Always contains high-priority packets which should always be accepted without much delay. These packets might originate from an attacker, and even be part of a DDOS attack, but we accept that situation. The processing of NTP packets has such a low overhead that even when packets are coming in at a very high speed, it won\u2019t take too much CPU resources. There are also no states preserved as with the TCP protocol which could cause buffer overflows. The only thing which might happen is saturation of the network, but that would happen with a DDOS attack independent of us accepting or dropping the incoming packets. - The next rule is equally important. It tells us to accept all packets of a connection which has previously been accepted as OK. Many firewall rule sets miss this particular rule, but from a performance point of view it is a very important one. Most TCP/IP protocols exchange a lot of packets over one single connection. If we effectively only test the first packet of that stream and then mark the stream as either accepted or dropped, there is just little overhead involved when processing accepted network connections. - The third rule accepts all traffic to the loop-back adapter. What many firewall rule sets seem to forget is that also traffic to and from the loop-back adapter is screened by the firewall. This can cause severe performance problems if the loop-back adapter is for example used for connections between an Apache web server and an SQL database. - In the next block we define our friends. As you probably have seen, the Friends chain is not called directly from the INPUT or OUTPUT chain. It is called by other rules which we will define later to accept access to specific ports from some IP addresses but not from others. The Friends rule set is called to check the source IP address against a number of possible matches. The first rule checks if the IP address equals 123.123.123.123. If this is the case, the packet is accepted without further processing of firewall rules. It then checks against a block of IP addresses ranging from 111.111.111.0 to 111.111.111.255. If the IP address are in this range, the packet is accepted. The third rule drops all packets which do not match any of the previous IP address matches. It is obvious that the IP addresses mentioned here are just examples. You should replace them by addresses where friendly traffic to your server is normally coming from, for example the IP address of your home ADSL connection or IP addresses from other servers in your network. You are allowed to add extra rules before the -A Friends -j DROP line. - Enemies can be identified in two ways. Either by their source IP address, or by their behavior. Unfortunately the firewall in the kernel operates at a low level in the networking hierarchy and many behavioral actions are only visible at the application level, for example password dictionary attacks on SSH servers or spam mails. But some behavior can be detected at the lowest networking level and one of this is port scanning. Port scanning is trying to connect to a number of TCP/IP ports of the attacked computer to search for open connections with SQL servers, control panels or other network aware applications. My experience has shown that many port scanners are looking for unprotected connections to MySQL and MSSQL servers. Because my servers only accept SQL requests from local applications, every request from the outside world is by definition an enemy. In the IP firewall I have setup rules which are triggered if requests are coming in from the network for port 1433, port 3306 and some other well known ports. Every IP address from which such a request originates is put in quarantine for a period of 60 seconds. This is done by the \u2013name psc \u2013set and checking with every subsequent packet of the last set action for that IP address was less than 60 seconds ago. The Linux kernel will maintain a list of port-scan IPs which can be accessed at the location /proc/net/ipt_recent/psc. I have used a 60 seconds grace time because in some occasions I might be testing something and accidentally look like a port scanner according to the firewall. In that case I will only lockout myself for a period of one minute. But it is also possible to increase this value to for example 15 minutes or one hour. Other enemies are enemies which are known by IP address or IP address range. I have added the address 99.99.99.99 as an example, but this could also be ranges like 111.111.111.0/24.","title":"Firewall Script for Home Server"},{"location":"learning/linux/router/#adding-and-removing-enemies-from-the-firewall","text":"Friends come and go, but enemies accumulate . Many enemies are temporary by nature. These are computers infected by malware, or servers with configuration flaws like open mail or web proxies. Once these problems are fixed, an enemy can turn in a friend again. #!/bin/sh iptables -L -n -v | grep -q $1 RETVAL = $? if [ $RETVAL -eq 0 ] ; then echo \"IP address already in blocklist: $1 \" exit 0 fi echo \"Adding enemy $1 \" iptables -A Enemies -s $1 -j DROP - This scripts first makes a dump of the current firewall rules and checks if the IP address or IP range matches an existing rule. If this is the case a warning is shown and the script does nothing. If no matching rule has been found, the IP address or IP range is added to the list of enemies. By checking the whole rules list against the new IP address, we have some basic protection against adding one of our own IPs to the firewall because this script will also fail if it finds the IP address or range in the Friends table. But there is no check of individual IPs against IP ranges so you still have to be careful about what you do. #!/bin/sh echo \"Deleting enemy $1 \" iptables -D Enemies -s $1 -j DROP - This scripts deletes a rule from the Enemies table which matches the IP address or IP range.","title":"Adding and removing enemies from the firewall"},{"location":"learning/linux/router/#install-dd-wrt","text":"First method: Copy the factory-to-ddwrt file in the firware upgrade section of http://192.168.1.1/ router IP. Connect the Laptop to LAN Port 1 of the router. Wait for the proces to complete. Second method: In case the process is interrupted or to flash back to TP-LINK firware back. Override custom TP-LINK software from your router, use TFTP service on a Network LAN server. You need to set up a TFTP Server on your computer with IP 192.168.0.66 . You can do this by linking the MAC address of the computer with the 192.168.0.66 IP and restart the WI-FI connection. Verify if the computer has the correct IP. Once verified, copy the DD-WRT software to /tftpboot directory on the computer. Rename the file from tl-wr841nd-webflash.bin to wr841nv10_tp_recovery.bin . Start checking the tftp server logs tail -f /var/log/syslog to check the request Shutdown the router and connect the WAN port to main router. Hold down the Reset button on the back of the router and switch it on again, release the reset button when there pop-up a flashing window. Wait for the router to connect to the TFTP service and download the recovery.bin file. This will kickstart the router installation process. Wait for 10 mins for the process to complete. Now using the LAN port, connect the router to the computer and access the IP assigned by the main router to start DD-WRT configuration.","title":"Install DD-WRT"},{"location":"learning/linux/router/#network-configuration","text":"One of your routers will be connected to your ISP. This router needs to be assigned the internal LAN IP 192.168.0.1, and hand out DHCP addresses on something like 192.168.0.100 through 192.168.0.200. Call this router Router A. Connect a cable from one of the LAN ports on router A to the WAN port on router B. Set Router B's WAN IP address to 192.168.0.201, and its internal LAN IP to 192.168.1.1. Tell it to hand out DHCP addresses on something like 192.168.1.100 through 192.168.1.200. Set the DNS settings on this router's DHCP to what you want for your kids. Use Static WAN IP on Router B to set the IP 192.168.0.201. Router A should work normally. If you have things you need accessible via the Internet, make sure it's connected to Router A and setup your port forwarding on Router A like you would on any normal router. Router B will \"get internet\" through Router A. Router B will be double NATed. This means it's very difficult for machines from the Internet to connect to anything behind router B. To get Internet passed through to Router B servers in the second network, Static Route on Router A. # Network Destination: 192.168.1.0 # Subnet Mask: 255.255.255.0 # Default Gateway: 192.168.0.201 # Interface: LAN/WLAN # Description: Internal Network Setup Router B configuration . Backup Initial settings, incase we mess up the configurations later on. [Downloads/tp-wr841n/dd-wrt /DD-WRT_TP-Link_TL-WR841ND_v10-Config-Backup.bin] ssh root@192.168.1.1 -i ~/.ssh/ansible-user https://wiki.dd-wrt.com/wiki/index.php/Separate_LAN_and_WLAN WLAN Separate From LAN, With Independent Dhcp, Etc https://wiki.dd-wrt.com/wiki/index.php/Linking_Subnets_with_Static_Routes","title":"Network Configuration"},{"location":"learning/linux/router/#install-dd-wrt-in-virtualbox","text":"Get the public DD-WRT image Convert DD-WRT x86 Image to VDI cd /home/leslie/Downloads/tp-wr841n vboxmanage convertdd dd-wrt_public_vga.image dd-wrt.vdi # resize the vdi to 256MB vboxmanage modifyhd --resize 256 dd-wrt.vdi Launch Virtualbox and Create a New VM by selecting Machine > New # Name: dd-wrt # Machine Folder: /home/leslie/machines # Type: Linux # Version: Other Linux (32-bit) # Memory Size: 256 MB # Hard disk: Do not add a virtual hard disk # Click Create # Copy the dd-wrt.vdi file from downloads into /home/leslie/machines/dd-wrt # Select the VM and Click Settings # Select Storage # Click Add Storage Attachment > Add Hard Disk > Choose existing disk # Click Add and browse to /home/leslie/machines/dd-wrt/dd-wrt.vdi # Click OK # Select Network # Set Attached to: Bridge Adapter (Keeping it NAT for testing) # Make sure the DD-WRT VM is selected and click Start > Normal # Wait for the text to stop scrolling and press Enter # Login with root/admin # Set the IP address to something in your current subnet so you can reach it # ifconfig br0 192.168.56.1 (Not required) Launch your web browser of choice and access the DD-WRT web UI with the IP address in the previous step","title":"Install DD-WRT in Virtualbox"},{"location":"learning/linux/router/#setup-virtual-network-lab","text":"Configure Virtual Box # Open Virtual Box and select Tool --> Menu --> Network --> Properties # Create new Network vboxnet1 # Change the Default Address from 192.168.56.1 to 192.168.57.2 # x.x.x.2 is so that we can have an Edge Router which will have DHCP enabled and will give out IP. # So if the VMs have 99.x number range, we know the Virtual Edge router has given the address and not Virtual Box DHCP. # Disable DHCP in the next tab for the above reason. # Save the interface twice, so the changes are stored and not re-enabled.","title":"Setup Virtual Network Lab"},{"location":"learning/linux/router/#install-open-wrt-in-virtualbox","text":"Setup Router in Virtualbox # Download the Stable x86 - 64 bit - combined ext4 (as that has the fewest limitations) https://downloads.openwrt.org/releases/22.03.2/targets/x86/64/ # Extract the file in a directory openwrt # Execute the below command VBoxManage convertfromraw --format VDI openwrt-22.03.2-x86-64-generic-ext4-combined.img openwrt.vdi # resize the vdi to 512MB VBoxManage modifymedium openwrt.vdi --resize 512 # Copy the directory to /home/leslie/machines - Launch Virtualbox and Create a New VM by selecting Machine > New # Name: OpenWRT Edge Router # Machine Folder: /home/leslie/machines -> Default # Type: Linux # Version: Linux 2.6/3.x/4.x (64 Bit) --> The latest Linux 64bit # Memory Size: 512 MB # Hard disk: Do not add a virtual hard disk # Click Create # Copy the openwrt.vdi file from downloads into /home/leslie/machines/openwrt # Add the HDD to the VirtualBox Media --> Tools -- Hard Disks --> Add -- Browse to /home/leslie/machines/openwrt and select openwrt.vdi # Select the VM and Click Settings # Select Storage # Click Add Storage Attachment > Add Hard Disk > Choose existing disk # Note: Add this under Controller: SATA # Click Add openwrt.vdi # Click OK - Configure Edge Router # On the VirtualBox interface, open Settings --> Network # Adapter1 --> Host-Only Adapter and vboxnet1 # This will allow it to act as LAN and allow access to the internal network. # So my Laptop IP will be assigned on this interface, allowing access to my original WAN # Adapter2 --> Bridge Adapter and select the Wireless Network 'wslp01'. # This will connect to the Internet. - Start the Openwrt VM. - Adjust the window size \u2192 View \u2192 Adjust Window Size - Configure Openwrt IP uci show network # This will show lan.ipaddr=192.168.1.1 # Change the Network LAN IP Address to match vboxnet1, but to 57.1 so it can act as Edge Router uci set network.lan.ipaddr = 192 .168.57.2 uci show network # Confirm the change uci commit # Commit the change reboot # Reboot - Testing the network connectivity ping 192 .168.57.2 # Ping a IP of the Host ping www.google.com # Ping a internet address # If both works, we should be able to open the Router's page on the browser and access luci (Router web UI) # Open http://192.168.57.1 # Login and verify the Network --> Interfaces. WAN is the Main Address and LAN is x.x.57.1 and both are up. # Stop the openwrt server halt - Shutdown the machine and take a snaphot. - OpenWRT \u2192 Machine \u2192 Tools \u2192 Snapshots \u2192 Take - LAN and WAN working after Initial Setup Configure Internal Router # Click Machine and Clone the Edge Router and Rename to OpenWRT Internal Router # Path: /home/leslie/machines/openwrt # MAC Address Policy: Generate new MAC Address for all Network Adapters # Clone Type: Full clone # Snapshots: Everything # Click Clone # Remove the WAN Adpater as its not required to access Internet # Settings --> Network --> Disable Adpater 2 # Start the machine # Configure server to move from Fixed IP to DHCP (which it will get from Edge Router) uci set network.lan.proto = dhcp uci show network # Confirm the change uci commit # Commit the change reboot # Reboot Start the Edge Router to confirm if Internal Router gets the IP address correctly In the Browser status page, we can see internal router has an address assigned. Rename the Openwrt server to give a definte name Settings - Change the Hostname - Openwrt-Edge Similarly of the Internal router - Openwrt-Internal Reboot both machines to confirm the hostname changes. Take a snapshot of the Internal Router and delete the orignal Snapshot 1 as its not required any more. Adding more machines to the LAN, ensure only Adapter1 is added as HostOnly. This would mean, servers are on the LAN and go through the Internal router and then through the Edge router to access the Internet.","title":"Install Open-WRT in Virtualbox"},{"location":"learning/linux/router/#setup-the-servers","text":"Add the route to reach the Gateway for the Ethernet interface ifconfig # Identify the Ethernet interface route -n # Check the default route, n shows the IP address resolution traceroute google.com # Add the route for the eth0 interface netstat -r","title":"Setup the Servers"},{"location":"learning/linux/router/#testing","text":"Combining tcpdump with Wireshark is a powerful combination, particularly when you wish to dig into full application layer sessions as the decoders can assemble the full stream. # On Server 1 - Setup netcat listner (which acts as Server) nc -nlvp 22 # On Server 2 - Which needs to be tested (which acts as Client) nc 192 .168.57.11 22 # This will initiate a TCP connection and start a chat session # To kill the netcat session - Ctrl + d # OR # Server 1 sudo tcpdump -i enp0s8 # All traffic can be seen sudo tcpdump -i enp0s8 -v port 22 # Filter only SSH traffic # Server 2 sudo nmap -p 22 192 .168.57.11 # OR Check all ports sudo nmap -v -Pn 192 .168.57.11 # Check which service has opened which ports netstat -anp # Do a network scan to see if any other machine can be accessed sudo nmap -Pn -sV 192 .168.0.0/16 # When such a scan is run, no alert or monitoring is generated logging the time of this activity # Capture UDP traffic sudo tcpdump -i eth0 udp # DHCP Example # Monitoring DHCP request and reply. DHCP requests are seen on port 67 and the reply is on 68. Using the verbose parameter -v we get to see the protocol options and other details. sudo tcpdump -v -n port 67 or 68 # host filter will capture traffic going to (destination) and from (source) the IP address. # In this example, an network traffic from 10.10.1.1 can be captured on the localhost. # Run the below command in terminal 1. Start a new terminal and ping 10.10.1.1, you will see the output in terminal 1. sudo tcpdump -i eth0 host 10 .10.1.1 # capture only packets going one way using src or dst. # This will capture traffic only in one direction sudo tcpdump -i eth0 dst 10 .10.1.20 # Another example is to use domain name sudo tcpdump -i eth0 dst medium.com -n # -n will not convert the IP address to domain name # In another terminal curl medium.com or open a brower and type medium.com to test traffic # To capture all network traffic in the subnet use net option sudo tcpdump -i eth0 net 192 .168.0.0/24 # Capture HTTPS traffic using port option sudo tcpdump -i eth0 port 443 -vv -n # using the `or` operator sudo tcpdump -i eth0 port 80 or port 443 -vv -n # Capture all ICMP packets sudo tcpdump -n icmp # Capture ICP traffic to and from the DNS # To test this, in another terminal, ping 8.8.8.8 which is ICMP traffic sudo tcpdump -i eth0 -n icmp and host 8 .8.8.8 # Filter on the icmp type to select on icmp packets that are not standard ping packets. sudo tcpdump 'icmp[icmptype] != icmp-echo and icmp[icmptype] != icmp-echoreply' # See the NTP query and response. sudo tcpdump dst port 123 # Detect Port Scan in Network Traffic tcpdump -nn # Capture DNS Request and Response # Outbound DNS request to Google public DNS and the A record (ip address) response can be seen in this capture. sudo tcpdump -i wlp58s0 -s0 port 53 -vv -n # Capture DNS request going through port 53 sudo tcpdump -i eth0 dst port 53 -vv -n # Capture with tcpdump and view in Wireshark # This tip is a favorite, pipe the raw tcpdump output right into wireshark on your local machine. Don't forget the not port 22 so you are not capturing your SSH traffic. ssh root@remotesystem 'tcpdump -s0 -c 1000 -nn -w - not port 22' | wireshark -k -i - # Another tip is to use count -c on the remote tcpdump to allow the capture to finish otherwise hitting ctrl-c will not only kill tcpdump but also Wireshark and your capture. # Top Hosts by Packets sudo tcpdump -nnn -t -c 200 | cut -f 1 ,2,3,4 -d '.' | sort | uniq -c | sort -nr | head -n 20","title":"Testing"},{"location":"learning/linux/router/#firewall","text":"# My goal was to migrate my existing linux box with an ethernet interface and a wifi card which was acting as a crude AP, but keep the same functionality: firewalling, separate subnets for wired and wireless, separate dhcp, etc. # I wanted my wifi on a separate subnet from my LAN, with its own DHCP scope. In these examples, lan is 192.168.7.0/24, and wifi is 192.168.8.0/24 ## wan: vlan1 ## lan: br0 - 192.168.7.1 ## wifi: eth1 - 192.168.8.1 ## permit incoming connections from WLAN iptables -I INPUT 2 -i eth1 -m state --state NEW -j logaccept ## fixup forwarding table ## the lan2wan target didn't work for me, replace it with straight accept iptables -R FORWARD 5 -i br0 -o vlan1 -j ACCEPT ## permit WLAN -> WAN iptables -I FORWARD 7 -i eth1 -o vlan1 -j ACCEPT ## disallow WLAN -> LAN iptables -I FORWARD 7 -i eth1 -o br0 -m state --state NEW -j DROP ## disallow LAN -> WLAN iptables -I FORWARD -i br0 -o eth1 -m state --state NEW -j DROP ## disallow WLAN -> WAN subnet iptables -I FORWARD -i eth1 -d ` nvram get wan_ipaddr ` / ` nvram get wan_netmask ` -m state --state NEW -j DROP ## disallow WLAN -> direct router access iptables -I INPUT -i eth1 -m state --state NEW -j DROP ## Allow WLAN -> DHCP on the router iptables -I INPUT -i eth1 -p udp --dport 67 -j ACCEPT ## Allow WLAN -> DNS on the router iptables -I INPUT -i eth1 -p udp --dport 53 -j ACCEPT iptables -I INPUT -i eth1 -p tcp --dport 53 -j ACCEPT # This is for allowing netwrok access between 2 networks # Allow the entire 192.168.0.0/16 block to be forwarded through the router iptables -I FORWARD -s 192 .168.0.0/16 -j ACCEPT # OR # Allow Router2 to forward traffic from Router1 and Router3's subnets iptables -I FORWARD -s 192 .168.1.0/24 -j ACCEPT iptables -I FORWARD -s 192 .168.3.0/24 -j ACCEPT # Allow Router3 to forward traffic from Router1 and Router2's subnets iptables -I FORWARD -s 192 .168.1.0/24 -j ACCEPT iptables -I FORWARD -s 192 .168.2.0/24 -j ACCEPT","title":"Firewall"},{"location":"learning/linux/security/","text":"Introduction \u00b6 Linux Security Confernce Videos Hardening Guides from CIS, DISA, Ubuntu Add Ubuntu mailing list for security updates Linux is only as secure as you make it! \u00b6 # I trust `ME` more than anyone else with my Data, Infrastructure, Time and Value - Nothing is perfectly secure. - Security is a series of trade-offs. - convenience vs security # No passwords = easy to use, not secure. # System powered off = secure, not usable. - Examples: - Linux can be configured to be unsecure. - Users may employ lax file permissions. - System administration mistakes. - Users could use easy to guess passwords. - Data transmitted in the clear. - Malicious software installed on the system. - Lack of training or security awareness. Continous Improvement \u00b6 Just because you are using Linux, doesn\u2019t mean you are \u201csecure.\u201d Security is an ongoing process. Stay vigilant! Risk Assessment \u00b6 What is the severity of the risk? What is the probability of the risk occurring? What is the cost to mitigate the risk? What is the effectiveness of the countermeasure? Multiuser System \u00b6 Linux is a multiuser system. The superuser is the root account. root is all powerful. Required to install system-wide software, configure networking, manager users, etc. All other accounts are \u201cnormal\u201d accounts. Can be used by people or applications (services). Advantages to a Multiuser System. \u00b6 File permissions \u00b6 Every file has an owner. Permissions can be granted to other accounts and users as needed. Breaking into one account does not necessarily compromise the entire system. Process permissions. \u00b6 Every process has an owner. Each account can manage their processes. Exception to above rule: root can do anything Breaking into one account does not necessarily compromise the entire system. Security Guidelines \u00b6 Principle of Least Privilege Use encryption Shared accounts (Yes, root can be a shared account!) Multifactor authentication Firewall Monitoring logs Minimize Software and Services \u00b6 If you don\u2019t need a piece of software, don\u2019t install it. If you don\u2019t need a service, don\u2019t start it. If you no longer need the software or service, stop and uninstall it. Run Services on Separate Systems \u00b6 Minimizes the risk of one compromised service leading to other compromised services. Encrypt Data Transmissions \u00b6 Protect against eavesdropping and man-in-the middle attacks. Examples: Protocol \u2192 Replace with FTP \u2192 SFTP telnet \u2192 SSH SNMP v1/v2 \u2192 SNMP v3 HTTP \u2192 HTTPS Avoid Shared Accounts \u00b6 Each person should have their own account. Each service should have its own account. Shared accounts make security auditing difficult. Lack of accountability with shared accounts. Avoid Direct root Logins \u00b6 Do not allow direct login of shared accounts. Users must login to their personal accounts and then switch to the shared account. Control and monitor access with sudo. Maintain Accounts \u00b6 Create and use a process for removing access. Use Multifactor Authentication \u00b6 Something you know (password) + something you have (phone) or something you are(fingerprints). Examples: account password + phone to receive the one time password (OTP). account password + fingerprint The Principle of Least Privilege \u00b6 AKA, the Principle of Least Authority. Examples: Only use root privileges when required. Avoid running services as the root user. Use restrictive permissions that allow people and services enough access to do their jobs. Monitor System Activity \u00b6 Routinely review logs. Send logs to a central logging system. Use a Firewall \u00b6 Linux has a built-in firewall. Netfilters + iptables. Only allow network connections from desired sources. Encrypt Your Data \u00b6 Encryption protects your data while it is \u201cat rest\u201d (on disk). Physical Security \u00b6 Physical Security Is Linux Security \u00b6 Physical access poses a great security threat to your Linux system! Single user mode - Allows unrestricted access. Only allow physical access when necessary. Systems Not Under Your Control \u00b6 Data centers / colos - Like \u201cbanks\u201d of data. Possible targets for attackers Needs processes, procedures, and controls in place toprotect your valuable data. Cloud \u00b6 At some point the cloud is real equipment. Physical security is still important. Your data is on their storage systems. The provider has access to your virtual disks. If encryption is available, use it. Protecting Linux Against Physical Attacks \u00b6 Gaining Access to a Linux System: Single User Mode & Power Resets Changing the shell command from sushell to sulogin will prompt for root password when entrering into single user mode # Securing Single User Mode and blank passwords by having root password at logins # Login to root and gain access to shell echo $$ # Shows the current process id i.e shell ps -fp <pid> # Shows detailed information about shell process including command executed at login cd /lib/systemd/system grep sulogin emergency.target # No output should be visible grep sulogin emergency.service # Should have sulogin in ExecStart command head -1 /etc/shadow # Shows id root password is set or not. If not set (!) is present in output 2nd column passwd # Set root password Securing the Boot Loader \u00b6 To prevent a person who as physical access from passing arguments to the Linux kernel at boot time, you should password protect the boot loader. Check examples how to secure this. Disk Encryption \u00b6 dm-crypt \u00b6 device mapper crypt Provides transparent disk encryption. Creates a new device in /dev/mapper. Use like any other block device. Manage with cryptsetup LUKS \u00b6 Linux Unified Key Setup. Front-end for dm-crypt. Multiple passphrase support. Portable as LUKS stores setup information in the partition header. Great for removable media, too. Encrypt During Install \u00b6 PRO: easy, with sane defaults. CON: you give up some control. Setting up LUKS on a New Device \u00b6 Use this process for any block device presented to your system that you want to encrypt. Following this procedure will remove all data on the partition (device) that you are encrypting! Implementing LUKS for Full Disk Encryption \u00b6 example Laptops, USB sudo apt-get install cryptsetup # Install LUKS tools lsblk # list the blocks sudo shread -v -n 1 /dev/sdb # Writes random data to he device sdb sudo fdisk /dev/sdb # partition the sdb drive and create a new partition called sdb1 sudo cryptsetup \u2013y luksFormat /dev/sdb1 # Allows to store encrypted data in this partition. Give a passphrase sudo cryptsetup luksDump /dev/sdb1 # Shows details of the encrypted partition sudo cryptsetup luksOpen /dev/sdb1 private_data # Assign a mapper called private_data and opens the device ls /dev/mapper # Shows the new mapper setup and that is a device ls -arlt /dev/mapper | tail # Shows the virtual block devices private_data sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted # Make an entry in the /etc/fstab to mount private_data to /mnt on each boot # Make an entry in /etc/crypttab to mount private_data at boot time # To close the encrypted device sudo cryptsetup luksClose private_data Encrypting device in Cloud \u00b6 Sometimes cloud providers do not give block level access to volumes. For such cases, we will encrypt the files like we do for volumes sudo mkdir /data sudo fallocate -l 100M /data/private_data # Creates a non sparse file sudo strings /data/private_data # Shows any string data in the file. It mostly is blank # Write random data, if=input, of=output, bs=byte size<1 Mb>, count=<size of file i.e 100Mb> sudo dd if = /dev/urandom of = /data_private_data bs = 1M count = 100 sudo strings /data/private_data sudo cryptsetup \u2013y luksFormat /data/private_data # Allows to store encrypted data in this file. Give a passphrase sudo cryptsetup luksOpen /data/private_data private_data # Assign a mapper called private_data and opens the device ls /dev/mapper # Shows the new mapper setup and that is a device ls -arlt /dev/mapper | tail # Shows the virtual block devices private_data sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted sudo df -h /mnt # Shows the size # Make an entry in the /etc/fstab to mount private_data to /mnt on each boot # Make an entry in /etc/crypttab to mount private_data at boot time Converting an Existing Device to LUKS \u00b6 # Backup the data. /home lives on /dev/sda3 # for example. # Wipe the device. # use shred or dd if=/dev/urandom of=/dev/sda3 # Setup LUKS. cryptsetup luksFormat /dev/sda3 cryptsetup luksOpen /dev/sda3 home mkfs -t ext4 /dev/mapper/home mount /dev/mapper/home /home # & restore from backup Disabling Ctrl+Alt+Del (Systemd) \u00b6 Attackers can gain access to the virtual terminal and send command ctrl+alt+delete to reboot the system. # Disabling reboot using ctrl-alt-delete command over remote connection systemctl mask ctrl-alt-del.target systemctl daemon-reload Account Security \u00b6 It's easier to attack a system from the inside. Privilege escalation attacks are a threat. Mitigation \u00b6 Keep unwanted users out. Secure accounts. PAM \u00b6 Pluggable Authentication Modules Used to delegate / abstract authentication of services / programs like login or sshd PAM Configuration files \u00b6 Location: /etc/pam.d Configuration file for login is /etc/pam.d/login Configuration file for sshd is /etc/pam.d/sshd Format: module_interface control_flag module_name module_args PAM Module Interfaces \u00b6 auth - Authenticates users. account - Verifies if access is permitted. password - Changes a user\u2019s password. session - Manages user\u2019s sessions. PAM Control Flags \u00b6 required - Module result must be successful to continue. requisite - Like required, but no other modules are invoked. sufficient - Authenticates user if no required modules have failed, otherwise ignored. optional - Only used when no other modules reference the interface. include - Includes configuration from another file. complex control flags - attribute=value PAM Configuration Example \u00b6 The directives listed in the PAM module are executed in sequential order *.so extension stands for shared objects #%PAM-1.0 # Comment auth required pam_securetty.so # 3 auth modules which need to pass auth required pam_unix.so nullok auth required pam_nologin.so account required pam_unix.so # Checks if the user account is valid password required pam_pwquality.so retry = 3 # Checks for password quality if acount has expired and gives 3 tries to set password password required pam_unix.so shadow \\ # Allows to use shadow file nullok use_authtok session required pam_unix.so # Logs when user logs in and out of the system PAM Documentation \u00b6 Configuration: account required pam_nologin.so session required pam_unix.so Getting help, drop the .so extension and use the man page to get additional help: man pam_nologin man pam_unix Linux Account Types \u00b6 root, the superuser \u00b6 Root can do anything. Always has the UID of 0. System accounts \u00b6 UIDs < 1,000 Configured in /etc/login.defs useradd -r system_account_name Normal User Accounts \u00b6 UIDs >= 1,000 Intended for human (interactive) use Password Security \u00b6 Enforce, not hope for, strong passwords. Use pam_pwquality, based on pam_cracklib. Configuration File: /etc/security/pwquality.conf PAM Usage: password requisite pam_pwquality.so Module attributes: man pam_pwquality # /etc/login.defs format # PASS_MAX_DAYS 99999 # PASS_MIN_DAYS 0 # PASS_MIN_LEN 5 # PASS_WARN_AGE 7 Use Shadow Passwords \u00b6 /etc/passwd unencrypted: root:$6$L3ZSmlM1H5:0:0:root:/root:/bin/bash /etc/passwd with shadow passwords: root:x:0:0:root:/root:/bin/bash /etc/shadow: root:$6$L3ZSmlM1H5::0:99999:7::: Converting Passwords \u00b6 pwconv - convert to shadow passwords. pwunconv - convert from shadow passwords. /etc/shadow format \u00b6 Username Hashed password Days since epoch of last password change Days until change allowed Days before change required Days warning for expiration Days before account inactive Days since epoch when account expires Reserved Display user account expiry info with chage \u00b6 chage -l <account-name> # Show account aging info. $ chage -l jason # Last password change : Apr 01, 2016 # Password expires : never # Password inactive : never # Account expires : never # Minimum number of days between password change : 0 # Maximum number of days between password change : 99999 # Number of days of warning before password expires : 7 Change user account expiry info with chage \u00b6 -M MAX_DAYS - Set the maximum number of days during which a password is valid. -E EXPIRE_DATE - Date on which the user\u2019s account will no longer be accessible. -d LAST_DAY - Set the last day the password was changed. Demo to change normal account to root \u00b6 head -n 1 /etc/passwd # Shows the root entry, UID is 3rd field delimited by : sudo useradd jim # Create normal account sudo passwd jim # Change password su - jim # Login to account whoami # Show logged in user details exit sudo vi /etc/passwd # Edit the UID of jim to 0 su - jim id whoami # Now jim account shows root # Show how many users have UID of 0 awk -F: '($3 == ' 0 ')' { print } # Delimit Field by :, take the 3rd field and check if its 0 and print the line # This will show 2 entries, one for root and other for jim # Undo the change by editing `/etc/passwd` and updating jim's UID to original Controlling Account Access \u00b6 Locking and Unlocking accounts \u00b6 passwd -l account passwd -u account Disabling logins for system and root accounts \u00b6 Locking with nologin as the Shell # Example /etc/passwd entries: for apache and www-data system accounts apache:x:48:48:Apache:/usr/share/httpd:/sbin/nologin www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin # Using chsh : chsh -s SHELL ACCOUNT chsh -s /sbin/nologin jason # Does not allow jason user to login using password Centralized Authentication \u00b6 Easy to manage users system-wide - lock account everywhere Example authentication systems: freeIPA, LDAP (openLDAP) Has drawbacks too. Disable Logins \u00b6 pam_nologin module Looks for /etc/nologin or /var/run/nologin Disables logins and displays contents of nologin file. Monitoring Authentication Logs \u00b6 last # All login data lastb # Failed authentication lastlog # Last logins # Depends on syslog configuration, logs are stored in following files: /var/log/messages /var/log/syslog /var/log/secure /var/log/auth.log Intrusion Prevention with fail2ban \u00b6 Monitors log files. Blocks IP address of attacker. Automatic unban. Not just for Linux logins. Multifactor Authentication \u00b6 Google Authenticator PAM module DuoSecurity\u2019s pam_duo module RSA SecurID PAM module Security by Account Type \u00b6 Account Security - root \u00b6 Use a normal account for normal activities. Avoid logging in as root. Use sudo instead of su. Avoid using the same root password. Ensure only the root account has UID 0 - awk -F: '($3 == \"0\") {print}' /etc/passwd Disabling root Logins \u00b6 /etc/securetty - Controls root logins using terminals. Normal user logins don't use this file # pam_securetty module - /etc/pam.d/login auth [ user_unknown = ignore success = ok ignore = ignore default = bad ] pam_securetty.so # Shows pam_securetty module is used w # Shows the current terminal, assume tty1 vi /etc/securetty # Remove tty1 entry from this file and save # login to system as root and it fails as tty1 (first terminal) has been removed from logging # Alt+Ctrl+F2 (This will use tty2 to login to the system instead of tty1 as that is no longer valid) # Similarly F3 for tty3, F4 for tty4 and so on # Now login as root and it works as tty2 is present in /etc/securetty file # Empty the securetty file of all entries and save it. # Now there is no way root can login to this system. # Login using a normal account and that will work System / Application Accounts \u00b6 Use one account per service - web service (httpd), web service account (apache) Don\u2019t activate the account. Don\u2019t allow direct logins from the account - sshd_config: DenyUsers account1 accountN Use sudo for all access. sudo -u apache apachectl configtest User Accounts \u00b6 One account per person. Deleting Accounts \u00b6 Determine the UID - id ACCOUNT Delete their account and home directory - userdel -r Find other files that belong to them on the system. find / -user UID find / -nouser Using and Configuring Sudo \u00b6 sudo vs su \u00b6 \u201cSuperUser Do\u201d or \u201cSubstitute User Do\u201d Use instead of the su command. Complete shell access with su. With su you need to know the password of the other account. Breaks the Principle of Least Privilege. Vague audit trail with su. Sudo (Super User Do) \u00b6 Elevation of Privileges - Giving users temporary root priviledges Fine grain controls. No need to share passwords. Clear audit trail. grep sudo /etc/group # Shows users in the sudo group at account setup # Once the access to root is granted, it is cached for 15 mins on the same terminal. sudo -k # Invalidates the cache, so next time password is asked again. Sudoers Format \u00b6 Sudoers configiration is stored in /etc/sudoers.d file. Change the default editor used by visudo using sudo update-alternatives --config editor and select 3 for vim basic. To avoid corrupting this file, open it in visudo editor as it validates the configuration before saving the file. User Specification Format: user host=(run_as) command # Examples: # User Priviledge jason webdev01 =( root ) /sbin/apachectl # Group Priviledge (Starts with %) %web web* =( root ) /sbin/apachectl %wheel ALL =( ALL ) ALL The last line in the file, includes permissions for other sudo users to make it maintainable. @includedir /etc/sudoers.d Sudo Authentication \u00b6 Sudo requires a user to authenticate. Default 5 minute grace period (timeout). You may not want to use a password. apache web* =( root ) NOPASSWD:/sbin/backup-web, # No password required to run backup-web PASSWD:/sbin/apachectl # Password required for apache Sudo Aliases \u00b6 User_Alias Runas_Alias Host_Alias Cmnd_Alias Format: Alias_Type NAME=item1,item2, ... # Add normal users to group webteam User_Alias WEBTEAM = jason,bob # Give permission to group WEBTEAM web* =( root ) /sbin/apachectl WEBTEAM web* =( apache ) /sbin/apachebackup # Run permissions to system accounts Runas_Alias WEBUSERS = apache,httpd WEBTEAM web* =( WEBUSERS ) /sbin/apachectl # Host permissions to user accounts Host_Alias WEBHOSTS = web*,prodweb01 WEBTEAM WEBHOSTS =( WEBUSERS ) /sbin/apachectl # Command permissions Cmnd_Alias WEBCMNDS = /sbin/apachectl WEBTEAM WEBHOSTS =( WEBUSERS ) WEBCMNDS # Optimized sudoers configuration User_Alias WEBTEAM = jason, bob Runas_Alias WEBUSERS = apache, httpd Host_Alias WEBHOSTS = web*, prodweb01 Cmnd_Alias WEBCMNDS = /sbin/apachectl WEBTEAM WEBHOSTS =( root ) /sbin/apachebackup WEBTEAM WEBHOSTS =( WEBUSERS ) WEBCMNDS Displaying the Sudo Configuration \u00b6 List commands you are allowed to run: sudo -l Verbose listing of commands: sudo -ll List commands another USER is allowed: sudo -l -U user # Sudo configuration export EDITOR = nano visudo # Give Bob the rights to run yum command at the end of the sudoers file bob ALL =( root ) /usr/bin/yum # save and exit sudo -l -U bob # List sudo permissions for bob sudo -ll -U bob # More verbose output su - bob # Login as bob sudo -l # Shows current permissions sudo yum install dstat -y # It will work without password exit su - visudo -f /etc/sudoers.d/bob # Creates a new file inside sudoers.d bob ALL =( ALL ) /usr/bin/whoami # Give permission to run whoami. save and exit su - bob whoami # Gives bob as output sudo -u jason whoami # Pass user as jason who runs whoami and it works as well. # As one user can give another user access to run a command and this is dangerous. # All sudo operations are logged inside /var/log/secure # Allow the \u201cbob\u201d account to run the \u201creboot\u201d command only as the \u201croot\u201d user on the \u201clinuxsvr1\u201d system bob linuxsvr1 =( root ) /sbin/reboot Network Security \u00b6 Network Services \u00b6 Called -Network services, daemons, servers. Listen on network ports. Constantly running in the background. Output recorded in log files. Designed to perform a single task. Securing Network Services \u00b6 Use a dedicated user for each service. Take advantage of privilege separation. Ports below 1024 are privileged. Use root to open them, then drop privileges. Configuration controlled by each service. Stop and uninstall unused services. Avoid unsecure services. Use SSH instead of telnet, rlogin, rsh, and FTP Stay up to date with patches. Install services provided by your distro. Only listen on the required interfaces and addresses. Information Leakage: Avoid revealing information where possible. Examples: Web server banners or information in /etc/motd /etc/issue /etc/issue.net Stop and Disable Services systemctl stop SERVICE systemctl disable SERVICE # Example: systemctl stop httpd systemctl disable httpd List Listening Programs with netstat: netstat -nutlp or netstat -tupan Port Scanning: nmap HOSTNAME_OR_IP or lsof -i nmap localhost nmap 10 .11.12.13 Testing a Specific Port: telnet HOST_OR_ADDRESS PORT or nc -v HOST_OR_ADDRESS PORT Xinetd Controlled Services /etc/xinetd.d/SERVICE # To disable service: disable = yes # To disable xinetd: systemctl stop xinetd systemctl disable xinetd NMAP \u00b6 ########################## ## NMAP ########################## ##** SCAN ONLY YOUR OWN HOSTS AND SERVERS !!! **## ## Scanning Networks is your own responsibility ## # Syn Scan - Half Open Scanning (root only) nmap -sS 192 .168.0.1 # Connect Scan nmap -sT 192 .168.0.1 # Scanning all ports (0-65535) nmap -p- 192 .168.0.1 # Specifying the ports to scan nmap -p 20 ,22-100,443,1000-2000 192 .168.0.1 # Scan Version nmap -p 22 ,80 -sV 192 .168.0.1 # UDP Port scanning nmap -sU localhost # Ping scanning (entire Network) nmap -sP 192 .168.0.0/24 # Treat all hosts as online -- skip host discovery nmap -Pn 192 .168.0.101 # Excluding an IP nmap -sS 192 .168.0.0/24 --exclude 192 .168.0.10 # Saving the scanning report to a file nmap -oN output.txt 192 .168.0.1 # OS Detection nmap -O 192 .168.0.1 # Enable OS detection, version detection, script scanning, and traceroute nmap -A 192 .168.0.1 # https://nmap.org/book/performance-timing-templates.html # -T paranoid|sneaky|polite|normal|aggressive|insane (Set a timing template) # These templates allow the user to specify how aggressive they wish to be, while leaving Nmap to pick the exact # timing values. The templates also make some minor speed adjustments for which fine-grained control options do # not currently exist. # -A OS and service detection with faster execution nmap -A -T aggressive cloudflare.com # Using decoys to evade scan detection nmap -p 22 -sV 192 .168.0.101 -D 192 .168.0.1,192.168.0.21,192.168.0.100 # Where decoy IP is local address 192.168.0.1 and so on # reading the targets from a file (ip/name/network seperated by a new line or a whitespace) nmap -p 80 -iL hosts.txt # Where hosts.txt will contain 4 host 192 .168.0.100 192 .168.0.1 8 .8.8.8 vulnweb.com # google and vulnweb servers on the internet # exporting to out output file and disabling reverse DNS nmap -n -iL hosts.txt -p 80 -oN output.txt Securing SSH \u00b6 SSH = Secure SHell. Allows for key based authentication. Configuration: /etc/ssh/sshd_config # vi /etc/ssh/sshd_config PubkeyAuthentication yes PasswordAuthentication no # Force Key Authentication PermitRootLogin no # To disable root logins PermitRootLogin without-password # To only allow root to login with a key AllowUsers user1 user2 userN # Only Allow Certain Users SSH Access AllowGroups group1 group2 groupN # Only Allow Certain Groups SSH Access DenyUsers user1 user2 userN # Deny Certain Users SSH Access DenyGroups group1 group2 groupN # Deny Certain Groups SSH Access AllowTcpForwarding no # Disable TCP Port Forwarding GatewayPorts no Protocol 2 # Use SSHv2 instead of SSHv1 ListenAddress host_or_address1 # Bind SSH to a Specific Address ListenAddress host_or_addressN # Allow individual IP to connect to SSH, separate line for each IP Port 2222 # Change the Default Port. This masks port scanners Banner none # Disable the Banner, or update the banner in `/etc/issue.net` to remove identifiable data like versions # Reload the SSH Configuration systemctl reload sshd # Add the New Port to SELinux after changing ssh port number semanage port -a -t ssh_port_t -p tcp PORT semanage port -l | grep ssh # Additional information man ssh man sshd man sshd_config ########################## ## OpenSSH Hardening Example ########################## # 1. Installing OpenSSH (client and server) # Ubuntu sudo apt update && sudo apt install openssh-server openssh-client # connecting to the server ssh -p 22 username@server_ip # => Ex: ssh -p 2267 john@192.168.0.100 ssh -p 22 -l username server_ip ssh -v -p 22 username@server_ip # => verbose # 2. Controlling the SSHd daemon # checking its status sudo systemctl status ssh # => Ubuntu sudo systemctl status sshd # => CentOS # stopping the daemon sudo systemctl stop ssh # => Ubuntu sudo systemctl stop sshd # => CentOS # restarting the daemon sudo systemctl restart ssh # => Ubuntu sudo systemctl restart sshd # => CentOS # enabling at boot time sudo systemctl enable ssh # => Ubuntu sudo systemctl enable sshd # => CentOS sudo systemctl is-enabled ssh # => Ubuntu sudo systemctl is-enabled sshd # => CentOS # 3. Securing the SSHd daemon # change the configuration file (/etc/ssh/sshd_config) and then restart the server man sshd_config #a) Change the port Port 2278 #b) Disable direct root login PermitRootLogin no #c) Limit Users\u2019 SSH access AllowUsers stud u1 u2 john #d) Filter SSH access at the firewall level (iptables) #e) Activate Public Key Authentication and Disable Password Authentication #f) Use only SSH Protocol version 2 #g) Other configurations: ClientAliveInterval 300 ClientAliveCountMax 0 MaxAuthTries 2 MaxStartUps 3 LoginGraceTime 20 SSH Port Forwarding \u00b6 Expose service (database) to a client not in same network Forward traffic from client to a service running in remote # Client (Different network) ---> (SSH session on port 3306) ---> Server1 ---> (SSH session on port 22) ---> (Database on 127.0.0.0 and port 3306) ssh -L 3306 :127.0.0.1:3306 server1 # -L = forwarding # Similarly another application can connect to Client on port 3306 and access database on server 1 # Avoid this by having firewall block connections to client # Client (Different network) ---> (SSH session on port 8080) ---> Server1 ---> (SSH session on port 22) ---> Google.com (service on port 80) ssh -L 8080 :google.com:80 server1 # -L = forwarding # In this case google.com will understand that traffic is originating from server1 instead of client Dynamic Port Forwarding / SOCKS \u00b6 # Client (Different network) ---> (SSH session on port 8080) ---> Server1 ---> (SSH session on port 22) ---> (Internal Network) ssh -D 8080 server1 # -D = Dynamic forwarding # This allows users to connect to company network Reverse Port Forwarding \u00b6 Use a proxy to direct traffic to internal network Connect to a desktop inside corporate network to work from home # (Internal Network) (SSH session on port 22) <--- (Desktop having shell program) <--- (SSH session on port 22) (Port 2222 is open to internet) <--- Server1 <--- Client (Different network) ssh -R 2222 :127.0.0.0:22 # -R = Reverse forwarding File Security \u00b6 File Permissions \u00b6 first letter in ls -l output Regular file: - Directory: d Symbolic link: l Read: r Write: w Execute: x Files vs Directory \u00b6 r - Allows a file to be read. Allows file names in a directory to be read w - Allows a file to be modified. Allows entries to be modified within the directory x - Allows a file to be executed. Allows access to the contents and metdadata for entries Groups \u00b6 Every user in linux is in atleast one group Users can belong to many groups group command shows a user's group. Can also use id -Gn # Changing file permissions ls -l sales.data # Long list file permissions chmod g+w sales.data # Add Write permission to group chmod g-w sales.data # Remove Write permission to group chmod u+rwx,g-x sales.data # Add user all permissions and remove execute permission for group chmod a = r sales.data # Give all groups read permissions chmod o = sales.data # removes all permissions for others Directory permissions \u00b6 Permissions on a directory can affect the files in the directory If the file permissions look correct, start checking the directory permissions Work your way up to the root Use chgrp to change group of a file # Changing directory permissions ls -l sales.data # Long list to show current group membership groups # Shows current user's groups (one is user and other is sales group) chgrp sales sales.data # Change file ownership from user to sales group chmod g+w sales.data # Give other members in sales group write access mkdir -p /usr/local/sales # Create a common directory for sales group mv sales.data /usr/local/sales # Now file and directory have the same permissions # Verify directory permissions mkdir -p my-cat # Create new directory touch my-cat/cat # Create new file chmod 755 my-cat # Modify permissions ls -ld my-cat # Shows permissions chmod 400 my-cat # Give only read permissions ls -ld my-cat # Gives permission error chmod 500 my-cat # Give write permission as well ls -ld my-cat # Gives output cat my-cat/cat # Shows file output File Creation Mask \u00b6 File creation mask are set by the admins This can be changed by per user basis File creation mask determines default permissions. If no mask were to be specified: deafault permissions would be 777 for directories and 666 for files umask command \u00b6 Sets the file creation mask to mode, if given Use -S for symbolic notation Format: umask [-S] [mode] mode of umask is opposite to chmod chmod - adds permissions. chmod 660 will give read and write permissions to file umask - subtracts permissions. umask 007 will give read an write permissions to file Common umask modes: 002, 022, 077, 007 # Test umask mkdir testumask cd testumask umask # Shows creation umask 0022 umask -S # Shows umask in symbolic mode mkdir newdir touch newfile ls -l # Shows dir permissions = 0755 and file permissions = 0644 rm newfile rmdir newdir umask 007 # Set new umask mkdir newdir touch newfile ls -l # Shows dir permissions = 0770 and file permissions = 0660 Special Modes \u00b6 Setuid \u00b6 When a process is started, it runs using the starting user's UID and GID. setuid = Set User ID upon execution. Examples of binaries running with root and setuid -rwsr-xr-x 1 root root /usr/bin/passwd # Requires root permissions to modify passwd file ping chsh setuid files are an attack surface. Not honored on shell scripts. Octal Permissions: setuid=4, setgid=2 and sticky=1 # Adding the Setuid Attribute chmod u+s /path/to/file chmod 4755 /path/to/file # Removing the Setuid Attribute chmod u-s /path/to/file chmod 0755 /path/to/file # Finding Setuid Files find / -perm /4000 -ls # ls also lists files # Only the Owner Should Edit Setuid Files Good: Symbolic Octal -rwsr-xr-x 4755 Really bad: -rwsrwxrwx 4777 Setgid \u00b6 setgid = Set Group ID upon execution. -rwxr-sr-x 1 root tty /usr/bin/wall # s - set in group field crw--w---- 1 bob tty /dev/pts/0 # Adding the Setgid Attribute chmod g+s /path/to/file chmod 2755 /path/to/file # Adding the Setuid & Setgid Attributes chmod ug+s /path/to/file chmod 6755 /path/to/file # Removing the Setgid Attribute chmod g-s /path/to/file chmod 0755 /path/to/file # Finding Setgid Files find / -perm /2000 -ls Setgid on Directories \u00b6 setgid on a directory causes new files to inherit the group of the directory. setgid causes directories to inherit the setgid bit. Is not retroactive. Does not apply on existing files, but new files in directories. Great for working with groups. Use an Integrity Checker \u00b6 Other options to find . Tripwire AIDE (Advanced Intrusion Detection Environment) OSSEC Samhain Package managers Sticky Bit \u00b6 Use on a directory to only allow the owner of the file/directory to delete it. Required on files or directories whcih are shared with multiple users or programs # Used on /tmp: drwxrwxrwt 10 root root 4096 Feb 1 09 :47 /tmp # Adding the Sticky Bit chmod o+s /path/to/directory chmod 1777 /path/to/directory # Removing the Sticky Bit chmod o-t /path/to/directory chmod 0777 /path/to/directory Reading ls Output \u00b6 A capitalized special permission means the underlying normal permission is not set. A lowercase special permission means the underlying normal permission set. # execute permission is not set for user -rwSr--r-- 1 root root 0 Feb 14 11 :21 test # execute permission is set for the user -rwsr--r-- 1 root root 0 Feb 14 # execute permission is not set for group -rwxrwSr-- 1 root root 0 Feb 14 11 :21 test # execute permission is not set for others drwxr-xr-T 2 root root 0 Feb 14 11 :30 testd File Attributes \u00b6 xattr: Supported by many file systems. ext2, ext3, ext4, XFS Attribute: i immutable \u00b6 The file cannot be: modified, deleted, renamed, hard linked to Unset the attribute in order to delete it. Attribute: a append \u00b6 Append only. Existing contents cannot be modified. Cannot be deleted while attribute is set. Use this attribute on log files. Safeguard the audit trail. Other Attributes \u00b6 Not every attribute is supported. man ext4, man xfs, man brtfs, etc. Example: s secure deletion Modifying Attributes \u00b6 Use the chattr command. adds attributes. removes attributes. = sets the exact attributes. # Making the hosts file immutable lsattr /etc/hosts # No attributes present chattr +i /etc/hosts # Add immutable attribute vi /etc/hosts # Not able to write and save rm /etc/hosts # Not able to delete the file chattr -i /etc/hosts # Remove immutable attribute lsattr /etc/hosts # No attributes present vi /etc/hosts # Now able to write and save chattr +i /etc/hosts # Add immutable attribute back again # Making the apache logs files append only cd /var/log/httpd chattr = a * # Adding append attribute to all the files inside lsattr echo test >> access_log # Data will be appended cat access_log vi access_log # Not able to write and save Access Control Lists \u00b6 ACL = Access Control List Provides additional control Example: Give one user access to a file. Traditional solution is to create another group. Increases management overhead of groups. Ensure file system mounted with ACL support XFS supports this by default # Mount files with ACL support to get this feature mount -o acl /path/to/dev /path/to/mount tune2fs -o acl /path/to/dev tune2fs -l /path/to/dev | grep options # Verify ACL Types of ACLs \u00b6 Access - Control access to a specific file or directory. Default - Used on directories only. Files without access rules use the default ACL rules. Not retroactive. Optional. ACLs Can Be Configured: \u00b6 Per user Per group For users not in the file\u2019s group Via the effective rights mask Creating ACLs \u00b6 Use the setfacl command. May need to install the ACL tools. Modify or add ACLs: setfacl -m ACL FILE_OR_DIRECTORY Detecting Files with ACLs: + at the end of ls -l permissions output # User ACLs / Rules # u:uid:perms Set the access ACL for a user. setfacl -m u:jason:rwx start.sh setfacl -m u:sam:xr start.sh # Group ACLs / Rules # g:gid:perms Sets the access ACL for a group. setfacl -m g:sales:rw sales.txt # Mask ACLs / Rules # m:perms Sets the effective rights mask. setfacl -m m:rx sales.txt # Other ACLs / Rules # o:perms Sets the access ACL for others. setfacl -m o:r sales.txt # Creating Multiple ACLs at Once setfacl -m u:bob:r,g:sales:rw sales.txt # Default ACLs # d:[ugo]:perms Sets the default ACL. setfacl -m d:g:sales:rw sales # Setting ACLs Recursively (-R) setfacl -R -m g:sales:rw sales # Removing ACLs for particular group or user setfacl -x ACL FILE_OR_DIRECTORY setfacl -x u:jason sales.txt # Note: Not supposed to give permissions when deleting ACL setfacl -x g:sales sales.txt # Removing All ACLs setfacl -b FILE_OR_DIRECTORY setfacl -b sales.txt # Viewing ACLs getfacl sales.txt # ACL for a shared project directory # Logged in as root user mkdir /usr/local/project cd /usr/local chgrp project project/ chmod 775 project/ # Give the project members access to the directory ls -ld project/ # Directory owned by root and project members have access su - sam # Switch user groups # same is in project group cd usr/local/project echo stuff > notes.txt ls -l notes.txt # sam is the owner of this file # Give bob who is not a member of project group access to this file setfacl -m u:bob:rw notes.txt # Modify access by sam to give read and write access getfacl notes.txt # List the permisions for bob su -bob # Switch user vi notes.txt # bob is able to write su -sam # Switch user setfacl -x u:bob notes.txt # Remove the rw permissions getfacl notes.txt # No permissions visble for bob su - # Switch to root cd project touch root-was-here # Create a new file owned by root in shared directory ls -l # No ACL is applied even though its in shared directory su sam # Switch to group user echo >> root-was-here # Permission denied su - # Switch to root cd /usr/local/project setfacl -m d:g:project:rw . # Set default ACL to project directory ls -ld . # Default ACL are added to directory denoted by + getfacl # Default ACL are added touch test # Create new file getfacl su sam vi test # Edit is possible setfacl -R -m g:project:rw . # Add recursively permissions to root-was-here file as well Cracking Passwords \u00b6 # CRACKING PASSWORD HASHES USING JOHN THE RIPPER # Installing JTR apt install john # combining /etc/passwd and /etc/shadow in a single file unshadow /etc/passwd /etc/shadow > unshadowed.txt # cracking in single mode john -single unshadowed.txt # brute-force and dictionary attack john --wordlist = /usr/share/john/password.lst --rules unshadowed.txt # dictionary files: # /usr/share/dict # /usr/share/metasploit-framework/data/wordlists # -> on Kali Linux # showing cracked hashes (~/.john/john.pot) john --show unshadowed.txt # to continue an interrupted (ctrl+c) session, run in the same directory: john -restore # cracking only accounts with specific shells (valid shells) john --wordlist = mydict.txt --rules --shell = bash,sh unshadowed.txt # cracking only some accounts john --wordlist = mydict.txt --rules --users = admin,mark unshadowed.txt # cracking in incremental mode (/etc/john/john.conf) john --incremental unshadowed.txt Rootkits \u00b6 Software used to gain root access and remain undetected. They attempt to hide from system administrators and antivirus software. User space rootkits replace common commands such as ls, ps, find, netstat, etc Kernel space rootkits add or replace parts of the core operating system. Rootkit Detection \u00b6 Use a file integrity checker for user space rootkits. (AIDE, tripwire, OSSEC, etc.) Identify inconsistent behavior of a system. High CPU utilization without corresponding processes. High network load or unusual connections. Kernel mode rootkits have to be running in order to hide themselves. Halt the system and examine the storage. Use a known good operating system to do the investigation. Use bootable media, for example. Rootkit Hunter / RKHunter /ChkRootKit \u00b6 Shell script that searches for rootkits. rkhunter --update # Update database rkhunter --propupd # Update RKHunter configurations rkhunter -c # Interactive Check Mode cat /var/log/rkhunter.log # Log file location rkhunter -c --rwo # Report only warnings rkhunter --cronjob # Run as cronjob # After a fresh installation of OS # Install RK Hunter and then update the properties and database # Configure it to run everyday and report any issues by configuring it to log all output in a central place # Configure Cronjob crontab -e # Create a new cronjob # At the end run at modnight everyday # Redirect the output to single file 0 0 * * * /usr/local/bin/rkhunter --cronjob --update > /var/tmp/rkhunter.cronlog 2 > & 1 # installing chkrootkit apt install chkrootkit # running a scan chkrootkit chkrootkit -q # Reports only warnings OSSEC \u00b6 Host Intrusion Detection System (HIDS) More than just rookit detection: log analysis, file integrity checking, alerting. Syscheck module - user mode rootkit detection. Rootcheck module - both user mode and kernel mode rootkit detection. Rootkit Removal \u00b6 Keep a copy of the data if possible. Learn how to keep it from happening again. Reinstall core OS components and start investigating. Not recommended. Easy to make a mistake. Safest is to reinstall the OS from trusted media. Rootkit Prevention \u00b6 Use good security practices: Physical, Account, Network Use file integrity monitoring: AIDE, Tripware, OSSEC Keep your systems patched. AIDE - Advanced Intrusion Detection \u00b6 What security incident can AIDE detect? A virus installed in system directory A configuration file that was changed by a hacker A command that was replaced by a hacker # installing AIDE apt update && apt install aide aide -v # getting help aide --help /etc/aide/aide.conf # => config file ### SEARCHING FOR CHANGES ### # initializing the AIDE database => /var/lib/aide/aide.db.new aideinit # moving the db to the one that will be checked by AIDE mv /var/lib/aide/aide.db.new /var/lib/aide/aide.db # creating a runtime config file => /var/lib/aide/aide.conf.autogenerated update-aide.conf # this is a command to run # detecting changes aide -c /var/lib/aide/aide.conf.autogenerated --check > report.txt # updating the db aide -c /var/lib/aide/aide.conf.autogenerated --update # copying the newly created database as the baseline database cp /var/lib/aide/aide.db.new /var/lib/aide/aide.db ## CREATING A CUSTOM aide.conf FILE (Example: /root/aide.conf) ## database = file:/var/lib/aide/aide.db database_out = file:/var/lib/aide/aide.db.new MYRULE = u+g+p+n+s+m+sha256 /etc MYRULE /usr MYRULE /root MYRULE !/usr/.* !/usr/share/file$ # initializing the new AIDE db aide -c /root/aide.conf --init # moving the new db to the baseline db mv /var/lib/aide/aide.db.new /var/lib/aide/aide.db # checking for any changes aide -c /root/aide.conf --check # --limit option aide -c /root/aide.conf --limit /etc --check # AIDE will check for changes only in /etc directory Anitvirus - ClamAV \u00b6 Virus is not an issue for Linux, but it can be a host to spread Malware and Virus to Windows machines in the network. To protect Windows machines from themselves, install antivirus solution on Linux. # installing clamav sudo apt install && sudo apt install clamav clamav-daemon # checking the status systemctl status clamav-freshclam systemctl status clamav-daemon # starting the clamav daemon systemctl start clamav-daemon # enabling the daemon to start and boot systemctl enable clamav-daemon # getting a test virus wget www.eicar.org/download/eicar.com # scanning a directory using clamdscan clamdscan --fdpass /root/ # moving found viruses to a quarantine directory clamdscan --move = /quarantine --fdpass /root # scanning a directory using clamscan clamscan --recursive /etc clamscan --recursive --infected /quarantine clamscan --recursive --infected --remove /quarantine/","title":"Linux_Security"},{"location":"learning/linux/security/#introduction","text":"Linux Security Confernce Videos Hardening Guides from CIS, DISA, Ubuntu Add Ubuntu mailing list for security updates","title":"Introduction"},{"location":"learning/linux/security/#linux-is-only-as-secure-as-you-make-it","text":"# I trust `ME` more than anyone else with my Data, Infrastructure, Time and Value - Nothing is perfectly secure. - Security is a series of trade-offs. - convenience vs security # No passwords = easy to use, not secure. # System powered off = secure, not usable. - Examples: - Linux can be configured to be unsecure. - Users may employ lax file permissions. - System administration mistakes. - Users could use easy to guess passwords. - Data transmitted in the clear. - Malicious software installed on the system. - Lack of training or security awareness.","title":"Linux is only as secure as you make it!"},{"location":"learning/linux/security/#continous-improvement","text":"Just because you are using Linux, doesn\u2019t mean you are \u201csecure.\u201d Security is an ongoing process. Stay vigilant!","title":"Continous Improvement"},{"location":"learning/linux/security/#risk-assessment","text":"What is the severity of the risk? What is the probability of the risk occurring? What is the cost to mitigate the risk? What is the effectiveness of the countermeasure?","title":"Risk Assessment"},{"location":"learning/linux/security/#multiuser-system","text":"Linux is a multiuser system. The superuser is the root account. root is all powerful. Required to install system-wide software, configure networking, manager users, etc. All other accounts are \u201cnormal\u201d accounts. Can be used by people or applications (services).","title":"Multiuser System"},{"location":"learning/linux/security/#advantages-to-a-multiuser-system","text":"","title":"Advantages to a Multiuser System."},{"location":"learning/linux/security/#file-permissions","text":"Every file has an owner. Permissions can be granted to other accounts and users as needed. Breaking into one account does not necessarily compromise the entire system.","title":"File permissions"},{"location":"learning/linux/security/#process-permissions","text":"Every process has an owner. Each account can manage their processes. Exception to above rule: root can do anything Breaking into one account does not necessarily compromise the entire system.","title":"Process permissions."},{"location":"learning/linux/security/#security-guidelines","text":"Principle of Least Privilege Use encryption Shared accounts (Yes, root can be a shared account!) Multifactor authentication Firewall Monitoring logs","title":"Security Guidelines"},{"location":"learning/linux/security/#minimize-software-and-services","text":"If you don\u2019t need a piece of software, don\u2019t install it. If you don\u2019t need a service, don\u2019t start it. If you no longer need the software or service, stop and uninstall it.","title":"Minimize Software and Services"},{"location":"learning/linux/security/#run-services-on-separate-systems","text":"Minimizes the risk of one compromised service leading to other compromised services.","title":"Run Services on Separate Systems"},{"location":"learning/linux/security/#encrypt-data-transmissions","text":"Protect against eavesdropping and man-in-the middle attacks. Examples: Protocol \u2192 Replace with FTP \u2192 SFTP telnet \u2192 SSH SNMP v1/v2 \u2192 SNMP v3 HTTP \u2192 HTTPS","title":"Encrypt Data Transmissions"},{"location":"learning/linux/security/#avoid-shared-accounts","text":"Each person should have their own account. Each service should have its own account. Shared accounts make security auditing difficult. Lack of accountability with shared accounts.","title":"Avoid Shared Accounts"},{"location":"learning/linux/security/#avoid-direct-root-logins","text":"Do not allow direct login of shared accounts. Users must login to their personal accounts and then switch to the shared account. Control and monitor access with sudo.","title":"Avoid Direct root Logins"},{"location":"learning/linux/security/#maintain-accounts","text":"Create and use a process for removing access.","title":"Maintain Accounts"},{"location":"learning/linux/security/#use-multifactor-authentication","text":"Something you know (password) + something you have (phone) or something you are(fingerprints). Examples: account password + phone to receive the one time password (OTP). account password + fingerprint","title":"Use Multifactor Authentication"},{"location":"learning/linux/security/#the-principle-of-least-privilege","text":"AKA, the Principle of Least Authority. Examples: Only use root privileges when required. Avoid running services as the root user. Use restrictive permissions that allow people and services enough access to do their jobs.","title":"The Principle of Least Privilege"},{"location":"learning/linux/security/#monitor-system-activity","text":"Routinely review logs. Send logs to a central logging system.","title":"Monitor System Activity"},{"location":"learning/linux/security/#use-a-firewall","text":"Linux has a built-in firewall. Netfilters + iptables. Only allow network connections from desired sources.","title":"Use a Firewall"},{"location":"learning/linux/security/#encrypt-your-data","text":"Encryption protects your data while it is \u201cat rest\u201d (on disk).","title":"Encrypt Your Data"},{"location":"learning/linux/security/#physical-security","text":"","title":"Physical Security"},{"location":"learning/linux/security/#physical-security-is-linux-security","text":"Physical access poses a great security threat to your Linux system! Single user mode - Allows unrestricted access. Only allow physical access when necessary.","title":"Physical Security Is Linux Security"},{"location":"learning/linux/security/#systems-not-under-your-control","text":"Data centers / colos - Like \u201cbanks\u201d of data. Possible targets for attackers Needs processes, procedures, and controls in place toprotect your valuable data.","title":"Systems Not Under Your Control"},{"location":"learning/linux/security/#cloud","text":"At some point the cloud is real equipment. Physical security is still important. Your data is on their storage systems. The provider has access to your virtual disks. If encryption is available, use it.","title":"Cloud"},{"location":"learning/linux/security/#protecting-linux-against-physical-attacks","text":"Gaining Access to a Linux System: Single User Mode & Power Resets Changing the shell command from sushell to sulogin will prompt for root password when entrering into single user mode # Securing Single User Mode and blank passwords by having root password at logins # Login to root and gain access to shell echo $$ # Shows the current process id i.e shell ps -fp <pid> # Shows detailed information about shell process including command executed at login cd /lib/systemd/system grep sulogin emergency.target # No output should be visible grep sulogin emergency.service # Should have sulogin in ExecStart command head -1 /etc/shadow # Shows id root password is set or not. If not set (!) is present in output 2nd column passwd # Set root password","title":"Protecting Linux Against Physical Attacks"},{"location":"learning/linux/security/#securing-the-boot-loader","text":"To prevent a person who as physical access from passing arguments to the Linux kernel at boot time, you should password protect the boot loader. Check examples how to secure this.","title":"Securing the Boot Loader"},{"location":"learning/linux/security/#disk-encryption","text":"","title":"Disk Encryption"},{"location":"learning/linux/security/#dm-crypt","text":"device mapper crypt Provides transparent disk encryption. Creates a new device in /dev/mapper. Use like any other block device. Manage with cryptsetup","title":"dm-crypt"},{"location":"learning/linux/security/#luks","text":"Linux Unified Key Setup. Front-end for dm-crypt. Multiple passphrase support. Portable as LUKS stores setup information in the partition header. Great for removable media, too.","title":"LUKS"},{"location":"learning/linux/security/#encrypt-during-install","text":"PRO: easy, with sane defaults. CON: you give up some control.","title":"Encrypt During Install"},{"location":"learning/linux/security/#setting-up-luks-on-a-new-device","text":"Use this process for any block device presented to your system that you want to encrypt. Following this procedure will remove all data on the partition (device) that you are encrypting!","title":"Setting up LUKS on a New Device"},{"location":"learning/linux/security/#implementing-luks-for-full-disk-encryption","text":"example Laptops, USB sudo apt-get install cryptsetup # Install LUKS tools lsblk # list the blocks sudo shread -v -n 1 /dev/sdb # Writes random data to he device sdb sudo fdisk /dev/sdb # partition the sdb drive and create a new partition called sdb1 sudo cryptsetup \u2013y luksFormat /dev/sdb1 # Allows to store encrypted data in this partition. Give a passphrase sudo cryptsetup luksDump /dev/sdb1 # Shows details of the encrypted partition sudo cryptsetup luksOpen /dev/sdb1 private_data # Assign a mapper called private_data and opens the device ls /dev/mapper # Shows the new mapper setup and that is a device ls -arlt /dev/mapper | tail # Shows the virtual block devices private_data sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted # Make an entry in the /etc/fstab to mount private_data to /mnt on each boot # Make an entry in /etc/crypttab to mount private_data at boot time # To close the encrypted device sudo cryptsetup luksClose private_data","title":"Implementing LUKS for Full Disk Encryption"},{"location":"learning/linux/security/#encrypting-device-in-cloud","text":"Sometimes cloud providers do not give block level access to volumes. For such cases, we will encrypt the files like we do for volumes sudo mkdir /data sudo fallocate -l 100M /data/private_data # Creates a non sparse file sudo strings /data/private_data # Shows any string data in the file. It mostly is blank # Write random data, if=input, of=output, bs=byte size<1 Mb>, count=<size of file i.e 100Mb> sudo dd if = /dev/urandom of = /data_private_data bs = 1M count = 100 sudo strings /data/private_data sudo cryptsetup \u2013y luksFormat /data/private_data # Allows to store encrypted data in this file. Give a passphrase sudo cryptsetup luksOpen /data/private_data private_data # Assign a mapper called private_data and opens the device ls /dev/mapper # Shows the new mapper setup and that is a device ls -arlt /dev/mapper | tail # Shows the virtual block devices private_data sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted sudo df -h /mnt # Shows the size # Make an entry in the /etc/fstab to mount private_data to /mnt on each boot # Make an entry in /etc/crypttab to mount private_data at boot time","title":"Encrypting device in Cloud"},{"location":"learning/linux/security/#converting-an-existing-device-to-luks","text":"# Backup the data. /home lives on /dev/sda3 # for example. # Wipe the device. # use shred or dd if=/dev/urandom of=/dev/sda3 # Setup LUKS. cryptsetup luksFormat /dev/sda3 cryptsetup luksOpen /dev/sda3 home mkfs -t ext4 /dev/mapper/home mount /dev/mapper/home /home # & restore from backup","title":"Converting an Existing Device to LUKS"},{"location":"learning/linux/security/#disabling-ctrlaltdel-systemd","text":"Attackers can gain access to the virtual terminal and send command ctrl+alt+delete to reboot the system. # Disabling reboot using ctrl-alt-delete command over remote connection systemctl mask ctrl-alt-del.target systemctl daemon-reload","title":"Disabling Ctrl+Alt+Del (Systemd)"},{"location":"learning/linux/security/#account-security","text":"It's easier to attack a system from the inside. Privilege escalation attacks are a threat.","title":"Account Security"},{"location":"learning/linux/security/#mitigation","text":"Keep unwanted users out. Secure accounts.","title":"Mitigation"},{"location":"learning/linux/security/#pam","text":"Pluggable Authentication Modules Used to delegate / abstract authentication of services / programs like login or sshd","title":"PAM"},{"location":"learning/linux/security/#pam-configuration-files","text":"Location: /etc/pam.d Configuration file for login is /etc/pam.d/login Configuration file for sshd is /etc/pam.d/sshd Format: module_interface control_flag module_name module_args","title":"PAM Configuration files"},{"location":"learning/linux/security/#pam-module-interfaces","text":"auth - Authenticates users. account - Verifies if access is permitted. password - Changes a user\u2019s password. session - Manages user\u2019s sessions.","title":"PAM Module Interfaces"},{"location":"learning/linux/security/#pam-control-flags","text":"required - Module result must be successful to continue. requisite - Like required, but no other modules are invoked. sufficient - Authenticates user if no required modules have failed, otherwise ignored. optional - Only used when no other modules reference the interface. include - Includes configuration from another file. complex control flags - attribute=value","title":"PAM Control Flags"},{"location":"learning/linux/security/#pam-configuration-example","text":"The directives listed in the PAM module are executed in sequential order *.so extension stands for shared objects #%PAM-1.0 # Comment auth required pam_securetty.so # 3 auth modules which need to pass auth required pam_unix.so nullok auth required pam_nologin.so account required pam_unix.so # Checks if the user account is valid password required pam_pwquality.so retry = 3 # Checks for password quality if acount has expired and gives 3 tries to set password password required pam_unix.so shadow \\ # Allows to use shadow file nullok use_authtok session required pam_unix.so # Logs when user logs in and out of the system","title":"PAM Configuration Example"},{"location":"learning/linux/security/#pam-documentation","text":"Configuration: account required pam_nologin.so session required pam_unix.so Getting help, drop the .so extension and use the man page to get additional help: man pam_nologin man pam_unix","title":"PAM Documentation"},{"location":"learning/linux/security/#linux-account-types","text":"","title":"Linux Account Types"},{"location":"learning/linux/security/#root-the-superuser","text":"Root can do anything. Always has the UID of 0.","title":"root, the superuser"},{"location":"learning/linux/security/#system-accounts","text":"UIDs < 1,000 Configured in /etc/login.defs useradd -r system_account_name","title":"System accounts"},{"location":"learning/linux/security/#normal-user-accounts","text":"UIDs >= 1,000 Intended for human (interactive) use","title":"Normal User Accounts"},{"location":"learning/linux/security/#password-security","text":"Enforce, not hope for, strong passwords. Use pam_pwquality, based on pam_cracklib. Configuration File: /etc/security/pwquality.conf PAM Usage: password requisite pam_pwquality.so Module attributes: man pam_pwquality # /etc/login.defs format # PASS_MAX_DAYS 99999 # PASS_MIN_DAYS 0 # PASS_MIN_LEN 5 # PASS_WARN_AGE 7","title":"Password Security"},{"location":"learning/linux/security/#use-shadow-passwords","text":"/etc/passwd unencrypted: root:$6$L3ZSmlM1H5:0:0:root:/root:/bin/bash /etc/passwd with shadow passwords: root:x:0:0:root:/root:/bin/bash /etc/shadow: root:$6$L3ZSmlM1H5::0:99999:7:::","title":"Use Shadow Passwords"},{"location":"learning/linux/security/#converting-passwords","text":"pwconv - convert to shadow passwords. pwunconv - convert from shadow passwords.","title":"Converting Passwords"},{"location":"learning/linux/security/#etcshadow-format","text":"Username Hashed password Days since epoch of last password change Days until change allowed Days before change required Days warning for expiration Days before account inactive Days since epoch when account expires Reserved","title":"/etc/shadow format"},{"location":"learning/linux/security/#display-user-account-expiry-info-with-chage","text":"chage -l <account-name> # Show account aging info. $ chage -l jason # Last password change : Apr 01, 2016 # Password expires : never # Password inactive : never # Account expires : never # Minimum number of days between password change : 0 # Maximum number of days between password change : 99999 # Number of days of warning before password expires : 7","title":"Display user account expiry info with chage"},{"location":"learning/linux/security/#change-user-account-expiry-info-with-chage","text":"-M MAX_DAYS - Set the maximum number of days during which a password is valid. -E EXPIRE_DATE - Date on which the user\u2019s account will no longer be accessible. -d LAST_DAY - Set the last day the password was changed.","title":"Change user account expiry info with chage"},{"location":"learning/linux/security/#demo-to-change-normal-account-to-root","text":"head -n 1 /etc/passwd # Shows the root entry, UID is 3rd field delimited by : sudo useradd jim # Create normal account sudo passwd jim # Change password su - jim # Login to account whoami # Show logged in user details exit sudo vi /etc/passwd # Edit the UID of jim to 0 su - jim id whoami # Now jim account shows root # Show how many users have UID of 0 awk -F: '($3 == ' 0 ')' { print } # Delimit Field by :, take the 3rd field and check if its 0 and print the line # This will show 2 entries, one for root and other for jim # Undo the change by editing `/etc/passwd` and updating jim's UID to original","title":"Demo to change normal account to root"},{"location":"learning/linux/security/#controlling-account-access","text":"","title":"Controlling Account Access"},{"location":"learning/linux/security/#locking-and-unlocking-accounts","text":"passwd -l account passwd -u account","title":"Locking and Unlocking accounts"},{"location":"learning/linux/security/#disabling-logins-for-system-and-root-accounts","text":"Locking with nologin as the Shell # Example /etc/passwd entries: for apache and www-data system accounts apache:x:48:48:Apache:/usr/share/httpd:/sbin/nologin www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin # Using chsh : chsh -s SHELL ACCOUNT chsh -s /sbin/nologin jason # Does not allow jason user to login using password","title":"Disabling logins for system and root accounts"},{"location":"learning/linux/security/#centralized-authentication","text":"Easy to manage users system-wide - lock account everywhere Example authentication systems: freeIPA, LDAP (openLDAP) Has drawbacks too.","title":"Centralized Authentication"},{"location":"learning/linux/security/#disable-logins","text":"pam_nologin module Looks for /etc/nologin or /var/run/nologin Disables logins and displays contents of nologin file.","title":"Disable Logins"},{"location":"learning/linux/security/#monitoring-authentication-logs","text":"last # All login data lastb # Failed authentication lastlog # Last logins # Depends on syslog configuration, logs are stored in following files: /var/log/messages /var/log/syslog /var/log/secure /var/log/auth.log","title":"Monitoring Authentication Logs"},{"location":"learning/linux/security/#intrusion-prevention-with-fail2ban","text":"Monitors log files. Blocks IP address of attacker. Automatic unban. Not just for Linux logins.","title":"Intrusion Prevention with fail2ban"},{"location":"learning/linux/security/#multifactor-authentication","text":"Google Authenticator PAM module DuoSecurity\u2019s pam_duo module RSA SecurID PAM module","title":"Multifactor Authentication"},{"location":"learning/linux/security/#security-by-account-type","text":"","title":"Security by Account Type"},{"location":"learning/linux/security/#account-security---root","text":"Use a normal account for normal activities. Avoid logging in as root. Use sudo instead of su. Avoid using the same root password. Ensure only the root account has UID 0 - awk -F: '($3 == \"0\") {print}' /etc/passwd","title":"Account Security - root"},{"location":"learning/linux/security/#disabling-root-logins","text":"/etc/securetty - Controls root logins using terminals. Normal user logins don't use this file # pam_securetty module - /etc/pam.d/login auth [ user_unknown = ignore success = ok ignore = ignore default = bad ] pam_securetty.so # Shows pam_securetty module is used w # Shows the current terminal, assume tty1 vi /etc/securetty # Remove tty1 entry from this file and save # login to system as root and it fails as tty1 (first terminal) has been removed from logging # Alt+Ctrl+F2 (This will use tty2 to login to the system instead of tty1 as that is no longer valid) # Similarly F3 for tty3, F4 for tty4 and so on # Now login as root and it works as tty2 is present in /etc/securetty file # Empty the securetty file of all entries and save it. # Now there is no way root can login to this system. # Login using a normal account and that will work","title":"Disabling root Logins"},{"location":"learning/linux/security/#system--application-accounts","text":"Use one account per service - web service (httpd), web service account (apache) Don\u2019t activate the account. Don\u2019t allow direct logins from the account - sshd_config: DenyUsers account1 accountN Use sudo for all access. sudo -u apache apachectl configtest","title":"System / Application Accounts"},{"location":"learning/linux/security/#user-accounts","text":"One account per person.","title":"User Accounts"},{"location":"learning/linux/security/#deleting-accounts","text":"Determine the UID - id ACCOUNT Delete their account and home directory - userdel -r Find other files that belong to them on the system. find / -user UID find / -nouser","title":"Deleting Accounts"},{"location":"learning/linux/security/#using-and-configuring-sudo","text":"","title":"Using and Configuring Sudo"},{"location":"learning/linux/security/#sudo-vs-su","text":"\u201cSuperUser Do\u201d or \u201cSubstitute User Do\u201d Use instead of the su command. Complete shell access with su. With su you need to know the password of the other account. Breaks the Principle of Least Privilege. Vague audit trail with su.","title":"sudo vs su"},{"location":"learning/linux/security/#sudo-super-user-do","text":"Elevation of Privileges - Giving users temporary root priviledges Fine grain controls. No need to share passwords. Clear audit trail. grep sudo /etc/group # Shows users in the sudo group at account setup # Once the access to root is granted, it is cached for 15 mins on the same terminal. sudo -k # Invalidates the cache, so next time password is asked again.","title":"Sudo (Super User Do)"},{"location":"learning/linux/security/#sudoers-format","text":"Sudoers configiration is stored in /etc/sudoers.d file. Change the default editor used by visudo using sudo update-alternatives --config editor and select 3 for vim basic. To avoid corrupting this file, open it in visudo editor as it validates the configuration before saving the file. User Specification Format: user host=(run_as) command # Examples: # User Priviledge jason webdev01 =( root ) /sbin/apachectl # Group Priviledge (Starts with %) %web web* =( root ) /sbin/apachectl %wheel ALL =( ALL ) ALL The last line in the file, includes permissions for other sudo users to make it maintainable. @includedir /etc/sudoers.d","title":"Sudoers Format"},{"location":"learning/linux/security/#sudo-authentication","text":"Sudo requires a user to authenticate. Default 5 minute grace period (timeout). You may not want to use a password. apache web* =( root ) NOPASSWD:/sbin/backup-web, # No password required to run backup-web PASSWD:/sbin/apachectl # Password required for apache","title":"Sudo Authentication"},{"location":"learning/linux/security/#sudo-aliases","text":"User_Alias Runas_Alias Host_Alias Cmnd_Alias Format: Alias_Type NAME=item1,item2, ... # Add normal users to group webteam User_Alias WEBTEAM = jason,bob # Give permission to group WEBTEAM web* =( root ) /sbin/apachectl WEBTEAM web* =( apache ) /sbin/apachebackup # Run permissions to system accounts Runas_Alias WEBUSERS = apache,httpd WEBTEAM web* =( WEBUSERS ) /sbin/apachectl # Host permissions to user accounts Host_Alias WEBHOSTS = web*,prodweb01 WEBTEAM WEBHOSTS =( WEBUSERS ) /sbin/apachectl # Command permissions Cmnd_Alias WEBCMNDS = /sbin/apachectl WEBTEAM WEBHOSTS =( WEBUSERS ) WEBCMNDS # Optimized sudoers configuration User_Alias WEBTEAM = jason, bob Runas_Alias WEBUSERS = apache, httpd Host_Alias WEBHOSTS = web*, prodweb01 Cmnd_Alias WEBCMNDS = /sbin/apachectl WEBTEAM WEBHOSTS =( root ) /sbin/apachebackup WEBTEAM WEBHOSTS =( WEBUSERS ) WEBCMNDS","title":"Sudo Aliases"},{"location":"learning/linux/security/#displaying-the-sudo-configuration","text":"List commands you are allowed to run: sudo -l Verbose listing of commands: sudo -ll List commands another USER is allowed: sudo -l -U user # Sudo configuration export EDITOR = nano visudo # Give Bob the rights to run yum command at the end of the sudoers file bob ALL =( root ) /usr/bin/yum # save and exit sudo -l -U bob # List sudo permissions for bob sudo -ll -U bob # More verbose output su - bob # Login as bob sudo -l # Shows current permissions sudo yum install dstat -y # It will work without password exit su - visudo -f /etc/sudoers.d/bob # Creates a new file inside sudoers.d bob ALL =( ALL ) /usr/bin/whoami # Give permission to run whoami. save and exit su - bob whoami # Gives bob as output sudo -u jason whoami # Pass user as jason who runs whoami and it works as well. # As one user can give another user access to run a command and this is dangerous. # All sudo operations are logged inside /var/log/secure # Allow the \u201cbob\u201d account to run the \u201creboot\u201d command only as the \u201croot\u201d user on the \u201clinuxsvr1\u201d system bob linuxsvr1 =( root ) /sbin/reboot","title":"Displaying the Sudo Configuration"},{"location":"learning/linux/security/#network-security","text":"","title":"Network Security"},{"location":"learning/linux/security/#network-services","text":"Called -Network services, daemons, servers. Listen on network ports. Constantly running in the background. Output recorded in log files. Designed to perform a single task.","title":"Network Services"},{"location":"learning/linux/security/#securing-network-services","text":"Use a dedicated user for each service. Take advantage of privilege separation. Ports below 1024 are privileged. Use root to open them, then drop privileges. Configuration controlled by each service. Stop and uninstall unused services. Avoid unsecure services. Use SSH instead of telnet, rlogin, rsh, and FTP Stay up to date with patches. Install services provided by your distro. Only listen on the required interfaces and addresses. Information Leakage: Avoid revealing information where possible. Examples: Web server banners or information in /etc/motd /etc/issue /etc/issue.net Stop and Disable Services systemctl stop SERVICE systemctl disable SERVICE # Example: systemctl stop httpd systemctl disable httpd List Listening Programs with netstat: netstat -nutlp or netstat -tupan Port Scanning: nmap HOSTNAME_OR_IP or lsof -i nmap localhost nmap 10 .11.12.13 Testing a Specific Port: telnet HOST_OR_ADDRESS PORT or nc -v HOST_OR_ADDRESS PORT Xinetd Controlled Services /etc/xinetd.d/SERVICE # To disable service: disable = yes # To disable xinetd: systemctl stop xinetd systemctl disable xinetd","title":"Securing Network Services"},{"location":"learning/linux/security/#nmap","text":"########################## ## NMAP ########################## ##** SCAN ONLY YOUR OWN HOSTS AND SERVERS !!! **## ## Scanning Networks is your own responsibility ## # Syn Scan - Half Open Scanning (root only) nmap -sS 192 .168.0.1 # Connect Scan nmap -sT 192 .168.0.1 # Scanning all ports (0-65535) nmap -p- 192 .168.0.1 # Specifying the ports to scan nmap -p 20 ,22-100,443,1000-2000 192 .168.0.1 # Scan Version nmap -p 22 ,80 -sV 192 .168.0.1 # UDP Port scanning nmap -sU localhost # Ping scanning (entire Network) nmap -sP 192 .168.0.0/24 # Treat all hosts as online -- skip host discovery nmap -Pn 192 .168.0.101 # Excluding an IP nmap -sS 192 .168.0.0/24 --exclude 192 .168.0.10 # Saving the scanning report to a file nmap -oN output.txt 192 .168.0.1 # OS Detection nmap -O 192 .168.0.1 # Enable OS detection, version detection, script scanning, and traceroute nmap -A 192 .168.0.1 # https://nmap.org/book/performance-timing-templates.html # -T paranoid|sneaky|polite|normal|aggressive|insane (Set a timing template) # These templates allow the user to specify how aggressive they wish to be, while leaving Nmap to pick the exact # timing values. The templates also make some minor speed adjustments for which fine-grained control options do # not currently exist. # -A OS and service detection with faster execution nmap -A -T aggressive cloudflare.com # Using decoys to evade scan detection nmap -p 22 -sV 192 .168.0.101 -D 192 .168.0.1,192.168.0.21,192.168.0.100 # Where decoy IP is local address 192.168.0.1 and so on # reading the targets from a file (ip/name/network seperated by a new line or a whitespace) nmap -p 80 -iL hosts.txt # Where hosts.txt will contain 4 host 192 .168.0.100 192 .168.0.1 8 .8.8.8 vulnweb.com # google and vulnweb servers on the internet # exporting to out output file and disabling reverse DNS nmap -n -iL hosts.txt -p 80 -oN output.txt","title":"NMAP"},{"location":"learning/linux/security/#securing-ssh","text":"SSH = Secure SHell. Allows for key based authentication. Configuration: /etc/ssh/sshd_config # vi /etc/ssh/sshd_config PubkeyAuthentication yes PasswordAuthentication no # Force Key Authentication PermitRootLogin no # To disable root logins PermitRootLogin without-password # To only allow root to login with a key AllowUsers user1 user2 userN # Only Allow Certain Users SSH Access AllowGroups group1 group2 groupN # Only Allow Certain Groups SSH Access DenyUsers user1 user2 userN # Deny Certain Users SSH Access DenyGroups group1 group2 groupN # Deny Certain Groups SSH Access AllowTcpForwarding no # Disable TCP Port Forwarding GatewayPorts no Protocol 2 # Use SSHv2 instead of SSHv1 ListenAddress host_or_address1 # Bind SSH to a Specific Address ListenAddress host_or_addressN # Allow individual IP to connect to SSH, separate line for each IP Port 2222 # Change the Default Port. This masks port scanners Banner none # Disable the Banner, or update the banner in `/etc/issue.net` to remove identifiable data like versions # Reload the SSH Configuration systemctl reload sshd # Add the New Port to SELinux after changing ssh port number semanage port -a -t ssh_port_t -p tcp PORT semanage port -l | grep ssh # Additional information man ssh man sshd man sshd_config ########################## ## OpenSSH Hardening Example ########################## # 1. Installing OpenSSH (client and server) # Ubuntu sudo apt update && sudo apt install openssh-server openssh-client # connecting to the server ssh -p 22 username@server_ip # => Ex: ssh -p 2267 john@192.168.0.100 ssh -p 22 -l username server_ip ssh -v -p 22 username@server_ip # => verbose # 2. Controlling the SSHd daemon # checking its status sudo systemctl status ssh # => Ubuntu sudo systemctl status sshd # => CentOS # stopping the daemon sudo systemctl stop ssh # => Ubuntu sudo systemctl stop sshd # => CentOS # restarting the daemon sudo systemctl restart ssh # => Ubuntu sudo systemctl restart sshd # => CentOS # enabling at boot time sudo systemctl enable ssh # => Ubuntu sudo systemctl enable sshd # => CentOS sudo systemctl is-enabled ssh # => Ubuntu sudo systemctl is-enabled sshd # => CentOS # 3. Securing the SSHd daemon # change the configuration file (/etc/ssh/sshd_config) and then restart the server man sshd_config #a) Change the port Port 2278 #b) Disable direct root login PermitRootLogin no #c) Limit Users\u2019 SSH access AllowUsers stud u1 u2 john #d) Filter SSH access at the firewall level (iptables) #e) Activate Public Key Authentication and Disable Password Authentication #f) Use only SSH Protocol version 2 #g) Other configurations: ClientAliveInterval 300 ClientAliveCountMax 0 MaxAuthTries 2 MaxStartUps 3 LoginGraceTime 20","title":"Securing SSH"},{"location":"learning/linux/security/#ssh-port-forwarding","text":"Expose service (database) to a client not in same network Forward traffic from client to a service running in remote # Client (Different network) ---> (SSH session on port 3306) ---> Server1 ---> (SSH session on port 22) ---> (Database on 127.0.0.0 and port 3306) ssh -L 3306 :127.0.0.1:3306 server1 # -L = forwarding # Similarly another application can connect to Client on port 3306 and access database on server 1 # Avoid this by having firewall block connections to client # Client (Different network) ---> (SSH session on port 8080) ---> Server1 ---> (SSH session on port 22) ---> Google.com (service on port 80) ssh -L 8080 :google.com:80 server1 # -L = forwarding # In this case google.com will understand that traffic is originating from server1 instead of client","title":"SSH Port Forwarding"},{"location":"learning/linux/security/#dynamic-port-forwarding--socks","text":"# Client (Different network) ---> (SSH session on port 8080) ---> Server1 ---> (SSH session on port 22) ---> (Internal Network) ssh -D 8080 server1 # -D = Dynamic forwarding # This allows users to connect to company network","title":"Dynamic Port Forwarding / SOCKS"},{"location":"learning/linux/security/#reverse-port-forwarding","text":"Use a proxy to direct traffic to internal network Connect to a desktop inside corporate network to work from home # (Internal Network) (SSH session on port 22) <--- (Desktop having shell program) <--- (SSH session on port 22) (Port 2222 is open to internet) <--- Server1 <--- Client (Different network) ssh -R 2222 :127.0.0.0:22 # -R = Reverse forwarding","title":"Reverse Port Forwarding"},{"location":"learning/linux/security/#file-security","text":"","title":"File Security"},{"location":"learning/linux/security/#file-permissions_1","text":"first letter in ls -l output Regular file: - Directory: d Symbolic link: l Read: r Write: w Execute: x","title":"File Permissions"},{"location":"learning/linux/security/#files-vs-directory","text":"r - Allows a file to be read. Allows file names in a directory to be read w - Allows a file to be modified. Allows entries to be modified within the directory x - Allows a file to be executed. Allows access to the contents and metdadata for entries","title":"Files vs Directory"},{"location":"learning/linux/security/#groups","text":"Every user in linux is in atleast one group Users can belong to many groups group command shows a user's group. Can also use id -Gn # Changing file permissions ls -l sales.data # Long list file permissions chmod g+w sales.data # Add Write permission to group chmod g-w sales.data # Remove Write permission to group chmod u+rwx,g-x sales.data # Add user all permissions and remove execute permission for group chmod a = r sales.data # Give all groups read permissions chmod o = sales.data # removes all permissions for others","title":"Groups"},{"location":"learning/linux/security/#directory-permissions","text":"Permissions on a directory can affect the files in the directory If the file permissions look correct, start checking the directory permissions Work your way up to the root Use chgrp to change group of a file # Changing directory permissions ls -l sales.data # Long list to show current group membership groups # Shows current user's groups (one is user and other is sales group) chgrp sales sales.data # Change file ownership from user to sales group chmod g+w sales.data # Give other members in sales group write access mkdir -p /usr/local/sales # Create a common directory for sales group mv sales.data /usr/local/sales # Now file and directory have the same permissions # Verify directory permissions mkdir -p my-cat # Create new directory touch my-cat/cat # Create new file chmod 755 my-cat # Modify permissions ls -ld my-cat # Shows permissions chmod 400 my-cat # Give only read permissions ls -ld my-cat # Gives permission error chmod 500 my-cat # Give write permission as well ls -ld my-cat # Gives output cat my-cat/cat # Shows file output","title":"Directory permissions"},{"location":"learning/linux/security/#file-creation-mask","text":"File creation mask are set by the admins This can be changed by per user basis File creation mask determines default permissions. If no mask were to be specified: deafault permissions would be 777 for directories and 666 for files","title":"File Creation Mask"},{"location":"learning/linux/security/#umask-command","text":"Sets the file creation mask to mode, if given Use -S for symbolic notation Format: umask [-S] [mode] mode of umask is opposite to chmod chmod - adds permissions. chmod 660 will give read and write permissions to file umask - subtracts permissions. umask 007 will give read an write permissions to file Common umask modes: 002, 022, 077, 007 # Test umask mkdir testumask cd testumask umask # Shows creation umask 0022 umask -S # Shows umask in symbolic mode mkdir newdir touch newfile ls -l # Shows dir permissions = 0755 and file permissions = 0644 rm newfile rmdir newdir umask 007 # Set new umask mkdir newdir touch newfile ls -l # Shows dir permissions = 0770 and file permissions = 0660","title":"umask command"},{"location":"learning/linux/security/#special-modes","text":"","title":"Special Modes"},{"location":"learning/linux/security/#setuid","text":"When a process is started, it runs using the starting user's UID and GID. setuid = Set User ID upon execution. Examples of binaries running with root and setuid -rwsr-xr-x 1 root root /usr/bin/passwd # Requires root permissions to modify passwd file ping chsh setuid files are an attack surface. Not honored on shell scripts. Octal Permissions: setuid=4, setgid=2 and sticky=1 # Adding the Setuid Attribute chmod u+s /path/to/file chmod 4755 /path/to/file # Removing the Setuid Attribute chmod u-s /path/to/file chmod 0755 /path/to/file # Finding Setuid Files find / -perm /4000 -ls # ls also lists files # Only the Owner Should Edit Setuid Files Good: Symbolic Octal -rwsr-xr-x 4755 Really bad: -rwsrwxrwx 4777","title":"Setuid"},{"location":"learning/linux/security/#setgid","text":"setgid = Set Group ID upon execution. -rwxr-sr-x 1 root tty /usr/bin/wall # s - set in group field crw--w---- 1 bob tty /dev/pts/0 # Adding the Setgid Attribute chmod g+s /path/to/file chmod 2755 /path/to/file # Adding the Setuid & Setgid Attributes chmod ug+s /path/to/file chmod 6755 /path/to/file # Removing the Setgid Attribute chmod g-s /path/to/file chmod 0755 /path/to/file # Finding Setgid Files find / -perm /2000 -ls","title":"Setgid"},{"location":"learning/linux/security/#setgid-on-directories","text":"setgid on a directory causes new files to inherit the group of the directory. setgid causes directories to inherit the setgid bit. Is not retroactive. Does not apply on existing files, but new files in directories. Great for working with groups.","title":"Setgid on Directories"},{"location":"learning/linux/security/#use-an-integrity-checker","text":"Other options to find . Tripwire AIDE (Advanced Intrusion Detection Environment) OSSEC Samhain Package managers","title":"Use an Integrity Checker"},{"location":"learning/linux/security/#sticky-bit","text":"Use on a directory to only allow the owner of the file/directory to delete it. Required on files or directories whcih are shared with multiple users or programs # Used on /tmp: drwxrwxrwt 10 root root 4096 Feb 1 09 :47 /tmp # Adding the Sticky Bit chmod o+s /path/to/directory chmod 1777 /path/to/directory # Removing the Sticky Bit chmod o-t /path/to/directory chmod 0777 /path/to/directory","title":"Sticky Bit"},{"location":"learning/linux/security/#reading-ls-output","text":"A capitalized special permission means the underlying normal permission is not set. A lowercase special permission means the underlying normal permission set. # execute permission is not set for user -rwSr--r-- 1 root root 0 Feb 14 11 :21 test # execute permission is set for the user -rwsr--r-- 1 root root 0 Feb 14 # execute permission is not set for group -rwxrwSr-- 1 root root 0 Feb 14 11 :21 test # execute permission is not set for others drwxr-xr-T 2 root root 0 Feb 14 11 :30 testd","title":"Reading ls Output"},{"location":"learning/linux/security/#file-attributes","text":"xattr: Supported by many file systems. ext2, ext3, ext4, XFS","title":"File Attributes"},{"location":"learning/linux/security/#attribute-i-immutable","text":"The file cannot be: modified, deleted, renamed, hard linked to Unset the attribute in order to delete it.","title":"Attribute: i immutable"},{"location":"learning/linux/security/#attribute-a-append","text":"Append only. Existing contents cannot be modified. Cannot be deleted while attribute is set. Use this attribute on log files. Safeguard the audit trail.","title":"Attribute: a append"},{"location":"learning/linux/security/#other-attributes","text":"Not every attribute is supported. man ext4, man xfs, man brtfs, etc. Example: s secure deletion","title":"Other Attributes"},{"location":"learning/linux/security/#modifying-attributes","text":"Use the chattr command. adds attributes. removes attributes. = sets the exact attributes. # Making the hosts file immutable lsattr /etc/hosts # No attributes present chattr +i /etc/hosts # Add immutable attribute vi /etc/hosts # Not able to write and save rm /etc/hosts # Not able to delete the file chattr -i /etc/hosts # Remove immutable attribute lsattr /etc/hosts # No attributes present vi /etc/hosts # Now able to write and save chattr +i /etc/hosts # Add immutable attribute back again # Making the apache logs files append only cd /var/log/httpd chattr = a * # Adding append attribute to all the files inside lsattr echo test >> access_log # Data will be appended cat access_log vi access_log # Not able to write and save","title":"Modifying Attributes"},{"location":"learning/linux/security/#access-control-lists","text":"ACL = Access Control List Provides additional control Example: Give one user access to a file. Traditional solution is to create another group. Increases management overhead of groups. Ensure file system mounted with ACL support XFS supports this by default # Mount files with ACL support to get this feature mount -o acl /path/to/dev /path/to/mount tune2fs -o acl /path/to/dev tune2fs -l /path/to/dev | grep options # Verify ACL","title":"Access Control Lists"},{"location":"learning/linux/security/#types-of-acls","text":"Access - Control access to a specific file or directory. Default - Used on directories only. Files without access rules use the default ACL rules. Not retroactive. Optional.","title":"Types of ACLs"},{"location":"learning/linux/security/#acls-can-be-configured","text":"Per user Per group For users not in the file\u2019s group Via the effective rights mask","title":"ACLs Can Be Configured:"},{"location":"learning/linux/security/#creating-acls","text":"Use the setfacl command. May need to install the ACL tools. Modify or add ACLs: setfacl -m ACL FILE_OR_DIRECTORY Detecting Files with ACLs: + at the end of ls -l permissions output # User ACLs / Rules # u:uid:perms Set the access ACL for a user. setfacl -m u:jason:rwx start.sh setfacl -m u:sam:xr start.sh # Group ACLs / Rules # g:gid:perms Sets the access ACL for a group. setfacl -m g:sales:rw sales.txt # Mask ACLs / Rules # m:perms Sets the effective rights mask. setfacl -m m:rx sales.txt # Other ACLs / Rules # o:perms Sets the access ACL for others. setfacl -m o:r sales.txt # Creating Multiple ACLs at Once setfacl -m u:bob:r,g:sales:rw sales.txt # Default ACLs # d:[ugo]:perms Sets the default ACL. setfacl -m d:g:sales:rw sales # Setting ACLs Recursively (-R) setfacl -R -m g:sales:rw sales # Removing ACLs for particular group or user setfacl -x ACL FILE_OR_DIRECTORY setfacl -x u:jason sales.txt # Note: Not supposed to give permissions when deleting ACL setfacl -x g:sales sales.txt # Removing All ACLs setfacl -b FILE_OR_DIRECTORY setfacl -b sales.txt # Viewing ACLs getfacl sales.txt # ACL for a shared project directory # Logged in as root user mkdir /usr/local/project cd /usr/local chgrp project project/ chmod 775 project/ # Give the project members access to the directory ls -ld project/ # Directory owned by root and project members have access su - sam # Switch user groups # same is in project group cd usr/local/project echo stuff > notes.txt ls -l notes.txt # sam is the owner of this file # Give bob who is not a member of project group access to this file setfacl -m u:bob:rw notes.txt # Modify access by sam to give read and write access getfacl notes.txt # List the permisions for bob su -bob # Switch user vi notes.txt # bob is able to write su -sam # Switch user setfacl -x u:bob notes.txt # Remove the rw permissions getfacl notes.txt # No permissions visble for bob su - # Switch to root cd project touch root-was-here # Create a new file owned by root in shared directory ls -l # No ACL is applied even though its in shared directory su sam # Switch to group user echo >> root-was-here # Permission denied su - # Switch to root cd /usr/local/project setfacl -m d:g:project:rw . # Set default ACL to project directory ls -ld . # Default ACL are added to directory denoted by + getfacl # Default ACL are added touch test # Create new file getfacl su sam vi test # Edit is possible setfacl -R -m g:project:rw . # Add recursively permissions to root-was-here file as well","title":"Creating ACLs"},{"location":"learning/linux/security/#cracking-passwords","text":"# CRACKING PASSWORD HASHES USING JOHN THE RIPPER # Installing JTR apt install john # combining /etc/passwd and /etc/shadow in a single file unshadow /etc/passwd /etc/shadow > unshadowed.txt # cracking in single mode john -single unshadowed.txt # brute-force and dictionary attack john --wordlist = /usr/share/john/password.lst --rules unshadowed.txt # dictionary files: # /usr/share/dict # /usr/share/metasploit-framework/data/wordlists # -> on Kali Linux # showing cracked hashes (~/.john/john.pot) john --show unshadowed.txt # to continue an interrupted (ctrl+c) session, run in the same directory: john -restore # cracking only accounts with specific shells (valid shells) john --wordlist = mydict.txt --rules --shell = bash,sh unshadowed.txt # cracking only some accounts john --wordlist = mydict.txt --rules --users = admin,mark unshadowed.txt # cracking in incremental mode (/etc/john/john.conf) john --incremental unshadowed.txt","title":"Cracking Passwords"},{"location":"learning/linux/security/#rootkits","text":"Software used to gain root access and remain undetected. They attempt to hide from system administrators and antivirus software. User space rootkits replace common commands such as ls, ps, find, netstat, etc Kernel space rootkits add or replace parts of the core operating system.","title":"Rootkits"},{"location":"learning/linux/security/#rootkit-detection","text":"Use a file integrity checker for user space rootkits. (AIDE, tripwire, OSSEC, etc.) Identify inconsistent behavior of a system. High CPU utilization without corresponding processes. High network load or unusual connections. Kernel mode rootkits have to be running in order to hide themselves. Halt the system and examine the storage. Use a known good operating system to do the investigation. Use bootable media, for example.","title":"Rootkit Detection"},{"location":"learning/linux/security/#rootkit-hunter--rkhunter-chkrootkit","text":"Shell script that searches for rootkits. rkhunter --update # Update database rkhunter --propupd # Update RKHunter configurations rkhunter -c # Interactive Check Mode cat /var/log/rkhunter.log # Log file location rkhunter -c --rwo # Report only warnings rkhunter --cronjob # Run as cronjob # After a fresh installation of OS # Install RK Hunter and then update the properties and database # Configure it to run everyday and report any issues by configuring it to log all output in a central place # Configure Cronjob crontab -e # Create a new cronjob # At the end run at modnight everyday # Redirect the output to single file 0 0 * * * /usr/local/bin/rkhunter --cronjob --update > /var/tmp/rkhunter.cronlog 2 > & 1 # installing chkrootkit apt install chkrootkit # running a scan chkrootkit chkrootkit -q # Reports only warnings","title":"Rootkit Hunter / RKHunter /ChkRootKit"},{"location":"learning/linux/security/#ossec","text":"Host Intrusion Detection System (HIDS) More than just rookit detection: log analysis, file integrity checking, alerting. Syscheck module - user mode rootkit detection. Rootcheck module - both user mode and kernel mode rootkit detection.","title":"OSSEC"},{"location":"learning/linux/security/#rootkit-removal","text":"Keep a copy of the data if possible. Learn how to keep it from happening again. Reinstall core OS components and start investigating. Not recommended. Easy to make a mistake. Safest is to reinstall the OS from trusted media.","title":"Rootkit Removal"},{"location":"learning/linux/security/#rootkit-prevention","text":"Use good security practices: Physical, Account, Network Use file integrity monitoring: AIDE, Tripware, OSSEC Keep your systems patched.","title":"Rootkit Prevention"},{"location":"learning/linux/security/#aide---advanced-intrusion-detection","text":"What security incident can AIDE detect? A virus installed in system directory A configuration file that was changed by a hacker A command that was replaced by a hacker # installing AIDE apt update && apt install aide aide -v # getting help aide --help /etc/aide/aide.conf # => config file ### SEARCHING FOR CHANGES ### # initializing the AIDE database => /var/lib/aide/aide.db.new aideinit # moving the db to the one that will be checked by AIDE mv /var/lib/aide/aide.db.new /var/lib/aide/aide.db # creating a runtime config file => /var/lib/aide/aide.conf.autogenerated update-aide.conf # this is a command to run # detecting changes aide -c /var/lib/aide/aide.conf.autogenerated --check > report.txt # updating the db aide -c /var/lib/aide/aide.conf.autogenerated --update # copying the newly created database as the baseline database cp /var/lib/aide/aide.db.new /var/lib/aide/aide.db ## CREATING A CUSTOM aide.conf FILE (Example: /root/aide.conf) ## database = file:/var/lib/aide/aide.db database_out = file:/var/lib/aide/aide.db.new MYRULE = u+g+p+n+s+m+sha256 /etc MYRULE /usr MYRULE /root MYRULE !/usr/.* !/usr/share/file$ # initializing the new AIDE db aide -c /root/aide.conf --init # moving the new db to the baseline db mv /var/lib/aide/aide.db.new /var/lib/aide/aide.db # checking for any changes aide -c /root/aide.conf --check # --limit option aide -c /root/aide.conf --limit /etc --check # AIDE will check for changes only in /etc directory","title":"AIDE - Advanced Intrusion Detection"},{"location":"learning/linux/security/#anitvirus---clamav","text":"Virus is not an issue for Linux, but it can be a host to spread Malware and Virus to Windows machines in the network. To protect Windows machines from themselves, install antivirus solution on Linux. # installing clamav sudo apt install && sudo apt install clamav clamav-daemon # checking the status systemctl status clamav-freshclam systemctl status clamav-daemon # starting the clamav daemon systemctl start clamav-daemon # enabling the daemon to start and boot systemctl enable clamav-daemon # getting a test virus wget www.eicar.org/download/eicar.com # scanning a directory using clamdscan clamdscan --fdpass /root/ # moving found viruses to a quarantine directory clamdscan --move = /quarantine --fdpass /root # scanning a directory using clamscan clamscan --recursive /etc clamscan --recursive --infected /quarantine clamscan --recursive --infected --remove /quarantine/","title":"Anitvirus - ClamAV"},{"location":"learning/linux/troubleshooting/","text":"Troubleshooting \u00b6 Scenarios OS Debugging uname # Shows kernal uname -o # Shows OS uname -m # Shows computer architecture x86_64 (64 bit), x86 (32 bit) lsb_release -a # Distro version # kernel logs will be shown in dmesg command that are in memory. Will be lost at server restart. tail -f /var/log/kern.log # Shows the kernel logs that are persisted even after restart. /usr/include/linux/capabilities.h # Shows the Linux capabilities that are enabled Server Not reachable # Check server connectivity with outside world ping google.com # check for a connection nslookup google.com # Identify IP if DNS resolution is not working ping 172 .10.1.2 # IP address of google # Check DNS resolution issues of the server cat /etc/hosts # IP to DNS mapping ping localhost # Check connection to itself cat /etc/resolv.conf # Local DNS server details. Mostly router cat /etc/nsswitch.conf # Controls the flow of traffic # `hosts: files dns myhostname # This entry in `nsswitch.conf`, tells the first check in hosts file # If IP entry is not found, go to DNS Resolver i.e resolv.conf # Check if server has an IP address ifcongfig # Shows the Network configurations # Ping Gateway to check local connectivity netstat -rnv # Shows Gateway address # Netstat also shows the routing to the outside world. # This should show the destination 0.0.0.0 route to outside world route -n # Shows the path taken by the server to reach destination # Trace the route taken by the server, this needs additional traceroute package traceroute google.com # Shows the network path taken to reach the server # The router should have IPV4 routing enabled to forward packets to the next destination cat /etc/sysctl.conf | grep -i ipv4 # ipv4.forward should be 1 and not 0 on the router echo 1 > /proc/sys/net/ipv4/ip_forward # Enable routing Application / Webserver Not reachable # Server is reachable but application is not curl <IP>:<Port> # If no response, start the service # Login to the server and start the service ps -eaf | grep ntp # Shows if ntp process is running ps -eaf | grep -i network # Check if Network manager is running ps -eaf | grep httpd # If apache is down start it SSH Login Errors for root or normal users # Login to the server # Check Root can login using password. This would be off and should also be. # If Login is allowed, check if the password is wrong or account is locked # Check /var/log/secure more /var/log/secure # Check the error messages # Check if User does not exist or has nologin shell id bob # Shows user exists # Check sudo permissions sudo -l vi /etc/passwd # Shows nologin is set or not `/sbin/nologin` # Check if user acoount is disabled vi /etc/passwd # User information is not found or is commented # Check if ssh service is running ps -eaf | grep ssh systemctl restart sshd Firewall # Connection Refused errors service iptables status # Check if firewall service is running ps -eaf | grep iptables # Double check if service is up systemctl status ufw # Firewall service systemctl stop ufw # Temporray stop firewall to check connectivity find /var/log -type f -mmin -1 # This will show any log file modified in the last 1 min cd chmod o+x secret/ # Executable permission is required to cd into a directory for any group su - sam # sam is in others group and has execute permission on dir cd secret # No error chmod o-x secret/ cd secret # Will fail # Read allows one to see the contents. ls -ltr # Will fail if read permission is not there for sam su - # Login to root chmod a+x /root # Give everyone permission to cd into root home directory su -sam cd /root # Allowed ls -ltr # Denied Read file or Execute a script Errors # Read permission is required to read a file chmod o-r README.md su - sam cat README.md # This will give permission denied # Reading directory will aslo throw an error chmod a+x script # Executable permission is required to execute a script for any group # 2 ways to execute this script # Give absolute path pwd /home/sam/script # This will execute # Give the current folder using dot notation ./script # This will execute # Error: If none of the above is done, \"Command not found\" error will be thrown as its not in path Finding Files or Directories Errors su - # Login as root find / -name \"hello.txt\" # Search for a file in the entire file system # Throws a permission denied error for /run/user filesystem which can be ignored Create Links Errors # Format: ln -s <source> <destination> # Important: source and directory has to be ABSOLUTE path ln -s hello.txt /tmp/greetings.txt # Link will get created cat /tmp/greetings.txt # No such file or directory. Why? Full path for hello.txt not given rm /tmp/greetings.txt # Remove the link pwd ln -s /home/sam/hello.txt /tmp/greetings.txt # Link will get created cat /tmp/greetings.txt # No Error and file is read # If you switch the destination and source position, it will throw an error ln -s /tmp/greetings.txt /home/sam/hello.txt # Error: File already exists # Source file should have read permissions # Owner of the source file can only create symbolic links # Destination where the file will land should also have read permissions for the group or others sudo grep sudo /etc/group # Shows users in the sudo group at account setup nmap # Always output the data into a file nmap 10 .211.55.6 -O # Server's operating system nmap 10 .211.55.6 -p- # Scans all 65535 TCP ports, helpful for finding services listening on weird, high ports like 12380 in this case. nmap 10 .211.55.6 -sC -o /tmp/output.txt # Runs a set list of default scripts against your target. nmap 10 .211.55.6 -sU \u2013top-ports 250 # scan the 250 most common UDP ports Docker # Find running process ps faux | grep 'agent' # For a matched process, look at the environment variables xargs -0 -L1 -a /proc/1099/environ #1099 is the process id # Find sensitive files find / -name authorized_keys 2 > /dev/null find / -name id_rsa 2 > /dev/null find / -type f -path '*.kube/*' -name 'config' 2 > /dev/null find / -type f -path '*.docker/*' -name 'config.json' 2 > /dev/null find / -name kubeconfig 2 > /dev/null find / -name .gitconfig 2 > /dev/null # Docker logs on the machine sudo find /var/lib/docker/containers/ -name *.log # CAP_SYS_MODULE # If this capabilitcy is available for user, it can easily load a new module and setup a reverse shell. capsh --print # shows if enabled # More details on this can be found in https://greencashew.dev/posts/how-to-add-reverseshell-to-host-from-privileged-container # Docker or not? # Check /proc/1/group. Inside container it will be /docker/ cat /proc/1/cgroup cat /proc/self/cgroup # Logging agent configuration cat /etc/ryslog.d/<any conf file> # Check this later Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility","title":"Troubleshooting"},{"location":"learning/linux/troubleshooting/#troubleshooting","text":"Scenarios OS Debugging uname # Shows kernal uname -o # Shows OS uname -m # Shows computer architecture x86_64 (64 bit), x86 (32 bit) lsb_release -a # Distro version # kernel logs will be shown in dmesg command that are in memory. Will be lost at server restart. tail -f /var/log/kern.log # Shows the kernel logs that are persisted even after restart. /usr/include/linux/capabilities.h # Shows the Linux capabilities that are enabled Server Not reachable # Check server connectivity with outside world ping google.com # check for a connection nslookup google.com # Identify IP if DNS resolution is not working ping 172 .10.1.2 # IP address of google # Check DNS resolution issues of the server cat /etc/hosts # IP to DNS mapping ping localhost # Check connection to itself cat /etc/resolv.conf # Local DNS server details. Mostly router cat /etc/nsswitch.conf # Controls the flow of traffic # `hosts: files dns myhostname # This entry in `nsswitch.conf`, tells the first check in hosts file # If IP entry is not found, go to DNS Resolver i.e resolv.conf # Check if server has an IP address ifcongfig # Shows the Network configurations # Ping Gateway to check local connectivity netstat -rnv # Shows Gateway address # Netstat also shows the routing to the outside world. # This should show the destination 0.0.0.0 route to outside world route -n # Shows the path taken by the server to reach destination # Trace the route taken by the server, this needs additional traceroute package traceroute google.com # Shows the network path taken to reach the server # The router should have IPV4 routing enabled to forward packets to the next destination cat /etc/sysctl.conf | grep -i ipv4 # ipv4.forward should be 1 and not 0 on the router echo 1 > /proc/sys/net/ipv4/ip_forward # Enable routing Application / Webserver Not reachable # Server is reachable but application is not curl <IP>:<Port> # If no response, start the service # Login to the server and start the service ps -eaf | grep ntp # Shows if ntp process is running ps -eaf | grep -i network # Check if Network manager is running ps -eaf | grep httpd # If apache is down start it SSH Login Errors for root or normal users # Login to the server # Check Root can login using password. This would be off and should also be. # If Login is allowed, check if the password is wrong or account is locked # Check /var/log/secure more /var/log/secure # Check the error messages # Check if User does not exist or has nologin shell id bob # Shows user exists # Check sudo permissions sudo -l vi /etc/passwd # Shows nologin is set or not `/sbin/nologin` # Check if user acoount is disabled vi /etc/passwd # User information is not found or is commented # Check if ssh service is running ps -eaf | grep ssh systemctl restart sshd Firewall # Connection Refused errors service iptables status # Check if firewall service is running ps -eaf | grep iptables # Double check if service is up systemctl status ufw # Firewall service systemctl stop ufw # Temporray stop firewall to check connectivity find /var/log -type f -mmin -1 # This will show any log file modified in the last 1 min cd chmod o+x secret/ # Executable permission is required to cd into a directory for any group su - sam # sam is in others group and has execute permission on dir cd secret # No error chmod o-x secret/ cd secret # Will fail # Read allows one to see the contents. ls -ltr # Will fail if read permission is not there for sam su - # Login to root chmod a+x /root # Give everyone permission to cd into root home directory su -sam cd /root # Allowed ls -ltr # Denied Read file or Execute a script Errors # Read permission is required to read a file chmod o-r README.md su - sam cat README.md # This will give permission denied # Reading directory will aslo throw an error chmod a+x script # Executable permission is required to execute a script for any group # 2 ways to execute this script # Give absolute path pwd /home/sam/script # This will execute # Give the current folder using dot notation ./script # This will execute # Error: If none of the above is done, \"Command not found\" error will be thrown as its not in path Finding Files or Directories Errors su - # Login as root find / -name \"hello.txt\" # Search for a file in the entire file system # Throws a permission denied error for /run/user filesystem which can be ignored Create Links Errors # Format: ln -s <source> <destination> # Important: source and directory has to be ABSOLUTE path ln -s hello.txt /tmp/greetings.txt # Link will get created cat /tmp/greetings.txt # No such file or directory. Why? Full path for hello.txt not given rm /tmp/greetings.txt # Remove the link pwd ln -s /home/sam/hello.txt /tmp/greetings.txt # Link will get created cat /tmp/greetings.txt # No Error and file is read # If you switch the destination and source position, it will throw an error ln -s /tmp/greetings.txt /home/sam/hello.txt # Error: File already exists # Source file should have read permissions # Owner of the source file can only create symbolic links # Destination where the file will land should also have read permissions for the group or others sudo grep sudo /etc/group # Shows users in the sudo group at account setup nmap # Always output the data into a file nmap 10 .211.55.6 -O # Server's operating system nmap 10 .211.55.6 -p- # Scans all 65535 TCP ports, helpful for finding services listening on weird, high ports like 12380 in this case. nmap 10 .211.55.6 -sC -o /tmp/output.txt # Runs a set list of default scripts against your target. nmap 10 .211.55.6 -sU \u2013top-ports 250 # scan the 250 most common UDP ports Docker # Find running process ps faux | grep 'agent' # For a matched process, look at the environment variables xargs -0 -L1 -a /proc/1099/environ #1099 is the process id # Find sensitive files find / -name authorized_keys 2 > /dev/null find / -name id_rsa 2 > /dev/null find / -type f -path '*.kube/*' -name 'config' 2 > /dev/null find / -type f -path '*.docker/*' -name 'config.json' 2 > /dev/null find / -name kubeconfig 2 > /dev/null find / -name .gitconfig 2 > /dev/null # Docker logs on the machine sudo find /var/lib/docker/containers/ -name *.log # CAP_SYS_MODULE # If this capabilitcy is available for user, it can easily load a new module and setup a reverse shell. capsh --print # shows if enabled # More details on this can be found in https://greencashew.dev/posts/how-to-add-reverseshell-to-host-from-privileged-container # Docker or not? # Check /proc/1/group. Inside container it will be /docker/ cat /proc/1/cgroup cat /proc/self/cgroup # Logging agent configuration cat /etc/ryslog.d/<any conf file> # Check this later Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility Utility","title":"Troubleshooting"},{"location":"learning/linux/volumes/","text":"Why use LVM? \u00b6 Flexible Capacity \u00b6 One benefit of using LVM is that you can create file systems that extend across multiple storage devices. With LVM, you can aggregate multiple storage devices into a single logical volume. Easily Resize Storage While Online \u00b6 LVM allows you to expand or shrink filesystems in real-time while the data remains online and fully accessible. Without LVM you would have to reformat and repartition the underlying storage devices. Of course, you would have to take the file system offline to perform that work. LVM eliminates this problem. Online Data Relocation \u00b6 LVM also allows you to easily migrate data from one storage device to another while online. For example, if you want to deploy newer, faster, or more resilient storage, you can move your existing data from the current storage devices to the new ones while your system is active. Convenient Device Naming \u00b6 Instead of using abstract disk numbers, you can use human-readable device names of your choosing. Instead of wondering what data is on /dev/sdb, you can name your data with a descriptive name. Disk Striping \u00b6 With LVM, you can stripe data across two or more disks. This can dramatically increase throughput by allowing your system to read data in parallel. Data Redundancy / Data Mirroring \u00b6 If you want to increase fault tolerance and reliability, use LVM to mirror your data so that you always have more than one copy of your data. Using LVM mirroring prevents single points of failure. If one storage device fails, your data can be accessed via another storage device. You can then fix or replace the failed storage device to restore your mirror, without downtime. Snapshots \u00b6 LVM gives you the ability to create point-in-time snapshots of your filesystems. This can be perfect for when you need consistent backups. For example, you could pause writes to a database, take a snapshot of the logical volume where the database data resides, then resume writes to the database. That way you ensure your data is in a known-good state when you perform the backup of the snapshot. Layers of Abstraction in LVM \u00b6 The logical volume manager introduces extra layers of abstraction between the storage devices and the file systems placed on those storage devices. The first layer of abstraction is physical volumes. These are storage devices that are used by LVM. The name is a bit of a legacy name. To be clear, these storage devices do not have to be physical. They just have to be made available to the Linux operating system. In other words, as long as Linux sees the device as a block storage device, it can be used as a physical volume (PV). Physical hard drives, iSCSI devices, SAN disks, and so on can be PVs. You can allocate an entire storage device as a PV or you can partition a storage device and use just that one partition as a PV. The next layer of abstraction is the Volume Group. A volume group is made up of one or more PVs. You can think of a volume group as a pool of storage. If you want to increase the size of the pool, you simply add more PVs. Keep in mind that you can have different types of storage in the same volume group if you want. For example, you could have some PVs that are backed by hard drives and other PVs that are backed by san disks. The next layer of abstraction is the Logical Volume layer. Logical Volumes are created from a volume group. File systems are created on Logical Volumes. Without LVM you would create a file system on a disk partition, but with LVM you create a file system on a logical volume. As long as there is free space in the Volume Group, logical volumes can be extended. You can also shrink logical volumes to reclaim unused space if you want, but typically you'll find yourself extending logical volumes. Logical Volume Creation Process \u00b6 At a high level, the process for creating a logical volume is this: 1. Create one or more physical volumes. 2. Create a volume group from those one or more physical volumes. 3. Finally, you can create one or more logical volumes from the volume group. Creating Physical Volumes \u00b6 # Verify which devices are used su - lvmdiskscan # Shows all the storage devices that have the ability to be used with LVM. lsblk -p # Shows the partitions df -h # Display sizes in a human readable format. # Once verified which devices are not used # Create the physical volumes pvcreate /dev/sdb # initializes the disk for use by the logical volume manager. pvs # list of pvs Creating Volume Groups \u00b6 vgcreate vg_app /dev/sdb # Volume group naming convention of \"vg_\". vgs # view the volume groups # It shows that we have 1 physical volume. # The size of the volume group is 50GB and we have 50GB free in the volume group. # If we look at our pvs with the pvs command, we'll now see what VG our PV belongs to. Creating Logical Volumes \u00b6 lvcreate -L 20G -n lv_data vg_app # Logical Volume naming convention of \"lv_\". # Note captial L is to give human readable volume size lvs # view logical volumes lvdisplay # Also to view lv, but it provides different output. # For example, notice the LV Path: \u200b /dev/vg_app/lv_data\u200b # It's easy to tell that the \u200b lv_data\u200b logical volume belongs to the vg_app\u200b volume group. Creating File Systems \u00b6 Now that we have a logical volume, we can treat it like we would a normal disk partition. So, let's put a file system on our logical volume and mount it. mkfs -t ext4 /dev/vg_app/lv_data mkdir /data mount /dev/vg_app/lv_data /data df -h /data Creating Multiple Logical Volumes in Volume Groups \u00b6 lvcreate -L 5G -n lv_app vg_app # Now you can see we have two logical volumes in vg_app lvs # We can put a file system on this logical volume and mount it. mkfs -t ext4 /dev/vg_app/lv_app mkdir /app # Add Logical Volumes in the \u200b/etc/fstab\u200b so that it gets mounted at boot time. # vi /etc/fstab /dev/vg_app/lv_app /app ext4 defaults 0 0 # Save and quit mount /app df -h /app df -h # You also access your logical volume through the device mapper path as shown in the df output. # For example, \u200b /dev/vg_app/lv_app\u200b can be accessed via \u200b /dev/mapper/vg_app-lv_app\u200b . ls -l /dev/vg_app/lv_app ls -l /dev/mapper/vg_app-lv_app Logical Extents and Physical Extents \u00b6 There is yet another layer of abstraction that we haven't talked about. Well, two layers of abstraction, really. Each of our LVs is actually divided up into LEs, which stands for logical extents. Or if we look at it from the other direction, a collection of Logical Extents makes up a Logical Volume. This is how LVM allows us to expand or shrink a logical volume \u2013 the logical volume manager just changes the number of underlying Logical Extents for the logical volume. lvdisplay -m # Show a map of the logical volume. # This map view tells us that the logical extents for the \u200blv_app\u200b LV resize on the \u200b/dev/sdb\u200b disk. # Like an LV is divided into LE, a PV is divided into PE, physical extents. # There is a one-to-one mapping of LEs to PEs. pvdisplay -m # Shows a map from the view of the disk Create Logical Volumes using percentage \u00b6 lvcreate -l 100 %FREE -n lv_logs vg_app # Note small l is used with % or passing free extents # We can squeeze every possible bit of space out of your volume group and put it into a logical volume. Extending Volume Groups \u00b6 Let's say that the \u200blv_data\u200b logical volume is getting full and we need to extend it. If we look at our volume group, we'll see we've already allocated all our space to the existing logical volumes. vgs # Output shows VFREE = 0 # In this case, we need to extend the volume group itself # before we can extend the logical volume within that volume group. lvmdiskscan # Shows sdc is free pvcreate /dev/sdc vgextend vg_app /dev/sdc # add this PV to our VG vgs # Output shows VFREE = 50G pvs # We have one pv that is complete full and one that is completely free. # Before we extend our logical volume into this free space, # let's look at the space from the file system level. df -h /data Extending Logical Volumes \u00b6 Now, let's use the lvextend command to add 5GB of space to that logical volume. In addition to increasing the size of the logical volume, we also need to grow the file system on that logical volume to fill up this new space. That's what the \u200b -r\u200b option is for. lvextend -L +5G -r /dev/vg_app/lv_data df -h /data # Shows filesystem has increased by 5G # If you forget to use the \u200b-r\u200b option to lvextend to perform the resize, you'll have to do that after the fact. lvextend -L +5G /dev/vg_app/lv_data lvs # lv size has increased df -h /data # The file system is still the same size, and there is no 5G increase # To fix this you'll have to use a resize tool for the specific filesystem you're working with. # For ext file systems, that tool is \u200bresize2fs\u200b. # We give it the path the the underlying device, which is a logical volume in our case. resize2fs /dev/vg_app/lv_data df -h /data # Shows filesystem has increased by 5G lvdisplay -m /dev/vg_app/lv_data # We see that some of the extents live on \u200b/dev/sdb\u200b while other extents live on \u200b/dev/sdc\u200b . Creating Mirrored Logical Volumes \u00b6 Mirrored logical volume will ensure that an exact copy of the data will be stored on two different storage devices. lvmdiskscan # We have two more disks that we can use sdd and sde. pvcreate /dev/sdd /dev/sde vgcreate vg_safe /dev/sdd /dev/sde lvcreate -m 1 -L 5G -n lv_secrets vg_safe lvs # Copy%Sync column indicates if the mirror is synced. # When it finished syncing the data, it would be at 100%. lvs -a # The logical volume we created is actually RAID 1. So a mirror and raid 1 are the same thing. # Let's create a file system on that logical volume and mount it. mkfs -t ext4 /dev/vg_safe/lv_secrets mkdir /secrets mount /dev/vg_safe/lv_secrets /secrets df -h /secrets # So we write to \u200b /dev/vg_safe/lv_secrets\u200b and let the logical volume manager handle the mirroring. # We just use this file system like any other. Deleting Logical Volumes, Volume Groups, and Physical Volumes \u00b6 # unmount the file system that is mounted inside logical volumes. umount /secrets # we can remove the underlying logical volume. lvremove /dev/vg_safe/lv_secrets # If you want to remove a pv from a vg, use the vgreduce command. vgs vgreduce vg_safe /dev/sde pvs pvremove /dev/sde # Let's finish destroying the \u200b vg_safe\u200b volume group with \u200b vgremove\u200b . vgremove vg_safe vgs pvs pvremove /dev/sdd Migrating Data While Online \u00b6 Early I mentioned how easy it is to move data from one storage device to another with LVM. Let's say that the storage device attached to our system at \u200b /dev/sde \u200b is faster and has more space. Let's say we want to move the data that currently resides on \u200b /dev/sdb\u200b to that new disk. To do that, we'll just add \u200b /dev/sde \u200b to the volume group and migrate the data over. # we \u200bpvcreate\u200b the device. pvcreate /dev/sde # Now we add it to the volume group. vgextend vg_app /dev/sde pvs # Finally, we use the \u200bpvmove\u200b command to move all the data from \u200b/dev/sdb\u200b to \u200b/dev/sde\u200b pvmove /dev/sdb /dev/sde Once the \u200b pvmove\u200b command is complete, all the data the used to live on \u200b /dev/sdb\u200b now lives on \u200b /dev/sde\u200b . And the whole time this was happening, any logical volumes and file systems that where on \u200b /dev/sdb remained online and available throughout this entire process. There's no need to take an outage for this process. pvs # Now \u200b/dev/sdb\u200b is unused pvdisplay /dev/sdb # We see that \"Allocated PE\" is zero. # Now that we're done with this disk we can remove it from the volume group with \u200bvgreduce\u200b vgreduce vg_app /dev/sdb pvremove /dev/sdb pvs","title":"Why use LVM?"},{"location":"learning/linux/volumes/#why-use-lvm","text":"","title":"Why use LVM?"},{"location":"learning/linux/volumes/#flexible-capacity","text":"One benefit of using LVM is that you can create file systems that extend across multiple storage devices. With LVM, you can aggregate multiple storage devices into a single logical volume.","title":"Flexible Capacity"},{"location":"learning/linux/volumes/#easily-resize-storage-while-online","text":"LVM allows you to expand or shrink filesystems in real-time while the data remains online and fully accessible. Without LVM you would have to reformat and repartition the underlying storage devices. Of course, you would have to take the file system offline to perform that work. LVM eliminates this problem.","title":"Easily Resize Storage While Online"},{"location":"learning/linux/volumes/#online-data-relocation","text":"LVM also allows you to easily migrate data from one storage device to another while online. For example, if you want to deploy newer, faster, or more resilient storage, you can move your existing data from the current storage devices to the new ones while your system is active.","title":"Online Data Relocation"},{"location":"learning/linux/volumes/#convenient-device-naming","text":"Instead of using abstract disk numbers, you can use human-readable device names of your choosing. Instead of wondering what data is on /dev/sdb, you can name your data with a descriptive name.","title":"Convenient Device Naming"},{"location":"learning/linux/volumes/#disk-striping","text":"With LVM, you can stripe data across two or more disks. This can dramatically increase throughput by allowing your system to read data in parallel.","title":"Disk Striping"},{"location":"learning/linux/volumes/#data-redundancy--data-mirroring","text":"If you want to increase fault tolerance and reliability, use LVM to mirror your data so that you always have more than one copy of your data. Using LVM mirroring prevents single points of failure. If one storage device fails, your data can be accessed via another storage device. You can then fix or replace the failed storage device to restore your mirror, without downtime.","title":"Data Redundancy / Data Mirroring"},{"location":"learning/linux/volumes/#snapshots","text":"LVM gives you the ability to create point-in-time snapshots of your filesystems. This can be perfect for when you need consistent backups. For example, you could pause writes to a database, take a snapshot of the logical volume where the database data resides, then resume writes to the database. That way you ensure your data is in a known-good state when you perform the backup of the snapshot.","title":"Snapshots"},{"location":"learning/linux/volumes/#layers-of-abstraction-in-lvm","text":"The logical volume manager introduces extra layers of abstraction between the storage devices and the file systems placed on those storage devices. The first layer of abstraction is physical volumes. These are storage devices that are used by LVM. The name is a bit of a legacy name. To be clear, these storage devices do not have to be physical. They just have to be made available to the Linux operating system. In other words, as long as Linux sees the device as a block storage device, it can be used as a physical volume (PV). Physical hard drives, iSCSI devices, SAN disks, and so on can be PVs. You can allocate an entire storage device as a PV or you can partition a storage device and use just that one partition as a PV. The next layer of abstraction is the Volume Group. A volume group is made up of one or more PVs. You can think of a volume group as a pool of storage. If you want to increase the size of the pool, you simply add more PVs. Keep in mind that you can have different types of storage in the same volume group if you want. For example, you could have some PVs that are backed by hard drives and other PVs that are backed by san disks. The next layer of abstraction is the Logical Volume layer. Logical Volumes are created from a volume group. File systems are created on Logical Volumes. Without LVM you would create a file system on a disk partition, but with LVM you create a file system on a logical volume. As long as there is free space in the Volume Group, logical volumes can be extended. You can also shrink logical volumes to reclaim unused space if you want, but typically you'll find yourself extending logical volumes.","title":"Layers of Abstraction in LVM"},{"location":"learning/linux/volumes/#logical-volume-creation-process","text":"At a high level, the process for creating a logical volume is this: 1. Create one or more physical volumes. 2. Create a volume group from those one or more physical volumes. 3. Finally, you can create one or more logical volumes from the volume group.","title":"Logical Volume Creation Process"},{"location":"learning/linux/volumes/#creating-physical-volumes","text":"# Verify which devices are used su - lvmdiskscan # Shows all the storage devices that have the ability to be used with LVM. lsblk -p # Shows the partitions df -h # Display sizes in a human readable format. # Once verified which devices are not used # Create the physical volumes pvcreate /dev/sdb # initializes the disk for use by the logical volume manager. pvs # list of pvs","title":"Creating Physical Volumes"},{"location":"learning/linux/volumes/#creating-volume-groups","text":"vgcreate vg_app /dev/sdb # Volume group naming convention of \"vg_\". vgs # view the volume groups # It shows that we have 1 physical volume. # The size of the volume group is 50GB and we have 50GB free in the volume group. # If we look at our pvs with the pvs command, we'll now see what VG our PV belongs to.","title":"Creating Volume Groups"},{"location":"learning/linux/volumes/#creating-logical-volumes","text":"lvcreate -L 20G -n lv_data vg_app # Logical Volume naming convention of \"lv_\". # Note captial L is to give human readable volume size lvs # view logical volumes lvdisplay # Also to view lv, but it provides different output. # For example, notice the LV Path: \u200b /dev/vg_app/lv_data\u200b # It's easy to tell that the \u200b lv_data\u200b logical volume belongs to the vg_app\u200b volume group.","title":"Creating Logical Volumes"},{"location":"learning/linux/volumes/#creating-file-systems","text":"Now that we have a logical volume, we can treat it like we would a normal disk partition. So, let's put a file system on our logical volume and mount it. mkfs -t ext4 /dev/vg_app/lv_data mkdir /data mount /dev/vg_app/lv_data /data df -h /data","title":"Creating File Systems"},{"location":"learning/linux/volumes/#creating-multiple-logical-volumes-in-volume-groups","text":"lvcreate -L 5G -n lv_app vg_app # Now you can see we have two logical volumes in vg_app lvs # We can put a file system on this logical volume and mount it. mkfs -t ext4 /dev/vg_app/lv_app mkdir /app # Add Logical Volumes in the \u200b/etc/fstab\u200b so that it gets mounted at boot time. # vi /etc/fstab /dev/vg_app/lv_app /app ext4 defaults 0 0 # Save and quit mount /app df -h /app df -h # You also access your logical volume through the device mapper path as shown in the df output. # For example, \u200b /dev/vg_app/lv_app\u200b can be accessed via \u200b /dev/mapper/vg_app-lv_app\u200b . ls -l /dev/vg_app/lv_app ls -l /dev/mapper/vg_app-lv_app","title":"Creating Multiple Logical Volumes in Volume Groups"},{"location":"learning/linux/volumes/#logical-extents-and-physical-extents","text":"There is yet another layer of abstraction that we haven't talked about. Well, two layers of abstraction, really. Each of our LVs is actually divided up into LEs, which stands for logical extents. Or if we look at it from the other direction, a collection of Logical Extents makes up a Logical Volume. This is how LVM allows us to expand or shrink a logical volume \u2013 the logical volume manager just changes the number of underlying Logical Extents for the logical volume. lvdisplay -m # Show a map of the logical volume. # This map view tells us that the logical extents for the \u200blv_app\u200b LV resize on the \u200b/dev/sdb\u200b disk. # Like an LV is divided into LE, a PV is divided into PE, physical extents. # There is a one-to-one mapping of LEs to PEs. pvdisplay -m # Shows a map from the view of the disk","title":"Logical Extents and Physical Extents"},{"location":"learning/linux/volumes/#create-logical-volumes-using-percentage","text":"lvcreate -l 100 %FREE -n lv_logs vg_app # Note small l is used with % or passing free extents # We can squeeze every possible bit of space out of your volume group and put it into a logical volume.","title":"Create Logical Volumes using percentage"},{"location":"learning/linux/volumes/#extending-volume-groups","text":"Let's say that the \u200blv_data\u200b logical volume is getting full and we need to extend it. If we look at our volume group, we'll see we've already allocated all our space to the existing logical volumes. vgs # Output shows VFREE = 0 # In this case, we need to extend the volume group itself # before we can extend the logical volume within that volume group. lvmdiskscan # Shows sdc is free pvcreate /dev/sdc vgextend vg_app /dev/sdc # add this PV to our VG vgs # Output shows VFREE = 50G pvs # We have one pv that is complete full and one that is completely free. # Before we extend our logical volume into this free space, # let's look at the space from the file system level. df -h /data","title":"Extending Volume Groups"},{"location":"learning/linux/volumes/#extending-logical-volumes","text":"Now, let's use the lvextend command to add 5GB of space to that logical volume. In addition to increasing the size of the logical volume, we also need to grow the file system on that logical volume to fill up this new space. That's what the \u200b -r\u200b option is for. lvextend -L +5G -r /dev/vg_app/lv_data df -h /data # Shows filesystem has increased by 5G # If you forget to use the \u200b-r\u200b option to lvextend to perform the resize, you'll have to do that after the fact. lvextend -L +5G /dev/vg_app/lv_data lvs # lv size has increased df -h /data # The file system is still the same size, and there is no 5G increase # To fix this you'll have to use a resize tool for the specific filesystem you're working with. # For ext file systems, that tool is \u200bresize2fs\u200b. # We give it the path the the underlying device, which is a logical volume in our case. resize2fs /dev/vg_app/lv_data df -h /data # Shows filesystem has increased by 5G lvdisplay -m /dev/vg_app/lv_data # We see that some of the extents live on \u200b/dev/sdb\u200b while other extents live on \u200b/dev/sdc\u200b .","title":"Extending Logical Volumes"},{"location":"learning/linux/volumes/#creating-mirrored-logical-volumes","text":"Mirrored logical volume will ensure that an exact copy of the data will be stored on two different storage devices. lvmdiskscan # We have two more disks that we can use sdd and sde. pvcreate /dev/sdd /dev/sde vgcreate vg_safe /dev/sdd /dev/sde lvcreate -m 1 -L 5G -n lv_secrets vg_safe lvs # Copy%Sync column indicates if the mirror is synced. # When it finished syncing the data, it would be at 100%. lvs -a # The logical volume we created is actually RAID 1. So a mirror and raid 1 are the same thing. # Let's create a file system on that logical volume and mount it. mkfs -t ext4 /dev/vg_safe/lv_secrets mkdir /secrets mount /dev/vg_safe/lv_secrets /secrets df -h /secrets # So we write to \u200b /dev/vg_safe/lv_secrets\u200b and let the logical volume manager handle the mirroring. # We just use this file system like any other.","title":"Creating Mirrored Logical Volumes"},{"location":"learning/linux/volumes/#deleting-logical-volumes-volume-groups-and-physical-volumes","text":"# unmount the file system that is mounted inside logical volumes. umount /secrets # we can remove the underlying logical volume. lvremove /dev/vg_safe/lv_secrets # If you want to remove a pv from a vg, use the vgreduce command. vgs vgreduce vg_safe /dev/sde pvs pvremove /dev/sde # Let's finish destroying the \u200b vg_safe\u200b volume group with \u200b vgremove\u200b . vgremove vg_safe vgs pvs pvremove /dev/sdd","title":"Deleting Logical Volumes, Volume Groups, and Physical Volumes"},{"location":"learning/linux/volumes/#migrating-data-while-online","text":"Early I mentioned how easy it is to move data from one storage device to another with LVM. Let's say that the storage device attached to our system at \u200b /dev/sde \u200b is faster and has more space. Let's say we want to move the data that currently resides on \u200b /dev/sdb\u200b to that new disk. To do that, we'll just add \u200b /dev/sde \u200b to the volume group and migrate the data over. # we \u200bpvcreate\u200b the device. pvcreate /dev/sde # Now we add it to the volume group. vgextend vg_app /dev/sde pvs # Finally, we use the \u200bpvmove\u200b command to move all the data from \u200b/dev/sdb\u200b to \u200b/dev/sde\u200b pvmove /dev/sdb /dev/sde Once the \u200b pvmove\u200b command is complete, all the data the used to live on \u200b /dev/sdb\u200b now lives on \u200b /dev/sde\u200b . And the whole time this was happening, any logical volumes and file systems that where on \u200b /dev/sdb remained online and available throughout this entire process. There's no need to take an outage for this process. pvs # Now \u200b/dev/sdb\u200b is unused pvdisplay /dev/sdb # We see that \"Allocated PE\" is zero. # Now that we're done with this disk we can remove it from the volume group with \u200bvgreduce\u200b vgreduce vg_app /dev/sdb pvremove /dev/sdb pvs","title":"Migrating Data While Online"},{"location":"learning/tools/","text":"Knwoledge base \u00b6 This is the collection of all notes, reminders and whatnot I gathered during the years. Conventions \u00b6 Use sh as document language instead of shell when writing shell snippets in code blocks: - ```shell + ```sh #!/usr/bin/env zsh The local renderer just displays them better like this.","title":"Knwoledge base"},{"location":"learning/tools/#knwoledge-base","text":"This is the collection of all notes, reminders and whatnot I gathered during the years.","title":"Knwoledge base"},{"location":"learning/tools/#conventions","text":"Use sh as document language instead of shell when writing shell snippets in code blocks: - ```shell + ```sh #!/usr/bin/env zsh The local renderer just displays them better like this.","title":"Conventions"},{"location":"learning/tools/1password-cli/","text":"1password-cli \u00b6 TL;DR \u00b6 # installation brew cask install 1password-cli # first login op signin company.1password.com user.name@company.com # subsequent logins op signin company # automatically set environment variables # needed to run the export command manually eval $( op signin company ) # show all the items in the account op list items Gotchas \u00b6 After you have signed in the first time, you can sign in again using your account shorthand, which is your sign-in address subdomain (in this example, company ); op signin will prompt you for your password and output a command that can save your session token to an environment variable: op signin company Session tokens expire after 30 minutes of inactivity, after which you'll need to sign in again Further readings \u00b6 CLI guide Sources \u00b6 CLI getting started guide","title":"1password-cli"},{"location":"learning/tools/1password-cli/#1password-cli","text":"","title":"1password-cli"},{"location":"learning/tools/1password-cli/#tldr","text":"# installation brew cask install 1password-cli # first login op signin company.1password.com user.name@company.com # subsequent logins op signin company # automatically set environment variables # needed to run the export command manually eval $( op signin company ) # show all the items in the account op list items","title":"TL;DR"},{"location":"learning/tools/1password-cli/#gotchas","text":"After you have signed in the first time, you can sign in again using your account shorthand, which is your sign-in address subdomain (in this example, company ); op signin will prompt you for your password and output a command that can save your session token to an environment variable: op signin company Session tokens expire after 30 minutes of inactivity, after which you'll need to sign in again","title":"Gotchas"},{"location":"learning/tools/1password-cli/#further-readings","text":"CLI guide","title":"Further readings"},{"location":"learning/tools/1password-cli/#sources","text":"CLI getting started guide","title":"Sources"},{"location":"learning/tools/access%20a%20jar%20file%20contents/","text":"Access a jar file's contents \u00b6 A .jar file is nothing more than an archive. You can find all the files it contains just unzipping it: $ unzip file.jar Archive: file.jar creating: META-INF/ inflating: META-INF/MANIFEST.MF creating: org/ \u2026 inflating: META-INF/maven/org.apache.hive/hive-contrib/pom.properties","title":"Access a jar file's contents"},{"location":"learning/tools/access%20a%20jar%20file%20contents/#access-a-jar-files-contents","text":"A .jar file is nothing more than an archive. You can find all the files it contains just unzipping it: $ unzip file.jar Archive: file.jar creating: META-INF/ inflating: META-INF/MANIFEST.MF creating: org/ \u2026 inflating: META-INF/maven/org.apache.hive/hive-contrib/pom.properties","title":"Access a jar file's contents"},{"location":"learning/tools/acl/","text":"Access Control Lists assignment \u00b6 TL;DR \u00b6 # show acls of file test/declarations.h getfacl test/declarations.h # set permissions for user awe-user setfacl -m u:awe-user:rwx test/declarations.h # set permissions for group awe-group setfacl -m \"g:awe-group:r-x\" test/declarations.h # make children files and directories inherit acls # sets default acls setfacl -d -m u:dummy:rw test # remove a specific acl setfacl -x u:dummy:rw test # remove all acls setfacl -b test/declarations.h Sources \u00b6 Access Control Lists (ACL) in Linux","title":"Access Control Lists assignment"},{"location":"learning/tools/acl/#access-control-lists-assignment","text":"","title":"Access Control Lists assignment"},{"location":"learning/tools/acl/#tldr","text":"# show acls of file test/declarations.h getfacl test/declarations.h # set permissions for user awe-user setfacl -m u:awe-user:rwx test/declarations.h # set permissions for group awe-group setfacl -m \"g:awe-group:r-x\" test/declarations.h # make children files and directories inherit acls # sets default acls setfacl -d -m u:dummy:rw test # remove a specific acl setfacl -x u:dummy:rw test # remove all acls setfacl -b test/declarations.h","title":"TL;DR"},{"location":"learning/tools/acl/#sources","text":"Access Control Lists (ACL) in Linux","title":"Sources"},{"location":"learning/tools/ansible/","text":"Ansible \u00b6 TL;DR Templating Tests Loops Roles Get roles Role dependencies Output formatting Troubleshooting Print all known variables Force notified handlers to run at a specific point Run specific tasks even in check mode Dry-run only specific tasks Set up recursive permissions on a directory so that directories are set to 755 and files to 644 Only run a task when another has a specific result Define when a task changed or failed Set environment variables for a play, role or task Set variables to the value of environment variables Check if a list contains an item and fail otherwise Define different values for true / false / null Force a task or play to use a specific Python interpreter Further readings Sources TL;DR \u00b6 # Install. pip3 install --user ansible && port install sshpass # darwin sudo pamac install ansible sshpass # manjaro linux # Show hosts' ansible facts. ansible -i hostfile -m setup all ansible -i host1,hostn, -m setup host1 -u remote-user ansible -i localhost, -c local -km setup localhost # Check the syntax of a playbook. # This will *not* execute the plays inside it. ansible-playbook path/to/playbook.yml --syntax-check # Execute a playbook. ansible-playbook path/to/playbook.yml -i hosts.list ansible-playbook path/to/playbook.yml -i host1,host2,hostn, -l hosts,list ansible-playbook path/to/playbook.yml -i host1,host2,other, -l hosts-pattern # Show what changes (with details) a play whould apply to the local machine. ansible-playbook path/to/playbook.yml -i localhost, -c local -vvC # Only execute tasks with specific tags. ansible-playbook path/to/playbook.yml --tags \"configuration,packages\" # Avoid executing tasks with specific tags. ansible-playbook path/to/playbook.yml --skip-tags \"system,user\" # Check what tasks will be executed. ansible-playbook example.yml --list-tasks ansible-playbook example.yml --list-tasks --tags \"configuration,packages\" ansible-playbook example.yml --list-tasks --skip-tags \"system,user\" # List roles installed from Galaxy. ansible-galaxy list # Install roles from Galaxy. ansible-galaxy install namespace.role ansible-galaxy install --roles-path ~/ansible-roles namespace.role ansible-galaxy install namespace.role,v1.0.0 ansible-galaxy install git+https://github.com/namespace/role.git,commit-hash ansible-galaxy install -r requirements.yml # Remove roles installed from Galaxy. ansible-galaxy remove namespace.role Templating \u00b6 Ansible leverages Jinja2 templating , which can be used directly in tasks or through the template module. All Jinja2's standard filters and tests can be used, with the addition of: specialized filters for selecting and transforming data tests for evaluating template expressions lookup plugins for retrieving data from external sources for use in templating All templating happens on the Ansible controller , before the task is sent and executed on the target machine. Updated examples are available. # Remove empty or false values from a list piping it to 'select()'. # Returns [\"string\"]. - vars : list : [ \"\" , \"string\" , 0 , false ] ansible.builtin.debug : var : list | select # Remove only empty strings from a list 'reject()'ing them. # Returns [\"string\", 0, false]. - vars : list : [ \"\" , \"string\" , 0 , false ] ansible.builtin.debug : var : list | reject('match', '^$') # Merge two lists. # Returns [\"a\", \"b\", \"c\", \"d\"]. - vars : list1 : [ \"a\" , \"b\" ] list2 : [ \"c\" , \"d\" ] ansible.builtin.debug : var : list1 + list2 # Dedupe elements in a list. # Returns [\"a\", \"b\"]. - vars : list : [ \"a\" , \"b\" , \"b\" , \"a\" ] ansible.builtin.debug : var : list | unique # Sort a list by version number (not lexicographically). # Returns ['2.7.0', '2.8.0', '2.9.0', '2.10.0' '2.11.0']. - vars : list : [ '2.8.0' , '2.11.0' , '2.7.0' , '2.10.0' , '2.9.0' ] ansible.builtin.debug : var : list | community.general.version_sort # Generate a random password. # Returns a random string following the specifications. - vars : password : \"{{ lookup('password', '/dev/null length=32 chars=ascii_letters,digits,punctuation') }}\" ansible.builtin.debug : var : password # Hash a password. # Returns a hash of the requested type. - vars : password : abcd salt : \"{{ lookup('community.general.random_string', special=false) }}\" ansible.builtin.debug : var : password | password_hash('sha512', salt) # Get a variable's type. - ansible.builtin.debug : var : \"'string' | type_debug\" Tests \u00b6 Return a boolean result. # Compare semver version numbers. - ansible.builtin.debug : var : \"'2.0.0-rc.1+build.123' is version('2.1.0-rc.2+build.423', 'ge', version_type='semver')\" Loops \u00b6 # Get the values of some special variables. # See the 'Further readings' section for the full list. - ansible.builtin.debug : var : \"{{ item }}\" with_items : [ \"ansible_local\" , \"playbook_dir\" , \"role_path\" ] # Fail when any of the given variables is an empty string. # Returns the ones which are empty. - when : lookup('vars', item) == '' ansible.builtin.fail : msg : \"The {{ item }} variable is an empty string\" loop : - variable1 - variableN # Iterate thrugh nested loops. - vars : middles : - 'middle1' - 'middle2' ansible.builtin.debug : msg : \"{{ item[0] }}, {{ item[1] }}, {{ item[2] }}\" with_nested : - [ 'outer1' , 'outer2' ] - \"{{ middles }}\" - [ 'inner1' , 'inner2' ] Roles \u00b6 Get roles \u00b6 Roles can be either created : ansible-galaxy init role-name or installed from Ansible Galaxy : --- # requirements.yml collections : - community.docker ansible-galaxy install mcereda.boinc_client ansible-galaxy install --roles-path ~/ansible-roles namespace.role ansible-galaxy install namespace.role,v1.0.0 ansible-galaxy install git+https://github.com/namespace/role.git,0b7cd353c0250e87a26e0499e59e7fd265cc2f25 ansible-galaxy install -r requirements.yml Role dependencies \u00b6 --- # role/meta/main.yml dependencies : - role : common vars : some_parameter : 3 - role : postgres vars : dbname : blarg other_parameter : 12 Output formatting \u00b6 Introduced in Ansible 2.5 Change Ansible's output setting the stdout callback to json or yaml : ANSIBLE_STDOUT_CALLBACK = yaml # ansible.cfg [defaults] stdout_callback = json yaml will set tasks output only to be in the defined format: $ ANSIBLE_STDOUT_CALLBACK=yaml ansible-playbook --inventory=localhost.localdomain, ansible/localhost.configure.yml -vv --check PLAY [Configure localhost] ******************************************************************* TASK [Upgrade system packages] *************************************************************** task path: /home/user/localhost.configure.yml:7 ok: [localhost.localdomain] => changed=false cmd: - /usr/bin/zypper - --quiet - --non-interactive \u2026 update_cache: false The json output format will be a single, long JSON file: $ ANSIBLE_STDOUT_CALLBACK=yaml ansible-playbook --inventory=localhost.localdomain, ansible/localhost.configure.yml -vv --check { \"custom_stats\": {}, \"global_custom_stats\": {}, \"plays\": [ { \"play\": { \u2026 \"name\": \"Configure localhost\" }, \"tasks\": [ { \"hosts\": { \"localhost.localdomain\": { \u2026 \"action\": \"community.general.zypper\", \"changed\": false, \u2026 \"update_cache\": false } } \u2026 \u2026 } Troubleshooting \u00b6 Print all known variables \u00b6 Print the special variable vars as a task: - name : Debug all variables ansible.builtin.debug : var=vars Force notified handlers to run at a specific point \u00b6 Use the meta plugin with the flush_handlers option: - name : Force all notified handlers to run at this point, not waiting for normal sync points ansible.builtin.meta : flush_handlers Run specific tasks even in check mode \u00b6 Add the check_mode: false pair to the task: - name : this task will make changes to the system even in check mode check_mode : false ansible.builtin.command : /something/to/run --even-in-check-mode Dry-run only specific tasks \u00b6 Add the check_mode: true pair to the task: - name : This task will always run under checkmode and not change the system check_mode : true ansible.builtin.lineinfile : line : \"important file\" dest : /path/to/file.conf state : present Set up recursive permissions on a directory so that directories are set to 755 and files to 644 \u00b6 Use the special X mode setting in the file plugin: - name : Fix files and directories' permissions ansible.builtin.file : dest : /path/to/some/dir mode : u=rwX,g=rX,o=rX recurse : yes Only run a task when another has a specific result \u00b6 When a task executes, it also stores the two special values changed and failed in its results. You can use those as conditions to execute the next ones: - name : Trigger task ansible.builtin.command : any register : trigger_task ignore_errors : true - name : Run only on change when : trigger_task.changed ansible.builtin.debug : msg=\"The trigger task changed\" - name : Run only on failure when : trigger_task.failed ansible.builtin.debug : msg=\"The trigger task failed\" Alternatively, you can use special checks built for this: - name : Run only on success when : trigger_task is succeeded ansible.builtin.debug : msg=\"The trigger task succeeded\" - name : Run only on change when : trigger_task is changed ansible.builtin.debug : msg=\"The trigger task changed\" - name : Run only on failure when : trigger_task is failed ansible.builtin.debug : msg=\"The trigger task failed\" - name : Run only on skip when : trigger_task is skipped ansible.builtin.debug : msg=\"The trigger task skipped\" Define when a task changed or failed \u00b6 This lets you avoid using ignore_errors . Use the changed_when and failed_when attributes to define your own conditions: - name : Task with custom results ansible.builtin.command : any register : result changed_when : - result.rc == 2 - result.stderr | regex_search('things changed') failed_when : - result.rc != 0 - not (result.stderr | regex_search('all good')) Set environment variables for a play, role or task \u00b6 Environment variables can be set at a play, block, or task level using the environment keyword: - name : Use environment variables for a task environment : HTTP_PROXY : http://example.proxy ansible.builtin.command : curl ifconfig.io The environment keyword does not affect Ansible itself or its configuration settings, the environment for other users, or the execution of other plugins like lookups and filters; variables set with environment do not automatically become Ansible facts, even when set at the play level. Set variables to the value of environment variables \u00b6 Use the lookup() plugin with the env option: - name : Use a local environment variable ansible.builtin.debug : msg=\"HOME={{ lookup('env', 'HOME') }}\" Check if a list contains an item and fail otherwise \u00b6 - name : Check if a list contains an item and fail otherwise when : item not in list ansible.builtin.fail : msg=\"item not in list\" Define different values for true / false / null \u00b6 Create a test and define two values: the first will be returned when the test returns true , the second will be returned when the test returns false (Ansible 1.9+): {{ (ansible_pkg_mgr == 'zypper') | ternary('gnu_parallel' , 'parallel' )) }} Since Ansible 2.8 you can define a third value to be returned when the test returns null : {{ autoscaling_enabled | ternary(true , false , omit) }} Force a task or play to use a specific Python interpreter \u00b6 Just set it in the Play's or Task's variables: vars : ansible_python_interpreter : /usr/local/bin/python3.9 Further readings \u00b6 Templating Roles Tests Special variables Automating Helm using Ansible Edit .ini file in other servers using Ansible PlayBook Yes and No, True and False Ansible Galaxy user guide Windows playbook example Sources \u00b6 Removing empty values from a list and assigning it to a new list Human-Readable Output Format How to append to lists Check if a list contains an item in ansible Working with versions How to install SSHpass on Mac Include task only if file exists Unique filter of list in jinja2 Only do something if another action changed How to recursively set directory and file permissions","title":"Ansible"},{"location":"learning/tools/ansible/#ansible","text":"TL;DR Templating Tests Loops Roles Get roles Role dependencies Output formatting Troubleshooting Print all known variables Force notified handlers to run at a specific point Run specific tasks even in check mode Dry-run only specific tasks Set up recursive permissions on a directory so that directories are set to 755 and files to 644 Only run a task when another has a specific result Define when a task changed or failed Set environment variables for a play, role or task Set variables to the value of environment variables Check if a list contains an item and fail otherwise Define different values for true / false / null Force a task or play to use a specific Python interpreter Further readings Sources","title":"Ansible"},{"location":"learning/tools/ansible/#tldr","text":"# Install. pip3 install --user ansible && port install sshpass # darwin sudo pamac install ansible sshpass # manjaro linux # Show hosts' ansible facts. ansible -i hostfile -m setup all ansible -i host1,hostn, -m setup host1 -u remote-user ansible -i localhost, -c local -km setup localhost # Check the syntax of a playbook. # This will *not* execute the plays inside it. ansible-playbook path/to/playbook.yml --syntax-check # Execute a playbook. ansible-playbook path/to/playbook.yml -i hosts.list ansible-playbook path/to/playbook.yml -i host1,host2,hostn, -l hosts,list ansible-playbook path/to/playbook.yml -i host1,host2,other, -l hosts-pattern # Show what changes (with details) a play whould apply to the local machine. ansible-playbook path/to/playbook.yml -i localhost, -c local -vvC # Only execute tasks with specific tags. ansible-playbook path/to/playbook.yml --tags \"configuration,packages\" # Avoid executing tasks with specific tags. ansible-playbook path/to/playbook.yml --skip-tags \"system,user\" # Check what tasks will be executed. ansible-playbook example.yml --list-tasks ansible-playbook example.yml --list-tasks --tags \"configuration,packages\" ansible-playbook example.yml --list-tasks --skip-tags \"system,user\" # List roles installed from Galaxy. ansible-galaxy list # Install roles from Galaxy. ansible-galaxy install namespace.role ansible-galaxy install --roles-path ~/ansible-roles namespace.role ansible-galaxy install namespace.role,v1.0.0 ansible-galaxy install git+https://github.com/namespace/role.git,commit-hash ansible-galaxy install -r requirements.yml # Remove roles installed from Galaxy. ansible-galaxy remove namespace.role","title":"TL;DR"},{"location":"learning/tools/ansible/#templating","text":"Ansible leverages Jinja2 templating , which can be used directly in tasks or through the template module. All Jinja2's standard filters and tests can be used, with the addition of: specialized filters for selecting and transforming data tests for evaluating template expressions lookup plugins for retrieving data from external sources for use in templating All templating happens on the Ansible controller , before the task is sent and executed on the target machine. Updated examples are available. # Remove empty or false values from a list piping it to 'select()'. # Returns [\"string\"]. - vars : list : [ \"\" , \"string\" , 0 , false ] ansible.builtin.debug : var : list | select # Remove only empty strings from a list 'reject()'ing them. # Returns [\"string\", 0, false]. - vars : list : [ \"\" , \"string\" , 0 , false ] ansible.builtin.debug : var : list | reject('match', '^$') # Merge two lists. # Returns [\"a\", \"b\", \"c\", \"d\"]. - vars : list1 : [ \"a\" , \"b\" ] list2 : [ \"c\" , \"d\" ] ansible.builtin.debug : var : list1 + list2 # Dedupe elements in a list. # Returns [\"a\", \"b\"]. - vars : list : [ \"a\" , \"b\" , \"b\" , \"a\" ] ansible.builtin.debug : var : list | unique # Sort a list by version number (not lexicographically). # Returns ['2.7.0', '2.8.0', '2.9.0', '2.10.0' '2.11.0']. - vars : list : [ '2.8.0' , '2.11.0' , '2.7.0' , '2.10.0' , '2.9.0' ] ansible.builtin.debug : var : list | community.general.version_sort # Generate a random password. # Returns a random string following the specifications. - vars : password : \"{{ lookup('password', '/dev/null length=32 chars=ascii_letters,digits,punctuation') }}\" ansible.builtin.debug : var : password # Hash a password. # Returns a hash of the requested type. - vars : password : abcd salt : \"{{ lookup('community.general.random_string', special=false) }}\" ansible.builtin.debug : var : password | password_hash('sha512', salt) # Get a variable's type. - ansible.builtin.debug : var : \"'string' | type_debug\"","title":"Templating"},{"location":"learning/tools/ansible/#tests","text":"Return a boolean result. # Compare semver version numbers. - ansible.builtin.debug : var : \"'2.0.0-rc.1+build.123' is version('2.1.0-rc.2+build.423', 'ge', version_type='semver')\"","title":"Tests"},{"location":"learning/tools/ansible/#loops","text":"# Get the values of some special variables. # See the 'Further readings' section for the full list. - ansible.builtin.debug : var : \"{{ item }}\" with_items : [ \"ansible_local\" , \"playbook_dir\" , \"role_path\" ] # Fail when any of the given variables is an empty string. # Returns the ones which are empty. - when : lookup('vars', item) == '' ansible.builtin.fail : msg : \"The {{ item }} variable is an empty string\" loop : - variable1 - variableN # Iterate thrugh nested loops. - vars : middles : - 'middle1' - 'middle2' ansible.builtin.debug : msg : \"{{ item[0] }}, {{ item[1] }}, {{ item[2] }}\" with_nested : - [ 'outer1' , 'outer2' ] - \"{{ middles }}\" - [ 'inner1' , 'inner2' ]","title":"Loops"},{"location":"learning/tools/ansible/#roles","text":"","title":"Roles"},{"location":"learning/tools/ansible/#get-roles","text":"Roles can be either created : ansible-galaxy init role-name or installed from Ansible Galaxy : --- # requirements.yml collections : - community.docker ansible-galaxy install mcereda.boinc_client ansible-galaxy install --roles-path ~/ansible-roles namespace.role ansible-galaxy install namespace.role,v1.0.0 ansible-galaxy install git+https://github.com/namespace/role.git,0b7cd353c0250e87a26e0499e59e7fd265cc2f25 ansible-galaxy install -r requirements.yml","title":"Get roles"},{"location":"learning/tools/ansible/#role-dependencies","text":"--- # role/meta/main.yml dependencies : - role : common vars : some_parameter : 3 - role : postgres vars : dbname : blarg other_parameter : 12","title":"Role dependencies"},{"location":"learning/tools/ansible/#output-formatting","text":"Introduced in Ansible 2.5 Change Ansible's output setting the stdout callback to json or yaml : ANSIBLE_STDOUT_CALLBACK = yaml # ansible.cfg [defaults] stdout_callback = json yaml will set tasks output only to be in the defined format: $ ANSIBLE_STDOUT_CALLBACK=yaml ansible-playbook --inventory=localhost.localdomain, ansible/localhost.configure.yml -vv --check PLAY [Configure localhost] ******************************************************************* TASK [Upgrade system packages] *************************************************************** task path: /home/user/localhost.configure.yml:7 ok: [localhost.localdomain] => changed=false cmd: - /usr/bin/zypper - --quiet - --non-interactive \u2026 update_cache: false The json output format will be a single, long JSON file: $ ANSIBLE_STDOUT_CALLBACK=yaml ansible-playbook --inventory=localhost.localdomain, ansible/localhost.configure.yml -vv --check { \"custom_stats\": {}, \"global_custom_stats\": {}, \"plays\": [ { \"play\": { \u2026 \"name\": \"Configure localhost\" }, \"tasks\": [ { \"hosts\": { \"localhost.localdomain\": { \u2026 \"action\": \"community.general.zypper\", \"changed\": false, \u2026 \"update_cache\": false } } \u2026 \u2026 }","title":"Output formatting"},{"location":"learning/tools/ansible/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/ansible/#print-all-known-variables","text":"Print the special variable vars as a task: - name : Debug all variables ansible.builtin.debug : var=vars","title":"Print all known variables"},{"location":"learning/tools/ansible/#force-notified-handlers-to-run-at-a-specific-point","text":"Use the meta plugin with the flush_handlers option: - name : Force all notified handlers to run at this point, not waiting for normal sync points ansible.builtin.meta : flush_handlers","title":"Force notified handlers to run at a specific point"},{"location":"learning/tools/ansible/#run-specific-tasks-even-in-check-mode","text":"Add the check_mode: false pair to the task: - name : this task will make changes to the system even in check mode check_mode : false ansible.builtin.command : /something/to/run --even-in-check-mode","title":"Run specific tasks even in check mode"},{"location":"learning/tools/ansible/#dry-run-only-specific-tasks","text":"Add the check_mode: true pair to the task: - name : This task will always run under checkmode and not change the system check_mode : true ansible.builtin.lineinfile : line : \"important file\" dest : /path/to/file.conf state : present","title":"Dry-run only specific tasks"},{"location":"learning/tools/ansible/#set-up-recursive-permissions-on-a-directory-so-that-directories-are-set-to-755-and-files-to-644","text":"Use the special X mode setting in the file plugin: - name : Fix files and directories' permissions ansible.builtin.file : dest : /path/to/some/dir mode : u=rwX,g=rX,o=rX recurse : yes","title":"Set up recursive permissions on a directory so that directories are set to 755 and files to 644"},{"location":"learning/tools/ansible/#only-run-a-task-when-another-has-a-specific-result","text":"When a task executes, it also stores the two special values changed and failed in its results. You can use those as conditions to execute the next ones: - name : Trigger task ansible.builtin.command : any register : trigger_task ignore_errors : true - name : Run only on change when : trigger_task.changed ansible.builtin.debug : msg=\"The trigger task changed\" - name : Run only on failure when : trigger_task.failed ansible.builtin.debug : msg=\"The trigger task failed\" Alternatively, you can use special checks built for this: - name : Run only on success when : trigger_task is succeeded ansible.builtin.debug : msg=\"The trigger task succeeded\" - name : Run only on change when : trigger_task is changed ansible.builtin.debug : msg=\"The trigger task changed\" - name : Run only on failure when : trigger_task is failed ansible.builtin.debug : msg=\"The trigger task failed\" - name : Run only on skip when : trigger_task is skipped ansible.builtin.debug : msg=\"The trigger task skipped\"","title":"Only run a task when another has a specific result"},{"location":"learning/tools/ansible/#define-when-a-task-changed-or-failed","text":"This lets you avoid using ignore_errors . Use the changed_when and failed_when attributes to define your own conditions: - name : Task with custom results ansible.builtin.command : any register : result changed_when : - result.rc == 2 - result.stderr | regex_search('things changed') failed_when : - result.rc != 0 - not (result.stderr | regex_search('all good'))","title":"Define when a task changed or failed"},{"location":"learning/tools/ansible/#set-environment-variables-for-a-play-role-or-task","text":"Environment variables can be set at a play, block, or task level using the environment keyword: - name : Use environment variables for a task environment : HTTP_PROXY : http://example.proxy ansible.builtin.command : curl ifconfig.io The environment keyword does not affect Ansible itself or its configuration settings, the environment for other users, or the execution of other plugins like lookups and filters; variables set with environment do not automatically become Ansible facts, even when set at the play level.","title":"Set environment variables for a play, role or task"},{"location":"learning/tools/ansible/#set-variables-to-the-value-of-environment-variables","text":"Use the lookup() plugin with the env option: - name : Use a local environment variable ansible.builtin.debug : msg=\"HOME={{ lookup('env', 'HOME') }}\"","title":"Set variables to the value of environment variables"},{"location":"learning/tools/ansible/#check-if-a-list-contains-an-item-and-fail-otherwise","text":"- name : Check if a list contains an item and fail otherwise when : item not in list ansible.builtin.fail : msg=\"item not in list\"","title":"Check if a list contains an item and fail otherwise"},{"location":"learning/tools/ansible/#define-different-values-for-truefalsenull","text":"Create a test and define two values: the first will be returned when the test returns true , the second will be returned when the test returns false (Ansible 1.9+): {{ (ansible_pkg_mgr == 'zypper') | ternary('gnu_parallel' , 'parallel' )) }} Since Ansible 2.8 you can define a third value to be returned when the test returns null : {{ autoscaling_enabled | ternary(true , false , omit) }}","title":"Define different values for true/false/null"},{"location":"learning/tools/ansible/#force-a-task-or-play-to-use-a-specific-python-interpreter","text":"Just set it in the Play's or Task's variables: vars : ansible_python_interpreter : /usr/local/bin/python3.9","title":"Force a task or play to use a specific Python interpreter"},{"location":"learning/tools/ansible/#further-readings","text":"Templating Roles Tests Special variables Automating Helm using Ansible Edit .ini file in other servers using Ansible PlayBook Yes and No, True and False Ansible Galaxy user guide Windows playbook example","title":"Further readings"},{"location":"learning/tools/ansible/#sources","text":"Removing empty values from a list and assigning it to a new list Human-Readable Output Format How to append to lists Check if a list contains an item in ansible Working with versions How to install SSHpass on Mac Include task only if file exists Unique filter of list in jinja2 Only do something if another action changed How to recursively set directory and file permissions","title":"Sources"},{"location":"learning/tools/antigen/","text":"Antigen \u00b6 Troubleshooting \u00b6 While loading, a completion fails with error No such file or directory . \u00b6 Example: tee: /Users/user/.antigen/bundles/robbyrussell/oh-my-zsh/cache//completions/_helm: No such file or directory /Users/user/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/helm/helm.plugin.zsh:source:9: no such file or directory: /Users/user/.antigen/bundles/robbyrussell/oh-my-zsh/cache//completions/_helm The issue is due of the $ZSH_CACHE_DIR/completions being missing and tee not creating it on Mac OS X. Create the missing completions directory and re-run antigen: mkdir -p $ZSH_CACHE_DIR /completions antigen apply Further readings \u00b6 Github 's repository Antigen's Wiki","title":"Antigen"},{"location":"learning/tools/antigen/#antigen","text":"","title":"Antigen"},{"location":"learning/tools/antigen/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/antigen/#while-loading-a-completion-fails-with-error-no-such-file-or-directory","text":"Example: tee: /Users/user/.antigen/bundles/robbyrussell/oh-my-zsh/cache//completions/_helm: No such file or directory /Users/user/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/helm/helm.plugin.zsh:source:9: no such file or directory: /Users/user/.antigen/bundles/robbyrussell/oh-my-zsh/cache//completions/_helm The issue is due of the $ZSH_CACHE_DIR/completions being missing and tee not creating it on Mac OS X. Create the missing completions directory and re-run antigen: mkdir -p $ZSH_CACHE_DIR /completions antigen apply","title":"While loading, a completion fails with error No such file or directory."},{"location":"learning/tools/antigen/#further-readings","text":"Github 's repository Antigen's Wiki","title":"Further readings"},{"location":"learning/tools/apk/","text":"APK \u00b6 TL;DR \u00b6 # Update the package lists. apk update # Search for packages apk search duperemove apk search -a parallel apk --no-cache search -v # Get information about packages. apk info htop apk --no-cache info -a curl # List installed packages. apk list -I # Install packages. apk add zstd apk --no-cache add -i zfs = 2 .1.4-r0 xz> = 5 .2.0 apk -s add --allow-untrusted path/to/foo.apk # Upgrade packages. apk upgrade apk --no-cache add -iu apk-tools apk -s add -u --allow-untrusted path/to/foo.apk # Remove packages. apk del php7 # Remove cache. apk cache clean apk cache -v sync # Find what package provides a file. apk info --who-owns /etc/passwd # List files included in a package. apk info -L zsh # Check if a package is installed. apk info -e fdupes # List packages dependencies. apk info -R atop # List packages depending on a package. apk info -r bash # Show the installed size of an installed package. apk info -s top # Get an installed package's description. apk info -d parallel Sources \u00b6 10 Alpine Linux apk Command Examples","title":"APK"},{"location":"learning/tools/apk/#apk","text":"","title":"APK"},{"location":"learning/tools/apk/#tldr","text":"# Update the package lists. apk update # Search for packages apk search duperemove apk search -a parallel apk --no-cache search -v # Get information about packages. apk info htop apk --no-cache info -a curl # List installed packages. apk list -I # Install packages. apk add zstd apk --no-cache add -i zfs = 2 .1.4-r0 xz> = 5 .2.0 apk -s add --allow-untrusted path/to/foo.apk # Upgrade packages. apk upgrade apk --no-cache add -iu apk-tools apk -s add -u --allow-untrusted path/to/foo.apk # Remove packages. apk del php7 # Remove cache. apk cache clean apk cache -v sync # Find what package provides a file. apk info --who-owns /etc/passwd # List files included in a package. apk info -L zsh # Check if a package is installed. apk info -e fdupes # List packages dependencies. apk info -R atop # List packages depending on a package. apk info -r bash # Show the installed size of an installed package. apk info -s top # Get an installed package's description. apk info -d parallel","title":"TL;DR"},{"location":"learning/tools/apk/#sources","text":"10 Alpine Linux apk Command Examples","title":"Sources"},{"location":"learning/tools/apt/","text":"The APT package manager \u00b6 TL;DR \u00b6 # mark all packages as non-explicitly installed apt-mark auto $( sudo apt-mark showmanual ) # remove orphaned packages apt autoremove --purge Further readings \u00b6 Apt configuration Configuring Apt sources","title":"The APT package manager"},{"location":"learning/tools/apt/#the-apt-package-manager","text":"","title":"The APT package manager"},{"location":"learning/tools/apt/#tldr","text":"# mark all packages as non-explicitly installed apt-mark auto $( sudo apt-mark showmanual ) # remove orphaned packages apt autoremove --purge","title":"TL;DR"},{"location":"learning/tools/apt/#further-readings","text":"Apt configuration Configuring Apt sources","title":"Further readings"},{"location":"learning/tools/arch%20linux/","text":"Arch linux \u00b6 # Manually install packages from the AUR. git clone https://aur.archlinux.org/jdupes.git \\ && cd jdupes \\ && makepkg -sirc && git clean -dfX Further readings \u00b6 Suspend and hibernate Arch User Repository","title":"Arch linux"},{"location":"learning/tools/arch%20linux/#arch-linux","text":"# Manually install packages from the AUR. git clone https://aur.archlinux.org/jdupes.git \\ && cd jdupes \\ && makepkg -sirc && git clean -dfX","title":"Arch linux"},{"location":"learning/tools/arch%20linux/#further-readings","text":"Suspend and hibernate Arch User Repository","title":"Further readings"},{"location":"learning/tools/asdf/","text":"ASDF \u00b6 asdf is a CLI tool to manage multiple language runtime versions on a per-project basis. It works like gvm , nvm , rbenv and pyenv (and more) all in one. TL;DR \u00b6 # list installed plugins asdf plugin list # list available plugins asdf plugin list all # install plugins asdf plugin add helm # update plugins asdf plugin update kubectl asdf plugin update --all # remove plugins asdf plugin remove terraform # list installed versions asdf list elixir # list available versions asdf list all elixir # install a version asdf install erlang latest asdf install terraform 1 .1.1 # set a specific installed version to use asdf global helm 3 .3 3 .2 asdf shell erlang latest asdf local elixir system # uninstall a version asdf uninstall helm 3 .3 # show the current status asdf current asdf current helm Installation \u00b6 # install the program brew install asdf # load its shell file and completion # or just load oh-my-zsh's plugin . $( brew --prefix asdf ) /asdf.sh Plugins management \u00b6 # list installed plugins asdf plugin list asdf plugin list --urls # list all plugins (available too) asdf plugin list all # asdf plugin add $PLUGIN_NAME [$PLUGIN_URL] asdf plugin add helm asdf plugin update --all asdf plugin update $PLUGIN_NAME asdf plugin remove $PLUGIN_NAME Plugins gotchas \u00b6 asdf plugin list all or asdf plugin add $PLUGIN_NAME also trigger a sync to the plugins repository. Versions management \u00b6 # list installed versions for a plugin # asdf list $PLUGIN_NAME asdf list elixir # list all available versions for a plugin # asdf list all $PLUGIN_NAME asdf list all elixir # install a plugin version # asdf install $PLUGIN_NAME $PLUGIN_VERSION asdf install erlang latest # check current plugin version # asdf current [$PLUGIN_NAME] asdf current asdf current helm # set plugin version # asdf global|shell|local $PLUGIN_NAME $PLUGIN_VERSION [$PLUGIN_VERSION,...] asdf global helm 3 .3 3 .2 asdf shell erlang latest asdf local elixir latest # fallback to system version # asdf local $PLUGIN_NAME system asdf local python system # uninstall a version # asdf uninstall $PLUGIN_NAME $PLUGIN_VERSION asdf uninstall helm 3 .3 Further readings \u00b6 the project's homepage the project's github page plugins list","title":"ASDF"},{"location":"learning/tools/asdf/#asdf","text":"asdf is a CLI tool to manage multiple language runtime versions on a per-project basis. It works like gvm , nvm , rbenv and pyenv (and more) all in one.","title":"ASDF"},{"location":"learning/tools/asdf/#tldr","text":"# list installed plugins asdf plugin list # list available plugins asdf plugin list all # install plugins asdf plugin add helm # update plugins asdf plugin update kubectl asdf plugin update --all # remove plugins asdf plugin remove terraform # list installed versions asdf list elixir # list available versions asdf list all elixir # install a version asdf install erlang latest asdf install terraform 1 .1.1 # set a specific installed version to use asdf global helm 3 .3 3 .2 asdf shell erlang latest asdf local elixir system # uninstall a version asdf uninstall helm 3 .3 # show the current status asdf current asdf current helm","title":"TL;DR"},{"location":"learning/tools/asdf/#installation","text":"# install the program brew install asdf # load its shell file and completion # or just load oh-my-zsh's plugin . $( brew --prefix asdf ) /asdf.sh","title":"Installation"},{"location":"learning/tools/asdf/#plugins-management","text":"# list installed plugins asdf plugin list asdf plugin list --urls # list all plugins (available too) asdf plugin list all # asdf plugin add $PLUGIN_NAME [$PLUGIN_URL] asdf plugin add helm asdf plugin update --all asdf plugin update $PLUGIN_NAME asdf plugin remove $PLUGIN_NAME","title":"Plugins management"},{"location":"learning/tools/asdf/#plugins-gotchas","text":"asdf plugin list all or asdf plugin add $PLUGIN_NAME also trigger a sync to the plugins repository.","title":"Plugins gotchas"},{"location":"learning/tools/asdf/#versions-management","text":"# list installed versions for a plugin # asdf list $PLUGIN_NAME asdf list elixir # list all available versions for a plugin # asdf list all $PLUGIN_NAME asdf list all elixir # install a plugin version # asdf install $PLUGIN_NAME $PLUGIN_VERSION asdf install erlang latest # check current plugin version # asdf current [$PLUGIN_NAME] asdf current asdf current helm # set plugin version # asdf global|shell|local $PLUGIN_NAME $PLUGIN_VERSION [$PLUGIN_VERSION,...] asdf global helm 3 .3 3 .2 asdf shell erlang latest asdf local elixir latest # fallback to system version # asdf local $PLUGIN_NAME system asdf local python system # uninstall a version # asdf uninstall $PLUGIN_NAME $PLUGIN_VERSION asdf uninstall helm 3 .3","title":"Versions management"},{"location":"learning/tools/asdf/#further-readings","text":"the project's homepage the project's github page plugins list","title":"Further readings"},{"location":"learning/tools/atom/","text":"Atom \u00b6 Troubleshooting \u00b6 Zsh terminal icons are not getting displayed in Atom PlatformIO Ide Terminal \u00b6 Change font to NotoSansMono Nerd Font in PlatformIO Ide Terminal's settings. See Why Zsh terminal icons are not getting displayed in Atom PlatformIO Ide Terminal? Sources \u00b6 Why Zsh terminal icons are not getting displayed in Atom PlatformIO Ide Terminal?","title":"Atom"},{"location":"learning/tools/atom/#atom","text":"","title":"Atom"},{"location":"learning/tools/atom/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/atom/#zsh-terminal-icons-are-not-getting-displayed-in-atom-platformio-ide-terminal","text":"Change font to NotoSansMono Nerd Font in PlatformIO Ide Terminal's settings. See Why Zsh terminal icons are not getting displayed in Atom PlatformIO Ide Terminal?","title":"Zsh terminal icons are not getting displayed in Atom PlatformIO Ide Terminal"},{"location":"learning/tools/atom/#sources","text":"Why Zsh terminal icons are not getting displayed in Atom PlatformIO Ide Terminal?","title":"Sources"},{"location":"learning/tools/authenticate%20without%20using%20sudo/","text":"Authenticate without using sudo \u00b6 Polkit \u00b6 Requires [polkit] to be: installed configured to authorize and authenticate the users pkexec COMMAND Further readings \u00b6 pkexec Sources \u00b6 How to get gui sudo password prompt without command line","title":"Authenticate without using sudo"},{"location":"learning/tools/authenticate%20without%20using%20sudo/#authenticate-without-using-sudo","text":"","title":"Authenticate without using sudo"},{"location":"learning/tools/authenticate%20without%20using%20sudo/#polkit","text":"Requires [polkit] to be: installed configured to authorize and authenticate the users pkexec COMMAND","title":"Polkit"},{"location":"learning/tools/authenticate%20without%20using%20sudo/#further-readings","text":"pkexec","title":"Further readings"},{"location":"learning/tools/authenticate%20without%20using%20sudo/#sources","text":"How to get gui sudo password prompt without command line","title":"Sources"},{"location":"learning/tools/aws/","text":"AWS CLI \u00b6 TL;DR \u00b6 # Install the CLI. brew install awscli # Configure a profile. aws configure aws configure --profile work # Use a specific profile for the rest of this shell session. export AWS_PROFILE = \"work\" Profiles \u00b6 # Initialize the default profile. # Not specifying a profile means to configure the default profile. $ aws configure AWS Access Key ID [ None ] : AKIA\u2026 AWS Secret Access Key [ None ] : je7MtG\u2026 Default region name [ None ] : us-east-1 Default output format [ None ] : text # Initialize a specific profile. $ aws configure --profile work AWS Access Key ID [ None ] : AKIA\u2026 AWS Secret Access Key [ None ] : LB88Mt\u2026 Default region name [ None ] : us-west-1 Default output format [ None ] : json # Use a specific profile for the rest of this session. $ export AWS_PROFILE = \"work\" Sources \u00b6 CLI quickstart Configure profiles in the CLI","title":"AWS CLI"},{"location":"learning/tools/aws/#aws-cli","text":"","title":"AWS CLI"},{"location":"learning/tools/aws/#tldr","text":"# Install the CLI. brew install awscli # Configure a profile. aws configure aws configure --profile work # Use a specific profile for the rest of this shell session. export AWS_PROFILE = \"work\"","title":"TL;DR"},{"location":"learning/tools/aws/#profiles","text":"# Initialize the default profile. # Not specifying a profile means to configure the default profile. $ aws configure AWS Access Key ID [ None ] : AKIA\u2026 AWS Secret Access Key [ None ] : je7MtG\u2026 Default region name [ None ] : us-east-1 Default output format [ None ] : text # Initialize a specific profile. $ aws configure --profile work AWS Access Key ID [ None ] : AKIA\u2026 AWS Secret Access Key [ None ] : LB88Mt\u2026 Default region name [ None ] : us-west-1 Default output format [ None ] : json # Use a specific profile for the rest of this session. $ export AWS_PROFILE = \"work\"","title":"Profiles"},{"location":"learning/tools/aws/#sources","text":"CLI quickstart Configure profiles in the CLI","title":"Sources"},{"location":"learning/tools/az/","text":"Azure CLI \u00b6 TL;DR Pipelines APIs Further readings Sources TL;DR \u00b6 # Install the CLI. brew install 'azure-cli' asdf plugin add 'azure-cli' && asdf install 'azure-cli' '2.37.0' # Disable certificates check upon connection. # Use it for proxies with doubtful certificates. export AZURE_CLI_DISABLE_CONNECTION_VERIFICATION = 1 # Login to Azure. az login az login -u 'username' -p 'password' az login --identity --username 'client_id__or__object_id__or__resource_id' az login --service-principal \\ -u 'app_id' -p 'password_or_certificate' --tenant 'tenant_id' # Gather information on the current user. az ad signed-in-user show az ad signed-in-user list-owned-objects # Gather information on another user. az ad user show --id 'user@email.org' # Check a User's permissions. az ad user get-member-groups --id 'user@email.org' # Get the ID of a Service Principal from its Display Name. az ad sp list --display-name 'service_principal_name' --query 'id' -o 'tsv' # Get the Display Name of a Service Principal from its ID. az ad sp show -o 'tsv' \\ --id '12345678-abcd-0987-fedc-567890abcdef' --query 'displayName' # Get a Resource Group's ID. az group show 'resource_group_name' # List Subscriptions available to the current User. az account list --refresh --output 'table' # Get the current User's default Subscription's ID. az account show --query 'id' --output 'tsv' # Get the current User's default Subscription. az account set --subscription 'subscription_uuid_or_name' # Set the current User's default Resource Group. az configure --defaults 'group=resource_group_name' # List available Locations. az account list-locations -o 'table' # Create an Access Token for the current User. az account get-access-token # List role assignments. az role assignment list az role assignment list --all az role assignment list --scope 'scope_id' --role 'role_id_or_name' # List the names of all keys in a KeyVault. az keyvault key list --query '[].name' -o tsv --vault-name 'key_vault_name' # Get a password from a KeyVault. az keyvault secret show --query 'value' \\ --name 'secret_name' --vault-name 'key_vault_name' # List LogAnalytics' Workspaces. az monitor log-analytics workspace list \\ --resource-group 'resource_group_name' \\ | jq -r '.[].name' - # Login to Azure DevOps with a PAT. az devops login --organization 'https://dev.azure.com/organization_name' # Get the names of all the Pipelines the current user has access to. az pipelines list --organization 'organization_id_or_name' az pipelines list --detect 'true' --query '[].name' -o 'tsv' # Show a specific Pipeline information. az pipelines show --id 'pipeline_id' az pipelines show --name 'pipeline_name' # Start a run. az pipelines run --name 'pipeline_name' \\ --parameters 'system.debug=True' agent.diagnostic = \"True\" # Get the status of a run of a Pipeline. az pipelines build show --id 'pipeline_id' az pipelines build show --detect true -o 'tsv' \\ --project 'project_name' --id 'pipeline_id' --query 'result' # Validate a bicep template to create a Deployment Group. az deployment group validate \\ -n 'deployment_group_name' -g 'resource_group_name' \\ -f 'template.bicep' -p 'parameter1=value' parameter2 = \"value\" # Check what a bicep template would do. az deployment group what-if \u2026 # Create a Deployment Group from a template. az deployment group create \u2026 # Cancel the current operation on a Deployment Group. az deployment group cancel \\ -n 'deployment_group_name' -g 'resource_group_name' # Delete a Deployment Group. az deployment group delete \\ -n 'deployment_group_name' -g 'resource_group_name' # Login to an ACR. az acr login --name 'acr_name' # Diagnose container registry connectivity issues. # Requires Docker being running. # Will run a hello-world image locally. az acr check-health -n 'acr_name' -s 'subscription_uuid_or_name' # List helm charts in an ACR. az acr helm list -n 'acr_name' -s 'subscription_uuid_or_name' # Get the 5 latest versions of a helm chart in an ACR. az acr helm list -n 'acr_name' -s 'subscription_uuid_or_name' -o 'json' \\ | jq \\ --arg CHART_REGEXP 'chart_name_or_regex' \\ 'to_entries | map(select(.key|test($CHART_REGEXP)))[].value[] | { version: .version, created: .created }' - \\ | yq -sy 'sort_by(.created) | reverse | .[0:5]' - # Push a helm chart to an ACR. az acr helm push -n 'acr_name' 'chart.tgz' --force # List the available AKS versions. az aks get-versions --location 'location' -o table # Get credentials for an AKS cluster. az aks get-credentials \\ --resource-group 'resource_group_name' --name 'cluster_name' az aks get-credentials \u2026 --overwrite-existing --admin # Move the cluster to its goal state *without* changing its configuration. # Can be used to move out of a non succeeded state. az aks update --resource-group 'resource_group_name' --name 'cluster_name' --yes # Validate an ACR is accessible from an AKS cluster. az aks check-acr --acr 'acr_name' \\ --resource-group 'resource_group_name' --name 'cluster_name' az aks check-acr \u2026 --node-name 'node_name' # Check if the current User is member of a given Group. az rest -u 'https://graph.microsoft.com/v1.0/me/checkMemberObjects' \\ -m post -b '{\"ids\":[\"group_id\"]}' # Check if a Service Principal is member of a given Group. az rest -u 'https://graph.microsoft.com/v1.0/servicePrincipals/service_principal_id/checkMemberObjects' \\ -m post -b '{\"ids\":[\"group_id\"]}' # Query the Graph APIs for a specific Member in a Group. az rest -m 'get' \\ -u 'https://graph.microsoft.com/beta/groups/group_id/members?$search=\"displayName:group_display_name\"&$select=displayName' \\ --headers 'consistencylevel=eventual' # Remove a Member from an AAD Group. # If '/$ref' is missing from the request, the user will be **deleted from AAD** # if the appropriate permissions are used, otherwise a '403 Forbidden' error is # returned. az rest -m 'delete' \\ -u 'https://graph.microsoft.com/beta/groups/group_id/members/member_id/$ref' # List the PATs of a User. # 'displayFilterOptions' are 'active' (default), 'all', 'expired' or 'revoked'. # If more then 20, results are paged and a 'continuationToken' will be returned. az rest -m 'get' \\ --headers Authorization = 'Bearer ey\u2026pw' \\ -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats?api-version=7.1-preview.1' az rest \u2026 -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats?api-version=7.1-preview.1&displayFilterOption=revoked&isSortAscending=false' az rest \u2026 -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats' \\ --url-parameters 'api-version=7.1-preview.1' 'displayFilterOption=expired' continuationToken = 'Hr\u2026in=' # Extend a PAT. # Works with expired PATs too. az rest \\ -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats?api-version=7.1-preview.1' \\ -m 'put' \\ --headers Authorization = 'Bearer ey\u2026pw' Content-Type = 'application/json' \\ -b '{ \"authorizationId\": \"01234567-abcd-0987-fedc-0123456789ab\", \"validTo\": \"2021-12-31T23:46:23.319Z\" }' az rest \u2026 -b @ 'file.json' Pipelines \u00b6 Give the --organization parameter, or use --detect true if running the command from a git repository to have it guessed automatically. --detect already defaults to true . APIs \u00b6 One can directly call the APIs with the rest command: az rest \\ -u 'https://graph.microsoft.com/v1.0/me/checkMemberObjects' \\ --headers Authorization = 'Bearer ey\u2026pw' \\ -m 'post' \\ -b '{\"ids\": [\"group_id\"]}' az rest \\ -u 'https://graph.microsoft.com/beta/groups/group_id/members/member_id/$ref' \\ -m 'delete' az rest \\ -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats?api-version=7.1-preview.1' \\ -m 'put' \\ --headers \\ 'Authorization=Bearer ey\u2026pw' \\ 'Content-Type=application/json' \\ -b '{ \"authorizationId\": \"01234567-abcd-0987-fedc-0123456789ab\", \"validTo\": \"2021-12-31T23:46:23.319Z\" }' az rest \\ -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats' \\ -m 'get' --url-parameters \\ 'api-version=7.1-preview.1' \\ 'displayFilterOption=expired' \\ 'continuationToken=Hr\u2026in=' Further readings \u00b6 Pat token APIs Sources \u00b6 Install Azure CLI on macOS Get started with Azure CLI Sign in with Azure CLI How to manage Azure subscriptions with the Azure CLI Authenticate with an Azure container registry Remove a member az aks reference Create and manage Azure Pipelines from the command line","title":"Azure CLI"},{"location":"learning/tools/az/#azure-cli","text":"TL;DR Pipelines APIs Further readings Sources","title":"Azure CLI"},{"location":"learning/tools/az/#tldr","text":"# Install the CLI. brew install 'azure-cli' asdf plugin add 'azure-cli' && asdf install 'azure-cli' '2.37.0' # Disable certificates check upon connection. # Use it for proxies with doubtful certificates. export AZURE_CLI_DISABLE_CONNECTION_VERIFICATION = 1 # Login to Azure. az login az login -u 'username' -p 'password' az login --identity --username 'client_id__or__object_id__or__resource_id' az login --service-principal \\ -u 'app_id' -p 'password_or_certificate' --tenant 'tenant_id' # Gather information on the current user. az ad signed-in-user show az ad signed-in-user list-owned-objects # Gather information on another user. az ad user show --id 'user@email.org' # Check a User's permissions. az ad user get-member-groups --id 'user@email.org' # Get the ID of a Service Principal from its Display Name. az ad sp list --display-name 'service_principal_name' --query 'id' -o 'tsv' # Get the Display Name of a Service Principal from its ID. az ad sp show -o 'tsv' \\ --id '12345678-abcd-0987-fedc-567890abcdef' --query 'displayName' # Get a Resource Group's ID. az group show 'resource_group_name' # List Subscriptions available to the current User. az account list --refresh --output 'table' # Get the current User's default Subscription's ID. az account show --query 'id' --output 'tsv' # Get the current User's default Subscription. az account set --subscription 'subscription_uuid_or_name' # Set the current User's default Resource Group. az configure --defaults 'group=resource_group_name' # List available Locations. az account list-locations -o 'table' # Create an Access Token for the current User. az account get-access-token # List role assignments. az role assignment list az role assignment list --all az role assignment list --scope 'scope_id' --role 'role_id_or_name' # List the names of all keys in a KeyVault. az keyvault key list --query '[].name' -o tsv --vault-name 'key_vault_name' # Get a password from a KeyVault. az keyvault secret show --query 'value' \\ --name 'secret_name' --vault-name 'key_vault_name' # List LogAnalytics' Workspaces. az monitor log-analytics workspace list \\ --resource-group 'resource_group_name' \\ | jq -r '.[].name' - # Login to Azure DevOps with a PAT. az devops login --organization 'https://dev.azure.com/organization_name' # Get the names of all the Pipelines the current user has access to. az pipelines list --organization 'organization_id_or_name' az pipelines list --detect 'true' --query '[].name' -o 'tsv' # Show a specific Pipeline information. az pipelines show --id 'pipeline_id' az pipelines show --name 'pipeline_name' # Start a run. az pipelines run --name 'pipeline_name' \\ --parameters 'system.debug=True' agent.diagnostic = \"True\" # Get the status of a run of a Pipeline. az pipelines build show --id 'pipeline_id' az pipelines build show --detect true -o 'tsv' \\ --project 'project_name' --id 'pipeline_id' --query 'result' # Validate a bicep template to create a Deployment Group. az deployment group validate \\ -n 'deployment_group_name' -g 'resource_group_name' \\ -f 'template.bicep' -p 'parameter1=value' parameter2 = \"value\" # Check what a bicep template would do. az deployment group what-if \u2026 # Create a Deployment Group from a template. az deployment group create \u2026 # Cancel the current operation on a Deployment Group. az deployment group cancel \\ -n 'deployment_group_name' -g 'resource_group_name' # Delete a Deployment Group. az deployment group delete \\ -n 'deployment_group_name' -g 'resource_group_name' # Login to an ACR. az acr login --name 'acr_name' # Diagnose container registry connectivity issues. # Requires Docker being running. # Will run a hello-world image locally. az acr check-health -n 'acr_name' -s 'subscription_uuid_or_name' # List helm charts in an ACR. az acr helm list -n 'acr_name' -s 'subscription_uuid_or_name' # Get the 5 latest versions of a helm chart in an ACR. az acr helm list -n 'acr_name' -s 'subscription_uuid_or_name' -o 'json' \\ | jq \\ --arg CHART_REGEXP 'chart_name_or_regex' \\ 'to_entries | map(select(.key|test($CHART_REGEXP)))[].value[] | { version: .version, created: .created }' - \\ | yq -sy 'sort_by(.created) | reverse | .[0:5]' - # Push a helm chart to an ACR. az acr helm push -n 'acr_name' 'chart.tgz' --force # List the available AKS versions. az aks get-versions --location 'location' -o table # Get credentials for an AKS cluster. az aks get-credentials \\ --resource-group 'resource_group_name' --name 'cluster_name' az aks get-credentials \u2026 --overwrite-existing --admin # Move the cluster to its goal state *without* changing its configuration. # Can be used to move out of a non succeeded state. az aks update --resource-group 'resource_group_name' --name 'cluster_name' --yes # Validate an ACR is accessible from an AKS cluster. az aks check-acr --acr 'acr_name' \\ --resource-group 'resource_group_name' --name 'cluster_name' az aks check-acr \u2026 --node-name 'node_name' # Check if the current User is member of a given Group. az rest -u 'https://graph.microsoft.com/v1.0/me/checkMemberObjects' \\ -m post -b '{\"ids\":[\"group_id\"]}' # Check if a Service Principal is member of a given Group. az rest -u 'https://graph.microsoft.com/v1.0/servicePrincipals/service_principal_id/checkMemberObjects' \\ -m post -b '{\"ids\":[\"group_id\"]}' # Query the Graph APIs for a specific Member in a Group. az rest -m 'get' \\ -u 'https://graph.microsoft.com/beta/groups/group_id/members?$search=\"displayName:group_display_name\"&$select=displayName' \\ --headers 'consistencylevel=eventual' # Remove a Member from an AAD Group. # If '/$ref' is missing from the request, the user will be **deleted from AAD** # if the appropriate permissions are used, otherwise a '403 Forbidden' error is # returned. az rest -m 'delete' \\ -u 'https://graph.microsoft.com/beta/groups/group_id/members/member_id/$ref' # List the PATs of a User. # 'displayFilterOptions' are 'active' (default), 'all', 'expired' or 'revoked'. # If more then 20, results are paged and a 'continuationToken' will be returned. az rest -m 'get' \\ --headers Authorization = 'Bearer ey\u2026pw' \\ -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats?api-version=7.1-preview.1' az rest \u2026 -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats?api-version=7.1-preview.1&displayFilterOption=revoked&isSortAscending=false' az rest \u2026 -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats' \\ --url-parameters 'api-version=7.1-preview.1' 'displayFilterOption=expired' continuationToken = 'Hr\u2026in=' # Extend a PAT. # Works with expired PATs too. az rest \\ -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats?api-version=7.1-preview.1' \\ -m 'put' \\ --headers Authorization = 'Bearer ey\u2026pw' Content-Type = 'application/json' \\ -b '{ \"authorizationId\": \"01234567-abcd-0987-fedc-0123456789ab\", \"validTo\": \"2021-12-31T23:46:23.319Z\" }' az rest \u2026 -b @ 'file.json'","title":"TL;DR"},{"location":"learning/tools/az/#pipelines","text":"Give the --organization parameter, or use --detect true if running the command from a git repository to have it guessed automatically. --detect already defaults to true .","title":"Pipelines"},{"location":"learning/tools/az/#apis","text":"One can directly call the APIs with the rest command: az rest \\ -u 'https://graph.microsoft.com/v1.0/me/checkMemberObjects' \\ --headers Authorization = 'Bearer ey\u2026pw' \\ -m 'post' \\ -b '{\"ids\": [\"group_id\"]}' az rest \\ -u 'https://graph.microsoft.com/beta/groups/group_id/members/member_id/$ref' \\ -m 'delete' az rest \\ -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats?api-version=7.1-preview.1' \\ -m 'put' \\ --headers \\ 'Authorization=Bearer ey\u2026pw' \\ 'Content-Type=application/json' \\ -b '{ \"authorizationId\": \"01234567-abcd-0987-fedc-0123456789ab\", \"validTo\": \"2021-12-31T23:46:23.319Z\" }' az rest \\ -u 'https://vssps.dev.azure.com/organization_name/_apis/tokens/pats' \\ -m 'get' --url-parameters \\ 'api-version=7.1-preview.1' \\ 'displayFilterOption=expired' \\ 'continuationToken=Hr\u2026in='","title":"APIs"},{"location":"learning/tools/az/#further-readings","text":"Pat token APIs","title":"Further readings"},{"location":"learning/tools/az/#sources","text":"Install Azure CLI on macOS Get started with Azure CLI Sign in with Azure CLI How to manage Azure subscriptions with the Azure CLI Authenticate with an Azure container registry Remove a member az aks reference Create and manage Azure Pipelines from the command line","title":"Sources"},{"location":"learning/tools/bash/","text":"Bourne Again SHell \u00b6 TL;DR \u00b6 # Declare functions. functionName () { \u2026 } function functionName { \u2026 } # Declare functions on a single line. functionName () { command1 ; \u2026 ; command N ; } # Run a command or function on exit, kill or error. trap \"rm -f $tempfile \" EXIT SIGTERM ERR trap function -name EXIT SIGTERM ERR # Disable CTRL-C. trap \"\" SIGINT # Re-enable CTRL-C. trap - SIGINT # Bash 3 and `sh` have no built-in means to convert case of a string, but the # `awk`, `sed` or `tr` tools can be used instead. echo $( echo \" $name \" | tr '[:upper:]' '[:lower:]' ) echo $( tr '[:upper:]' '[:lower:]' <<< \" $name \" ) # Bash 5 has a special parameter expansion for upper- and lowercasing strings. echo ${ name ,, } echo ${ name ^^ } Startup files loading order \u00b6 On startup: (if login shell) /etc/profile (if interactive and non login shell) /etc/bashrc (if login shell) ~/.bash_profile (if login shell and ~/.bash_profile not found) ~/.bash_login (if login shell and no ~/.bash_profile nor ~/.bash_login found) ~/.profile (if interactive and non login shell) ~/.bashrc Upon exit: (if login shell) ~/.bash_logout (if login shell) /etc/bash_logout Functions \u00b6 A function automatically returns the exit code of the last command in it. Check if a script is sourced by another \u00b6 ( return 0 2 >/dev/null ) && echo \"this script is not sourced\" || echo \"this script is sourced\" Further readings \u00b6 Trap Upper- or lower-casing strings Sources \u00b6 The Bash trap command Bash startup files loading order How to detect if a script is being sourced","title":"Bourne Again SHell"},{"location":"learning/tools/bash/#bourne-again-shell","text":"","title":"Bourne Again SHell"},{"location":"learning/tools/bash/#tldr","text":"# Declare functions. functionName () { \u2026 } function functionName { \u2026 } # Declare functions on a single line. functionName () { command1 ; \u2026 ; command N ; } # Run a command or function on exit, kill or error. trap \"rm -f $tempfile \" EXIT SIGTERM ERR trap function -name EXIT SIGTERM ERR # Disable CTRL-C. trap \"\" SIGINT # Re-enable CTRL-C. trap - SIGINT # Bash 3 and `sh` have no built-in means to convert case of a string, but the # `awk`, `sed` or `tr` tools can be used instead. echo $( echo \" $name \" | tr '[:upper:]' '[:lower:]' ) echo $( tr '[:upper:]' '[:lower:]' <<< \" $name \" ) # Bash 5 has a special parameter expansion for upper- and lowercasing strings. echo ${ name ,, } echo ${ name ^^ }","title":"TL;DR"},{"location":"learning/tools/bash/#startup-files-loading-order","text":"On startup: (if login shell) /etc/profile (if interactive and non login shell) /etc/bashrc (if login shell) ~/.bash_profile (if login shell and ~/.bash_profile not found) ~/.bash_login (if login shell and no ~/.bash_profile nor ~/.bash_login found) ~/.profile (if interactive and non login shell) ~/.bashrc Upon exit: (if login shell) ~/.bash_logout (if login shell) /etc/bash_logout","title":"Startup files loading order"},{"location":"learning/tools/bash/#functions","text":"A function automatically returns the exit code of the last command in it.","title":"Functions"},{"location":"learning/tools/bash/#check-if-a-script-is-sourced-by-another","text":"( return 0 2 >/dev/null ) && echo \"this script is not sourced\" || echo \"this script is sourced\"","title":"Check if a script is sourced by another"},{"location":"learning/tools/bash/#further-readings","text":"Trap Upper- or lower-casing strings","title":"Further readings"},{"location":"learning/tools/bash/#sources","text":"The Bash trap command Bash startup files loading order How to detect if a script is being sourced","title":"Sources"},{"location":"learning/tools/bluetooth/","text":"Bluetooth \u00b6 Troubleshooting \u00b6 Bluetooth devices take a long time to connect \u00b6 Enabling this option will use more power. In /etc/bluetooth/main.conf , under the General section, set FastConnectable to true : [General] # Permanently enables the Fast Connectable setting for adapters that # support it. When enabled other devices can connect faster to us, # however the tradeoff is increased power consumptions. This feature # will fully work only on kernel version 4.1 and newer. Defaults to # 'false'. - #FastConnectable = true + FastConnectable = true Bluetooth devices cannot be used at login \u00b6 In /etc/bluetooth/main.conf , under the Policy section, set AutoEnable to true : [Policy] # AutoEnable defines option to enable all controllers when they are found. # This includes adapters present on start as well as adapters that are plugged # in later on. Defaults to 'false'. - #AutoEnable = false + AutoEnable = true Sources \u00b6 Failing to use bluetooth keyboard at login","title":"Bluetooth"},{"location":"learning/tools/bluetooth/#bluetooth","text":"","title":"Bluetooth"},{"location":"learning/tools/bluetooth/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/bluetooth/#bluetooth-devices-take-a-long-time-to-connect","text":"Enabling this option will use more power. In /etc/bluetooth/main.conf , under the General section, set FastConnectable to true : [General] # Permanently enables the Fast Connectable setting for adapters that # support it. When enabled other devices can connect faster to us, # however the tradeoff is increased power consumptions. This feature # will fully work only on kernel version 4.1 and newer. Defaults to # 'false'. - #FastConnectable = true + FastConnectable = true","title":"Bluetooth devices take a long time to connect"},{"location":"learning/tools/bluetooth/#bluetooth-devices-cannot-be-used-at-login","text":"In /etc/bluetooth/main.conf , under the Policy section, set AutoEnable to true : [Policy] # AutoEnable defines option to enable all controllers when they are found. # This includes adapters present on start as well as adapters that are plugged # in later on. Defaults to 'false'. - #AutoEnable = false + AutoEnable = true","title":"Bluetooth devices cannot be used at login"},{"location":"learning/tools/bluetooth/#sources","text":"Failing to use bluetooth keyboard at login","title":"Sources"},{"location":"learning/tools/boinc/","text":"BOINC \u00b6 TL;DR Client management Use the GPU for computation On OpenSUSE Further readings TL;DR \u00b6 # Install. flatpak install 'edu.berkeley.BOINC' sudo zypper install 'boinc-client' 'boinc-manager' # Set the GUI RPC communications port. boinc --gui_rpc_port 30000 Client management \u00b6 Name Type Description BOINC Manager Graphical boinccmd Command line boinctui Text Use the GPU for computation \u00b6 Also see AMD Linux drivers and Radeon\u2122 Software for Linux\u00ae Installation . The BOINC client seems to need to be added to the video group to be able to use the drivers correctly - this is something I still need to check. On OpenSUSE \u00b6 Install the amdgpu-install package from AMD's Linux drivers page, then execute it. # Previous versions of the package (like the one in the official documentation # at the time of writing) made DKMS fail. sudo zypper install 'https://repo.radeon.com/amdgpu-install/22.20.3/sle/15.4/amdgpu-install-22.20.50203-1.noarch.rpm' sudo amdgpu-install At the next restart of the boinc-client, something similar to this line should appear in the client's logs: Oct 09 23:09:40 hostnameHere boinc[1709]: 09-Oct-2022 23:09:40 [---] OpenCL: AMD/ATI GPU 0: gfx90c:xnack- (driver version 3452.0 (HSA1.1,LC), device ve> Further readings \u00b6 BOINC Manager boinccmd boinctui GUI RPC bind to port 31416 failed: 98","title":"BOINC"},{"location":"learning/tools/boinc/#boinc","text":"TL;DR Client management Use the GPU for computation On OpenSUSE Further readings","title":"BOINC"},{"location":"learning/tools/boinc/#tldr","text":"# Install. flatpak install 'edu.berkeley.BOINC' sudo zypper install 'boinc-client' 'boinc-manager' # Set the GUI RPC communications port. boinc --gui_rpc_port 30000","title":"TL;DR"},{"location":"learning/tools/boinc/#client-management","text":"Name Type Description BOINC Manager Graphical boinccmd Command line boinctui Text","title":"Client management"},{"location":"learning/tools/boinc/#use-the-gpu-for-computation","text":"Also see AMD Linux drivers and Radeon\u2122 Software for Linux\u00ae Installation . The BOINC client seems to need to be added to the video group to be able to use the drivers correctly - this is something I still need to check.","title":"Use the GPU for computation"},{"location":"learning/tools/boinc/#on-opensuse","text":"Install the amdgpu-install package from AMD's Linux drivers page, then execute it. # Previous versions of the package (like the one in the official documentation # at the time of writing) made DKMS fail. sudo zypper install 'https://repo.radeon.com/amdgpu-install/22.20.3/sle/15.4/amdgpu-install-22.20.50203-1.noarch.rpm' sudo amdgpu-install At the next restart of the boinc-client, something similar to this line should appear in the client's logs: Oct 09 23:09:40 hostnameHere boinc[1709]: 09-Oct-2022 23:09:40 [---] OpenCL: AMD/ATI GPU 0: gfx90c:xnack- (driver version 3452.0 (HSA1.1,LC), device ve>","title":"On OpenSUSE"},{"location":"learning/tools/boinc/#further-readings","text":"BOINC Manager boinccmd boinctui GUI RPC bind to port 31416 failed: 98","title":"Further readings"},{"location":"learning/tools/boinccmd/","text":"Boinccmd \u00b6 TL;DR \u00b6 # Use a project manager. boinccmd --acct_mgr attach 'http://bam.boincstats.com' 'username' 'password' boinccmd --acct_mgr info boinccmd --acct_mgr sync boinccmd --acct_mgr detach # get the host status boinccmd --get_simple_gui_info boinccmd --get_state # list the current tasks boinccmd --get_tasks boinccmd --get_tasks | grep -i -C 8 executing # toggle getting work units from a project boinccmd --project http://www.worldcommunitygrid.org/ allowmorework boinccmd --get_project_status | grep \"master URL\" | awk -F \": \" '{print $2}' | xargs -n 1 -t -I {} boinccmd --project {} nomorework Gotchas \u00b6 boinccmd looks for the gui_rpc_auth.cfg file in the same directory it is launched from.","title":"Boinccmd"},{"location":"learning/tools/boinccmd/#boinccmd","text":"","title":"Boinccmd"},{"location":"learning/tools/boinccmd/#tldr","text":"# Use a project manager. boinccmd --acct_mgr attach 'http://bam.boincstats.com' 'username' 'password' boinccmd --acct_mgr info boinccmd --acct_mgr sync boinccmd --acct_mgr detach # get the host status boinccmd --get_simple_gui_info boinccmd --get_state # list the current tasks boinccmd --get_tasks boinccmd --get_tasks | grep -i -C 8 executing # toggle getting work units from a project boinccmd --project http://www.worldcommunitygrid.org/ allowmorework boinccmd --get_project_status | grep \"master URL\" | awk -F \": \" '{print $2}' | xargs -n 1 -t -I {} boinccmd --project {} nomorework","title":"TL;DR"},{"location":"learning/tools/boinccmd/#gotchas","text":"boinccmd looks for the gui_rpc_auth.cfg file in the same directory it is launched from.","title":"Gotchas"},{"location":"learning/tools/btrfs/","text":"BTRFS \u00b6 TL;DR \u00b6 # Create a volume with single metadata and double data blocks # Useless in practice but a good example. sudo mkfs.btrfs --metadata single --data dup /dev/sdb # Sparse a volume on multiple devices. sudo mkfs.btrfs --label data /dev/sd { a,c,d,e,f,g } --force \\ && echo \"LABEL=data /mnt/data btrfs compress=zstd 0 0\" | tee -a /etc/fstab # List all btrfs file systems. sudo btrfs filesystem show # Show detailed `df` analogue for a filesystem. sudo btrfs filesystem df path/to/filesystem # Give more details about usage. sudo btrfs filesystem usage path/to/filesystem # Resize online volumes. # -2g decreases, +2g increases. sudo btrfs filesystem resize -2g path/to/volume sudo btrfs filesystem resize max path/to/volume # Add new devices to a filesystem. sudo btrfs device add /dev/sdf /mnt # Remove devices from a filesystem. sudo btrfs device delete missing /mnt # List subvolumes. sudo btrfs subvolume list /mnt # Create subvolumes. btrfs subvolume create ~/subvolume sudo btrfs subvolume create /mnt/subvolume # Create a readonly snapshot of a subvolume. btrfs subvolume snapshot ~/subvolume ~/snapshot sudo btrfs subvolume snapshot -r /mnt/volume/subvolume /mnt/volume/snapshot # Mount subvolumes without mounting their main filesystem. sudo mount -o subvol = sv1 /dev/sdb /mnt # Delete a subvolume. sudo btrfs subvolume delete --commit-each /mnt/volume/subvolume # Deduplicate a volume's blocks. sudo duperemove -Adrh --hashfile = /tmp/dr.hash /mnt/volume1 /media volume2 sudo jdupes --dedupe -rZ /mnt/volume1 /media volume2 # Send and receive snapshots. sudo btrfs send /source/.snapshots/snap \\ | sudo btrfs receive /destination/.snapshots/ # Show the properties of a subvolume/filesystem/inode/device. btrfs property get -ts /path/to/subvolume btrfs property get -tf /path/to/filesystem btrfs property get -ti /path/to/inode btrfs property get -td /path/to/device btrfs property get /path/to/autoselected/type/of/resource # Change a subvolume to RO on the fly. btrfs property set -ts /path/to/subvolume ro true # Show a volume's information. sudo btrfs subvolume show /path/to/subvolume # Check the compress ratio of a compressed volume. sudo compsize /mnt/volume # Show the status of a running or paused balance operation. sudo btrfs balance status path/to/filesystem # Balance all block groups. # Slow: rewrites all blocks in filesystem. sudo btrfs balance start path/to/filesystem sudo btrfs balance start path/to/filesystem --bg --enqueue # Balance data block groups which are less than 15% utilized. # Run the operation in the background sudo btrfs balance start --bg -dusage = 15 path/to/filesystem # Balance a max of 10 metadata chunks with less than 20% utilization and at # least 1 chunk on a given device 'devid'. # Get the device's devid with `btrfs filesystem show`. sudo btrfs balance start -musage = 20 ,limit = 10 ,devid = devid path/to/filesystem # Convert data blocks to the raid6 profile, and metadata to raid1c3. sudo btrfs balance start -dconvert = raid6 -mconvert = raid1c3 path/to/filesystem # Convert data blocks to raid1 skipping already converted chunks. # Useful after a previous cancelled conversion operation. sudo btrfs balance start -dconvert = raid1,soft path/to/filesystem # Cancel, pause or resume a running or paused balance operation. sudo btrfs balance cancel path/to/filesystem sudo btrfs balance pause path/to/filesystem sudo btrfs balance resume path/to/filesystem # Enable quota. sudo btrfs quota enable path/to/subvolume # Show quota. sudo btrfs qgroup show path/to/subvolume # Convert ext3/ext4 to btrfs. btrfs-convert /dev/sdb1 # Convert btrfs to ext3/ext4. btrfs-convert -r /dev/sdb1 Check differences between 2 snapshots \u00b6 See also snapper . sudo btrfs send --no-data -p /old/snapshot /new/snapshot | sudo btrfs receive --dump # requires you to be using snapper for your snapshots sudo snapper -c config diff 445 ..446 Further readings \u00b6 Gentoo wiki Snapper Sources \u00b6 cheat.sh does btrfs have an efficient way to compare snapshots? determine if a btrfs subvolume is read-only","title":"BTRFS"},{"location":"learning/tools/btrfs/#btrfs","text":"","title":"BTRFS"},{"location":"learning/tools/btrfs/#tldr","text":"# Create a volume with single metadata and double data blocks # Useless in practice but a good example. sudo mkfs.btrfs --metadata single --data dup /dev/sdb # Sparse a volume on multiple devices. sudo mkfs.btrfs --label data /dev/sd { a,c,d,e,f,g } --force \\ && echo \"LABEL=data /mnt/data btrfs compress=zstd 0 0\" | tee -a /etc/fstab # List all btrfs file systems. sudo btrfs filesystem show # Show detailed `df` analogue for a filesystem. sudo btrfs filesystem df path/to/filesystem # Give more details about usage. sudo btrfs filesystem usage path/to/filesystem # Resize online volumes. # -2g decreases, +2g increases. sudo btrfs filesystem resize -2g path/to/volume sudo btrfs filesystem resize max path/to/volume # Add new devices to a filesystem. sudo btrfs device add /dev/sdf /mnt # Remove devices from a filesystem. sudo btrfs device delete missing /mnt # List subvolumes. sudo btrfs subvolume list /mnt # Create subvolumes. btrfs subvolume create ~/subvolume sudo btrfs subvolume create /mnt/subvolume # Create a readonly snapshot of a subvolume. btrfs subvolume snapshot ~/subvolume ~/snapshot sudo btrfs subvolume snapshot -r /mnt/volume/subvolume /mnt/volume/snapshot # Mount subvolumes without mounting their main filesystem. sudo mount -o subvol = sv1 /dev/sdb /mnt # Delete a subvolume. sudo btrfs subvolume delete --commit-each /mnt/volume/subvolume # Deduplicate a volume's blocks. sudo duperemove -Adrh --hashfile = /tmp/dr.hash /mnt/volume1 /media volume2 sudo jdupes --dedupe -rZ /mnt/volume1 /media volume2 # Send and receive snapshots. sudo btrfs send /source/.snapshots/snap \\ | sudo btrfs receive /destination/.snapshots/ # Show the properties of a subvolume/filesystem/inode/device. btrfs property get -ts /path/to/subvolume btrfs property get -tf /path/to/filesystem btrfs property get -ti /path/to/inode btrfs property get -td /path/to/device btrfs property get /path/to/autoselected/type/of/resource # Change a subvolume to RO on the fly. btrfs property set -ts /path/to/subvolume ro true # Show a volume's information. sudo btrfs subvolume show /path/to/subvolume # Check the compress ratio of a compressed volume. sudo compsize /mnt/volume # Show the status of a running or paused balance operation. sudo btrfs balance status path/to/filesystem # Balance all block groups. # Slow: rewrites all blocks in filesystem. sudo btrfs balance start path/to/filesystem sudo btrfs balance start path/to/filesystem --bg --enqueue # Balance data block groups which are less than 15% utilized. # Run the operation in the background sudo btrfs balance start --bg -dusage = 15 path/to/filesystem # Balance a max of 10 metadata chunks with less than 20% utilization and at # least 1 chunk on a given device 'devid'. # Get the device's devid with `btrfs filesystem show`. sudo btrfs balance start -musage = 20 ,limit = 10 ,devid = devid path/to/filesystem # Convert data blocks to the raid6 profile, and metadata to raid1c3. sudo btrfs balance start -dconvert = raid6 -mconvert = raid1c3 path/to/filesystem # Convert data blocks to raid1 skipping already converted chunks. # Useful after a previous cancelled conversion operation. sudo btrfs balance start -dconvert = raid1,soft path/to/filesystem # Cancel, pause or resume a running or paused balance operation. sudo btrfs balance cancel path/to/filesystem sudo btrfs balance pause path/to/filesystem sudo btrfs balance resume path/to/filesystem # Enable quota. sudo btrfs quota enable path/to/subvolume # Show quota. sudo btrfs qgroup show path/to/subvolume # Convert ext3/ext4 to btrfs. btrfs-convert /dev/sdb1 # Convert btrfs to ext3/ext4. btrfs-convert -r /dev/sdb1","title":"TL;DR"},{"location":"learning/tools/btrfs/#check-differences-between-2-snapshots","text":"See also snapper . sudo btrfs send --no-data -p /old/snapshot /new/snapshot | sudo btrfs receive --dump # requires you to be using snapper for your snapshots sudo snapper -c config diff 445 ..446","title":"Check differences between 2 snapshots"},{"location":"learning/tools/btrfs/#further-readings","text":"Gentoo wiki Snapper","title":"Further readings"},{"location":"learning/tools/btrfs/#sources","text":"cheat.sh does btrfs have an efficient way to compare snapshots? determine if a btrfs subvolume is read-only","title":"Sources"},{"location":"learning/tools/cheat.sh/","text":"cheat.sh \u00b6 TL;DR \u00b6 # using the exact name of the command curl cheat.sh/tar curl cht.sh/curl curl https://cheat.sh/rsync curl https://cht.sh/tr # search for a command curl cht.sh/~snapshot # programming languages have special namespaces curl cht.sh/go/Pointers curl cht.sh/scala/Functions curl cht.sh/python/lambda # list cheatsheets for a specific programming language curl cht.sh/go/:list Further readings \u00b6 website","title":"cheat.sh"},{"location":"learning/tools/cheat.sh/#cheatsh","text":"","title":"cheat.sh"},{"location":"learning/tools/cheat.sh/#tldr","text":"# using the exact name of the command curl cheat.sh/tar curl cht.sh/curl curl https://cheat.sh/rsync curl https://cht.sh/tr # search for a command curl cht.sh/~snapshot # programming languages have special namespaces curl cht.sh/go/Pointers curl cht.sh/scala/Functions curl cht.sh/python/lambda # list cheatsheets for a specific programming language curl cht.sh/go/:list","title":"TL;DR"},{"location":"learning/tools/cheat.sh/#further-readings","text":"website","title":"Further readings"},{"location":"learning/tools/check%20a%20pod%20can%20connect%20to%20an%20external%20db/","text":"Check a Pod can connect to an external DB \u00b6 TL;DR \u00b6 # access a test container kubectl run --generator = run-pod/v1 --limits 'cpu=200m,memory=512Mi' --requests 'cpu=200m,memory=512Mi' --image alpine ${ USER } -mysql-test -it -- sh # install programs apk --no-cache add mysql-client netcat-openbsd # test plain connectivity nc -vz -w3 10 .0.2.15 3306 # test the client can connect mysql --host 10 .0.2.15 --port 3306 --user root","title":"Check a Pod can connect to an external DB"},{"location":"learning/tools/check%20a%20pod%20can%20connect%20to%20an%20external%20db/#check-a-pod-can-connect-to-an-external-db","text":"","title":"Check a Pod can connect to an external DB"},{"location":"learning/tools/check%20a%20pod%20can%20connect%20to%20an%20external%20db/#tldr","text":"# access a test container kubectl run --generator = run-pod/v1 --limits 'cpu=200m,memory=512Mi' --requests 'cpu=200m,memory=512Mi' --image alpine ${ USER } -mysql-test -it -- sh # install programs apk --no-cache add mysql-client netcat-openbsd # test plain connectivity nc -vz -w3 10 .0.2.15 3306 # test the client can connect mysql --host 10 .0.2.15 --port 3306 --user root","title":"TL;DR"},{"location":"learning/tools/check%20the%20temperature%20of%20components/","text":"Check the temperature of components \u00b6 See lm-sensors , hddtemp or nvme-cli . Further readings \u00b6 [LM sensors] Hddtemp nvme-cli Sources \u00b6 How to check CPU temperature on Ubuntu Linux","title":"Check the temperature of components"},{"location":"learning/tools/check%20the%20temperature%20of%20components/#check-the-temperature-of-components","text":"See lm-sensors , hddtemp or nvme-cli .","title":"Check the temperature of components"},{"location":"learning/tools/check%20the%20temperature%20of%20components/#further-readings","text":"[LM sensors] Hddtemp nvme-cli","title":"Further readings"},{"location":"learning/tools/check%20the%20temperature%20of%20components/#sources","text":"How to check CPU temperature on Ubuntu Linux","title":"Sources"},{"location":"learning/tools/chezmoi/","text":"Chezmoi \u00b6 A multi-machine dotfile manager, written in Go. TL;DR \u00b6 # initialize chezmoi chezmoi init chezmoi init https://github.com/username/dotfiles.git # initialize, checkout and apply chezmoi init --apply --verbose https://github.com/username/dotfiles.git # add a file chezmoi add .gitconfig chezmoi add --follow --template .vimrc # follow symlinks, add as template chezmoi add --encrypt .ssh/id_ed25519 # add encrypted # edit a file # the file needs to be added first chezmoi edit .tmux.conf # check what files would change during an apply chezmoi apply --dry-run --verbose # check what contents would change chezmoi diff # apply changes chezmoi apply # show the full list of variables # includes custom data from the configuration file chezmoi data # test a template chezmoi execute-template < .local/share/chezmoi/dot_gitconfig.tmpl chezmoi execute-template --init --promptString email = me@home.org < ~/.local/share/chezmoi/.chezmoi.yaml.tmpl # use git on chezmoi's data storage chezmoi git add -- . chezmoi git commit -- --message \"commit message\" chezmoi git pull -- --rebase chezmoi git push -- --set-upstream origin main # fetch the latest changes from a remote repository chezmoi update Save the current data to a remote repository \u00b6 $ chezmoi cd chezmoi $> git remote add origin https://github.com/username/dotfiles.git chezmoi $> git push -u origin main chezmoi $> exit $ Gotchas \u00b6 templating uses the Go text/template library due to a feature of a library used by chezmoi, all custom variable names in the configuration file are converted to lowercase; see the custom data fields appear as all lowercase strings GitHub issue for more information. # configuration file [data] AwesomeCustomField = \"my Awesome custom Value\" normallookingcustomfield = \"normalLookingValue\" map[awesomecustomfield:my Awesome custom Value chezmoi:\u2026 normallookingcustomfield:normalLookingValue] Snippets \u00b6 {{ - /* Overwrite settings from the host-specific configuration files, if existing. */ }} {{ - $ hostConfigFiles := list ( print \".chezmoi_\" . chezmoi . hostname \".yaml\" ) }} {{ - range $ f := $ hostConfigFiles }} {{ - if stat ( joinPath $ . chezmoi . sourceDir $ f ) }} {{ - $ hostConfig := dict }} {{ - $ hostConfig = include $ f | fromYaml }} {{ - $ config = mergeOverwrite $ config $ hostConfig }} {{ - end }} {{ - end }} {{ - $ hostEncryptedConfigFiles := list ( print \"encrypted_chezmoi_\" . chezmoi . hostname \".yaml\" ( dig \"age\" \"suffix\" \".age\" .)) ( print \"encrypted_chezmoi_\" . chezmoi . hostname \".yaml\" ( dig \"gpg\" \"suffix\" \".asc\" .)) }} {{ - /* A value for .encryption *must* be set *before execution* to be able to decrypt values. */ }} {{ - /* Ignore this step if .encryption is not set. */ }} {{ - if hasKey . \"encryption\" }} {{ - range $ f := $ hostEncryptedConfigFiles }} {{ - if stat ( joinPath $ . chezmoi . sourceDir $ f ) }} {{ - $ hostConfig := dict }} {{ - $ hostConfig = include $ f | decrypt | fromYaml }} {{ - $ config = mergeOverwrite $ config $ hostConfig }} {{ - end }} {{ - end }} {{ - end }} Further readings \u00b6 Chezmoi user guide Go text/template Sprig Sources \u00b6 cheat.sh custom data fields appear as all lowercase strings","title":"Chezmoi"},{"location":"learning/tools/chezmoi/#chezmoi","text":"A multi-machine dotfile manager, written in Go.","title":"Chezmoi"},{"location":"learning/tools/chezmoi/#tldr","text":"# initialize chezmoi chezmoi init chezmoi init https://github.com/username/dotfiles.git # initialize, checkout and apply chezmoi init --apply --verbose https://github.com/username/dotfiles.git # add a file chezmoi add .gitconfig chezmoi add --follow --template .vimrc # follow symlinks, add as template chezmoi add --encrypt .ssh/id_ed25519 # add encrypted # edit a file # the file needs to be added first chezmoi edit .tmux.conf # check what files would change during an apply chezmoi apply --dry-run --verbose # check what contents would change chezmoi diff # apply changes chezmoi apply # show the full list of variables # includes custom data from the configuration file chezmoi data # test a template chezmoi execute-template < .local/share/chezmoi/dot_gitconfig.tmpl chezmoi execute-template --init --promptString email = me@home.org < ~/.local/share/chezmoi/.chezmoi.yaml.tmpl # use git on chezmoi's data storage chezmoi git add -- . chezmoi git commit -- --message \"commit message\" chezmoi git pull -- --rebase chezmoi git push -- --set-upstream origin main # fetch the latest changes from a remote repository chezmoi update","title":"TL;DR"},{"location":"learning/tools/chezmoi/#save-the-current-data-to-a-remote-repository","text":"$ chezmoi cd chezmoi $> git remote add origin https://github.com/username/dotfiles.git chezmoi $> git push -u origin main chezmoi $> exit $","title":"Save the current data to a remote repository"},{"location":"learning/tools/chezmoi/#gotchas","text":"templating uses the Go text/template library due to a feature of a library used by chezmoi, all custom variable names in the configuration file are converted to lowercase; see the custom data fields appear as all lowercase strings GitHub issue for more information. # configuration file [data] AwesomeCustomField = \"my Awesome custom Value\" normallookingcustomfield = \"normalLookingValue\" map[awesomecustomfield:my Awesome custom Value chezmoi:\u2026 normallookingcustomfield:normalLookingValue]","title":"Gotchas"},{"location":"learning/tools/chezmoi/#snippets","text":"{{ - /* Overwrite settings from the host-specific configuration files, if existing. */ }} {{ - $ hostConfigFiles := list ( print \".chezmoi_\" . chezmoi . hostname \".yaml\" ) }} {{ - range $ f := $ hostConfigFiles }} {{ - if stat ( joinPath $ . chezmoi . sourceDir $ f ) }} {{ - $ hostConfig := dict }} {{ - $ hostConfig = include $ f | fromYaml }} {{ - $ config = mergeOverwrite $ config $ hostConfig }} {{ - end }} {{ - end }} {{ - $ hostEncryptedConfigFiles := list ( print \"encrypted_chezmoi_\" . chezmoi . hostname \".yaml\" ( dig \"age\" \"suffix\" \".age\" .)) ( print \"encrypted_chezmoi_\" . chezmoi . hostname \".yaml\" ( dig \"gpg\" \"suffix\" \".asc\" .)) }} {{ - /* A value for .encryption *must* be set *before execution* to be able to decrypt values. */ }} {{ - /* Ignore this step if .encryption is not set. */ }} {{ - if hasKey . \"encryption\" }} {{ - range $ f := $ hostEncryptedConfigFiles }} {{ - if stat ( joinPath $ . chezmoi . sourceDir $ f ) }} {{ - $ hostConfig := dict }} {{ - $ hostConfig = include $ f | decrypt | fromYaml }} {{ - $ config = mergeOverwrite $ config $ hostConfig }} {{ - end }} {{ - end }} {{ - end }}","title":"Snippets"},{"location":"learning/tools/chezmoi/#further-readings","text":"Chezmoi user guide Go text/template Sprig","title":"Further readings"},{"location":"learning/tools/chezmoi/#sources","text":"cheat.sh custom data fields appear as all lowercase strings","title":"Sources"},{"location":"learning/tools/citrix/","text":"Citrix \u00b6 TL;DR \u00b6 # Disable autostart on Mac OS X. find /Library/LaunchAgents /Library/LaunchDaemons \\ -iname \"*.citrix.*.plist\" \\ -exec sudo -p 'sudo password: ' mv -fv {} {} .backup ';'","title":"Citrix"},{"location":"learning/tools/citrix/#citrix","text":"","title":"Citrix"},{"location":"learning/tools/citrix/#tldr","text":"# Disable autostart on Mac OS X. find /Library/LaunchAgents /Library/LaunchDaemons \\ -iname \"*.citrix.*.plist\" \\ -exec sudo -p 'sudo password: ' mv -fv {} {} .backup ';'","title":"TL;DR"},{"location":"learning/tools/clamav/","text":"ClamAV \u00b6 TL;DR \u00b6 # manually update the virus definitions # do it once **before** starting a scan or the daemon # the definitions updater deamon must be stopped to avoid complaints from it sudo systemctl stop clamav-freshclam \\ && sudo freshclam \\ && sudo systemctl enable --now clamav-freshclam # scan a file or directory clamscan path/to/file clamscan --recursive path/to/dir # only return specific files clamscan --infected /home/ clamscan --suppress-ok-results Downloads/ # save results to file clamscan --bell -i -r /home -l output.txt # scan files in a list clamscan -i -f /tmp/scan.list # remove infected files clamscan -r --remove /home/user clamscan -r -i --move = /home/user/infected /home/ # limit cpu usage nice -n 15 clamscan && clamscan --bell -i -r /home # use multiple threads Gotchas \u00b6 The --fdpass option of clamdscan (notice the d in the command) sends a file descriptor to clamd rather than a path name, avoiding the need for the clamav user to be able to read everyone's files clamscan is designed to be single-threaded, so when scanning a file or directory from the command line only a single CPU thread is used; use xargs or another executor to run a scan in parallel: find . -type f -printf \"'%p' \" | xargs -P $( nproc ) -n 1 clamscan find . -type f | parallel --group --jobs 0 -d '\\n' clamscan {} Further readings \u00b6 Gentoo Wiki Sources \u00b6 Install ClamAV on Fedora Linux 35","title":"ClamAV"},{"location":"learning/tools/clamav/#clamav","text":"","title":"ClamAV"},{"location":"learning/tools/clamav/#tldr","text":"# manually update the virus definitions # do it once **before** starting a scan or the daemon # the definitions updater deamon must be stopped to avoid complaints from it sudo systemctl stop clamav-freshclam \\ && sudo freshclam \\ && sudo systemctl enable --now clamav-freshclam # scan a file or directory clamscan path/to/file clamscan --recursive path/to/dir # only return specific files clamscan --infected /home/ clamscan --suppress-ok-results Downloads/ # save results to file clamscan --bell -i -r /home -l output.txt # scan files in a list clamscan -i -f /tmp/scan.list # remove infected files clamscan -r --remove /home/user clamscan -r -i --move = /home/user/infected /home/ # limit cpu usage nice -n 15 clamscan && clamscan --bell -i -r /home # use multiple threads","title":"TL;DR"},{"location":"learning/tools/clamav/#gotchas","text":"The --fdpass option of clamdscan (notice the d in the command) sends a file descriptor to clamd rather than a path name, avoiding the need for the clamav user to be able to read everyone's files clamscan is designed to be single-threaded, so when scanning a file or directory from the command line only a single CPU thread is used; use xargs or another executor to run a scan in parallel: find . -type f -printf \"'%p' \" | xargs -P $( nproc ) -n 1 clamscan find . -type f | parallel --group --jobs 0 -d '\\n' clamscan {}","title":"Gotchas"},{"location":"learning/tools/clamav/#further-readings","text":"Gentoo Wiki","title":"Further readings"},{"location":"learning/tools/clamav/#sources","text":"Install ClamAV on Fedora Linux 35","title":"Sources"},{"location":"learning/tools/cloud-init/","text":"Cloud init \u00b6 TL;DR Merge 2 or more files or parts In Terraform Further readings Sources TL;DR \u00b6 # Get the current status. cloud-init status cloud-init status --wait # Verify that cloud-init received the expected user data. cloud-init query userdata # Assert the user data we provided is a valid cloud-config. # From version 22.2, drops the 'devel' command. cloud-init devel schema --system --annotate cloud-init devel schema --config-file '/tmp/user-data' # Check the raw logs. cat '/var/log/cloud-init.log' cat '/var/log/cloud-init-output.log' # Parse and organize the events in the log file by stage. cloud-init analyze show # Manually run a single cloud-config module once after the instance has booted. sudo cloud-init single --name 'cc_ssh' --frequency 'always' # Clean up everything so `cloud-init` can run again. sudo cloud-init clean # Re-run everything. sudo cloud-init init # Check the user scripts. ls '/var/lib/cloud/instance/scripts' #cloud-config # Add the Docker repository yum_repos : docker-ce : name : Docker CE Stable - $basearch enabled : true baseurl : https://download.docker.com/linux/rhel/$releasever/$basearch/stable priority : 1 gpgcheck : true gpgkey : https://download.docker.com/linux/rhel/gpg # Upgrade the instance package_upgrade : true package_reboot_if_required : true # Install required packages # docker-ce already depends on docker-ce-cli and containerd.io packages : - docker-ce - jq - unzip # Enable and start the service after installation runcmd : - systemctl daemon-reload - systemctl enable --now docker.service Merge 2 or more files or parts \u00b6 FIXME See Merging User-Data sections for details. #cloud-config packages : - jq - unzip --- merge_how : - name : list settings : [ append ] - name : dict settings : [ no_replace , recurse_list ] packages : - parallel --- packages : - vim merge_type : 'list(append)+dict(recurse_array)+str()' In Terraform \u00b6 create a data resource containing the files in order, one per part: # https://registry.terraform.io/providers/hashicorp/cloudinit/latest/docs # https://github.com/chrusty/terraform-multipart-userdata/blob/master/example/cloudinit.tf data \"cloudinit_config\" \"vm\" { gzip = true base64_encode = true part { content = file(\"files/first.yaml\") content_type = \"text/cloud-config\" } \u2026 part { content = file(\"files/n-th.yaml\") content_type = \"text/cloud-config\" filename = \"n-th.yaml\" merge_type = \"dict(recurse_array,no_replace)+list(append)\" } } give its rendered form as input to a vm's userdata attribute or an output resource: resource \"azurerm_linux_virtual_machine\" \"vm\" { user_data = data.cloudinit_config.vm.rendered \u2026 } output \"cloudinit_config\" { value = data.cloudinit_config.vm.rendered } Further readings \u00b6 Website Modules Examples Merging User-Data sections cloud-init multipart encoding issues Test cloud-init with a multipass container Mime Multi Part Archive format Docker cloud init example Sources \u00b6 Debugging cloud-init Tutorial Cloud-Init configuration merging Terraform's cloud-init provider","title":"Cloud init"},{"location":"learning/tools/cloud-init/#cloud-init","text":"TL;DR Merge 2 or more files or parts In Terraform Further readings Sources","title":"Cloud init"},{"location":"learning/tools/cloud-init/#tldr","text":"# Get the current status. cloud-init status cloud-init status --wait # Verify that cloud-init received the expected user data. cloud-init query userdata # Assert the user data we provided is a valid cloud-config. # From version 22.2, drops the 'devel' command. cloud-init devel schema --system --annotate cloud-init devel schema --config-file '/tmp/user-data' # Check the raw logs. cat '/var/log/cloud-init.log' cat '/var/log/cloud-init-output.log' # Parse and organize the events in the log file by stage. cloud-init analyze show # Manually run a single cloud-config module once after the instance has booted. sudo cloud-init single --name 'cc_ssh' --frequency 'always' # Clean up everything so `cloud-init` can run again. sudo cloud-init clean # Re-run everything. sudo cloud-init init # Check the user scripts. ls '/var/lib/cloud/instance/scripts' #cloud-config # Add the Docker repository yum_repos : docker-ce : name : Docker CE Stable - $basearch enabled : true baseurl : https://download.docker.com/linux/rhel/$releasever/$basearch/stable priority : 1 gpgcheck : true gpgkey : https://download.docker.com/linux/rhel/gpg # Upgrade the instance package_upgrade : true package_reboot_if_required : true # Install required packages # docker-ce already depends on docker-ce-cli and containerd.io packages : - docker-ce - jq - unzip # Enable and start the service after installation runcmd : - systemctl daemon-reload - systemctl enable --now docker.service","title":"TL;DR"},{"location":"learning/tools/cloud-init/#merge-2-or-more-files-or-parts","text":"FIXME See Merging User-Data sections for details. #cloud-config packages : - jq - unzip --- merge_how : - name : list settings : [ append ] - name : dict settings : [ no_replace , recurse_list ] packages : - parallel --- packages : - vim merge_type : 'list(append)+dict(recurse_array)+str()'","title":"Merge 2 or more files or parts"},{"location":"learning/tools/cloud-init/#in-terraform","text":"create a data resource containing the files in order, one per part: # https://registry.terraform.io/providers/hashicorp/cloudinit/latest/docs # https://github.com/chrusty/terraform-multipart-userdata/blob/master/example/cloudinit.tf data \"cloudinit_config\" \"vm\" { gzip = true base64_encode = true part { content = file(\"files/first.yaml\") content_type = \"text/cloud-config\" } \u2026 part { content = file(\"files/n-th.yaml\") content_type = \"text/cloud-config\" filename = \"n-th.yaml\" merge_type = \"dict(recurse_array,no_replace)+list(append)\" } } give its rendered form as input to a vm's userdata attribute or an output resource: resource \"azurerm_linux_virtual_machine\" \"vm\" { user_data = data.cloudinit_config.vm.rendered \u2026 } output \"cloudinit_config\" { value = data.cloudinit_config.vm.rendered }","title":"In Terraform"},{"location":"learning/tools/cloud-init/#further-readings","text":"Website Modules Examples Merging User-Data sections cloud-init multipart encoding issues Test cloud-init with a multipass container Mime Multi Part Archive format Docker cloud init example","title":"Further readings"},{"location":"learning/tools/cloud-init/#sources","text":"Debugging cloud-init Tutorial Cloud-Init configuration merging Terraform's cloud-init provider","title":"Sources"},{"location":"learning/tools/combine%20multiple%20pdf%20files/","text":"Combine multiple PDF files \u00b6 Further readings \u00b6 pdftk","title":"Combine multiple PDF files"},{"location":"learning/tools/combine%20multiple%20pdf%20files/#combine-multiple-pdf-files","text":"","title":"Combine multiple PDF files"},{"location":"learning/tools/combine%20multiple%20pdf%20files/#further-readings","text":"pdftk","title":"Further readings"},{"location":"learning/tools/comm/","text":"comm \u00b6 comm requires the files it is working with to be pre-sorted. TL;DR \u00b6 # Print unique lines of file1 which are not present in file2. comm -23 < ( sort -u 'file1' ) < ( sort -u 'file2' ) # Check the whole content of file1 is present in file2. [[ $( comm -23 < ( sort -u 'file1' ) < ( sort -u 'file2' ) | wc -l ) -eq 0 ]] Sources \u00b6 Check whether all lines of file occur in different file","title":"comm"},{"location":"learning/tools/comm/#comm","text":"comm requires the files it is working with to be pre-sorted.","title":"comm"},{"location":"learning/tools/comm/#tldr","text":"# Print unique lines of file1 which are not present in file2. comm -23 < ( sort -u 'file1' ) < ( sort -u 'file2' ) # Check the whole content of file1 is present in file2. [[ $( comm -23 < ( sort -u 'file1' ) < ( sort -u 'file2' ) | wc -l ) -eq 0 ]]","title":"TL;DR"},{"location":"learning/tools/comm/#sources","text":"Check whether all lines of file occur in different file","title":"Sources"},{"location":"learning/tools/compare%20two%20pdf%20files/","text":"Compare two PDF files \u00b6 Install and use diffpdf (preferred) or diff-pdf : sudo pacman -S diffpdf sudo zypper install diff-pdf diffpdf file1 file2 As an alternative: # create a pdf with the diff as red pixels compare -verbose -debug coder $PDF_1 $PDF_2 -compose src /tmp/ $OUT_FILE .tmp # merge the diff-pdf with background PDF_1 pdftk /tmp/ $OUT_FILE .tmp background $PDF_1 output $OUT_FILE Sources \u00b6 Compare PDF Files With DiffPDF PDF compare on linux command line","title":"Compare two PDF files"},{"location":"learning/tools/compare%20two%20pdf%20files/#compare-two-pdf-files","text":"Install and use diffpdf (preferred) or diff-pdf : sudo pacman -S diffpdf sudo zypper install diff-pdf diffpdf file1 file2 As an alternative: # create a pdf with the diff as red pixels compare -verbose -debug coder $PDF_1 $PDF_2 -compose src /tmp/ $OUT_FILE .tmp # merge the diff-pdf with background PDF_1 pdftk /tmp/ $OUT_FILE .tmp background $PDF_1 output $OUT_FILE","title":"Compare two PDF files"},{"location":"learning/tools/compare%20two%20pdf%20files/#sources","text":"Compare PDF Files With DiffPDF PDF compare on linux command line","title":"Sources"},{"location":"learning/tools/cpupower/","text":"Cpufreq \u00b6 Default governor is ondemand for older CPUs and kernels and schedutil for new CPUs and kernels. TL;DR \u00b6 # Install. sudo dnf install kernel-tools # List the available governors. cpupower frequency-info --governors # Get the current active governor. cpupower frequency-info --policy # Set a new governor until reboot. sudo cpupower frequency-set -g performance sudo cpupower frequency-set --governor powersave sudo cpupower frequency-set --governor schedutil Further readings \u00b6 CPU frequency scaling Sources \u00b6 CPU governer settings ignore nice load","title":"Cpufreq"},{"location":"learning/tools/cpupower/#cpufreq","text":"Default governor is ondemand for older CPUs and kernels and schedutil for new CPUs and kernels.","title":"Cpufreq"},{"location":"learning/tools/cpupower/#tldr","text":"# Install. sudo dnf install kernel-tools # List the available governors. cpupower frequency-info --governors # Get the current active governor. cpupower frequency-info --policy # Set a new governor until reboot. sudo cpupower frequency-set -g performance sudo cpupower frequency-set --governor powersave sudo cpupower frequency-set --governor schedutil","title":"TL;DR"},{"location":"learning/tools/cpupower/#further-readings","text":"CPU frequency scaling","title":"Further readings"},{"location":"learning/tools/cpupower/#sources","text":"CPU governer settings ignore nice load","title":"Sources"},{"location":"learning/tools/crontab/","text":"Crontab \u00b6 TL;DR \u00b6 # List existing jobs. crontab -l sudo crontab -l -u other_user # Edit crontab files. crontab -e sudo crontab -e -u other_user # Replace the current crontab with the contents of a given file. crontab path/to/file sudo crontab -u other_user path/to/file # Remove all cron jobs. crontab -r sudo crontab -r -u other_user # Run 'pwd' every day at 10PM. 0 22 * * * pwd # Run 'ls' every 10 minutes. */10 * * * * ls # Run a script at 02:34 every Friday. 34 2 * * Fri /absolute/path/to/script.sh Sources \u00b6 cheat.sh","title":"Crontab"},{"location":"learning/tools/crontab/#crontab","text":"","title":"Crontab"},{"location":"learning/tools/crontab/#tldr","text":"# List existing jobs. crontab -l sudo crontab -l -u other_user # Edit crontab files. crontab -e sudo crontab -e -u other_user # Replace the current crontab with the contents of a given file. crontab path/to/file sudo crontab -u other_user path/to/file # Remove all cron jobs. crontab -r sudo crontab -r -u other_user # Run 'pwd' every day at 10PM. 0 22 * * * pwd # Run 'ls' every 10 minutes. */10 * * * * ls # Run a script at 02:34 every Friday. 34 2 * * Fri /absolute/path/to/script.sh","title":"TL;DR"},{"location":"learning/tools/crontab/#sources","text":"cheat.sh","title":"Sources"},{"location":"learning/tools/curl/","text":"cURL \u00b6 TL;DR \u00b6 # Send a single GET request and show its output on stdout. curl 'http://url.of/file' # Be quiet. curl --silent 'https://www.example.com' curl -s --show-error 'https://www.example.com' # Download files. curl 'http://url.of/file' -o 'path/to/file' curl -O 'http://url.of/file1' -O 'http://url.of/file2' curl http://url.of/file [ 1 -24 ] # Resume downloads. curl -C - -o 'partial_file' 'http://url.of/file' # Limit downloads bandwidth. curl --limit-rate '1000B' -O 'http://url.of/file' # Follow redirects. curl -L 'http://url.of/file' # Only fetch HTTP headers from a response. curl -I 'http://example.com' # Only return the HTTP status code. curl -o '/dev/null' -w '%{http_code}\\n' -s -I 'http://example.com' # Send different request types. curl --request 'PUT' 'http://example.com' # Specify headers. curl 'http://example.com' -H 'Content-Type:application/json' 'http://example.com' # Skip certificate validation. curl --insecure 'https://example.com' # Pass certificates for a resource. curl --cert 'client.pem' --key 'key.pem' -k 'https://example.com' curl --cacert 'ca.pem' 'https://example.com' # Authenticate. curl -u 'username' : 'password' 'http://url.of/file' curl -u 'username' : 'password' -O 'ftp://url.of/file' curl 'ftp://username:password@example.com' # POST to a form. curl -F 'name=user' -F 'password=test' 'http://example.com' curl --data 'name=bob' 'http://example.com/form' # Send data. curl 'http://example.com' -H \"Content-Type:application/json\" -d '{\"name\":\"bob\"}' -X 'POST' curl \u2026 -d @file.json # Use a proxy. curl 'http://example.com' --proxy 'socks5://localhost:19999' # Forcefully resolve a host to a given address. curl 'https://example.com' --resolve 'example.com:443:google.com' Apply settings to all connections \u00b6 Unless the -q option is used, curl always checks for a default config file on invocation and uses it if found. The default configuration file is looked for in the following places, in this order: $CURL_HOME/.curlrc $XDG_CONFIG_HOME/.curlrc , added in 7.73.0 $HOME/.curlrc on Windows only: %USERPROFILE%\\.curlrc on Windows only: %APPDATA%\\.curlrc on Windows only: %USERPROFILE%\\Application Data\\.curlrc On Non-Windows hosts, curl uses getpwuid to find the user's home directory. On Windows, if curl finds no .curlrc file in the sequence described above, it will check for one in the same dir the curl executable is placed. # ~/.curlrc # Accepts both short and long options. # Options in long format are accepted without the leading two dashes to make it # easier to read. # Arguments must be provided on the same line of the option. # Arguments can be separated by space, '=' and ':' location --insecure --user-agent \"my-agent\" request = \"PUT\" config: \"~/.config/curl\" Sources \u00b6 cheat.sh How to ignore invalid and self signed ssl connection errors with curl Config file","title":"cURL"},{"location":"learning/tools/curl/#curl","text":"","title":"cURL"},{"location":"learning/tools/curl/#tldr","text":"# Send a single GET request and show its output on stdout. curl 'http://url.of/file' # Be quiet. curl --silent 'https://www.example.com' curl -s --show-error 'https://www.example.com' # Download files. curl 'http://url.of/file' -o 'path/to/file' curl -O 'http://url.of/file1' -O 'http://url.of/file2' curl http://url.of/file [ 1 -24 ] # Resume downloads. curl -C - -o 'partial_file' 'http://url.of/file' # Limit downloads bandwidth. curl --limit-rate '1000B' -O 'http://url.of/file' # Follow redirects. curl -L 'http://url.of/file' # Only fetch HTTP headers from a response. curl -I 'http://example.com' # Only return the HTTP status code. curl -o '/dev/null' -w '%{http_code}\\n' -s -I 'http://example.com' # Send different request types. curl --request 'PUT' 'http://example.com' # Specify headers. curl 'http://example.com' -H 'Content-Type:application/json' 'http://example.com' # Skip certificate validation. curl --insecure 'https://example.com' # Pass certificates for a resource. curl --cert 'client.pem' --key 'key.pem' -k 'https://example.com' curl --cacert 'ca.pem' 'https://example.com' # Authenticate. curl -u 'username' : 'password' 'http://url.of/file' curl -u 'username' : 'password' -O 'ftp://url.of/file' curl 'ftp://username:password@example.com' # POST to a form. curl -F 'name=user' -F 'password=test' 'http://example.com' curl --data 'name=bob' 'http://example.com/form' # Send data. curl 'http://example.com' -H \"Content-Type:application/json\" -d '{\"name\":\"bob\"}' -X 'POST' curl \u2026 -d @file.json # Use a proxy. curl 'http://example.com' --proxy 'socks5://localhost:19999' # Forcefully resolve a host to a given address. curl 'https://example.com' --resolve 'example.com:443:google.com'","title":"TL;DR"},{"location":"learning/tools/curl/#apply-settings-to-all-connections","text":"Unless the -q option is used, curl always checks for a default config file on invocation and uses it if found. The default configuration file is looked for in the following places, in this order: $CURL_HOME/.curlrc $XDG_CONFIG_HOME/.curlrc , added in 7.73.0 $HOME/.curlrc on Windows only: %USERPROFILE%\\.curlrc on Windows only: %APPDATA%\\.curlrc on Windows only: %USERPROFILE%\\Application Data\\.curlrc On Non-Windows hosts, curl uses getpwuid to find the user's home directory. On Windows, if curl finds no .curlrc file in the sequence described above, it will check for one in the same dir the curl executable is placed. # ~/.curlrc # Accepts both short and long options. # Options in long format are accepted without the leading two dashes to make it # easier to read. # Arguments must be provided on the same line of the option. # Arguments can be separated by space, '=' and ':' location --insecure --user-agent \"my-agent\" request = \"PUT\" config: \"~/.config/curl\"","title":"Apply settings to all connections"},{"location":"learning/tools/curl/#sources","text":"cheat.sh How to ignore invalid and self signed ssl connection errors with curl Config file","title":"Sources"},{"location":"learning/tools/dd/","text":"dd \u00b6 Convert and copy a file. TL;DR \u00b6 # Read 512 random Bytes for each iteration and save them . dd if = '/dev/urandom' of = 'output/file' count = 2 bs = 512 # Read 1000 Bytes for each iteration and save them while watching the progress. dd if = '/input/file' of = 'output/file' count = 4M bs = 1k status = 'progress' # Create a 1GiB file with nothing but zeros, ready to mkswap(8) it. dd if = '/dev/zero' of = '/swapfile' count = 1048576 bs = 1024 # Make a bootable USB drive from an isohybrid file. dd if = file.iso of = /dev/usb_drive status = progress # Clone a drive to another drive with 4 MiB block. # Ignore any error and show the progress. dd if = /dev/source_drive of = /dev/dest_drive bs = 4M conv = noerror status = progress # Generate a system backup into an IMG file and show the progress: dd if = /dev/drive_device of = path/to/file.img status = progress # Restore a drive from an IMG file and show the progress: dd if = path/to/file.img of = /dev/drive_device status = progress Sources \u00b6 cheat.sh","title":"dd"},{"location":"learning/tools/dd/#dd","text":"Convert and copy a file.","title":"dd"},{"location":"learning/tools/dd/#tldr","text":"# Read 512 random Bytes for each iteration and save them . dd if = '/dev/urandom' of = 'output/file' count = 2 bs = 512 # Read 1000 Bytes for each iteration and save them while watching the progress. dd if = '/input/file' of = 'output/file' count = 4M bs = 1k status = 'progress' # Create a 1GiB file with nothing but zeros, ready to mkswap(8) it. dd if = '/dev/zero' of = '/swapfile' count = 1048576 bs = 1024 # Make a bootable USB drive from an isohybrid file. dd if = file.iso of = /dev/usb_drive status = progress # Clone a drive to another drive with 4 MiB block. # Ignore any error and show the progress. dd if = /dev/source_drive of = /dev/dest_drive bs = 4M conv = noerror status = progress # Generate a system backup into an IMG file and show the progress: dd if = /dev/drive_device of = path/to/file.img status = progress # Restore a drive from an IMG file and show the progress: dd if = path/to/file.img of = /dev/drive_device status = progress","title":"TL;DR"},{"location":"learning/tools/dd/#sources","text":"cheat.sh","title":"Sources"},{"location":"learning/tools/debian%20linux/","text":"Debian GNU/Linux \u00b6 Further readings \u00b6 The APT package manager Dpkg Advice for new users on not breaking their Debian system","title":"Debian GNU/Linux"},{"location":"learning/tools/debian%20linux/#debian-gnulinux","text":"","title":"Debian GNU/Linux"},{"location":"learning/tools/debian%20linux/#further-readings","text":"The APT package manager Dpkg Advice for new users on not breaking their Debian system","title":"Further readings"},{"location":"learning/tools/dell%20hardware/","text":"Dell hardware \u00b6 Recovery \u00b6 Note: the Dell OS Recovery Tool identifies, downloads and creates a boot device with the original operating system that was preinstalled on a Dell computer only. It does not provide alternate operating systems for download. download and install the Dell OS Recovery Tool on a Windows machine open the application select a computer and click Next, then if using the same Dell computer, click This Computer if using another working computer, under Another computer enter the Service Tag of the Dell computer and click Search disconnect any USB drive/s that are connected to the computer for security connect a USB flash drive with at least 16 GB of free storage space to the computer; this will be formatted and used to create the USB recovery media under the USB drive, select the USB flash drive that you would like to use to create the USB recovery media and check the button I understand that the selected drive will be reformatted and existing data will be deleted. click Download to begin downloading the operating system recovery image; once the process is complete, a summary screen is shown Sources \u00b6 How to download and use the Dell operating system recovery image in Microsoft Windows","title":"Dell hardware"},{"location":"learning/tools/dell%20hardware/#dell-hardware","text":"","title":"Dell hardware"},{"location":"learning/tools/dell%20hardware/#recovery","text":"Note: the Dell OS Recovery Tool identifies, downloads and creates a boot device with the original operating system that was preinstalled on a Dell computer only. It does not provide alternate operating systems for download. download and install the Dell OS Recovery Tool on a Windows machine open the application select a computer and click Next, then if using the same Dell computer, click This Computer if using another working computer, under Another computer enter the Service Tag of the Dell computer and click Search disconnect any USB drive/s that are connected to the computer for security connect a USB flash drive with at least 16 GB of free storage space to the computer; this will be formatted and used to create the USB recovery media under the USB drive, select the USB flash drive that you would like to use to create the USB recovery media and check the button I understand that the selected drive will be reformatted and existing data will be deleted. click Download to begin downloading the operating system recovery image; once the process is complete, a summary screen is shown","title":"Recovery"},{"location":"learning/tools/dell%20hardware/#sources","text":"How to download and use the Dell operating system recovery image in Microsoft Windows","title":"Sources"},{"location":"learning/tools/diagrams/","text":"Diagrams \u00b6 Draws your cloud system architecture using Python code. Further readings \u00b6","title":"Diagrams"},{"location":"learning/tools/diagrams/#diagrams","text":"Draws your cloud system architecture using Python code.","title":"Diagrams"},{"location":"learning/tools/diagrams/#further-readings","text":"","title":"Further readings"},{"location":"learning/tools/diff-pdf/","text":"Diff-pdf \u00b6 TL;DR \u00b6 # just check diff-pdf --verbose file1.pdf file2.pdf # view differences in a window diff-pdf --view file1.pdf file2.pdf","title":"Diff-pdf"},{"location":"learning/tools/diff-pdf/#diff-pdf","text":"","title":"Diff-pdf"},{"location":"learning/tools/diff-pdf/#tldr","text":"# just check diff-pdf --verbose file1.pdf file2.pdf # view differences in a window diff-pdf --view file1.pdf file2.pdf","title":"TL;DR"},{"location":"learning/tools/diffpdf/","text":"DiffPDF \u00b6 TL;DR \u00b6 # Install it. sudo pacman -S diffpdf # Show differences visually. diffpdf file1 file2","title":"DiffPDF"},{"location":"learning/tools/diffpdf/#diffpdf","text":"","title":"DiffPDF"},{"location":"learning/tools/diffpdf/#tldr","text":"# Install it. sudo pacman -S diffpdf # Show differences visually. diffpdf file1 file2","title":"TL;DR"},{"location":"learning/tools/dig/","text":"Dig \u00b6 Sources \u00b6 Using dig +trace to Understand DNS Resolution from Start to Finish","title":"Dig"},{"location":"learning/tools/dig/#dig","text":"","title":"Dig"},{"location":"learning/tools/dig/#sources","text":"Using dig +trace to Understand DNS Resolution from Start to Finish","title":"Sources"},{"location":"learning/tools/docker/","text":"Docker \u00b6 TL;DR \u00b6 # Show locally available images. docker images -a # Search for images. docker search 'boinc' # Pull images. docker pull 'alpine:3.14' docker pull 'boinc/client:latest' # Login to registries. docker login docker login -u 'username' -p 'password' # Create containers. docker create -h 'alpine-test' --name 'alpine-test' 'alpine' # Start containers. docker start 'alpine-test' docker start 'bdbe3f45' # Create and start containers. docker run 'hello-world' docker run -ti --rm 'alpine' cat '/etc/apk/repositories' docker run -d --name 'boinc' --network = 'host' --pid = 'host' -v 'boinc:/var/lib/boinc' \\ -e BOINC_GUI_RPC_PASSWORD = '123' -e BOINC_CMD_LINE_OPTIONS = '--allow_remote_gui_rpc' \\ 'boinc/client' # Gracefully stop containers. docker stop 'alpine-test' docker stop 'bdbe3f45' # Kill containers. docker kill 'alpine-test' # Restart containers. docker restart 'alpine-test' docker restart 'bdbe3f45' # Show containers' status. docker ps docker ps --all # List containers with specific metadata values. docker ps -f 'name=pihole' -f 'status=running' -f 'health=healthy' -q # Execute commands inside *running* containers. docker exec 'app_web_1' tail 'logs/development.log' docker exec -ti 'alpine-test' 'sh' # Show containers' output. docker log 'alpine-test' # List processes running inside containers. docker top 'alpine-test' # Show information on containers. docker inspect 'alpine-test' # Build a docker image. docker build -t 'private/alpine:3.14' . # Tag images. docker tag 'alpine:3.14' 'private/alpine:3.14' # Push images. docker push 'private/alpine:3.14' # Export images to tarballs. docker save 'alpine:3.14' -o 'alpine.tar' docker save 'hello-world' > 'hw.tar' # Load images from tarballs. docker load -i 'hw.tar' # Delete containers. docker rm 'alpine-test' docker rm -f '87b27' # Cleanup. docker logout docker rmi 'alpine' docker image prune -a docker system prune -a Daemon configuration \u00b6 The docker daemon is configured using the /etc/docker/daemon.json file: { \"default-runtime\" : \"runc\" , \"dns\" : [ \"8.8.8.8\" , \"1.1.1.1\" ] } Containers configuration \u00b6 Docker mounts specific system files in all containers to forward its settings: 6a95fabde222$ mount \u2026 /dev/disk/by-uuid/1bb\u2026eb5 on /etc/resolv.conf type btrfs ( rw,\u2026 ) /dev/disk/by-uuid/1bb\u2026eb5 on /etc/hostname type btrfs ( rw,\u2026 ) /dev/disk/by-uuid/1bb\u2026eb5 on /etc/hosts type btrfs ( rw,\u2026 ) \u2026 Those files come from the volume the docker container is using for its root, and are modified on the container's startup with the information from the CLI, the daemon itself and, when missing, the host. Gotchas \u00b6 Containers created with no specified name will be assigned one automatically: $ docker create 'hello-world' 8eaaae8c0c720ac220abac763ad4b477d807be4522d58e334337b1b74a14d0bd $ docker create --name 'alpine' 'alpine' 63b1a0a3e557094eba7f18424fd50d49b36cacbc21f1df60b918b375b857f809 $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 63b1a0a3e557 alpine \"/bin/sh\" 24 seconds ago Created alpine 8eaaae8c0c72 hello-world \"/hello\" 21 seconds ago Created sleepy_brown When referring to a container or image using their ID, you just need to use as many characters you need to uniquely specify a single one of them: $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 63b1a0a3e557 alpine \"/bin/sh\" 34 seconds ago Created alpine 8eaaae8c0c72 hello-world \"/hello\" 31 seconds ago Created sleepy_brown $ docker start 8 8 $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 63b1a0a3e557 alpine \"/bin/sh\" 48 seconds ago Created alpine 8eaaae8c0c72 hello-world \"/hello\" 45 seconds ago Exited ( 0 ) 10 seconds ago sleepy_brown Sources \u00b6 Arch Linux Wiki Configuring DNS Cheatsheet","title":"Docker"},{"location":"learning/tools/docker/#docker","text":"","title":"Docker"},{"location":"learning/tools/docker/#tldr","text":"# Show locally available images. docker images -a # Search for images. docker search 'boinc' # Pull images. docker pull 'alpine:3.14' docker pull 'boinc/client:latest' # Login to registries. docker login docker login -u 'username' -p 'password' # Create containers. docker create -h 'alpine-test' --name 'alpine-test' 'alpine' # Start containers. docker start 'alpine-test' docker start 'bdbe3f45' # Create and start containers. docker run 'hello-world' docker run -ti --rm 'alpine' cat '/etc/apk/repositories' docker run -d --name 'boinc' --network = 'host' --pid = 'host' -v 'boinc:/var/lib/boinc' \\ -e BOINC_GUI_RPC_PASSWORD = '123' -e BOINC_CMD_LINE_OPTIONS = '--allow_remote_gui_rpc' \\ 'boinc/client' # Gracefully stop containers. docker stop 'alpine-test' docker stop 'bdbe3f45' # Kill containers. docker kill 'alpine-test' # Restart containers. docker restart 'alpine-test' docker restart 'bdbe3f45' # Show containers' status. docker ps docker ps --all # List containers with specific metadata values. docker ps -f 'name=pihole' -f 'status=running' -f 'health=healthy' -q # Execute commands inside *running* containers. docker exec 'app_web_1' tail 'logs/development.log' docker exec -ti 'alpine-test' 'sh' # Show containers' output. docker log 'alpine-test' # List processes running inside containers. docker top 'alpine-test' # Show information on containers. docker inspect 'alpine-test' # Build a docker image. docker build -t 'private/alpine:3.14' . # Tag images. docker tag 'alpine:3.14' 'private/alpine:3.14' # Push images. docker push 'private/alpine:3.14' # Export images to tarballs. docker save 'alpine:3.14' -o 'alpine.tar' docker save 'hello-world' > 'hw.tar' # Load images from tarballs. docker load -i 'hw.tar' # Delete containers. docker rm 'alpine-test' docker rm -f '87b27' # Cleanup. docker logout docker rmi 'alpine' docker image prune -a docker system prune -a","title":"TL;DR"},{"location":"learning/tools/docker/#daemon-configuration","text":"The docker daemon is configured using the /etc/docker/daemon.json file: { \"default-runtime\" : \"runc\" , \"dns\" : [ \"8.8.8.8\" , \"1.1.1.1\" ] }","title":"Daemon configuration"},{"location":"learning/tools/docker/#containers-configuration","text":"Docker mounts specific system files in all containers to forward its settings: 6a95fabde222$ mount \u2026 /dev/disk/by-uuid/1bb\u2026eb5 on /etc/resolv.conf type btrfs ( rw,\u2026 ) /dev/disk/by-uuid/1bb\u2026eb5 on /etc/hostname type btrfs ( rw,\u2026 ) /dev/disk/by-uuid/1bb\u2026eb5 on /etc/hosts type btrfs ( rw,\u2026 ) \u2026 Those files come from the volume the docker container is using for its root, and are modified on the container's startup with the information from the CLI, the daemon itself and, when missing, the host.","title":"Containers configuration"},{"location":"learning/tools/docker/#gotchas","text":"Containers created with no specified name will be assigned one automatically: $ docker create 'hello-world' 8eaaae8c0c720ac220abac763ad4b477d807be4522d58e334337b1b74a14d0bd $ docker create --name 'alpine' 'alpine' 63b1a0a3e557094eba7f18424fd50d49b36cacbc21f1df60b918b375b857f809 $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 63b1a0a3e557 alpine \"/bin/sh\" 24 seconds ago Created alpine 8eaaae8c0c72 hello-world \"/hello\" 21 seconds ago Created sleepy_brown When referring to a container or image using their ID, you just need to use as many characters you need to uniquely specify a single one of them: $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 63b1a0a3e557 alpine \"/bin/sh\" 34 seconds ago Created alpine 8eaaae8c0c72 hello-world \"/hello\" 31 seconds ago Created sleepy_brown $ docker start 8 8 $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 63b1a0a3e557 alpine \"/bin/sh\" 48 seconds ago Created alpine 8eaaae8c0c72 hello-world \"/hello\" 45 seconds ago Exited ( 0 ) 10 seconds ago sleepy_brown","title":"Gotchas"},{"location":"learning/tools/docker/#sources","text":"Arch Linux Wiki Configuring DNS Cheatsheet","title":"Sources"},{"location":"learning/tools/dpkg/","text":"Dpkg \u00b6 TL;DR \u00b6 # add an extra architecture dpkg --add-architecture i386 # list extra architectures dpkg --print-foreign-architectures # list available extra architectures dpkg-architecture --list-known #list all installed packages of the i386 architecture dpkg --get-selections | grep i386 | awk '{print $1}' # remove the i386 architecture apt-get purge $( dpkg --get-selections | grep --color = never i386 | awk '{print $1}' ) dpkg --remove-architecture i386 Sources \u00b6 How to check if dpkg-architecture \u2013list has all the architectures?","title":"Dpkg"},{"location":"learning/tools/dpkg/#dpkg","text":"","title":"Dpkg"},{"location":"learning/tools/dpkg/#tldr","text":"# add an extra architecture dpkg --add-architecture i386 # list extra architectures dpkg --print-foreign-architectures # list available extra architectures dpkg-architecture --list-known #list all installed packages of the i386 architecture dpkg --get-selections | grep i386 | awk '{print $1}' # remove the i386 architecture apt-get purge $( dpkg --get-selections | grep --color = never i386 | awk '{print $1}' ) dpkg --remove-architecture i386","title":"TL;DR"},{"location":"learning/tools/dpkg/#sources","text":"How to check if dpkg-architecture \u2013list has all the architectures?","title":"Sources"},{"location":"learning/tools/drain%20a%20k8s%20cluster%20node/","text":"Drain a K8S cluster node \u00b6 mark the node as unschedulable ( cordon ): $ kubectl cordon kworker-rj2 node/kworker-rj2 cordoned remove pods running on the node: $ kubectl drain kworker-rj2 --grace-period = 300 --ignore-daemonsets = true node/kworker-rj2 already cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-fl8dl, kube-system/kube-proxy-95vdf evicting pod default/my-dep-557548758d-d2pmd pod/my-dep-557548758d-d2pmd evicted node/kworker-rj2 evicted do to the node what you need to do make the node available again: $ kubectl uncordon kworker-rj2 node/kworker-rj2 uncordoned Sources \u00b6 How to drain a node in Kubernetes","title":"Drain a K8S cluster node"},{"location":"learning/tools/drain%20a%20k8s%20cluster%20node/#drain-a-k8s-cluster-node","text":"mark the node as unschedulable ( cordon ): $ kubectl cordon kworker-rj2 node/kworker-rj2 cordoned remove pods running on the node: $ kubectl drain kworker-rj2 --grace-period = 300 --ignore-daemonsets = true node/kworker-rj2 already cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-fl8dl, kube-system/kube-proxy-95vdf evicting pod default/my-dep-557548758d-d2pmd pod/my-dep-557548758d-d2pmd evicted node/kworker-rj2 evicted do to the node what you need to do make the node available again: $ kubectl uncordon kworker-rj2 node/kworker-rj2 uncordoned","title":"Drain a K8S cluster node"},{"location":"learning/tools/drain%20a%20k8s%20cluster%20node/#sources","text":"How to drain a node in Kubernetes","title":"Sources"},{"location":"learning/tools/duperemove/","text":"Duperemove \u00b6 Finds duplicated extents and submits them for deduplication. When given a list of files, duperemove hashes their contents block by block and compares them. When given the -d option, duperemove also submits duplicated extents for deduplication using the Linux kernel extent-same ioctl. duperemove can store the hashes it computes in a hashfile. If given an existing hashfile in input, it only computes hashes for those files which have changed since the last run. This lets you run duperemove repeatedly on your data as it changes, without having to re-checksum unchanged data. duperemove can also take input from fdupes , given the --fdupes option. TL;DR \u00b6 # Recursively search for duplicated extents in a directory. duperemove -hr path/to/directory # Recursively deduplicate duplicated extents on a Btrfs or XFS filesystem. # XFS deduplication is still experimental at the time of writing. duperemove -Adhr path/to/directory # Store extent hashes in a file. # Hogs less memory and can be reused on subsequent runs. duperemove -Adhr --hashfile = path/to/hashfile path/to/directory # List the files tracked by hashfiles. duperemove -L --hashfile = path/to/hashfile # Limit threads; defaults are based on the host's cpus number. # I/O threads are used for hashing and in the deduplication stage. # CPU threads are used in the duplicate extent finding stage. duperemove -Adhr --hashfile = path/to/hashfile \\ --io-threads = N --cpu-threads = N \\ path/to/directory Sources \u00b6 Website cheat.sh manpage","title":"Duperemove"},{"location":"learning/tools/duperemove/#duperemove","text":"Finds duplicated extents and submits them for deduplication. When given a list of files, duperemove hashes their contents block by block and compares them. When given the -d option, duperemove also submits duplicated extents for deduplication using the Linux kernel extent-same ioctl. duperemove can store the hashes it computes in a hashfile. If given an existing hashfile in input, it only computes hashes for those files which have changed since the last run. This lets you run duperemove repeatedly on your data as it changes, without having to re-checksum unchanged data. duperemove can also take input from fdupes , given the --fdupes option.","title":"Duperemove"},{"location":"learning/tools/duperemove/#tldr","text":"# Recursively search for duplicated extents in a directory. duperemove -hr path/to/directory # Recursively deduplicate duplicated extents on a Btrfs or XFS filesystem. # XFS deduplication is still experimental at the time of writing. duperemove -Adhr path/to/directory # Store extent hashes in a file. # Hogs less memory and can be reused on subsequent runs. duperemove -Adhr --hashfile = path/to/hashfile path/to/directory # List the files tracked by hashfiles. duperemove -L --hashfile = path/to/hashfile # Limit threads; defaults are based on the host's cpus number. # I/O threads are used for hashing and in the deduplication stage. # CPU threads are used in the duplicate extent finding stage. duperemove -Adhr --hashfile = path/to/hashfile \\ --io-threads = N --cpu-threads = N \\ path/to/directory","title":"TL;DR"},{"location":"learning/tools/duperemove/#sources","text":"Website cheat.sh manpage","title":"Sources"},{"location":"learning/tools/enable%20bluetooth%20pairing%20on%20boot/","text":"Enable Bluetooth pairing on boot \u00b6 Bluetooth pairing on boot is enabled by default on Mac OS X, at least for Apple devices. On GNU/Linux: enable the bluetooth service on boot make sure the AutoEnable option in /etc/bluetooth/main.conf is set to true [optional] make sure the FastConnectable option in /etc/bluetooth/main.conf is set to true Further readings: \u00b6","title":"Enable Bluetooth pairing on boot"},{"location":"learning/tools/enable%20bluetooth%20pairing%20on%20boot/#enable-bluetooth-pairing-on-boot","text":"Bluetooth pairing on boot is enabled by default on Mac OS X, at least for Apple devices. On GNU/Linux: enable the bluetooth service on boot make sure the AutoEnable option in /etc/bluetooth/main.conf is set to true [optional] make sure the FastConnectable option in /etc/bluetooth/main.conf is set to true","title":"Enable Bluetooth pairing on boot"},{"location":"learning/tools/enable%20bluetooth%20pairing%20on%20boot/#further-readings","text":"","title":"Further readings:"},{"location":"learning/tools/enable%20kernel%20extensions%20on%20m1%20macs/","text":"Enable kernel extensions on M1 macs \u00b6 Fully shut down your system, press and hold the Touch ID or power button to launch Startup Security Utility, in Startup Security Utility, enable kernel extensions from the Security Policy button.","title":"Enable kernel extensions on M1 macs"},{"location":"learning/tools/enable%20kernel%20extensions%20on%20m1%20macs/#enable-kernel-extensions-on-m1-macs","text":"Fully shut down your system, press and hold the Touch ID or power button to launch Startup Security Utility, in Startup Security Utility, enable kernel extensions from the Security Policy button.","title":"Enable kernel extensions on M1 macs"},{"location":"learning/tools/envsubst/","text":"Envsubst \u00b6 Substitutes environment variables in shell format strings. TL;DR \u00b6 envsubst < input.file envsubst < input.file > output.file $ cat hello.file hello $NAME $ NAME = 'mek' envsubst < hello.file hello mek","title":"Envsubst"},{"location":"learning/tools/envsubst/#envsubst","text":"Substitutes environment variables in shell format strings.","title":"Envsubst"},{"location":"learning/tools/envsubst/#tldr","text":"envsubst < input.file envsubst < input.file > output.file $ cat hello.file hello $NAME $ NAME = 'mek' envsubst < hello.file hello mek","title":"TL;DR"},{"location":"learning/tools/export%20all%20variables%20in%20a%20envfile/","text":"Export all variables in an envfile \u00b6 TL;DR \u00b6 set -o allexport source envfile set +o allexport Sources \u00b6 Set environment variables from file of key/value pairs","title":"Export all variables in an envfile"},{"location":"learning/tools/export%20all%20variables%20in%20a%20envfile/#export-all-variables-in-an-envfile","text":"","title":"Export all variables in an envfile"},{"location":"learning/tools/export%20all%20variables%20in%20a%20envfile/#tldr","text":"set -o allexport source envfile set +o allexport","title":"TL;DR"},{"location":"learning/tools/export%20all%20variables%20in%20a%20envfile/#sources","text":"Set environment variables from file of key/value pairs","title":"Sources"},{"location":"learning/tools/extract%20attachments%20from%20multipart%20emails/","text":"Extract attachments from multipart emails \u00b6 When saved as plain text, emails may be saved as S/MIME files with attachments. In such cases the text file itself contains the multipart message body of the email, so the attachments are provided as base64 streams: 1 --------------060903090608060502040600 2 Content-Type: application/x-gzip; 3 name=\"myawesomefile.tar.gz\" 4 Content-Transfer-Encoding: base64 5 Content-Disposition: attachment; 6 filename=\"myawesomefile.tar.gz\" 7 8 qrsIAstukk4AA+17Wa+jSpZuPfMrrK6Hvi1qJ4OxsfuodQU2YLDBUJDGrvqBeR7MaPj1N7D3 9 OEmSxO8Wq7+3Y48dTWvXi8XvvKj8od6vPf9vKjWIv1v7nt3G/d8rEX5D/FdrDIxj2IrUPeE/ 10 j5Dv4g9+fPnTRcX006T++LdYYw7w+i... You can use munpack to easily extract attachments out of such text files and write them into a proper named files. $ munpack -f plaintext.eml myawesomefile.tar.gz ( application/x-gzip ) Sources \u00b6 Extract attachments from multipart messages","title":"Extract attachments from multipart emails"},{"location":"learning/tools/extract%20attachments%20from%20multipart%20emails/#extract-attachments-from-multipart-emails","text":"When saved as plain text, emails may be saved as S/MIME files with attachments. In such cases the text file itself contains the multipart message body of the email, so the attachments are provided as base64 streams: 1 --------------060903090608060502040600 2 Content-Type: application/x-gzip; 3 name=\"myawesomefile.tar.gz\" 4 Content-Transfer-Encoding: base64 5 Content-Disposition: attachment; 6 filename=\"myawesomefile.tar.gz\" 7 8 qrsIAstukk4AA+17Wa+jSpZuPfMrrK6Hvi1qJ4OxsfuodQU2YLDBUJDGrvqBeR7MaPj1N7D3 9 OEmSxO8Wq7+3Y48dTWvXi8XvvKj8od6vPf9vKjWIv1v7nt3G/d8rEX5D/FdrDIxj2IrUPeE/ 10 j5Dv4g9+fPnTRcX006T++LdYYw7w+i... You can use munpack to easily extract attachments out of such text files and write them into a proper named files. $ munpack -f plaintext.eml myawesomefile.tar.gz ( application/x-gzip )","title":"Extract attachments from multipart emails"},{"location":"learning/tools/extract%20attachments%20from%20multipart%20emails/#sources","text":"Extract attachments from multipart messages","title":"Sources"},{"location":"learning/tools/fedora%20linux/","text":"Fedora GNU/Linux \u00b6 Enable the RPM Fusion repositories \u00b6 RPM Fusion provides software that the Fedora Project or Red Hat doesn't want to ship. That software is provided as precompiled RPMs for all current Fedora versions and current Red Hat Enterprise Linux or clones versions; you can use the RPM Fusion repositories with tools like yum and PackageKit. These repositories are not available by default and need to be installed using a remote package: # All flavours but Silverblue-based ones. sudo dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release- $( rpm -E %fedora ) .noarch.rpm https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release- $( rpm -E %fedora ) .noarch.rpm # Silverblue-base flavours. sudo rpm-ostree install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release- $( rpm -E %fedora ) .noarch.rpm https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release- $( rpm -E %fedora ) .noarch.rpm After enabling the repositories, you can add their tainted versions for closed or restricted packages: sudo dnf install rpmfusion- { free,nonfree } -release-tainted sudo rpm-ostree install rpmfusion- { free,nonfree } -release-tainted Broadcom Wi-Fi drivers \u00b6 After enabling the normal and tainted RPM fusion repositores, just install the b43-firmware package. Enable ZFS management \u00b6 sudo dnf install http://download.zfsonlinux.org/fedora/zfs-release $( rpm -E %dist ) .noarch.rpm sudo dnf install kernel-devel zfs sudo systemctl start zfs-fuse.service sudo zpool import -a Use DNF from behind a proxy \u00b6 Either: - add the line sslverify=0 to /etc/dnf/dnf.conf ; not suggested , but a quick fix - add the proxie's certificate, in PEM format, to the /etc/pki/ca-trust/source/anchors/ folder and then run sudo update-ca-trust . Sources \u00b6 RPM fusion configuration DNF update from behind SSL inspection proxy","title":"Fedora GNU/Linux"},{"location":"learning/tools/fedora%20linux/#fedora-gnulinux","text":"","title":"Fedora GNU/Linux"},{"location":"learning/tools/fedora%20linux/#enable-the-rpm-fusion-repositories","text":"RPM Fusion provides software that the Fedora Project or Red Hat doesn't want to ship. That software is provided as precompiled RPMs for all current Fedora versions and current Red Hat Enterprise Linux or clones versions; you can use the RPM Fusion repositories with tools like yum and PackageKit. These repositories are not available by default and need to be installed using a remote package: # All flavours but Silverblue-based ones. sudo dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release- $( rpm -E %fedora ) .noarch.rpm https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release- $( rpm -E %fedora ) .noarch.rpm # Silverblue-base flavours. sudo rpm-ostree install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release- $( rpm -E %fedora ) .noarch.rpm https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release- $( rpm -E %fedora ) .noarch.rpm After enabling the repositories, you can add their tainted versions for closed or restricted packages: sudo dnf install rpmfusion- { free,nonfree } -release-tainted sudo rpm-ostree install rpmfusion- { free,nonfree } -release-tainted","title":"Enable the RPM Fusion repositories"},{"location":"learning/tools/fedora%20linux/#broadcom-wi-fi-drivers","text":"After enabling the normal and tainted RPM fusion repositores, just install the b43-firmware package.","title":"Broadcom Wi-Fi drivers"},{"location":"learning/tools/fedora%20linux/#enable-zfs-management","text":"sudo dnf install http://download.zfsonlinux.org/fedora/zfs-release $( rpm -E %dist ) .noarch.rpm sudo dnf install kernel-devel zfs sudo systemctl start zfs-fuse.service sudo zpool import -a","title":"Enable ZFS management"},{"location":"learning/tools/fedora%20linux/#use-dnf-from-behind-a-proxy","text":"Either: - add the line sslverify=0 to /etc/dnf/dnf.conf ; not suggested , but a quick fix - add the proxie's certificate, in PEM format, to the /etc/pki/ca-trust/source/anchors/ folder and then run sudo update-ca-trust .","title":"Use DNF from behind a proxy"},{"location":"learning/tools/fedora%20linux/#sources","text":"RPM fusion configuration DNF update from behind SSL inspection proxy","title":"Sources"},{"location":"learning/tools/ffmpeg/","text":"ffmpeg \u00b6 TL;DR \u00b6 # Convert a webm file to GIF. ffmpeg -y -i rec.webm -vf palettegen palette.png \\ && ffmpeg -y -i rec.webm -i palette.png \\ -filter_complex paletteuse -r 10 out.gif Format conversion \u00b6 Webm to GIF \u00b6 ffmpeg -y -i rec.webm -vf palettegen palette.png ffmpeg -y -i rec.webm -i palette.png -filter_complex paletteuse -r 10 out.gif Here rec.webm is the recorded video. The first command creates a palette out of the webm file. The second command converts the webm file to gif using the created palette. Sources \u00b6 Convert a webm file to gif How to convert a webm video to a gif on the command line","title":"ffmpeg"},{"location":"learning/tools/ffmpeg/#ffmpeg","text":"","title":"ffmpeg"},{"location":"learning/tools/ffmpeg/#tldr","text":"# Convert a webm file to GIF. ffmpeg -y -i rec.webm -vf palettegen palette.png \\ && ffmpeg -y -i rec.webm -i palette.png \\ -filter_complex paletteuse -r 10 out.gif","title":"TL;DR"},{"location":"learning/tools/ffmpeg/#format-conversion","text":"","title":"Format conversion"},{"location":"learning/tools/ffmpeg/#webm-to-gif","text":"ffmpeg -y -i rec.webm -vf palettegen palette.png ffmpeg -y -i rec.webm -i palette.png -filter_complex paletteuse -r 10 out.gif Here rec.webm is the recorded video. The first command creates a palette out of the webm file. The second command converts the webm file to gif using the created palette.","title":"Webm to GIF"},{"location":"learning/tools/ffmpeg/#sources","text":"Convert a webm file to gif How to convert a webm video to a gif on the command line","title":"Sources"},{"location":"learning/tools/find%20how%20long%20a%20process%20has%20run/","text":"Find how long a process has run in UNIX \u00b6 UNIX and Linux have commands for almost everything; if there is no command, you can check some important files in the /etc directory or in the /proc virtual filesystem to find out some useful information. The easy way \u00b6 If the program started today, ps also shows when: $ ps -ef UID PID PPID C STIME TTY TIME CMD user 11610 11578 0 Aug24 ? 00 :08:06 java -Djava.library.path = /usr/lib/jni:/usr/lib/alpha-linux-gnu/jni... user 17057 25803 0 13 :01 ? 00 :00:24 /usr/lib/chromium-browser/chromium-browser The hackish way \u00b6 Useful if the process started before today: find the process ID $ ps -ef | grep java user 22031 22029 0 Jan 29 ? 24 :53 java -Xms512M -Xmx512 Server $ pgrep -l java 22031 java look into the proc virtual filesystem for that process and check the creation date, which is when the process was started ls -ld /proc/22031 dr-x--x--x 5 user group 832 Jan 22 13 :09 /proc/22031 Sources \u00b6 how to find how long a process has run in unix","title":"Find how long a process has run in UNIX"},{"location":"learning/tools/find%20how%20long%20a%20process%20has%20run/#find-how-long-a-process-has-run-in-unix","text":"UNIX and Linux have commands for almost everything; if there is no command, you can check some important files in the /etc directory or in the /proc virtual filesystem to find out some useful information.","title":"Find how long a process has run in UNIX"},{"location":"learning/tools/find%20how%20long%20a%20process%20has%20run/#the-easy-way","text":"If the program started today, ps also shows when: $ ps -ef UID PID PPID C STIME TTY TIME CMD user 11610 11578 0 Aug24 ? 00 :08:06 java -Djava.library.path = /usr/lib/jni:/usr/lib/alpha-linux-gnu/jni... user 17057 25803 0 13 :01 ? 00 :00:24 /usr/lib/chromium-browser/chromium-browser","title":"The easy way"},{"location":"learning/tools/find%20how%20long%20a%20process%20has%20run/#the-hackish-way","text":"Useful if the process started before today: find the process ID $ ps -ef | grep java user 22031 22029 0 Jan 29 ? 24 :53 java -Xms512M -Xmx512 Server $ pgrep -l java 22031 java look into the proc virtual filesystem for that process and check the creation date, which is when the process was started ls -ld /proc/22031 dr-x--x--x 5 user group 832 Jan 22 13 :09 /proc/22031","title":"The hackish way"},{"location":"learning/tools/find%20how%20long%20a%20process%20has%20run/#sources","text":"how to find how long a process has run in unix","title":"Sources"},{"location":"learning/tools/find/","text":"Find \u00b6 TL;DR \u00b6 # Change the permissions of all files and directories in the current directory, # recursively. find . -type d -exec chmod 755 {} + find . -type f -exec chmod 644 {} + # Change the ownership of all files and directories owned by a specific user or # group, recursively. find . -type d -user harry -exec chown daisy {} + find . -type f -group users -exec chown :admin {} + # Delete all empty files and directories in the 'Documents' directory. find Documents -empty -delete # recursively find Documents -maxdepth 1 -empty -delete # non recursively # Get the extensions of all files larger than 1MB. find . -type f -size +1M -exec basename {} \\; | sed 's|.*\\.||' | sort -u # Find files last accessed exactly 5 hour ago. find . -type f -amin 300 find . -type f -atime 5h # Find files last modified in the last hour. find . -type f -mmin -60 find . -type f -mtime -1h # Find files created more than 2 days ago. find . -type f -ctime +2 # Find all empty directories in a git repository that are not from git itself. find path/to/repo -type d -empty -not -path \"./.git/*\" # Find broken symlinks in the given directories, recursively. find dir/1 dir/n -type l -exec test ! -e {} \\; -print find dir/1 dir/n -xtype l # gnu find only # Sort files by name, in numeric order, regardless of the directory they are in. find . -type f -o -type l \\ | awk 'BEGIN {FS=\"/\"; OFS=\"|\"} {print $NF,$0}' \\ | sort --field-separator '|' --numeric-sort \\ | cut -d '|' -f2 # Print quoted file paths. # %p is for path. find . -type f -printf '\"%p\"\\n' find . -type f -printf \"'%p'\\n\" # Sort files by size. # %s is for size, %p is for path. find . -type f -printf '%s %p\\n' | sort -nr | head -50 # Find files which are executable but not readable. find /sbin /usr/sbin -executable -not -readable -print # Find files which are writable by either their owner or their group. find . -perm /220 find . -perm /u+w,g+w find . -perm /u = w,g = w # Find files which are writable by both their owner and their group. find . -perm -220 find . -perm -g+w,u+w # Record set-user-ID files and directories into '/root/suid.txt', and large # files into 'big-files.txt' find / \\ \\( -perm -4000 -fprintf /root/suid.txt '%#m %u %p\\n' \\) , \\ \\( -size +100M -fprintf big-files.txt '%-10s %p\\n' \\) # Show files with hard links. find . -type f -not -links 1 find -type f -links +1 # Show files hard linked to a given file. # GNU extension. find -samefile path/to/file Time specifications \u00b6 Primaries used to check the difference between the file last access, creation or modification time and the time find was started. All time specification primaries take a numeric argument, and allow the number to be preceded by a plus sign ( + ) or a minus sign ( - ). A preceding plus sign means more than n , a preceding minus sign means less than n and neither means exactly n . Accepted time information: a for the file's last access time c for the time of last change of file status information (creation) m for the file's last modification time B for the file's inode creation time With the -Xmin form, times are rounded up to the next full minute . This is the same as using -Xtime Nm . With the -Xtime form, times depend on the given unit; if no unit is given, it defaults to full 24 hours periods (days). Accepted units: s for seconds m for minutes (60 seconds) h for hours (60 minutes) d for days (24 hours) w for weeks (7 days) Any number of units may be combined in one -Xtime argument. with the -newerXY file form, find checks if file has a more recent last access time (X=a), inode creation time (X=B), change time (X=c), or modification time (X=m) than the last access time (Y=a), inode creation time (Y=B), change time (Y=c), or modification time (Y=m). If Y=t, file is interpreted as a direct date specification of the form understood by cvs . Also, -newermm is the same as -newer . # Find files last accessed exactly 5 minutes ago. find /dir -amin 5 find /dir -atime 300s find /dir -atime 5m # Find files last accessed in the last 3 days. find /dir -atime -3 find /dir -atime -3d # Find files created in the last 1.5 hour. find /dir -cmin -90 find /dir -ctime -1h30m # Find files created more than 4 days ago. find /dir -ctime +4 # Find files modified less than 30 minutes ago. find /dir -mmin -30 find /dir -mtime -30m find /dir -mtime -.5h # gnu find only # Find files modified exactly 2 days ago. find /dir -mtime 2 find /dir -mtime 48h # Find files modified more than 4 weeks ago. find /dir -mtime +28 find /dir -mtime +4w # Find all files whose inode change time is more recent than the current time # minus one minute. find / -newerct '1 minute ago' # Find files owned by 'wnj' that are newer than 'file.txt'. find / -newer file.txt -user wnj -print Gotchas \u00b6 in GNU's find the path parameter defaults to the current directory and can be avoided # Delete all empty folders in the current directory only. find -maxdepth 1 -empty -delete GNU's find also understands fractional time specifications: # Find files modified in the last 1 hour and 30 minutes. find -mtime 1 .5h Sources \u00b6 How can I find broken symlinks? find . -type f -exec chmod 644 {} ; How to output file names surrounded with quotes in SINGLE line? How to find all hardlinks in a folder?","title":"Find"},{"location":"learning/tools/find/#find","text":"","title":"Find"},{"location":"learning/tools/find/#tldr","text":"# Change the permissions of all files and directories in the current directory, # recursively. find . -type d -exec chmod 755 {} + find . -type f -exec chmod 644 {} + # Change the ownership of all files and directories owned by a specific user or # group, recursively. find . -type d -user harry -exec chown daisy {} + find . -type f -group users -exec chown :admin {} + # Delete all empty files and directories in the 'Documents' directory. find Documents -empty -delete # recursively find Documents -maxdepth 1 -empty -delete # non recursively # Get the extensions of all files larger than 1MB. find . -type f -size +1M -exec basename {} \\; | sed 's|.*\\.||' | sort -u # Find files last accessed exactly 5 hour ago. find . -type f -amin 300 find . -type f -atime 5h # Find files last modified in the last hour. find . -type f -mmin -60 find . -type f -mtime -1h # Find files created more than 2 days ago. find . -type f -ctime +2 # Find all empty directories in a git repository that are not from git itself. find path/to/repo -type d -empty -not -path \"./.git/*\" # Find broken symlinks in the given directories, recursively. find dir/1 dir/n -type l -exec test ! -e {} \\; -print find dir/1 dir/n -xtype l # gnu find only # Sort files by name, in numeric order, regardless of the directory they are in. find . -type f -o -type l \\ | awk 'BEGIN {FS=\"/\"; OFS=\"|\"} {print $NF,$0}' \\ | sort --field-separator '|' --numeric-sort \\ | cut -d '|' -f2 # Print quoted file paths. # %p is for path. find . -type f -printf '\"%p\"\\n' find . -type f -printf \"'%p'\\n\" # Sort files by size. # %s is for size, %p is for path. find . -type f -printf '%s %p\\n' | sort -nr | head -50 # Find files which are executable but not readable. find /sbin /usr/sbin -executable -not -readable -print # Find files which are writable by either their owner or their group. find . -perm /220 find . -perm /u+w,g+w find . -perm /u = w,g = w # Find files which are writable by both their owner and their group. find . -perm -220 find . -perm -g+w,u+w # Record set-user-ID files and directories into '/root/suid.txt', and large # files into 'big-files.txt' find / \\ \\( -perm -4000 -fprintf /root/suid.txt '%#m %u %p\\n' \\) , \\ \\( -size +100M -fprintf big-files.txt '%-10s %p\\n' \\) # Show files with hard links. find . -type f -not -links 1 find -type f -links +1 # Show files hard linked to a given file. # GNU extension. find -samefile path/to/file","title":"TL;DR"},{"location":"learning/tools/find/#time-specifications","text":"Primaries used to check the difference between the file last access, creation or modification time and the time find was started. All time specification primaries take a numeric argument, and allow the number to be preceded by a plus sign ( + ) or a minus sign ( - ). A preceding plus sign means more than n , a preceding minus sign means less than n and neither means exactly n . Accepted time information: a for the file's last access time c for the time of last change of file status information (creation) m for the file's last modification time B for the file's inode creation time With the -Xmin form, times are rounded up to the next full minute . This is the same as using -Xtime Nm . With the -Xtime form, times depend on the given unit; if no unit is given, it defaults to full 24 hours periods (days). Accepted units: s for seconds m for minutes (60 seconds) h for hours (60 minutes) d for days (24 hours) w for weeks (7 days) Any number of units may be combined in one -Xtime argument. with the -newerXY file form, find checks if file has a more recent last access time (X=a), inode creation time (X=B), change time (X=c), or modification time (X=m) than the last access time (Y=a), inode creation time (Y=B), change time (Y=c), or modification time (Y=m). If Y=t, file is interpreted as a direct date specification of the form understood by cvs . Also, -newermm is the same as -newer . # Find files last accessed exactly 5 minutes ago. find /dir -amin 5 find /dir -atime 300s find /dir -atime 5m # Find files last accessed in the last 3 days. find /dir -atime -3 find /dir -atime -3d # Find files created in the last 1.5 hour. find /dir -cmin -90 find /dir -ctime -1h30m # Find files created more than 4 days ago. find /dir -ctime +4 # Find files modified less than 30 minutes ago. find /dir -mmin -30 find /dir -mtime -30m find /dir -mtime -.5h # gnu find only # Find files modified exactly 2 days ago. find /dir -mtime 2 find /dir -mtime 48h # Find files modified more than 4 weeks ago. find /dir -mtime +28 find /dir -mtime +4w # Find all files whose inode change time is more recent than the current time # minus one minute. find / -newerct '1 minute ago' # Find files owned by 'wnj' that are newer than 'file.txt'. find / -newer file.txt -user wnj -print","title":"Time specifications"},{"location":"learning/tools/find/#gotchas","text":"in GNU's find the path parameter defaults to the current directory and can be avoided # Delete all empty folders in the current directory only. find -maxdepth 1 -empty -delete GNU's find also understands fractional time specifications: # Find files modified in the last 1 hour and 30 minutes. find -mtime 1 .5h","title":"Gotchas"},{"location":"learning/tools/find/#sources","text":"How can I find broken symlinks? find . -type f -exec chmod 644 {} ; How to output file names surrounded with quotes in SINGLE line? How to find all hardlinks in a folder?","title":"Sources"},{"location":"learning/tools/firewalld/","text":"Firewalld \u00b6 Firewalld is a dynamically managed firewall with support for network/firewall zones that define the trust level of network connections or interfaces. It has support for IPv4, IPv6, firewall settings, ethernet bridges and IP sets. It also offers separation of runtime and permanent configuration options. It is the default firewall management tool for: - RHEL and CentOS 7 and newer - Fedora 18 and newer - (Open)SUSE 15 and newer TL;DR \u00b6 # Show which zone is currently selected as the default. firewall-cmd --get-default-zone # List all available zones. firewall-cmd --get-zones firewall-cmd --get-zones --permanent # List the currently active zones only. firewall-cmd --get-active-zones # Print the default zone's configuration. firewall-config --list-all # Change the default zone. sudo firewall-cmd --set-default-zone = home # Change an interface's zone assignment. sudo firewall-cmd --zone = home --change-interface = eth0 # List the available service definitions. firewall-cmd --get-services # List the allowed services in a zone. sudo firewall-cmd --list-services sudo firewall-cmd --list-services --zone = public sudo firewall-cmd --list-services --permanent # Temporarily allow services. sudo firewall-cmd --add-service = http sudo firewall-cmd --add-service = ssh --zone = public # Permanently allow services. sudo firewall-cmd --add-service = ssh --permanent sudo firewall-cmd --add-service = https --zone = public --permanent # List the open ports in a zone. sudo firewall-cmd --list-ports sudo firewall-cmd --list-ports --zone = public sudo firewall-cmd --list-ports --permanent # Temporarily open specific ports. sudo firewall-cmd --add-port = 1978 /tcp sudo firewall-cmd --add-port = 4990 -4999/udp --zone = public # Permanently open specific ports. sudo firewall-cmd --add-port = 22 /tcp --permanent sudo firewall-cmd --add-port = 4990 -4999/udp --zone = public --permanent # Close an open port. sudo firewall-cmd --add-port = 1978 /tcp sudo firewall-cmd --add-port = 1978 /tcp --zone = public # Create a new zone. sudo firewall-cmd --new-zone = publicweb --permanent # Make changes permament. sudo firewall-cmd --runtime-to-permanent # Reload the firewall. sudo firewall-cmd --reload Further readings \u00b6 Website Documentation Sources \u00b6 Open TCP Port on openSUSE Firewall How To Set Up a Firewall Using firewalld on CentOS 8","title":"Firewalld"},{"location":"learning/tools/firewalld/#firewalld","text":"Firewalld is a dynamically managed firewall with support for network/firewall zones that define the trust level of network connections or interfaces. It has support for IPv4, IPv6, firewall settings, ethernet bridges and IP sets. It also offers separation of runtime and permanent configuration options. It is the default firewall management tool for: - RHEL and CentOS 7 and newer - Fedora 18 and newer - (Open)SUSE 15 and newer","title":"Firewalld"},{"location":"learning/tools/firewalld/#tldr","text":"# Show which zone is currently selected as the default. firewall-cmd --get-default-zone # List all available zones. firewall-cmd --get-zones firewall-cmd --get-zones --permanent # List the currently active zones only. firewall-cmd --get-active-zones # Print the default zone's configuration. firewall-config --list-all # Change the default zone. sudo firewall-cmd --set-default-zone = home # Change an interface's zone assignment. sudo firewall-cmd --zone = home --change-interface = eth0 # List the available service definitions. firewall-cmd --get-services # List the allowed services in a zone. sudo firewall-cmd --list-services sudo firewall-cmd --list-services --zone = public sudo firewall-cmd --list-services --permanent # Temporarily allow services. sudo firewall-cmd --add-service = http sudo firewall-cmd --add-service = ssh --zone = public # Permanently allow services. sudo firewall-cmd --add-service = ssh --permanent sudo firewall-cmd --add-service = https --zone = public --permanent # List the open ports in a zone. sudo firewall-cmd --list-ports sudo firewall-cmd --list-ports --zone = public sudo firewall-cmd --list-ports --permanent # Temporarily open specific ports. sudo firewall-cmd --add-port = 1978 /tcp sudo firewall-cmd --add-port = 4990 -4999/udp --zone = public # Permanently open specific ports. sudo firewall-cmd --add-port = 22 /tcp --permanent sudo firewall-cmd --add-port = 4990 -4999/udp --zone = public --permanent # Close an open port. sudo firewall-cmd --add-port = 1978 /tcp sudo firewall-cmd --add-port = 1978 /tcp --zone = public # Create a new zone. sudo firewall-cmd --new-zone = publicweb --permanent # Make changes permament. sudo firewall-cmd --runtime-to-permanent # Reload the firewall. sudo firewall-cmd --reload","title":"TL;DR"},{"location":"learning/tools/firewalld/#further-readings","text":"Website Documentation","title":"Further readings"},{"location":"learning/tools/firewalld/#sources","text":"Open TCP Port on openSUSE Firewall How To Set Up a Firewall Using firewalld on CentOS 8","title":"Sources"},{"location":"learning/tools/flatpak/","text":"Flatpak \u00b6 TL;DR \u00b6 # List installed applications and runtimes. flatpak list flatpak list --app # List remotes. flatpak remotes # Add remotes. flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo flatpak remote-add --user --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo # Search for applications. flatpak search vscode # Install applications. flatpak install fedora org.stellarium.Stellarium flatpak install --user flathub com.visualstudio.code-oss flatpak install https://flathub.org/repo/appstream/org.gimp.GIMP.flatpakref # Run applications. flatpak run org.gimp.GIMP # Update applications. flatpak update # Uninstall applications. flatpak uninstall org.stellarium.Stellarium flatpak uninstall --unused flatpak uninstall --delete-data edu.berkeley.BOINC # Remove remotes. flatpak remote-delete flathub # Fix inconsitencies. flatpak repair # Reset applications' permissions. flatpak permission-reset org.gimp.GIMP # List operations. flatpak history Further readings \u00b6 Using Flatpak getting started guide on the official documentation Sources \u00b6 How to clean up Flatpak apps to clear disk space","title":"Flatpak"},{"location":"learning/tools/flatpak/#flatpak","text":"","title":"Flatpak"},{"location":"learning/tools/flatpak/#tldr","text":"# List installed applications and runtimes. flatpak list flatpak list --app # List remotes. flatpak remotes # Add remotes. flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo flatpak remote-add --user --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo # Search for applications. flatpak search vscode # Install applications. flatpak install fedora org.stellarium.Stellarium flatpak install --user flathub com.visualstudio.code-oss flatpak install https://flathub.org/repo/appstream/org.gimp.GIMP.flatpakref # Run applications. flatpak run org.gimp.GIMP # Update applications. flatpak update # Uninstall applications. flatpak uninstall org.stellarium.Stellarium flatpak uninstall --unused flatpak uninstall --delete-data edu.berkeley.BOINC # Remove remotes. flatpak remote-delete flathub # Fix inconsitencies. flatpak repair # Reset applications' permissions. flatpak permission-reset org.gimp.GIMP # List operations. flatpak history","title":"TL;DR"},{"location":"learning/tools/flatpak/#further-readings","text":"Using Flatpak getting started guide on the official documentation","title":"Further readings"},{"location":"learning/tools/flatpak/#sources","text":"How to clean up Flatpak apps to clear disk space","title":"Sources"},{"location":"learning/tools/freebsd/","text":"FreeBSD \u00b6 TL;DR \u00b6 # Initialize package managers. portsnap auto pkg bootstrap Utilities worth noticing \u00b6 bsdinstall bsdconfig NTP time sync \u00b6 # file /etc/rc.conf ntpd_enable=\"YES\" ntpd_sync_on_start=\"YES\" Sources \u00b6 Ports NTPdate - not updating to current time Boinc sbz's FreeBSD commands cheat-sheet","title":"FreeBSD"},{"location":"learning/tools/freebsd/#freebsd","text":"","title":"FreeBSD"},{"location":"learning/tools/freebsd/#tldr","text":"# Initialize package managers. portsnap auto pkg bootstrap","title":"TL;DR"},{"location":"learning/tools/freebsd/#utilities-worth-noticing","text":"bsdinstall bsdconfig","title":"Utilities worth noticing"},{"location":"learning/tools/freebsd/#ntp-time-sync","text":"# file /etc/rc.conf ntpd_enable=\"YES\" ntpd_sync_on_start=\"YES\"","title":"NTP time sync"},{"location":"learning/tools/freebsd/#sources","text":"Ports NTPdate - not updating to current time Boinc sbz's FreeBSD commands cheat-sheet","title":"Sources"},{"location":"learning/tools/fstab/","text":"Fstab \u00b6 TL;DR \u00b6 # by label # e2label /dev/sdc1 # mount -L seagate_2tb_usb /media/usb LABEL=seagate_2tb_usb /media/usb ext3 defaults 0 0 # by uuid # vol_id --uuid /dev/sdb2 # mount -U 41c22818-fbad-4da6-8196-c816df0b7aa8 /disk2p2 UUID=41c22818-fbad-4da6-8196-c816df0b7aa8 /disk2p2 ext3 defaults,errors=remount-ro 0 1 Sources \u00b6 Mount a disk partition using LABEL Mount a disk partition using UUID","title":"Fstab"},{"location":"learning/tools/fstab/#fstab","text":"","title":"Fstab"},{"location":"learning/tools/fstab/#tldr","text":"# by label # e2label /dev/sdc1 # mount -L seagate_2tb_usb /media/usb LABEL=seagate_2tb_usb /media/usb ext3 defaults 0 0 # by uuid # vol_id --uuid /dev/sdb2 # mount -U 41c22818-fbad-4da6-8196-c816df0b7aa8 /disk2p2 UUID=41c22818-fbad-4da6-8196-c816df0b7aa8 /disk2p2 ext3 defaults,errors=remount-ro 0 1","title":"TL;DR"},{"location":"learning/tools/fstab/#sources","text":"Mount a disk partition using LABEL Mount a disk partition using UUID","title":"Sources"},{"location":"learning/tools/funtoo%20linux/","text":"Funtoo GNU/Linux \u00b6 # Portage update. sudo ego sync sudo emerge --sync Further readings \u00b6 Installation guide Portage Profiles","title":"Funtoo GNU/Linux"},{"location":"learning/tools/funtoo%20linux/#funtoo-gnulinux","text":"# Portage update. sudo ego sync sudo emerge --sync","title":"Funtoo GNU/Linux"},{"location":"learning/tools/funtoo%20linux/#further-readings","text":"Installation guide Portage Profiles","title":"Further readings"},{"location":"learning/tools/fwupd/","text":"Fwupd \u00b6 TL;DR \u00b6 # Display all detected devices. fwupdmgr get-devices # Download the latest metadata from LVFS. fwupdmgr refresh fwupdmgr refresh --force # Display available updates, if present. fwupdmgr get-updates # Download and apply all available updates. fwupdmgr update # Report the status of an update. fwupdmgr report-history # Clear the local history of updates. fwupdmgr clear-history Further readings \u00b6 Website GitHub page","title":"Fwupd"},{"location":"learning/tools/fwupd/#fwupd","text":"","title":"Fwupd"},{"location":"learning/tools/fwupd/#tldr","text":"# Display all detected devices. fwupdmgr get-devices # Download the latest metadata from LVFS. fwupdmgr refresh fwupdmgr refresh --force # Display available updates, if present. fwupdmgr get-updates # Download and apply all available updates. fwupdmgr update # Report the status of an update. fwupdmgr report-history # Clear the local history of updates. fwupdmgr clear-history","title":"TL;DR"},{"location":"learning/tools/fwupd/#further-readings","text":"Website GitHub page","title":"Further readings"},{"location":"learning/tools/gcloud/","text":"Google cloud platform CLI \u00b6 TL;DR Further readings Sources TL;DR \u00b6 # Login. gcloud auth login gcloud auth login account # Set applications. gcloud auth application-default login gcloud auth application-default login --no-launch-browser # Activate a service account. gcloud auth activate-service-account \\ serviceaccount@gcpproject.iam.gserviceaccount.com \\ --key-file /tmp/sa.credentials.json # Configure the CLI. gcloud config set account serviceaccount@gcpproject.iam.gserviceaccount.com gcloud config set project project-id gcloud config set compute/region europe-west1 # List current settings. gcloud config list gcloud config list --configuration profile # Create a new profile. gcloud config configurations create new-active-profile gcloud config configurations create --no-activate new-inactive-profile # List available profiles. gcloud config configurations list # Switch to a different configuration. gcloud config configurations activate old-profile # SSH into a compute instance. gcloud compute ssh --zone zone instance --project project gcloud beta compute ssh --zone zone instance --project project # Show operations. # Filters are suggested. gcloud container operations list --filter = \"NOT status:DONE\" gcloud container operations list \\ --filter = \"name:operation-1513320920760-9c26cff5 AND status:RUNNING\" gcloud compute operations list --filter = \"region:europe-west4 AND -status:DONE\" gcloud compute operations list \\ --filter = \"region:(europe-west4 us-east2)\" \\ --filter = \"status!=DONE\" # Use a specific service account for an operation. # The service account must have been activated. gcloud config set account serviceaccount@gcpproject.iam.gserviceaccount.com \\ && gcloud auth application-default login --no-launch-browser \\ && gcloud compute instances list # Logout. gcloud auth revoke --all gcloud auth revoke account Further readings \u00b6 Gcloud cheat-sheet Kubectl cluster access Gcloud config configurations Sources \u00b6 How to run gcloud command line using a service account How to change the active configuration profile in gcloud","title":"Google cloud platform CLI <!-- omit in toc -->"},{"location":"learning/tools/gcloud/#google-cloud-platform-cli","text":"TL;DR Further readings Sources","title":"Google cloud platform CLI "},{"location":"learning/tools/gcloud/#tldr","text":"# Login. gcloud auth login gcloud auth login account # Set applications. gcloud auth application-default login gcloud auth application-default login --no-launch-browser # Activate a service account. gcloud auth activate-service-account \\ serviceaccount@gcpproject.iam.gserviceaccount.com \\ --key-file /tmp/sa.credentials.json # Configure the CLI. gcloud config set account serviceaccount@gcpproject.iam.gserviceaccount.com gcloud config set project project-id gcloud config set compute/region europe-west1 # List current settings. gcloud config list gcloud config list --configuration profile # Create a new profile. gcloud config configurations create new-active-profile gcloud config configurations create --no-activate new-inactive-profile # List available profiles. gcloud config configurations list # Switch to a different configuration. gcloud config configurations activate old-profile # SSH into a compute instance. gcloud compute ssh --zone zone instance --project project gcloud beta compute ssh --zone zone instance --project project # Show operations. # Filters are suggested. gcloud container operations list --filter = \"NOT status:DONE\" gcloud container operations list \\ --filter = \"name:operation-1513320920760-9c26cff5 AND status:RUNNING\" gcloud compute operations list --filter = \"region:europe-west4 AND -status:DONE\" gcloud compute operations list \\ --filter = \"region:(europe-west4 us-east2)\" \\ --filter = \"status!=DONE\" # Use a specific service account for an operation. # The service account must have been activated. gcloud config set account serviceaccount@gcpproject.iam.gserviceaccount.com \\ && gcloud auth application-default login --no-launch-browser \\ && gcloud compute instances list # Logout. gcloud auth revoke --all gcloud auth revoke account","title":"TL;DR"},{"location":"learning/tools/gcloud/#further-readings","text":"Gcloud cheat-sheet Kubectl cluster access Gcloud config configurations","title":"Further readings"},{"location":"learning/tools/gcloud/#sources","text":"How to run gcloud command line using a service account How to change the active configuration profile in gcloud","title":"Sources"},{"location":"learning/tools/gentoo%20linux/","text":"Gentoo GNU/Linux \u00b6 Further readings \u00b6 Portage","title":"Gentoo GNU/Linux"},{"location":"learning/tools/gentoo%20linux/#gentoo-gnulinux","text":"","title":"Gentoo GNU/Linux"},{"location":"learning/tools/gentoo%20linux/#further-readings","text":"Portage","title":"Further readings"},{"location":"learning/tools/get%20the%20environment%20of%20a%20process%20running%20in%20a%20container/","text":"Get the environment of a process running in a container \u00b6 TL;DR \u00b6 cat /proc/ ${ PID } /environ # Container in kubernetes. kubectl exec pod-name -- cat /proc/1/environ # Only works if the onboard `ps` is not from busybox. ps e -p $PID Sources \u00b6 Get the environment variables of running process in container","title":"Get the environment of a process running in a container"},{"location":"learning/tools/get%20the%20environment%20of%20a%20process%20running%20in%20a%20container/#get-the-environment-of-a-process-running-in-a-container","text":"","title":"Get the environment of a process running in a container"},{"location":"learning/tools/get%20the%20environment%20of%20a%20process%20running%20in%20a%20container/#tldr","text":"cat /proc/ ${ PID } /environ # Container in kubernetes. kubectl exec pod-name -- cat /proc/1/environ # Only works if the onboard `ps` is not from busybox. ps e -p $PID","title":"TL;DR"},{"location":"learning/tools/get%20the%20environment%20of%20a%20process%20running%20in%20a%20container/#sources","text":"Get the environment variables of running process in container","title":"Sources"},{"location":"learning/tools/ghostscript/","text":"Ghostscript \u00b6 TL;DR \u00b6 # Install. brew install ghostscript sudo port install ghostscript # Reduce the size of PDF files. gs -dNOPAUSE -dQUIET -dBATCH \\ -sDEVICE = pdfwrite -dPDFSETTINGS = /ebook -dCompatibilityLevel = 1 .4 \\ -sOutputFile = path/to/small/file.pdf \\ path/to/massive/file.pdf Reduce the size of PDF files \u00b6 Execute the following: gs -dNOPAUSE -dQUIET -dBATCH \\ -sDEVICE = pdfwrite -dPDFSETTINGS = /ebook -dCompatibilityLevel = 1 .4 \\ -sOutputFile = path/to/small/file.pdf \\ path/to/massive/file.pdf Use one of the following options for the value of -dPDFSETTINGS : Value Description /screen Screen-view-only quality, 72 dpi images /ebook Low quality, 150 dpi images /printer High quality, 300 dpi images /prepress High quality, color preserving, 300 dpi images /default Almost identical to /screen Sources \u00b6 Reducing PDF File size","title":"Ghostscript"},{"location":"learning/tools/ghostscript/#ghostscript","text":"","title":"Ghostscript"},{"location":"learning/tools/ghostscript/#tldr","text":"# Install. brew install ghostscript sudo port install ghostscript # Reduce the size of PDF files. gs -dNOPAUSE -dQUIET -dBATCH \\ -sDEVICE = pdfwrite -dPDFSETTINGS = /ebook -dCompatibilityLevel = 1 .4 \\ -sOutputFile = path/to/small/file.pdf \\ path/to/massive/file.pdf","title":"TL;DR"},{"location":"learning/tools/ghostscript/#reduce-the-size-of-pdf-files","text":"Execute the following: gs -dNOPAUSE -dQUIET -dBATCH \\ -sDEVICE = pdfwrite -dPDFSETTINGS = /ebook -dCompatibilityLevel = 1 .4 \\ -sOutputFile = path/to/small/file.pdf \\ path/to/massive/file.pdf Use one of the following options for the value of -dPDFSETTINGS : Value Description /screen Screen-view-only quality, 72 dpi images /ebook Low quality, 150 dpi images /printer High quality, 300 dpi images /prepress High quality, color preserving, 300 dpi images /default Almost identical to /screen","title":"Reduce the size of PDF files"},{"location":"learning/tools/ghostscript/#sources","text":"Reducing PDF File size","title":"Sources"},{"location":"learning/tools/git/","text":"Git \u00b6 TL;DR Authentication Configuration Remotes Push to multiple git remotes with the one command Aliases Manage changes Create a patch Apply a patch The stash stack Branches Checkout an existing remote branch Delete a branch Delete branches which have been merged or are otherwise absent from a remote. Merge the master branch into a feature branch Rebase a branch on top of another Tags Convert a normal repository to a bare one LFS extension Submodules Remove a file from a commit Remove a file from the repository Troubleshooting Debug GPG cannot sign a commit Git does not accept self-signed certificates Further readings Sources TL;DR \u00b6 # Set your identity. git config 'user.name' 'User Name' git config --global 'user.email' 'user@email.com' # Avoid issues when collaborating from different platforms. git config --local 'core.autocrlf' 'input' git config --local 'core.autocrlf' 'true' # Create aliases. git config --local 'alias.co' 'checkout' git config --global 'alias.unstage' 'reset HEAD --' git config 'alias.funct' '!f() { sh_command ; sh_command | sh_command ; } && f' # Show git's configuration. git config --list git config --list --show-scope git config --list --show-origin # Render all current settings' values. git config --list \\ | awk -F '=' '{print $1}' | sort -u \\ | xargs -I {} sh -c 'printf \"{}=\" && git config --get {}' # Get a default value if the setting has none. # Does not work on sections alone. git config --get --default 'not-set' 'filter.lfs.cleaned' # Create or reinitialize a repository. git init git init --initial-branch 'main' 'path/to/repo' git init --bare 'path/to/repo.git' # Clone a repository. git clone 'https://github.com:user/repo.git' git clone --bare 'git@github.com:user/repo.git' 'path/to/clone' git clone --recurse-submodules 'ssh@git.server:user/repo.git' git clone --depth 1 'ssh@git.server:user/repo.git' git clone 'https://token@github.com/user/repo' git \\ -c http.extraHeader = \"Authorization: Basic $( echo -n \"user:pat\" | base64 ) \" \\ clone 'https://dev.azure.com/org/project/_git/repo' # Convert a normal repository to a bare one. git clone --bare 'repository' 'path/to/bare/clone.git' # Unshallow a clone. git pull --unshallow # Get objects and refs but do not incorporate them. git fetch # Get changes and merge them. git pull --all git pull --verify-signatures git pull 'remote' 'branch' # Show what files changed. git status git status --verbose # Show changes in a repository. git diff git diff --staged 'commit_hash' git diff 'commit_hash_1..commit_hash_2' git diff 'branch_1' 'branch_2' git diff --word-diff = 'color' git log -p 'feature' --not 'master' # Just show changes between two files. git diff --no-index 'path/to/file/a' 'path/to/file/b' # Stage changes for commit. git add . git add --all git add 'path/to/file' # Interactively review chunks of changes. git add --patch 'path/to/file' # Commit changes. git commit --message 'message' git commit --message 'whatever' --gpg-sign git commit --allow-empty --allow-empty-message git commit --date = 'Jun 13 18:30:25 IST 2015' git commit --date = \" $( date --date = '2 days ago' ) \" # Edit the last commit's message. git commit --amend git commit --amend --message 'message' # Change the last commit's author. git config 'user.name' 'user name' git config 'user.email' 'user.email@mail.com' git commit --amend --reset-author # Sign the last commit. git commit --amend --no-edit --gpg-sign # Show commits which would be pushed. git log @ { u } .. # Revert a commit but keep the history of the event as a separate commit. git revert 'commit_hash' # Interactively rebase the last 7 commits. git rebase -i '@~7' # List remotes. git remote --verbose # Add a new remote. git remote add 'gitlab' 'git@gitlab.com:user/repo.git' # Set a new URL for an existing remote. git remote set-url 'gitlab' 'git@gitlab.com:user/repo.git' # Push committed changes. git push git push 'remote' 'branch_1' 'branch_N' git push 'git@github.com:user/repo.git' git push --all --force # Show the repository's history. git reflog git log -p # Visualize the repository's history. git log --graph --full-history --all --color --decorate --oneline # Show and verify signatures. git log --show-signature -1 # Remove staged and working directory changes. git reset --hard git reset --hard 'origin/main' # Go back 4 commits. git reset --hard 'HEAD~4' # Remove untracked files. git clean -f -d # Remove ignored files. git clean -f -d -x # Show who committed which line. git blame 'path/to/file' # List changed files in a given commit. git diff-tree --no-commit-id --name-only -r 'commit_hash' # Create patches. git diff > 'file.patch' git diff --output 'file.patch' --cached git format-patch -5 'commit_hash' git format-patch 'HEAD~3' -o 'dir' git format-patch 'HEAD~2' --stdout > 'single/file.patch' # Create a full patch of the unstaged changes. git add . && git commit -m 'uncommitted' \\ && git format-patch 'HEAD~1' && git reset 'HEAD~1' # Apply a patch to the current index. git apply 'file.patch' # Apply commits from a patch. git am 'file.patch' # Stash changes locally. git stash git stash push 'message' # List all the stashed changes. git stash list # Apply the most recent change and remove them from the stash stack. git stash pop # Apply a stash, but don't remove it from the stack. git stash apply stash@ { 6 } # Remove a single stash entry from the stash stack. # Defaults to the current one. git stash drop git stash drop stash@ { 2 } # Remove all the stash entries. # Those will then be pruned and may be impossible to recover. git stash clear # Apply only the changes made within a given commit. git cherry-pick 'commit_hash' # Create a branch. git branch 'branch_name' git switch -c 'branch_name' git checkout -b 'local_branch_name' 'remote/branch_name' # Create a bare branch without any commits. git checkout --orphan 'branch_name' # List branches. git branch -a # Rename a branch. git branch --move 'old_name' 'new_name' # Switch branches. git switch 'branch_name' git checkout 'branch_name' git checkout - # Set an existing branch to track a remote branch. git branch -u 'remote_name/upstream-branch' # Get the current branch. git branch --show-current # git > v2.22 git rev-parse --abbrev-ref 'HEAD' # Delete local branches. git branch --delete 'branch_name' git branch -D 'branch_name' # Delete remote branches. git push 'remote_name' ':branch_name' git push 'remote_name' --delete 'branch_name' # Delete both local and remote branches. git branch --delete --remotes 'branch_name' # Sync the local branch list. git fetch --prune # Remove all stale branches. git remote prune 'branch_name' # Delete branches which have been merged or are otherwise absent from a remote. git branch --merged | grep -vE '(^\\*|master|main|dev)' | xargs git branch -d git fetch -p \\ && awk '/origin/&&/gone/{print $1}' < ( git branch -vv ) | xargs git branch -d # List all tags. git tag # Create annotated tags. git tag --annotate 'v0.1.0' git tag -as 'v1.2.0-r0' -m 'signed annotated tag for v1.2.0 release 0' git tag -a '1.1.9' '9fceb02' # Create lightweight tags. git tag 'v0.1.1-rc0' git tag '1.12.1' 'HEAD' # Push specific tags. git push 'remote_name' 'v1.5' # Push annotated tags only. git push --follow-tags # Push all tags. git push --tags # Delete local tags. git tag -d 'v1.4-lw' # Delete remote tags. git push 'remote_name' --delete 'v1.4-lw' # Sync the local tags list. git fetch --prune-tags # Rebase a branch on top of another. git rebase 'branch_name' git rebase 'remote_name/upstream_branch_name' 'local-branch_name' git pull --rebase = 'interactive' 'remote_name' 'branch_name' # Change the date of an existing commit. git filter-branch --env-filter \\ 'if [ $GIT_COMMIT = 119f9ecf58069b265ab22f1f97d2b648faf932e0 ] then export GIT_AUTHOR_DATE=\"Fri Jan 2 21:38:53 2009 -0800\" export GIT_COMMITTER_DATE=\"Sat May 19 01:01:01 2007 -0700\" fi' # Sign all commits from now on. git config --global 'user.signingKey' 'KEY_ID_IN_SHORT_FORMAT' git config --local 'commit.gpgSign' true # Import commits from another repo. git --git-dir = 'path/to/other-repo/.git' format-patch -k -1 --stdout 'commit_hash' \\ | git am -3 -k # Get the top-level directory of the current repository. git rev-parse --show-toplevel # Update all submodules. git submodule update --init --recursive # Show the first commit that has the string \"cool\" in its message body. git show :/cool Authentication \u00b6 # Use credentials in the URL. git clone 'https://username:password@host/path/to/repo' git clone 'https://token@github.com/user/repo' # Use headers. BASIC_AUTH = 'username:password' # or 'username:token', or ':token' BASIC_AUTH_B64 = \" $( printf \" $BASIC_AUTH \" | base64 ) \" git \\ -c http.extraHeader = \"Authorization: Basic ${ BASIC_AUTH_B64 } \" clone 'https://dev.azure.com/organizationName/projectName/_git/repoName' Configuration \u00b6 # Required to be able to commit changes. git config --local 'user.email' 'me@me.info' git config --local 'user.name' 'Me' # Avoid issues when collaborating from different platforms. # 'input' on unix, 'true' on windows, 'false' only if you know what you are doing. git config --local 'core.autocrlf' 'input' # Sign commits by default. # Get the GPG key short ID with `gpg --list-keys --keyid-format short`. git config --local 'user.signingKey' 'KEY_ID_IN_SHORT_FORMAT' git config --local 'commit.gpgSign' true # Pull submodules by default. git config --global 'submodule.recurse' true # Use a Personal Access Token to authenticate. git config http.extraHeader = \"Authorization: Basic $( echo -n 'user:pat' | base64 ) \" To show the current configuration use the --list option: git config --list git config --list --show-scope git config --list --global --show-origin The configuration is shown in full for the requested scope (or all if not specified), but it might include the same setting multiple times if it shows up in multiple scopes. Render the current value of a setting using the --get option: # Get the current user.name value. git config --get 'user.name' # Render all current settings' values. # Gets the settings names, then requests the current value for each. git config --list \\ | awk -F '=' '{print $1}' | sort -u \\ | xargs -I {} sh -c 'printf \"{}=\" && git config --get {}' Remotes \u00b6 # Add a remote. git remote add 'gitlab' 'git@gitlab.com:user/my-awesome-repo.git' # Add other push URLs to an existing remote. git remote set-url --push --add 'origin' 'https://exampleuser@example.com/path/to/repo1' # Change a remote's URL. git remote set-url 'origin' 'git@github.com:user/new-repo-name.git' Push to multiple git remotes with the one command \u00b6 To always push to repo1 , repo2 , and repo3 , but always pull only from repo1 , set up the remote 'origin' as follows: git remote add origin https://exampleuser@example.com/path/to/repo1 git remote set-url --push --add origin https://exampleuser@example.com/path/to/repo1 git remote set-url --push --add origin https://exampleuser@example.com/path/to/repo2 git remote set-url --push --add origin https://exampleuser@example.com/path/to/repo3 [remote \"origin\"] url = https://exampleuser@example.com/path/to/repo1 pushUrl = https://exampleuser@example.com/path/to/repo1 pushUrl = https://exampleuser@example.com/path/to/repo2 pushUrl = https://exampleuser@example.com/path/to/repo3 fetch = +refs/heads/*:refs/remotes/origin/* To only pull from repo1 but push to repo1 and repo2 for a specific branch specialBranch : [remote \"origin\"] url = ssh://git@aaa.xxx.com:7999/yyy/repo1.git fetch = +refs/heads/*:refs/remotes/origin/* \u2026 [remote \"specialRemote\"] url = ssh://git@aaa.xxx.com:7999/yyy/repo1.git pushUrl = ssh://git@aaa.xxx.com:7999/yyy/repo1.git pushUrl = ssh://git@aaa.xxx.com:7999/yyy/repo2.git fetch = +refs/heads/*:refs/remotes/origin/* \u2026 [branch \"specialBranch\"] remote = origin pushRemote = specialRemote \u2026 See https://git-scm.com/docs/git-config#git-config-branchltnamegtremote . Aliases \u00b6 Simple aliases to git commands can be added like aliases to a shell: [alias] caa = commit -a --amend -C HEAD ls = log --oneline statsu = status But simple aliases have limitations: they can't have parameters you can't execute multiple git commands in a single alias you can't use | (pipes) or grep git allows you to escape to a shell using ! (bang); this opens a new world of possibilities for aliases: use shell expansions and parameters use multiple git commands use pipes and all command line tools Those commands need to be wrapped into a one-line function definition: [alias] new = !sh -c 'git log $1@{1}..$1@{0} \"$@\"' pull-from-all = \"!f() { \\ git remote show \\ | xargs -I{} -P0 -n1 git pull {} ${1-$(git branch --show-current)} \\ ; } && f\" subtree-add = \"!f() { git subtree add --prefix $2 $1 master --squash ; } ; f\" Manage changes \u00b6 # Show changes relative to the current index (not yet staged). git diff # Show changes in the staged files only. git diff --staged # Show changes relative to 'commit' (defaults to HEAD if not given). # Alias of `--staged`. git diff --cached 'commit_hash' # Show changes relative to a different branch. git diff 'branch_name' # Show changes between commits. # Separating the commits with `..` is optional. git diff 'commit_hash_1' 'commit_hash_2' git diff 'commit_hash_1..commit_hash_2' # Show changes between branches. # Separating the branches with `..` is optional. git diff 'branch_name_1' 'branch_name_2' git diff 'branch_name_1..branch_name_2' # Show a word diff using 'mode' to delimit changed words for emphasis. # 'mode' defaults to 'plain'. # 'mode' must be one of 'color', 'none', 'plain' or 'porcelain'. git diff --word-diff = 'porcelain' # Just show changes between two files. # DO NOT consider them part of of the repository. # This can be used to diff any two files. git diff --no-index 'path/to/file/A' 'path/to/file/B' Create a patch \u00b6 Just save the output from git diff to get a patch file: # Just the current changes. # No staged nor committed files. git diff > 'file.patch' # Staged files only. git diff --output 'file.patch' --cached The output from git diff just shows changes to text files by default, no metadata or other information about commits or branches. To get a whole commit with all its metadata and binary changes, use git format-patch : # Include 5 commits starting with 'commit' and going backwards. git format-patch -5 'commit_hash' # Include 3 commits starting from HEAD and save the patches in 'dir'. git format-patch 'HEAD~3' -o 'dir' # Include 2 commits from HEAD and save them as a single file. git format-patch 'HEAD~2' --stdout > 'single/file.patch' # Create a full patch of the unstaged changes. git add . && git commit -m 'uncommitted' \\ && git format-patch 'HEAD~1' && git reset 'HEAD~1' Apply a patch \u00b6 Use git apply to apply a patch file to the current index: git apply 'file.patch' The changes from the patch are unstaged and no commits are created. To apply all commits from a patch, use git am on a patch created with git format-patch : git am 'file.patch' The commits are applied one after the other and registered in the repository's logs. The stash stack \u00b6 The stash is a changelist separated from the one in the current working directory. git stash will save the current changes there and cleans the working directory. You can (re-)apply changes from the stash at any time: # Stash changes locally. git stash # Stash changes with a message. git stash save 'message' # List all the stashed changes. git stash list # Apply the most recent change and remove them from the stash stack. git stash pop # Apply a stash, but don't remove it from the stack. git stash apply stash@ { 6 } Branches \u00b6 Checkout an existing remote branch \u00b6 This creates a local branch tracking an existing remote branch. $ git checkout -b 'local-branch' 'remote/existing-branch' Branch 'local-branch' set up to track remote branch 'existing-branch' from 'remote' . Switched to a new branch 'local-branch' Delete a branch \u00b6 # Delete local branches. git branch --delete 'local-branch' git branch -D 'local-branch' # Delete remote branches. git push 'remote' ':feat-branch' git push 'remote' --delete 'feat-branch' # Delete both local and remote branches. git branch --delete --remotes 'feat-branch' Delete branches which have been merged or are otherwise absent from a remote. \u00b6 Command source [here][prune local branches that do not exist on remote anymore]. # Branches merged on the remote are tagged as 'gone' in `git branch -vv`'s output. git fetch -p \\ && awk '/origin/&&/gone/{print $1}' < ( git branch -vv ) | xargs git branch -d # Retain the current, 'master', 'main' and 'dev*' branches in all cases. git branch --merged | grep -vE '(^\\*|master|main|dev)' | xargs git branch -d Merge the master branch into a feature branch \u00b6 git stash pull git checkout 'master' git pull git checkout 'feature' git pull git merge --no-ff 'master' git stash pop git checkout 'feature' git pull 'origin' 'master' Rebase a branch on top of another \u00b6 git rebase takes the commits in a branch and appends them on top of the commits in a different branch. The commits to rebase are previously saved into a temporary area and then reapplied to the new branch, one by one, in order. # Rebase main on top of the current branch. git rebase 'main' # Rebase an upstream branch on top of a local branch. git rebase 'remote/upstream-branch' 'local-branch' # Rebase the current branch onto the *upstream* 'master' branch. git pull --rebase = 'interactive' 'origin' 'master' Tags \u00b6 Annotated tags are stored as full objects in git's database: # Create annotated tags. git tag --annotate 'v0.1.0' # Create and sign annotated tags. git tag -as 'v1.2.0-r0' -m \"signed annotated tag for v1.2.0 release 0\" # Tag specific commits. git tag -a '1.1.9' '9fceb02' # Push all annotated tags only. git push --follow-tags while lightweight tags are stored as a pointer to a specific commit: # Create lightweight tags. git tag 'v0.1.1-rc0' git tag '1.12.1' 'HEAD' Type-generic tag operations: # Push specific tags. git push 'origin' 'v1.5' # Push all tags git push --tags # Delete specific local tags only. git tag -d 'v1.4-lw' # Delete specific remote tags only. git push 'origin' --delete 'v1.4-lw' Convert a normal repository to a bare one \u00b6 The preferred method is to create a bare clone of the normal repository: git clone --bare 'repository' 'repository.git' LFS extension \u00b6 Install the extension: apt install 'git-lfs' brew install 'git-lfs' dnf install 'git-lfs' pacman -S 'git-lfs' If the package manager did not enable it system-wide, enable the extension for your user account: git lfs install Without any options, this will only setup the \"lfs\" smudge and clean filters if they are not already set. Configure file tracking from inside the repository: git lfs track \"*.exe\" git lfs track \"enormous_file.*\" Add the .gitattributes file to the traced files: git add '.gitattributes' git commit -m \"lfs configured\" Submodules \u00b6 See Git Submodules: Adding, Using, Removing, Updating for more information. # Add a submodule to an existing repository. git submodule add 'https://github.com/ohmyzsh/ohmyzsh' 'lib/ohmyzsh' # Clone a repository which has submodules. git clone --recursive 'keybase://public/bananas/dotfiles' git clone --recurse-submodules 'ohmyzsh' 'keybase://public/bananas/dotfiles' # Update an existing repository which has submodules. git pull --recurse-submodules To delete a submodule the procedure is more complicated: De-init the submodule: git submodule deinit 'lib/ohmyzsh' This wil also remove its entry from $REPO_ROOT/.git/config . Remove the submodule from the repository's index: git rm -rf 'lib/ohmyzsh' This wil also remove its entry from $REPO_ROOT/.gitmodules . Commit the changes. Remove a file from a commit \u00b6 See remove files from git commit . Remove a file from the repository \u00b6 Unstage the file using git reset ; specify HEAD as the source: git reset HEAD 'secret-file' Remove the file from the repository's index: git rm --cached 'secret-file' Check the file is no longer in the index: $ git ls-files | grep 'secret-file' $ Add the file to .gitignore or remove it from the working directory. Amend the most recent commit from your repository: git commit --amend Troubleshooting \u00b6 Debug \u00b6 When everything else fails, enable tracing: export GIT_TRACE = 1 GPG cannot sign a commit \u00b6 error: gpg failed to sign the data fatal: failed to write commit object If gnupg2 and gpg-agent 2.x are used, be sure to set the environment variable GPG_TTY , specially zsh users using Powerlevel10k with Instant Prompt enabled. export GPG_TTY = $( tty ) Git does not accept self-signed certificates \u00b6 Disable certificate verification: export GIT_SSL_NO_VERIFY = true git -c http.sslVerify = false \u2026 Further readings \u00b6 The official LFS website Git docs Tagging Getting Git on a Server git-config reference Sources \u00b6 How to get the current branch name in Git? Git Submodules: Adding, Using, Removing, Updating How to add and update git submodules Is there a way to make git pull automatically update submodules? How to change a git remote Why can't I delete a branch in a remote GitLab repository? How to Delete a Git Branch Both Locally and Remotely gpg failed to sign the data fatal: failed to write commit object Able to push to all git remotes with the one command? Create a git patch from the uncommitted changes in the current working directory Is there a way to gpg sign all previous commits? 10 Git tips we can't live without Coloring white space in git-diff's output Multiple git configuration How to improve git's diff highlighting? Get the repository's root directory How do I check out a remote Git branch on StackOverflow How to manage your secrets with git-crypt Question about how to rebase a local branch with remote master Question about how to merge master into a feature branch Question about how to [prune local branches that do not exist on remote anymore] Question about how to rebase remote branches Quick guide about git rebase Quick guide about how to remove files from git commit One weird trick for powerful Git aliases Cannot clone git from Azure DevOps using PAT","title":"Git"},{"location":"learning/tools/git/#git","text":"TL;DR Authentication Configuration Remotes Push to multiple git remotes with the one command Aliases Manage changes Create a patch Apply a patch The stash stack Branches Checkout an existing remote branch Delete a branch Delete branches which have been merged or are otherwise absent from a remote. Merge the master branch into a feature branch Rebase a branch on top of another Tags Convert a normal repository to a bare one LFS extension Submodules Remove a file from a commit Remove a file from the repository Troubleshooting Debug GPG cannot sign a commit Git does not accept self-signed certificates Further readings Sources","title":"Git"},{"location":"learning/tools/git/#tldr","text":"# Set your identity. git config 'user.name' 'User Name' git config --global 'user.email' 'user@email.com' # Avoid issues when collaborating from different platforms. git config --local 'core.autocrlf' 'input' git config --local 'core.autocrlf' 'true' # Create aliases. git config --local 'alias.co' 'checkout' git config --global 'alias.unstage' 'reset HEAD --' git config 'alias.funct' '!f() { sh_command ; sh_command | sh_command ; } && f' # Show git's configuration. git config --list git config --list --show-scope git config --list --show-origin # Render all current settings' values. git config --list \\ | awk -F '=' '{print $1}' | sort -u \\ | xargs -I {} sh -c 'printf \"{}=\" && git config --get {}' # Get a default value if the setting has none. # Does not work on sections alone. git config --get --default 'not-set' 'filter.lfs.cleaned' # Create or reinitialize a repository. git init git init --initial-branch 'main' 'path/to/repo' git init --bare 'path/to/repo.git' # Clone a repository. git clone 'https://github.com:user/repo.git' git clone --bare 'git@github.com:user/repo.git' 'path/to/clone' git clone --recurse-submodules 'ssh@git.server:user/repo.git' git clone --depth 1 'ssh@git.server:user/repo.git' git clone 'https://token@github.com/user/repo' git \\ -c http.extraHeader = \"Authorization: Basic $( echo -n \"user:pat\" | base64 ) \" \\ clone 'https://dev.azure.com/org/project/_git/repo' # Convert a normal repository to a bare one. git clone --bare 'repository' 'path/to/bare/clone.git' # Unshallow a clone. git pull --unshallow # Get objects and refs but do not incorporate them. git fetch # Get changes and merge them. git pull --all git pull --verify-signatures git pull 'remote' 'branch' # Show what files changed. git status git status --verbose # Show changes in a repository. git diff git diff --staged 'commit_hash' git diff 'commit_hash_1..commit_hash_2' git diff 'branch_1' 'branch_2' git diff --word-diff = 'color' git log -p 'feature' --not 'master' # Just show changes between two files. git diff --no-index 'path/to/file/a' 'path/to/file/b' # Stage changes for commit. git add . git add --all git add 'path/to/file' # Interactively review chunks of changes. git add --patch 'path/to/file' # Commit changes. git commit --message 'message' git commit --message 'whatever' --gpg-sign git commit --allow-empty --allow-empty-message git commit --date = 'Jun 13 18:30:25 IST 2015' git commit --date = \" $( date --date = '2 days ago' ) \" # Edit the last commit's message. git commit --amend git commit --amend --message 'message' # Change the last commit's author. git config 'user.name' 'user name' git config 'user.email' 'user.email@mail.com' git commit --amend --reset-author # Sign the last commit. git commit --amend --no-edit --gpg-sign # Show commits which would be pushed. git log @ { u } .. # Revert a commit but keep the history of the event as a separate commit. git revert 'commit_hash' # Interactively rebase the last 7 commits. git rebase -i '@~7' # List remotes. git remote --verbose # Add a new remote. git remote add 'gitlab' 'git@gitlab.com:user/repo.git' # Set a new URL for an existing remote. git remote set-url 'gitlab' 'git@gitlab.com:user/repo.git' # Push committed changes. git push git push 'remote' 'branch_1' 'branch_N' git push 'git@github.com:user/repo.git' git push --all --force # Show the repository's history. git reflog git log -p # Visualize the repository's history. git log --graph --full-history --all --color --decorate --oneline # Show and verify signatures. git log --show-signature -1 # Remove staged and working directory changes. git reset --hard git reset --hard 'origin/main' # Go back 4 commits. git reset --hard 'HEAD~4' # Remove untracked files. git clean -f -d # Remove ignored files. git clean -f -d -x # Show who committed which line. git blame 'path/to/file' # List changed files in a given commit. git diff-tree --no-commit-id --name-only -r 'commit_hash' # Create patches. git diff > 'file.patch' git diff --output 'file.patch' --cached git format-patch -5 'commit_hash' git format-patch 'HEAD~3' -o 'dir' git format-patch 'HEAD~2' --stdout > 'single/file.patch' # Create a full patch of the unstaged changes. git add . && git commit -m 'uncommitted' \\ && git format-patch 'HEAD~1' && git reset 'HEAD~1' # Apply a patch to the current index. git apply 'file.patch' # Apply commits from a patch. git am 'file.patch' # Stash changes locally. git stash git stash push 'message' # List all the stashed changes. git stash list # Apply the most recent change and remove them from the stash stack. git stash pop # Apply a stash, but don't remove it from the stack. git stash apply stash@ { 6 } # Remove a single stash entry from the stash stack. # Defaults to the current one. git stash drop git stash drop stash@ { 2 } # Remove all the stash entries. # Those will then be pruned and may be impossible to recover. git stash clear # Apply only the changes made within a given commit. git cherry-pick 'commit_hash' # Create a branch. git branch 'branch_name' git switch -c 'branch_name' git checkout -b 'local_branch_name' 'remote/branch_name' # Create a bare branch without any commits. git checkout --orphan 'branch_name' # List branches. git branch -a # Rename a branch. git branch --move 'old_name' 'new_name' # Switch branches. git switch 'branch_name' git checkout 'branch_name' git checkout - # Set an existing branch to track a remote branch. git branch -u 'remote_name/upstream-branch' # Get the current branch. git branch --show-current # git > v2.22 git rev-parse --abbrev-ref 'HEAD' # Delete local branches. git branch --delete 'branch_name' git branch -D 'branch_name' # Delete remote branches. git push 'remote_name' ':branch_name' git push 'remote_name' --delete 'branch_name' # Delete both local and remote branches. git branch --delete --remotes 'branch_name' # Sync the local branch list. git fetch --prune # Remove all stale branches. git remote prune 'branch_name' # Delete branches which have been merged or are otherwise absent from a remote. git branch --merged | grep -vE '(^\\*|master|main|dev)' | xargs git branch -d git fetch -p \\ && awk '/origin/&&/gone/{print $1}' < ( git branch -vv ) | xargs git branch -d # List all tags. git tag # Create annotated tags. git tag --annotate 'v0.1.0' git tag -as 'v1.2.0-r0' -m 'signed annotated tag for v1.2.0 release 0' git tag -a '1.1.9' '9fceb02' # Create lightweight tags. git tag 'v0.1.1-rc0' git tag '1.12.1' 'HEAD' # Push specific tags. git push 'remote_name' 'v1.5' # Push annotated tags only. git push --follow-tags # Push all tags. git push --tags # Delete local tags. git tag -d 'v1.4-lw' # Delete remote tags. git push 'remote_name' --delete 'v1.4-lw' # Sync the local tags list. git fetch --prune-tags # Rebase a branch on top of another. git rebase 'branch_name' git rebase 'remote_name/upstream_branch_name' 'local-branch_name' git pull --rebase = 'interactive' 'remote_name' 'branch_name' # Change the date of an existing commit. git filter-branch --env-filter \\ 'if [ $GIT_COMMIT = 119f9ecf58069b265ab22f1f97d2b648faf932e0 ] then export GIT_AUTHOR_DATE=\"Fri Jan 2 21:38:53 2009 -0800\" export GIT_COMMITTER_DATE=\"Sat May 19 01:01:01 2007 -0700\" fi' # Sign all commits from now on. git config --global 'user.signingKey' 'KEY_ID_IN_SHORT_FORMAT' git config --local 'commit.gpgSign' true # Import commits from another repo. git --git-dir = 'path/to/other-repo/.git' format-patch -k -1 --stdout 'commit_hash' \\ | git am -3 -k # Get the top-level directory of the current repository. git rev-parse --show-toplevel # Update all submodules. git submodule update --init --recursive # Show the first commit that has the string \"cool\" in its message body. git show :/cool","title":"TL;DR"},{"location":"learning/tools/git/#authentication","text":"# Use credentials in the URL. git clone 'https://username:password@host/path/to/repo' git clone 'https://token@github.com/user/repo' # Use headers. BASIC_AUTH = 'username:password' # or 'username:token', or ':token' BASIC_AUTH_B64 = \" $( printf \" $BASIC_AUTH \" | base64 ) \" git \\ -c http.extraHeader = \"Authorization: Basic ${ BASIC_AUTH_B64 } \" clone 'https://dev.azure.com/organizationName/projectName/_git/repoName'","title":"Authentication"},{"location":"learning/tools/git/#configuration","text":"# Required to be able to commit changes. git config --local 'user.email' 'me@me.info' git config --local 'user.name' 'Me' # Avoid issues when collaborating from different platforms. # 'input' on unix, 'true' on windows, 'false' only if you know what you are doing. git config --local 'core.autocrlf' 'input' # Sign commits by default. # Get the GPG key short ID with `gpg --list-keys --keyid-format short`. git config --local 'user.signingKey' 'KEY_ID_IN_SHORT_FORMAT' git config --local 'commit.gpgSign' true # Pull submodules by default. git config --global 'submodule.recurse' true # Use a Personal Access Token to authenticate. git config http.extraHeader = \"Authorization: Basic $( echo -n 'user:pat' | base64 ) \" To show the current configuration use the --list option: git config --list git config --list --show-scope git config --list --global --show-origin The configuration is shown in full for the requested scope (or all if not specified), but it might include the same setting multiple times if it shows up in multiple scopes. Render the current value of a setting using the --get option: # Get the current user.name value. git config --get 'user.name' # Render all current settings' values. # Gets the settings names, then requests the current value for each. git config --list \\ | awk -F '=' '{print $1}' | sort -u \\ | xargs -I {} sh -c 'printf \"{}=\" && git config --get {}'","title":"Configuration"},{"location":"learning/tools/git/#remotes","text":"# Add a remote. git remote add 'gitlab' 'git@gitlab.com:user/my-awesome-repo.git' # Add other push URLs to an existing remote. git remote set-url --push --add 'origin' 'https://exampleuser@example.com/path/to/repo1' # Change a remote's URL. git remote set-url 'origin' 'git@github.com:user/new-repo-name.git'","title":"Remotes"},{"location":"learning/tools/git/#push-to-multiple-git-remotes-with-the-one-command","text":"To always push to repo1 , repo2 , and repo3 , but always pull only from repo1 , set up the remote 'origin' as follows: git remote add origin https://exampleuser@example.com/path/to/repo1 git remote set-url --push --add origin https://exampleuser@example.com/path/to/repo1 git remote set-url --push --add origin https://exampleuser@example.com/path/to/repo2 git remote set-url --push --add origin https://exampleuser@example.com/path/to/repo3 [remote \"origin\"] url = https://exampleuser@example.com/path/to/repo1 pushUrl = https://exampleuser@example.com/path/to/repo1 pushUrl = https://exampleuser@example.com/path/to/repo2 pushUrl = https://exampleuser@example.com/path/to/repo3 fetch = +refs/heads/*:refs/remotes/origin/* To only pull from repo1 but push to repo1 and repo2 for a specific branch specialBranch : [remote \"origin\"] url = ssh://git@aaa.xxx.com:7999/yyy/repo1.git fetch = +refs/heads/*:refs/remotes/origin/* \u2026 [remote \"specialRemote\"] url = ssh://git@aaa.xxx.com:7999/yyy/repo1.git pushUrl = ssh://git@aaa.xxx.com:7999/yyy/repo1.git pushUrl = ssh://git@aaa.xxx.com:7999/yyy/repo2.git fetch = +refs/heads/*:refs/remotes/origin/* \u2026 [branch \"specialBranch\"] remote = origin pushRemote = specialRemote \u2026 See https://git-scm.com/docs/git-config#git-config-branchltnamegtremote .","title":"Push to multiple git remotes with the one command"},{"location":"learning/tools/git/#aliases","text":"Simple aliases to git commands can be added like aliases to a shell: [alias] caa = commit -a --amend -C HEAD ls = log --oneline statsu = status But simple aliases have limitations: they can't have parameters you can't execute multiple git commands in a single alias you can't use | (pipes) or grep git allows you to escape to a shell using ! (bang); this opens a new world of possibilities for aliases: use shell expansions and parameters use multiple git commands use pipes and all command line tools Those commands need to be wrapped into a one-line function definition: [alias] new = !sh -c 'git log $1@{1}..$1@{0} \"$@\"' pull-from-all = \"!f() { \\ git remote show \\ | xargs -I{} -P0 -n1 git pull {} ${1-$(git branch --show-current)} \\ ; } && f\" subtree-add = \"!f() { git subtree add --prefix $2 $1 master --squash ; } ; f\"","title":"Aliases"},{"location":"learning/tools/git/#manage-changes","text":"# Show changes relative to the current index (not yet staged). git diff # Show changes in the staged files only. git diff --staged # Show changes relative to 'commit' (defaults to HEAD if not given). # Alias of `--staged`. git diff --cached 'commit_hash' # Show changes relative to a different branch. git diff 'branch_name' # Show changes between commits. # Separating the commits with `..` is optional. git diff 'commit_hash_1' 'commit_hash_2' git diff 'commit_hash_1..commit_hash_2' # Show changes between branches. # Separating the branches with `..` is optional. git diff 'branch_name_1' 'branch_name_2' git diff 'branch_name_1..branch_name_2' # Show a word diff using 'mode' to delimit changed words for emphasis. # 'mode' defaults to 'plain'. # 'mode' must be one of 'color', 'none', 'plain' or 'porcelain'. git diff --word-diff = 'porcelain' # Just show changes between two files. # DO NOT consider them part of of the repository. # This can be used to diff any two files. git diff --no-index 'path/to/file/A' 'path/to/file/B'","title":"Manage changes"},{"location":"learning/tools/git/#create-a-patch","text":"Just save the output from git diff to get a patch file: # Just the current changes. # No staged nor committed files. git diff > 'file.patch' # Staged files only. git diff --output 'file.patch' --cached The output from git diff just shows changes to text files by default, no metadata or other information about commits or branches. To get a whole commit with all its metadata and binary changes, use git format-patch : # Include 5 commits starting with 'commit' and going backwards. git format-patch -5 'commit_hash' # Include 3 commits starting from HEAD and save the patches in 'dir'. git format-patch 'HEAD~3' -o 'dir' # Include 2 commits from HEAD and save them as a single file. git format-patch 'HEAD~2' --stdout > 'single/file.patch' # Create a full patch of the unstaged changes. git add . && git commit -m 'uncommitted' \\ && git format-patch 'HEAD~1' && git reset 'HEAD~1'","title":"Create a patch"},{"location":"learning/tools/git/#apply-a-patch","text":"Use git apply to apply a patch file to the current index: git apply 'file.patch' The changes from the patch are unstaged and no commits are created. To apply all commits from a patch, use git am on a patch created with git format-patch : git am 'file.patch' The commits are applied one after the other and registered in the repository's logs.","title":"Apply a patch"},{"location":"learning/tools/git/#the-stash-stack","text":"The stash is a changelist separated from the one in the current working directory. git stash will save the current changes there and cleans the working directory. You can (re-)apply changes from the stash at any time: # Stash changes locally. git stash # Stash changes with a message. git stash save 'message' # List all the stashed changes. git stash list # Apply the most recent change and remove them from the stash stack. git stash pop # Apply a stash, but don't remove it from the stack. git stash apply stash@ { 6 }","title":"The stash stack"},{"location":"learning/tools/git/#branches","text":"","title":"Branches"},{"location":"learning/tools/git/#checkout-an-existing-remote-branch","text":"This creates a local branch tracking an existing remote branch. $ git checkout -b 'local-branch' 'remote/existing-branch' Branch 'local-branch' set up to track remote branch 'existing-branch' from 'remote' . Switched to a new branch 'local-branch'","title":"Checkout an existing remote branch"},{"location":"learning/tools/git/#delete-a-branch","text":"# Delete local branches. git branch --delete 'local-branch' git branch -D 'local-branch' # Delete remote branches. git push 'remote' ':feat-branch' git push 'remote' --delete 'feat-branch' # Delete both local and remote branches. git branch --delete --remotes 'feat-branch'","title":"Delete a branch"},{"location":"learning/tools/git/#delete-branches-which-have-been-merged-or-are-otherwise-absent-from-a-remote","text":"Command source [here][prune local branches that do not exist on remote anymore]. # Branches merged on the remote are tagged as 'gone' in `git branch -vv`'s output. git fetch -p \\ && awk '/origin/&&/gone/{print $1}' < ( git branch -vv ) | xargs git branch -d # Retain the current, 'master', 'main' and 'dev*' branches in all cases. git branch --merged | grep -vE '(^\\*|master|main|dev)' | xargs git branch -d","title":"Delete branches which have been merged or are otherwise absent from a remote."},{"location":"learning/tools/git/#merge-the-master-branch-into-a-feature-branch","text":"git stash pull git checkout 'master' git pull git checkout 'feature' git pull git merge --no-ff 'master' git stash pop git checkout 'feature' git pull 'origin' 'master'","title":"Merge the master branch into a feature branch"},{"location":"learning/tools/git/#rebase-a-branch-on-top-of-another","text":"git rebase takes the commits in a branch and appends them on top of the commits in a different branch. The commits to rebase are previously saved into a temporary area and then reapplied to the new branch, one by one, in order. # Rebase main on top of the current branch. git rebase 'main' # Rebase an upstream branch on top of a local branch. git rebase 'remote/upstream-branch' 'local-branch' # Rebase the current branch onto the *upstream* 'master' branch. git pull --rebase = 'interactive' 'origin' 'master'","title":"Rebase a branch on top of another"},{"location":"learning/tools/git/#tags","text":"Annotated tags are stored as full objects in git's database: # Create annotated tags. git tag --annotate 'v0.1.0' # Create and sign annotated tags. git tag -as 'v1.2.0-r0' -m \"signed annotated tag for v1.2.0 release 0\" # Tag specific commits. git tag -a '1.1.9' '9fceb02' # Push all annotated tags only. git push --follow-tags while lightweight tags are stored as a pointer to a specific commit: # Create lightweight tags. git tag 'v0.1.1-rc0' git tag '1.12.1' 'HEAD' Type-generic tag operations: # Push specific tags. git push 'origin' 'v1.5' # Push all tags git push --tags # Delete specific local tags only. git tag -d 'v1.4-lw' # Delete specific remote tags only. git push 'origin' --delete 'v1.4-lw'","title":"Tags"},{"location":"learning/tools/git/#convert-a-normal-repository-to-a-bare-one","text":"The preferred method is to create a bare clone of the normal repository: git clone --bare 'repository' 'repository.git'","title":"Convert a normal repository to a bare one"},{"location":"learning/tools/git/#lfs-extension","text":"Install the extension: apt install 'git-lfs' brew install 'git-lfs' dnf install 'git-lfs' pacman -S 'git-lfs' If the package manager did not enable it system-wide, enable the extension for your user account: git lfs install Without any options, this will only setup the \"lfs\" smudge and clean filters if they are not already set. Configure file tracking from inside the repository: git lfs track \"*.exe\" git lfs track \"enormous_file.*\" Add the .gitattributes file to the traced files: git add '.gitattributes' git commit -m \"lfs configured\"","title":"LFS extension"},{"location":"learning/tools/git/#submodules","text":"See Git Submodules: Adding, Using, Removing, Updating for more information. # Add a submodule to an existing repository. git submodule add 'https://github.com/ohmyzsh/ohmyzsh' 'lib/ohmyzsh' # Clone a repository which has submodules. git clone --recursive 'keybase://public/bananas/dotfiles' git clone --recurse-submodules 'ohmyzsh' 'keybase://public/bananas/dotfiles' # Update an existing repository which has submodules. git pull --recurse-submodules To delete a submodule the procedure is more complicated: De-init the submodule: git submodule deinit 'lib/ohmyzsh' This wil also remove its entry from $REPO_ROOT/.git/config . Remove the submodule from the repository's index: git rm -rf 'lib/ohmyzsh' This wil also remove its entry from $REPO_ROOT/.gitmodules . Commit the changes.","title":"Submodules"},{"location":"learning/tools/git/#remove-a-file-from-a-commit","text":"See remove files from git commit .","title":"Remove a file from a commit"},{"location":"learning/tools/git/#remove-a-file-from-the-repository","text":"Unstage the file using git reset ; specify HEAD as the source: git reset HEAD 'secret-file' Remove the file from the repository's index: git rm --cached 'secret-file' Check the file is no longer in the index: $ git ls-files | grep 'secret-file' $ Add the file to .gitignore or remove it from the working directory. Amend the most recent commit from your repository: git commit --amend","title":"Remove a file from the repository"},{"location":"learning/tools/git/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/git/#debug","text":"When everything else fails, enable tracing: export GIT_TRACE = 1","title":"Debug"},{"location":"learning/tools/git/#gpg-cannot-sign-a-commit","text":"error: gpg failed to sign the data fatal: failed to write commit object If gnupg2 and gpg-agent 2.x are used, be sure to set the environment variable GPG_TTY , specially zsh users using Powerlevel10k with Instant Prompt enabled. export GPG_TTY = $( tty )","title":"GPG cannot sign a commit"},{"location":"learning/tools/git/#git-does-not-accept-self-signed-certificates","text":"Disable certificate verification: export GIT_SSL_NO_VERIFY = true git -c http.sslVerify = false \u2026","title":"Git does not accept self-signed certificates"},{"location":"learning/tools/git/#further-readings","text":"The official LFS website Git docs Tagging Getting Git on a Server git-config reference","title":"Further readings"},{"location":"learning/tools/git/#sources","text":"How to get the current branch name in Git? Git Submodules: Adding, Using, Removing, Updating How to add and update git submodules Is there a way to make git pull automatically update submodules? How to change a git remote Why can't I delete a branch in a remote GitLab repository? How to Delete a Git Branch Both Locally and Remotely gpg failed to sign the data fatal: failed to write commit object Able to push to all git remotes with the one command? Create a git patch from the uncommitted changes in the current working directory Is there a way to gpg sign all previous commits? 10 Git tips we can't live without Coloring white space in git-diff's output Multiple git configuration How to improve git's diff highlighting? Get the repository's root directory How do I check out a remote Git branch on StackOverflow How to manage your secrets with git-crypt Question about how to rebase a local branch with remote master Question about how to merge master into a feature branch Question about how to [prune local branches that do not exist on remote anymore] Question about how to rebase remote branches Quick guide about git rebase Quick guide about how to remove files from git commit One weird trick for powerful Git aliases Cannot clone git from Azure DevOps using PAT","title":"Sources"},{"location":"learning/tools/gnome%20shell/","text":"Gnome shell \u00b6 Restart while running \u00b6 This helps reloading extensions. press Alt + F2 insert r and press Enter","title":"Gnome shell"},{"location":"learning/tools/gnome%20shell/#gnome-shell","text":"","title":"Gnome shell"},{"location":"learning/tools/gnome%20shell/#restart-while-running","text":"This helps reloading extensions. press Alt + F2 insert r and press Enter","title":"Restart while running"},{"location":"learning/tools/google%20chrome/","text":"Google Chrome \u00b6 Troubleshooting \u00b6 No \"Proceed Anyway\" option on NET::ERR_CERT_INVALID in Chrome on MacOS \u00b6 See No \"Proceed Anyway\" option on NET::ERR_CERT_INVALID in Chrome on MacOS for more information. There's a secret passphrase built into the error page. Just make sure the page is selected (click anywhere on the screen), then just type thisisunsafe and wait for the page to reload. Sources \u00b6 No \"Proceed Anyway\" option on NET::ERR_CERT_INVALID in Chrome on MacOS","title":"Google Chrome"},{"location":"learning/tools/google%20chrome/#google-chrome","text":"","title":"Google Chrome"},{"location":"learning/tools/google%20chrome/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/google%20chrome/#no-proceed-anyway-option-on-neterr_cert_invalid-in-chrome-on-macos","text":"See No \"Proceed Anyway\" option on NET::ERR_CERT_INVALID in Chrome on MacOS for more information. There's a secret passphrase built into the error page. Just make sure the page is selected (click anywhere on the screen), then just type thisisunsafe and wait for the page to reload.","title":"No \"Proceed Anyway\" option on NET::ERR_CERT_INVALID in Chrome on MacOS"},{"location":"learning/tools/google%20chrome/#sources","text":"No \"Proceed Anyway\" option on NET::ERR_CERT_INVALID in Chrome on MacOS","title":"Sources"},{"location":"learning/tools/google%20search/","text":"Google Search \u00b6 TL;DR \u00b6 find an exact phrase enclosing it in quotation marks ( \"\" ); it will give back only results that include those same words in that same order: mama \"i just killed a man\" exclude a word from a search preceding it with a minus sign ( - ); it will filter out unwanted results: how to scrub -tips find results that include all of the words in a phrase using the asterisk symbol ( * ); it will give back results that include a specific phrase but may also have other words: how to clean * from sheets get results from a particular website only using the site: operator: site:wikipedia.org obama use the operator alone to determine whether Google has indexed a website: site:heyitsa.me Sources \u00b6 20 Googling Tricks 99% of People Don't Know About","title":"Google Search"},{"location":"learning/tools/google%20search/#google-search","text":"","title":"Google Search"},{"location":"learning/tools/google%20search/#tldr","text":"find an exact phrase enclosing it in quotation marks ( \"\" ); it will give back only results that include those same words in that same order: mama \"i just killed a man\" exclude a word from a search preceding it with a minus sign ( - ); it will filter out unwanted results: how to scrub -tips find results that include all of the words in a phrase using the asterisk symbol ( * ); it will give back results that include a specific phrase but may also have other words: how to clean * from sheets get results from a particular website only using the site: operator: site:wikipedia.org obama use the operator alone to determine whether Google has indexed a website: site:heyitsa.me","title":"TL;DR"},{"location":"learning/tools/google%20search/#sources","text":"20 Googling Tricks 99% of People Don't Know About","title":"Sources"},{"location":"learning/tools/gopass/","text":"Gopass \u00b6 TL;DR \u00b6 gopass init # multistore init gopass init --store private --path ~/.password-store.private gopass init --store work --path ~/.password-store.work Browsers integration \u00b6 Browserpass \u00b6 brew tap amar1729/formulae brew install browserpass for b in chromium chrome vivaldi brave firefox ; do PREFIX = '/usr/local/opt/browserpass' make hosts-chrome-user -f /usr/local/opt/browserpass/lib/browserpass/Makefile done Further readings \u00b6 GoPass features BrowserPass extension installation guide","title":"Gopass"},{"location":"learning/tools/gopass/#gopass","text":"","title":"Gopass"},{"location":"learning/tools/gopass/#tldr","text":"gopass init # multistore init gopass init --store private --path ~/.password-store.private gopass init --store work --path ~/.password-store.work","title":"TL;DR"},{"location":"learning/tools/gopass/#browsers-integration","text":"","title":"Browsers integration"},{"location":"learning/tools/gopass/#browserpass","text":"brew tap amar1729/formulae brew install browserpass for b in chromium chrome vivaldi brave firefox ; do PREFIX = '/usr/local/opt/browserpass' make hosts-chrome-user -f /usr/local/opt/browserpass/lib/browserpass/Makefile done","title":"Browserpass"},{"location":"learning/tools/gopass/#further-readings","text":"GoPass features BrowserPass extension installation guide","title":"Further readings"},{"location":"learning/tools/gpg/","text":"GnuPG \u00b6 TL;DR \u00b6 # List existing keys. gpg --list-keys gpg --list-keys --keyid-format 'short' gpg --list-secret-keys --with-keygrip # Generate a new key. gpg --gen-key gpg --generate-key gpg --full-generate-key gpg --expert --full-generate-key # Generate a new key unattended. # The non-interactive (--batch) option requires a settings file. gpg --generate-key --batch setting.txt gpg --generate-key --batch <<-EOF \u2026 EOF # Delete a key from the keyring. # The non-interactive (--batch) option requires the key fingerprint. gpg --delete-secret-key 'recipient' gpg --delete-key 'recipient' gpg --delete-keys --batch 'key_fingerprint' # Get a key's fingerprint information. gpg --fingerprint gpg --fingerprint 'recipient' # Encrypt files. gpg -e -o 'file.out.gpg' -r 'recipient' 'file.in' gpg --encrypt -o 'file.out.gpg' -u 'sender' -r 'recipient' 'file.in' gpg --encrypt-files --batch -r 'recipient' 'file.in.1' 'file.in.N' gpg -e --multifile --batch -r 'recipient' --yes 'file.in.1' 'file.in.N' # Decrypt files. gpg -d -o 'file.out' 'file.in.gpg' gpg --decrypt-files --batch 'file.in.gpg.1' 'file.in.gpg.N' gpg -d --multifile --batch --yes 'file.in.gpg.1' 'file.in.gpg.N' # Import keys from a file. gpg --import 'keys.asc' # Export keys to a file. gpg --armor --export > 'all.public-keys.asc' gpg --armor --export recipient > 'recipient.public-keys.asc' gpg --armor --export-secret-keys > 'all.private-keys.asc' gpg --armor --export-secret-keys recipient > 'recipient.private-keys.asc' # Generate a revoke certificate. gpg --gen-revoke # Get the short ID of the signing key only for a user. # Primarily usable for git's signingKey configuration. gpg --list-keys --keyid-format 'short' 'recipient' \\ | grep --extended-regexp '^pub[[:blank:]]+[[:alnum:]]+/[[:alnum:]]+[[:blank:]].*\\[[[:upper:]]*S[[:upper:]]*\\]' \\ | awk '{print $2}' \\ | cut -d '/' -f 2 # Install on Mac OS X. # Choose one. brew install --cask 'gpg-suite-no-mail' brew install 'gnupg' Encryption \u00b6 # Single file. gpg --output 'file.out.gpg' --encrypt --recipient 'recipient' 'file.in' gpg --armor --symmetric --output 'file.out.gpg' 'file.in' # All files found. find . -type 'f' -name 'secret.txt' \\ -exec gpg --batch --yes --encrypt-files --recipient 'recipient' {} ';' Decryption \u00b6 # Single file. gpg --output 'file.out' --decrypt 'file.in.gpg' # All files found. find . -type f -name \"*.gpg\" -exec gpg --decrypt-files {} + The second command will create the decrypted version of all files in the same directory. Each file will have the same name of the encrypted version, minus the .gpg extension. Key export \u00b6 As the original user, export all public keys to a base64-encoded text file and create an encrypted version of that file: # Export. gpg --armor --export > 'all.public-keys.asc' gpg --armor --export 'recipient' > 'recipient.public-keys.asc' # Encryption. gpg --output 'file.out.gpg' --encrypt --recipient 'recipient' 'file.in' gpg --armor --symmetric --output 'file.out.gpg' 'file.in' Export all encrypted private keys (which will also include corresponding public keys) to a text file and create an encrypted version of that file: # Export. gpg --armor --export-secret-keys > 'all.private-keys.asc' gpg --armor --export-secret-keys 'recipient' > 'recipient.private-keys.asc' # Encryption. gpg --output 'file.out.gpg' --encrypt --recipient 'recipient' 'file.in' gpg --armor --symmetric --output 'file.out.gpg' 'file.in' Optionally, also export gpg 's trustdb to a text file: gpg --export-ownertrust > 'otrust.txt' Key import \u00b6 As the new user execute gpg --import commands against the secured files, or the decrypted content of those files, and then check for the new keys with gpg -k and gpg -K , e.g.: gpg --output 'myprivatekeys.asc' --decrypt 'mysecretatedprivatekeys.sec.asc' && \\ gpg --import 'myprivatekeys.asc' gpg --output 'mypubkeys.asc' --decrypt 'mysecretatedpubkeys.sec.asc' gpg --import 'mypubkeys.asc' gpg --list-secret-keys gpg --list-keys Optionally import the trustdb file as well: gpg --import-ownertrust 'otrust.txt' Key trust \u00b6 $ gpg --edit-key 'key_fingerprint' gpg> trust gpg> quit Unattended key generation \u00b6 The non-interactive (\u2013batch) option requires a settings file. # basic key with default values gpg --batch --generate-key <<EOF %echo Generating a default key Key-Type: default Subkey-Type: default Name-Real: Joe Tester Name-Comment: with stupid passphrase Name-Email: joe@foo.bar Expire-Date: 0 Passphrase: abc # Do a commit here, so that we can later print \"done\" :-) %commit %echo done EOF Change a key's password \u00b6 $ gpg --edit-key 'key_fingerprint' gpg> passwd gpg> quit Put comments in a message or file \u00b6 One can put comments in an armored ASCII message or key block using the Comment keyword for each line: -----BEGIN PGP MESSAGE----- Comment: \u2026 Comment: \u2026 hQIMAwbYc\u2026 -----END PGP MESSAGE----- OpenPGP defines all text to be in UTF-8, so a comment may be any UTF-8 string. The whole point of armoring, however, is to provide seven-bit-clean data, so if a comment has characters that are outside the US-ASCII range of UTF they may very well not survive transport. Use a GPG key for SSH authentication \u00b6 Shamelessly copied over from How to enable SSH access using a GPG key for authentication . This exercise will use a GPG subkey with only the authentication capability enabled to complete SSH connections. You can create multiple subkeys as you would do for SSH keypairs. Create an authentication subkey \u00b6 You should already have a GPG key. If you don't, read one of the many fine tutorials available on this topic. You will create the subkey by editing your existing key in expert mode to get access to the appropriate options: $ gpg2 --expert --edit-key 'key_fingerprint' gpg> addkey Please select what kind of key you want: ( 3 ) DSA ( sign only ) ( 4 ) RSA ( sign only ) ( 5 ) Elgamal ( encrypt only ) ( 6 ) RSA ( encrypt only ) ( 7 ) DSA ( set your own capabilities ) ( 8 ) RSA ( set your own capabilities ) ( 10 ) ECC ( sign only ) ( 11 ) ECC ( set your own capabilities ) ( 12 ) ECC ( encrypt only ) ( 13 ) Existing key Your selection? 8 Possible actions for a RSA key: Sign Encrypt Authenticate Current allowed actions: Sign Encrypt ( S ) Toggle the sign capability ( E ) Toggle the encrypt capability ( A ) Toggle the authenticate capability ( Q ) Finished Your selection? s Your selection? e Your selection? a Possible actions for a RSA key: Sign Encrypt Authenticate Current allowed actions: Authenticate ( S ) Toggle the sign capability ( E ) Toggle the encrypt capability ( A ) Toggle the authenticate capability ( Q ) Finished Your selection? q RSA keys may be between 1024 and 4096 bits long. What keysize do you want? ( 4096 ) Requested keysize is 4096 bits Please specify how long the key should be valid. 0 = key does not expire <n> = key expires in n days <n>w = key expires in n weeks <n>m = key expires in n months <n>y = key expires in n years Key is valid for ? ( 0 ) Key does not expire at all Is this correct? ( y/N ) y Really create? ( y/N ) y sec rsa2048/8715AF32191DB135 created: 2019 -03-21 expires: 2021 -03-20 usage: SC trust: ultimate validity: ultimate ssb rsa2048/150F16909B9AA603 created: 2019 -03-21 expires: 2021 -03-20 usage: E ssb rsa2048/17E7403F18CB1123 created: 2019 -03-21 expires: never usage: A [ ultimate ] ( 1 ) . Brian Exelbierd gpg> quit Save changes? ( y/N ) y Enable SSH to use the GPG subkey \u00b6 When using SSH, ssh-agent is used to manage SSH keys. When using a GPG key, gpg-agent is used to manage GPG keys. To get gpg-agent to handle requests from SSH, you need to enable its SSH support: echo \"enable-ssh-support\" >> ~/.gnupg/gpg-agent.conf You can avoid usinig ssh-add to load the keys pre-specifying which GPG keys to use in the ~/.gnupg/sshcontrol file. The entries in this file are keygrips\u2014internal identifiers that gpg-agent uses to refer to the keys. A keygrip refers to both the public and private key. To find the keygrip use gpg -K --with-keygrip , then add that line to the ~/.gnupg/sshcontrol file: $ gpg2 -K --with-keygrip /home/bexelbie/.gnupg/pubring.kbx ------------------------------ sec rsa2048 2019 -03-21 [ SC ] [ expires: 2021 -03-20 ] 96F33EA7F4E0F7051D75FC208715AF32191DB135 Keygrip = 90E08830BC1AAD225E657AD4FBE638B3D8E50C9E uid [ ultimate ] Brian Exelbierd ssb rsa2048 2019 -03-21 [ E ] [ expires: 2021 -03-20 ] Keygrip = 5FA04ABEBFBC5089E50EDEB43198B4895BCA2136 ssb rsa2048 2019 -03-21 [ A ] Keygrip = 7710BA0643CC022B92544181FF2EAC2A290CDC0E $ echo 7710BA0643CC022B92544181FF2EAC2A290CDC0E >> ~/.gnupg/sshcontrol Now tell SSH how to access gpg-agent by setting the value of the SSH_AUTH_SOCK environment variable. export SSH_AUTH_SOCK = $( gpgconf --list-dirs agent-ssh-socket ) gpgconf --launch gpg-agent Share the GPG-SSH key \u00b6 Run ssh-add -L to list your public keys and copy them over manually to the remote host, or use ssh-copy-id as you would normally do. Troubleshooting \u00b6 gpg failed to sign the data; fatal: failed to write commit object \u00b6 Problem: git is instructed to sign a commit with gpg git commit fails with the following error: gpg failed to sign the data fatal: failed to write commit object Solution: if gnupg2 and gpg-agent 2.x are used, be sure to set the environment variable GPG_TTY : export GPG_TTY = $( tty ) Sources \u00b6 Decrypt multiple openpgp files in a directory ask redhat how can i remove the passphrase from a gpg2 private key? Unattended key generation How to enable SSH access using a GPG key for authentication gpg failed to sign the data fatal: failed to write commit object Can you manually add a comment to a PGP public key block and not break it?","title":"GnuPG"},{"location":"learning/tools/gpg/#gnupg","text":"","title":"GnuPG"},{"location":"learning/tools/gpg/#tldr","text":"# List existing keys. gpg --list-keys gpg --list-keys --keyid-format 'short' gpg --list-secret-keys --with-keygrip # Generate a new key. gpg --gen-key gpg --generate-key gpg --full-generate-key gpg --expert --full-generate-key # Generate a new key unattended. # The non-interactive (--batch) option requires a settings file. gpg --generate-key --batch setting.txt gpg --generate-key --batch <<-EOF \u2026 EOF # Delete a key from the keyring. # The non-interactive (--batch) option requires the key fingerprint. gpg --delete-secret-key 'recipient' gpg --delete-key 'recipient' gpg --delete-keys --batch 'key_fingerprint' # Get a key's fingerprint information. gpg --fingerprint gpg --fingerprint 'recipient' # Encrypt files. gpg -e -o 'file.out.gpg' -r 'recipient' 'file.in' gpg --encrypt -o 'file.out.gpg' -u 'sender' -r 'recipient' 'file.in' gpg --encrypt-files --batch -r 'recipient' 'file.in.1' 'file.in.N' gpg -e --multifile --batch -r 'recipient' --yes 'file.in.1' 'file.in.N' # Decrypt files. gpg -d -o 'file.out' 'file.in.gpg' gpg --decrypt-files --batch 'file.in.gpg.1' 'file.in.gpg.N' gpg -d --multifile --batch --yes 'file.in.gpg.1' 'file.in.gpg.N' # Import keys from a file. gpg --import 'keys.asc' # Export keys to a file. gpg --armor --export > 'all.public-keys.asc' gpg --armor --export recipient > 'recipient.public-keys.asc' gpg --armor --export-secret-keys > 'all.private-keys.asc' gpg --armor --export-secret-keys recipient > 'recipient.private-keys.asc' # Generate a revoke certificate. gpg --gen-revoke # Get the short ID of the signing key only for a user. # Primarily usable for git's signingKey configuration. gpg --list-keys --keyid-format 'short' 'recipient' \\ | grep --extended-regexp '^pub[[:blank:]]+[[:alnum:]]+/[[:alnum:]]+[[:blank:]].*\\[[[:upper:]]*S[[:upper:]]*\\]' \\ | awk '{print $2}' \\ | cut -d '/' -f 2 # Install on Mac OS X. # Choose one. brew install --cask 'gpg-suite-no-mail' brew install 'gnupg'","title":"TL;DR"},{"location":"learning/tools/gpg/#encryption","text":"# Single file. gpg --output 'file.out.gpg' --encrypt --recipient 'recipient' 'file.in' gpg --armor --symmetric --output 'file.out.gpg' 'file.in' # All files found. find . -type 'f' -name 'secret.txt' \\ -exec gpg --batch --yes --encrypt-files --recipient 'recipient' {} ';'","title":"Encryption"},{"location":"learning/tools/gpg/#decryption","text":"# Single file. gpg --output 'file.out' --decrypt 'file.in.gpg' # All files found. find . -type f -name \"*.gpg\" -exec gpg --decrypt-files {} + The second command will create the decrypted version of all files in the same directory. Each file will have the same name of the encrypted version, minus the .gpg extension.","title":"Decryption"},{"location":"learning/tools/gpg/#key-export","text":"As the original user, export all public keys to a base64-encoded text file and create an encrypted version of that file: # Export. gpg --armor --export > 'all.public-keys.asc' gpg --armor --export 'recipient' > 'recipient.public-keys.asc' # Encryption. gpg --output 'file.out.gpg' --encrypt --recipient 'recipient' 'file.in' gpg --armor --symmetric --output 'file.out.gpg' 'file.in' Export all encrypted private keys (which will also include corresponding public keys) to a text file and create an encrypted version of that file: # Export. gpg --armor --export-secret-keys > 'all.private-keys.asc' gpg --armor --export-secret-keys 'recipient' > 'recipient.private-keys.asc' # Encryption. gpg --output 'file.out.gpg' --encrypt --recipient 'recipient' 'file.in' gpg --armor --symmetric --output 'file.out.gpg' 'file.in' Optionally, also export gpg 's trustdb to a text file: gpg --export-ownertrust > 'otrust.txt'","title":"Key export"},{"location":"learning/tools/gpg/#key-import","text":"As the new user execute gpg --import commands against the secured files, or the decrypted content of those files, and then check for the new keys with gpg -k and gpg -K , e.g.: gpg --output 'myprivatekeys.asc' --decrypt 'mysecretatedprivatekeys.sec.asc' && \\ gpg --import 'myprivatekeys.asc' gpg --output 'mypubkeys.asc' --decrypt 'mysecretatedpubkeys.sec.asc' gpg --import 'mypubkeys.asc' gpg --list-secret-keys gpg --list-keys Optionally import the trustdb file as well: gpg --import-ownertrust 'otrust.txt'","title":"Key import"},{"location":"learning/tools/gpg/#key-trust","text":"$ gpg --edit-key 'key_fingerprint' gpg> trust gpg> quit","title":"Key trust"},{"location":"learning/tools/gpg/#unattended-key-generation","text":"The non-interactive (\u2013batch) option requires a settings file. # basic key with default values gpg --batch --generate-key <<EOF %echo Generating a default key Key-Type: default Subkey-Type: default Name-Real: Joe Tester Name-Comment: with stupid passphrase Name-Email: joe@foo.bar Expire-Date: 0 Passphrase: abc # Do a commit here, so that we can later print \"done\" :-) %commit %echo done EOF","title":"Unattended key generation"},{"location":"learning/tools/gpg/#change-a-keys-password","text":"$ gpg --edit-key 'key_fingerprint' gpg> passwd gpg> quit","title":"Change a key's password"},{"location":"learning/tools/gpg/#put-comments-in-a-message-or-file","text":"One can put comments in an armored ASCII message or key block using the Comment keyword for each line: -----BEGIN PGP MESSAGE----- Comment: \u2026 Comment: \u2026 hQIMAwbYc\u2026 -----END PGP MESSAGE----- OpenPGP defines all text to be in UTF-8, so a comment may be any UTF-8 string. The whole point of armoring, however, is to provide seven-bit-clean data, so if a comment has characters that are outside the US-ASCII range of UTF they may very well not survive transport.","title":"Put comments in a message or file"},{"location":"learning/tools/gpg/#use-a-gpg-key-for-ssh-authentication","text":"Shamelessly copied over from How to enable SSH access using a GPG key for authentication . This exercise will use a GPG subkey with only the authentication capability enabled to complete SSH connections. You can create multiple subkeys as you would do for SSH keypairs.","title":"Use a GPG key for SSH authentication"},{"location":"learning/tools/gpg/#create-an-authentication-subkey","text":"You should already have a GPG key. If you don't, read one of the many fine tutorials available on this topic. You will create the subkey by editing your existing key in expert mode to get access to the appropriate options: $ gpg2 --expert --edit-key 'key_fingerprint' gpg> addkey Please select what kind of key you want: ( 3 ) DSA ( sign only ) ( 4 ) RSA ( sign only ) ( 5 ) Elgamal ( encrypt only ) ( 6 ) RSA ( encrypt only ) ( 7 ) DSA ( set your own capabilities ) ( 8 ) RSA ( set your own capabilities ) ( 10 ) ECC ( sign only ) ( 11 ) ECC ( set your own capabilities ) ( 12 ) ECC ( encrypt only ) ( 13 ) Existing key Your selection? 8 Possible actions for a RSA key: Sign Encrypt Authenticate Current allowed actions: Sign Encrypt ( S ) Toggle the sign capability ( E ) Toggle the encrypt capability ( A ) Toggle the authenticate capability ( Q ) Finished Your selection? s Your selection? e Your selection? a Possible actions for a RSA key: Sign Encrypt Authenticate Current allowed actions: Authenticate ( S ) Toggle the sign capability ( E ) Toggle the encrypt capability ( A ) Toggle the authenticate capability ( Q ) Finished Your selection? q RSA keys may be between 1024 and 4096 bits long. What keysize do you want? ( 4096 ) Requested keysize is 4096 bits Please specify how long the key should be valid. 0 = key does not expire <n> = key expires in n days <n>w = key expires in n weeks <n>m = key expires in n months <n>y = key expires in n years Key is valid for ? ( 0 ) Key does not expire at all Is this correct? ( y/N ) y Really create? ( y/N ) y sec rsa2048/8715AF32191DB135 created: 2019 -03-21 expires: 2021 -03-20 usage: SC trust: ultimate validity: ultimate ssb rsa2048/150F16909B9AA603 created: 2019 -03-21 expires: 2021 -03-20 usage: E ssb rsa2048/17E7403F18CB1123 created: 2019 -03-21 expires: never usage: A [ ultimate ] ( 1 ) . Brian Exelbierd gpg> quit Save changes? ( y/N ) y","title":"Create an authentication subkey"},{"location":"learning/tools/gpg/#enable-ssh-to-use-the-gpg-subkey","text":"When using SSH, ssh-agent is used to manage SSH keys. When using a GPG key, gpg-agent is used to manage GPG keys. To get gpg-agent to handle requests from SSH, you need to enable its SSH support: echo \"enable-ssh-support\" >> ~/.gnupg/gpg-agent.conf You can avoid usinig ssh-add to load the keys pre-specifying which GPG keys to use in the ~/.gnupg/sshcontrol file. The entries in this file are keygrips\u2014internal identifiers that gpg-agent uses to refer to the keys. A keygrip refers to both the public and private key. To find the keygrip use gpg -K --with-keygrip , then add that line to the ~/.gnupg/sshcontrol file: $ gpg2 -K --with-keygrip /home/bexelbie/.gnupg/pubring.kbx ------------------------------ sec rsa2048 2019 -03-21 [ SC ] [ expires: 2021 -03-20 ] 96F33EA7F4E0F7051D75FC208715AF32191DB135 Keygrip = 90E08830BC1AAD225E657AD4FBE638B3D8E50C9E uid [ ultimate ] Brian Exelbierd ssb rsa2048 2019 -03-21 [ E ] [ expires: 2021 -03-20 ] Keygrip = 5FA04ABEBFBC5089E50EDEB43198B4895BCA2136 ssb rsa2048 2019 -03-21 [ A ] Keygrip = 7710BA0643CC022B92544181FF2EAC2A290CDC0E $ echo 7710BA0643CC022B92544181FF2EAC2A290CDC0E >> ~/.gnupg/sshcontrol Now tell SSH how to access gpg-agent by setting the value of the SSH_AUTH_SOCK environment variable. export SSH_AUTH_SOCK = $( gpgconf --list-dirs agent-ssh-socket ) gpgconf --launch gpg-agent","title":"Enable SSH to use the GPG subkey"},{"location":"learning/tools/gpg/#share-the-gpg-ssh-key","text":"Run ssh-add -L to list your public keys and copy them over manually to the remote host, or use ssh-copy-id as you would normally do.","title":"Share the GPG-SSH key"},{"location":"learning/tools/gpg/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/gpg/#gpg-failed-to-sign-the-data-fatal-failed-to-write-commit-object","text":"Problem: git is instructed to sign a commit with gpg git commit fails with the following error: gpg failed to sign the data fatal: failed to write commit object Solution: if gnupg2 and gpg-agent 2.x are used, be sure to set the environment variable GPG_TTY : export GPG_TTY = $( tty )","title":"gpg failed to sign the data; fatal: failed to write commit object"},{"location":"learning/tools/gpg/#sources","text":"Decrypt multiple openpgp files in a directory ask redhat how can i remove the passphrase from a gpg2 private key? Unattended key generation How to enable SSH access using a GPG key for authentication gpg failed to sign the data fatal: failed to write commit object Can you manually add a comment to a PGP public key block and not break it?","title":"Sources"},{"location":"learning/tools/grep%20the%20standard%20error%20stream/","text":"Grep the standard error stream \u00b6 If you're using bash or zsh you can employ anonymous pipes: ffmpeg -i 01 -Daemon.mp3 2 > > ( grep -i Duration ) If you want the filtered redirected output on stderr again, add the >&2 redirection to grep: command 2 > > ( grep something > & 2 ) 2> redirects stderr to a pipe, while >(command) reads from it. This is syntactic sugar to create a pipe (not a file) and remove it when the process completes. They are effectively anonymous, because they are not given a name in the filesystem. Bash calls this process substitution : Process substitution can also be used to capture output that would normally go to a file, and redirect it to the input of a process. You can exclude stdout and grep stderr redirecting it to null : command 1 >/dev/null 2 > > ( grep -oP \"(.*)(?=pattern)\" ) Do note that the target command of process substitution runs asynchronously . As a consequence, stderr lines that get through the grep filter may not appear at the place you would expect in the rest of the output, but even on your next command prompt. Further readings \u00b6 Knowledge base on grep Sources \u00b6 Answer on StackExchange about how to grep the standard error stream","title":"Grep the standard error stream"},{"location":"learning/tools/grep%20the%20standard%20error%20stream/#grep-the-standard-error-stream","text":"If you're using bash or zsh you can employ anonymous pipes: ffmpeg -i 01 -Daemon.mp3 2 > > ( grep -i Duration ) If you want the filtered redirected output on stderr again, add the >&2 redirection to grep: command 2 > > ( grep something > & 2 ) 2> redirects stderr to a pipe, while >(command) reads from it. This is syntactic sugar to create a pipe (not a file) and remove it when the process completes. They are effectively anonymous, because they are not given a name in the filesystem. Bash calls this process substitution : Process substitution can also be used to capture output that would normally go to a file, and redirect it to the input of a process. You can exclude stdout and grep stderr redirecting it to null : command 1 >/dev/null 2 > > ( grep -oP \"(.*)(?=pattern)\" ) Do note that the target command of process substitution runs asynchronously . As a consequence, stderr lines that get through the grep filter may not appear at the place you would expect in the rest of the output, but even on your next command prompt.","title":"Grep the standard error stream"},{"location":"learning/tools/grep%20the%20standard%20error%20stream/#further-readings","text":"Knowledge base on grep","title":"Further readings"},{"location":"learning/tools/grep%20the%20standard%20error%20stream/#sources","text":"Answer on StackExchange about how to grep the standard error stream","title":"Sources"},{"location":"learning/tools/grep/","text":"Grep \u00b6 TL;DR \u00b6 # base search grep 'pattern' path/to/search # recursive search grep -R 'pattern' path/to/search/recursively grep -R --exclude-dir excluded/dir 'pattern' path/to/search/recursively # gnu grep >= 2.5.2 # show line numbers grep -n 'pattern' path/to/search # parallel execution # mind the files with spaces in their name find . -type f | parallel -j 100 % grep 'pattern' find . -type f -print0 | xargs -0 -n 1 -P $( nproc ) grep 'pattern' Grep variants \u00b6 egrep to use regular expressions in search patterns, same as grep -E fgrep to use patterns as fixed strings, same as grep -F archive-related variants for searching into compressed files pdfgrep for searching into PDF files Archive-related variants \u00b6 xzgrep (with xzegrep and xzfgrep ) zstdgrep for zstd archives many many others PDFgrep \u00b6 For simple searches, you might want to use pdfgrep . Should you need more advanced grep capabilities not incorporated by pdfgrep, you might want to convert the file to text and search there. You can to this using pdftotext as shown in this example ([source][stackoverflow answer about how to search contents of multiple pdf files]): find /path -name '*.pdf' -exec sh -c 'pdftotext \"{}\" - | grep --with-filename --label=\"{}\" --color \"your pattern\"' ';' Gotchas \u00b6 Standard editions of grep run in a single thread; use another executor like parallel or xargs to parallelize grepping multiple files: find . -type f | parallel -j 100 % grep 'pattern' find . -type f -print0 | xargs -0 -n 1 -P $( nproc ) grep 'pattern' mind files with spaces in their name. Further readings \u00b6 [Grep the standard error stream] Knowledge base on pdfgrep [grep the standard error stream]: grep the standard error stream.md Sources \u00b6 Answer on StackOverflow about how to search contents of multiple pdf files Regular expressions in grep with examples Parallel grep","title":"Grep"},{"location":"learning/tools/grep/#grep","text":"","title":"Grep"},{"location":"learning/tools/grep/#tldr","text":"# base search grep 'pattern' path/to/search # recursive search grep -R 'pattern' path/to/search/recursively grep -R --exclude-dir excluded/dir 'pattern' path/to/search/recursively # gnu grep >= 2.5.2 # show line numbers grep -n 'pattern' path/to/search # parallel execution # mind the files with spaces in their name find . -type f | parallel -j 100 % grep 'pattern' find . -type f -print0 | xargs -0 -n 1 -P $( nproc ) grep 'pattern'","title":"TL;DR"},{"location":"learning/tools/grep/#grep-variants","text":"egrep to use regular expressions in search patterns, same as grep -E fgrep to use patterns as fixed strings, same as grep -F archive-related variants for searching into compressed files pdfgrep for searching into PDF files","title":"Grep variants"},{"location":"learning/tools/grep/#archive-related-variants","text":"xzgrep (with xzegrep and xzfgrep ) zstdgrep for zstd archives many many others","title":"Archive-related variants"},{"location":"learning/tools/grep/#pdfgrep","text":"For simple searches, you might want to use pdfgrep . Should you need more advanced grep capabilities not incorporated by pdfgrep, you might want to convert the file to text and search there. You can to this using pdftotext as shown in this example ([source][stackoverflow answer about how to search contents of multiple pdf files]): find /path -name '*.pdf' -exec sh -c 'pdftotext \"{}\" - | grep --with-filename --label=\"{}\" --color \"your pattern\"' ';'","title":"PDFgrep"},{"location":"learning/tools/grep/#gotchas","text":"Standard editions of grep run in a single thread; use another executor like parallel or xargs to parallelize grepping multiple files: find . -type f | parallel -j 100 % grep 'pattern' find . -type f -print0 | xargs -0 -n 1 -P $( nproc ) grep 'pattern' mind files with spaces in their name.","title":"Gotchas"},{"location":"learning/tools/grep/#further-readings","text":"[Grep the standard error stream] Knowledge base on pdfgrep [grep the standard error stream]: grep the standard error stream.md","title":"Further readings"},{"location":"learning/tools/grep/#sources","text":"Answer on StackOverflow about how to search contents of multiple pdf files Regular expressions in grep with examples Parallel grep","title":"Sources"},{"location":"learning/tools/gsutils/","text":"Gsutils \u00b6 TL;DR \u00b6 # delete a bucket and all its contents gsutil rm -r gs:// ${ BUCKET_NAME } # delete a bucket only if empty gsutil rb gs:// ${ BUCKET_NAME } Further readings \u00b6 Deleting buckets","title":"Gsutils"},{"location":"learning/tools/gsutils/#gsutils","text":"","title":"Gsutils"},{"location":"learning/tools/gsutils/#tldr","text":"# delete a bucket and all its contents gsutil rm -r gs:// ${ BUCKET_NAME } # delete a bucket only if empty gsutil rb gs:// ${ BUCKET_NAME }","title":"TL;DR"},{"location":"learning/tools/gsutils/#further-readings","text":"Deleting buckets","title":"Further readings"},{"location":"learning/tools/hashcat/","text":"Hashcat \u00b6 TL;DR \u00b6 # Install it. sudo zypper install hashcat # Add your user to the 'video' group to be able to use the GPU. # You'll still need the correct drivers to be installed. sudo usermod -a -G 'video' 'username' sudo gpasswd -a 'username' 'video' # hashcat [options]... hash|hashfile|hccapxfile [dictionary|mask|directory]... # Run a benchmark. hashcat -b # Run a benchmark on all hash modes. hashcat -b --benchmark-all # Show the expected speed for a particular hash mode. hashcat \\ -m 1800 -a3 -O -w4 --speed-only \\ $( mkpasswd -m sha512crypt '1029384756' ) \\ ?a?a?a?a?a?a?a?a # Try to brute-force (-a3) a `sha512crypt`ed (-m1800) string. # Only test 10-digits strings (--increment --increment-min 10 # --increment-max 10 ?d?d?d?d?d?d?d?d?d?d). # Use all the available resources possible (-w4), including optimized kernel # code (-O). hashcat \\ -m 1800 -a3 -O -w4 --increment --increment-min 10 --increment-max 10 \\ $( mkpasswd -m sha512crypt '1029384756' ) \\ ?d?d?d?d?d?d?d?d?d?d","title":"Hashcat"},{"location":"learning/tools/hashcat/#hashcat","text":"","title":"Hashcat"},{"location":"learning/tools/hashcat/#tldr","text":"# Install it. sudo zypper install hashcat # Add your user to the 'video' group to be able to use the GPU. # You'll still need the correct drivers to be installed. sudo usermod -a -G 'video' 'username' sudo gpasswd -a 'username' 'video' # hashcat [options]... hash|hashfile|hccapxfile [dictionary|mask|directory]... # Run a benchmark. hashcat -b # Run a benchmark on all hash modes. hashcat -b --benchmark-all # Show the expected speed for a particular hash mode. hashcat \\ -m 1800 -a3 -O -w4 --speed-only \\ $( mkpasswd -m sha512crypt '1029384756' ) \\ ?a?a?a?a?a?a?a?a # Try to brute-force (-a3) a `sha512crypt`ed (-m1800) string. # Only test 10-digits strings (--increment --increment-min 10 # --increment-max 10 ?d?d?d?d?d?d?d?d?d?d). # Use all the available resources possible (-w4), including optimized kernel # code (-O). hashcat \\ -m 1800 -a3 -O -w4 --increment --increment-min 10 --increment-max 10 \\ $( mkpasswd -m sha512crypt '1029384756' ) \\ ?d?d?d?d?d?d?d?d?d?d","title":"TL;DR"},{"location":"learning/tools/helm/","text":"Helm \u00b6 TL;DR TL;DR \u00b6 # List installed plugins. helm plugin list helm plugin ls # Install new plugins. helm plugin add https://github.com/author/plugin helm plugin install path/to/plugin # Update installed plugins. helm plugin update plugin-name helm plugin up plugin-name # Uninstall plugins. helm plugin rm plugin-name helm plugin remove plugin-name helm plugin uninstall plugin-name","title":"Helm"},{"location":"learning/tools/helm/#helm","text":"TL;DR","title":"Helm"},{"location":"learning/tools/helm/#tldr","text":"# List installed plugins. helm plugin list helm plugin ls # Install new plugins. helm plugin add https://github.com/author/plugin helm plugin install path/to/plugin # Update installed plugins. helm plugin update plugin-name helm plugin up plugin-name # Uninstall plugins. helm plugin rm plugin-name helm plugin remove plugin-name helm plugin uninstall plugin-name","title":"TL;DR"},{"location":"learning/tools/helmfile/","text":"Helmfile \u00b6 TL;DR TL;DR \u00b6 # Show what happens in the internal computations. helmfile --debug -e environment apply # Show the difference between the current state and what would be applied. # Requires `helm` to have the 'diff' plugin installed. helmfile -f custom.yml -e environment diff --values environment.yaml","title":"Helmfile"},{"location":"learning/tools/helmfile/#helmfile","text":"TL;DR","title":"Helmfile"},{"location":"learning/tools/helmfile/#tldr","text":"# Show what happens in the internal computations. helmfile --debug -e environment apply # Show the difference between the current state and what would be applied. # Requires `helm` to have the 'diff' plugin installed. helmfile -f custom.yml -e environment diff --values environment.yaml","title":"TL;DR"},{"location":"learning/tools/homebrew/","text":"Homebrew \u00b6 TL;DR \u00b6 # Install/uninstall on OS X. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/uninstall.sh ) \" # Search for formulae. brew search parallel brew search --cask gpg # Install something. brew install gettext brew install --cask spotify # Uninstall something brew uninstall --zap keybase # Get formulae's dependencies. brew deps brew deps --installed azure-cli brew deps --tree # Get information on formulae. brew info sponge # Prevent a formula from upgrading. brew pin gnupg2 # Bring an installation up to speed from a Brewfile. brew bundle brew bundle --global brew bundle --file $HOME /Brewfile --no-lock # Dump all installed casks/formulae/images/taps into a Brewfile in the current # directory. brew bundle dump Configuration \u00b6 # Require SHA check for casks. # Change cask installation dir to the Application folder in the user's HOME. export HOMEBREW_CASK_OPTS = \"--require-sha --appdir $HOME /Applications\" # Print install times for each formula at the end of the run. export HOMEBREW_DISPLAY_INSTALL_TIMES = 1 # Do not automatically update before running some commands. export HOMEBREW_NO_AUTO_UPDATE = 1 # Do not print HOMEBREW_INSTALL_BADGE on a successful build. export HOMEBREW_NO_EMOJI = 1 # Do not use the GitHub API. # Avoid searches or fetching relevant issues after a failed install. export HOMEBREW_NO_GITHUB_API = 1 # Forbid redirects from secure HTTPS to insecure HTTP. export HOMEBREW_NO_INSECURE_REDIRECT = 1 # Only list updates to installed software. export HOMEBREW_UPDATE_REPORT_ONLY_INSTALLED = 1 # Pass the -A option when calling sudo. export SUDO_ASKPASS = 1 Downgrade an application to a non-managed version \u00b6 The easy way \u00b6 brew unlink kubernetes-helm brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/ed9dcb2cb455a816f744c3ad4ab5c18a0d335763/Formula/kubernetes-helm.rb brew switch kubernetes-helm 2 .13.0 The hard way \u00b6 source alternative source formula_name = 'kubernetes-helm' formula_version = '2.13.1' cd $( brew --repository ) /Library/Tapshomebrew/homebrew-core git log master -S ${ formula_version } -- Formula/ ${ formula_name } .rb commit_id = '<something>' # insert commit id git checkout -b ${ formula_name } - ${ formula_version } ${ commit_id } HOMEBREW_NO_AUTO_UPDATE = 1 brew install ${ formula_name } # pin application if needed git checkout master git branch -d ${ formula_name } - ${ formula_version } Gotchas \u00b6 moreutils installs its own old version of parallel , which conflicts with the parallel formulae; install the standalone gettext , parallel and sponge to have their recent version Further readings \u00b6 manpage Homebrew bundle Sources \u00b6 How to stop homebrew from upgrading itself on every run macOS migrations with Brewfile","title":"Homebrew"},{"location":"learning/tools/homebrew/#homebrew","text":"","title":"Homebrew"},{"location":"learning/tools/homebrew/#tldr","text":"# Install/uninstall on OS X. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/uninstall.sh ) \" # Search for formulae. brew search parallel brew search --cask gpg # Install something. brew install gettext brew install --cask spotify # Uninstall something brew uninstall --zap keybase # Get formulae's dependencies. brew deps brew deps --installed azure-cli brew deps --tree # Get information on formulae. brew info sponge # Prevent a formula from upgrading. brew pin gnupg2 # Bring an installation up to speed from a Brewfile. brew bundle brew bundle --global brew bundle --file $HOME /Brewfile --no-lock # Dump all installed casks/formulae/images/taps into a Brewfile in the current # directory. brew bundle dump","title":"TL;DR"},{"location":"learning/tools/homebrew/#configuration","text":"# Require SHA check for casks. # Change cask installation dir to the Application folder in the user's HOME. export HOMEBREW_CASK_OPTS = \"--require-sha --appdir $HOME /Applications\" # Print install times for each formula at the end of the run. export HOMEBREW_DISPLAY_INSTALL_TIMES = 1 # Do not automatically update before running some commands. export HOMEBREW_NO_AUTO_UPDATE = 1 # Do not print HOMEBREW_INSTALL_BADGE on a successful build. export HOMEBREW_NO_EMOJI = 1 # Do not use the GitHub API. # Avoid searches or fetching relevant issues after a failed install. export HOMEBREW_NO_GITHUB_API = 1 # Forbid redirects from secure HTTPS to insecure HTTP. export HOMEBREW_NO_INSECURE_REDIRECT = 1 # Only list updates to installed software. export HOMEBREW_UPDATE_REPORT_ONLY_INSTALLED = 1 # Pass the -A option when calling sudo. export SUDO_ASKPASS = 1","title":"Configuration"},{"location":"learning/tools/homebrew/#downgrade-an-application-to-a-non-managed-version","text":"","title":"Downgrade an application to a non-managed version"},{"location":"learning/tools/homebrew/#the-easy-way","text":"brew unlink kubernetes-helm brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/ed9dcb2cb455a816f744c3ad4ab5c18a0d335763/Formula/kubernetes-helm.rb brew switch kubernetes-helm 2 .13.0","title":"The easy way"},{"location":"learning/tools/homebrew/#the-hard-way","text":"source alternative source formula_name = 'kubernetes-helm' formula_version = '2.13.1' cd $( brew --repository ) /Library/Tapshomebrew/homebrew-core git log master -S ${ formula_version } -- Formula/ ${ formula_name } .rb commit_id = '<something>' # insert commit id git checkout -b ${ formula_name } - ${ formula_version } ${ commit_id } HOMEBREW_NO_AUTO_UPDATE = 1 brew install ${ formula_name } # pin application if needed git checkout master git branch -d ${ formula_name } - ${ formula_version }","title":"The hard way"},{"location":"learning/tools/homebrew/#gotchas","text":"moreutils installs its own old version of parallel , which conflicts with the parallel formulae; install the standalone gettext , parallel and sponge to have their recent version","title":"Gotchas"},{"location":"learning/tools/homebrew/#further-readings","text":"manpage Homebrew bundle","title":"Further readings"},{"location":"learning/tools/homebrew/#sources","text":"How to stop homebrew from upgrading itself on every run macOS migrations with Brewfile","title":"Sources"},{"location":"learning/tools/imagemagick/","text":"ImageMagick \u00b6 Components: convert : image conversion tool TL;DR \u00b6 # scale an image to 50% its original size convert IMG_20200117_135049.jpg -adaptive-resize 50 % IMG_20200117_135049_resized.jpg # create a gif using images magick *.jpg images.gif # convert n images to individual pdf pages magick *.jpg +adjoin page-%d.pdf # convert n images to a single pdf document magick *.png out.pdf Further readings \u00b6 Website cheat.sh/convert Sources \u00b6 Converting Multiple Images into a PDF File How to Quickly Resize, Convert & Modify Images from the Linux Terminal","title":"ImageMagick"},{"location":"learning/tools/imagemagick/#imagemagick","text":"Components: convert : image conversion tool","title":"ImageMagick"},{"location":"learning/tools/imagemagick/#tldr","text":"# scale an image to 50% its original size convert IMG_20200117_135049.jpg -adaptive-resize 50 % IMG_20200117_135049_resized.jpg # create a gif using images magick *.jpg images.gif # convert n images to individual pdf pages magick *.jpg +adjoin page-%d.pdf # convert n images to a single pdf document magick *.png out.pdf","title":"TL;DR"},{"location":"learning/tools/imagemagick/#further-readings","text":"Website cheat.sh/convert","title":"Further readings"},{"location":"learning/tools/imagemagick/#sources","text":"Converting Multiple Images into a PDF File How to Quickly Resize, Convert & Modify Images from the Linux Terminal","title":"Sources"},{"location":"learning/tools/immutable%20oses/","text":"Immutable OSes \u00b6 Further readings \u00b6 Fedora Silverblue OpenSUSE MicroOS","title":"Immutable OSes"},{"location":"learning/tools/immutable%20oses/#immutable-oses","text":"","title":"Immutable OSes"},{"location":"learning/tools/immutable%20oses/#further-readings","text":"Fedora Silverblue OpenSUSE MicroOS","title":"Further readings"},{"location":"learning/tools/insomnia/","text":"Insomnia \u00b6 Troubleshooting \u00b6 Manually install plugins \u00b6 open the plugins folder in the terminal; get the path in Preferences > Plugins tab > Reveal Plugins Folder button use npm to install the plugin in that folder: npm i --prefix ./ insomnia-plugin-date-add Further readings \u00b6 Website Documentation Inso CLI , runner for Insomnia Postman , an alternative to Insomnia Sources \u00b6 NPM install module in current directory","title":"Insomnia <!-- omit in toc -->"},{"location":"learning/tools/insomnia/#insomnia","text":"","title":"Insomnia "},{"location":"learning/tools/insomnia/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/insomnia/#manually-install-plugins","text":"open the plugins folder in the terminal; get the path in Preferences > Plugins tab > Reveal Plugins Folder button use npm to install the plugin in that folder: npm i --prefix ./ insomnia-plugin-date-add","title":"Manually install plugins"},{"location":"learning/tools/insomnia/#further-readings","text":"Website Documentation Inso CLI , runner for Insomnia Postman , an alternative to Insomnia","title":"Further readings"},{"location":"learning/tools/insomnia/#sources","text":"NPM install module in current directory","title":"Sources"},{"location":"learning/tools/iperf/","text":"Iperf \u00b6 # on the server iperf3 -s iperf3 -s -p 7575 # on the client iperf3 -c iperf.server.ip iperf3 -c iperf.server.ip -p 7575 Sources \u00b6 How to use iPerf3 to test network bandwidth","title":"Iperf"},{"location":"learning/tools/iperf/#iperf","text":"# on the server iperf3 -s iperf3 -s -p 7575 # on the client iperf3 -c iperf.server.ip iperf3 -c iperf.server.ip -p 7575","title":"Iperf"},{"location":"learning/tools/iperf/#sources","text":"How to use iPerf3 to test network bandwidth","title":"Sources"},{"location":"learning/tools/iw/","text":"Iw \u00b6 TL;DR \u00b6 # Scan for available wireless networks. iw dev wlp scan # Join an open wireless network. iw dev wlp connect SSID # Disconnect from the current network. iw dev wlp disconnect # Show information about the current connection. iw dev wlp link Sources \u00b6 cheat.sh","title":"Iw"},{"location":"learning/tools/iw/#iw","text":"","title":"Iw"},{"location":"learning/tools/iw/#tldr","text":"# Scan for available wireless networks. iw dev wlp scan # Join an open wireless network. iw dev wlp connect SSID # Disconnect from the current network. iw dev wlp disconnect # Show information about the current connection. iw dev wlp link","title":"TL;DR"},{"location":"learning/tools/iw/#sources","text":"cheat.sh","title":"Sources"},{"location":"learning/tools/iwlist/","text":"Iwlist \u00b6 Not deprecated, but surpassed many times by [iw]. TL;DR \u00b6 # Scan for networks. iwlist wlan0 scan","title":"Iwlist"},{"location":"learning/tools/iwlist/#iwlist","text":"Not deprecated, but surpassed many times by [iw].","title":"Iwlist"},{"location":"learning/tools/iwlist/#tldr","text":"# Scan for networks. iwlist wlan0 scan","title":"TL;DR"},{"location":"learning/tools/jdupes/","text":"Jdupes \u00b6 TL;DR \u00b6 # Prompt to delete all duplicate files. jdupes -Zdr directory # Automatically replace duplicate files with hardlinks to the first encountered. jdupes -ONLr directory1 directory2 directory3 file # Quickly list all duplicate GZ archives. jdupes -rQX onlyext:gz directory # Delete all duplicates from a folder while keeping all other folders intact. # Usually needs multiple runs to delete all duplicates. find -mindepth 1 -maxdepth 1 -type d -not -name directoryWithDuplicates \\ | xargs -I {} -n 1 -t jdupes -drINOZ {} directoryWithDuplicates Options \u00b6 Short Long Description -@ --loud output annoying low-level debug info while running -B --dedupe issue the btrfs same-extents ioctl to trigger a deduplication on disk; jdupes must be built with btrfs support for this option to be available -D --debug show debugging statistics and info at the end of program execution; the feature must be compiled in for this option to work -d --delete prompt the user for files to preserve, deleting all others -I --isolate isolate each command-line parameter from one another; only return a match if the files are under different parameters -L --link-hard replace all duplicate files with hardlinks to the first file in each set of duplicates -m --summarize summarize duplicate file information -M --print-summarize print matches and summarize the duplicate file information at the end -N --no-prompt when used together with --delete , preserve the first file in each set of duplicates and delete the others without prompting the user -O --param-order parameter order preservation is more important than the chosen sort; this is particularly useful with the -N option to ensure that automatic deletion behaves in a controllable way -Q --quick skip byte-for-byte verification of duplicate pairs (use hashes only) -q --quiet hide progress indicator -r --recurse for every directory given follow subdirectories encountered within -S --size show size of duplicate files -s --symlinks follow symlinked directories -X --ext-filter=spec:info exclude/filter files based on specified criteria; see the filter format section -Z --soft-abort if the user aborts the program (as with CTRL-C), act on the matches that were found before the abort was received; the default behavior without -Z is to abort without taking any actions Filter format \u00b6 jdupes -X filter[:value][size_suffix] Some filters take no value or multiple values. Filters that can take a numeric option generally support the size multipliers K / M / G / T / P / E , with or without an added iB or B . Multipliers are binary-style unless the -B suffix is used, which will use decimal multipliers. For example, 16k or 16kib = 16384; 16kb = 16000. Multipliers are case-insensitive. Filters have cumulative effects: jdupes -X size+:99 -X size-:101 will cause only files of exactly 100 bytes in size to be included. Extension matching is case-insensitive. Path substring matching is case-sensitive. Supported filters: size[+-=]:number[suffix] : match only if size is greater (+), less than (-), or equal to (=) the specified number; the \u00b1 and = specifiers can be combined, i.e. size+=:4K will only consider files with a size greater than or equal to 4 kilobytes (4096 bytes) noext:ext1[,ext2,...] : exclude files with certain extension(s), specified as a comma-separated list; do not use a leading dot onlyext:ext1[,ext2,...] : only include files with certain extension(s), specified as a comma-separated list; do not use a leading dot nostr:text_string : exclude all paths containing the substring text_string ; this scans the full file path, so it can be used to match directories, i.e. -X nostr:dir_name/ onlystr:text_string : require all paths to contain the substring text_string ; this scans the full file path, so it can be used to match directories, i.e. -X onlystr:dir_name/ newer:datetime : only include files newer than the specified date; use the date/time format YYYY-MM-DD HH:MM:SS ; time is optional older:datetime : only include files older than the specified date; use the date/time format YYYY-MM-DD HH:MM:SS ; time is optional Further readings \u00b6 Jdupes' github page","title":"Jdupes"},{"location":"learning/tools/jdupes/#jdupes","text":"","title":"Jdupes"},{"location":"learning/tools/jdupes/#tldr","text":"# Prompt to delete all duplicate files. jdupes -Zdr directory # Automatically replace duplicate files with hardlinks to the first encountered. jdupes -ONLr directory1 directory2 directory3 file # Quickly list all duplicate GZ archives. jdupes -rQX onlyext:gz directory # Delete all duplicates from a folder while keeping all other folders intact. # Usually needs multiple runs to delete all duplicates. find -mindepth 1 -maxdepth 1 -type d -not -name directoryWithDuplicates \\ | xargs -I {} -n 1 -t jdupes -drINOZ {} directoryWithDuplicates","title":"TL;DR"},{"location":"learning/tools/jdupes/#options","text":"Short Long Description -@ --loud output annoying low-level debug info while running -B --dedupe issue the btrfs same-extents ioctl to trigger a deduplication on disk; jdupes must be built with btrfs support for this option to be available -D --debug show debugging statistics and info at the end of program execution; the feature must be compiled in for this option to work -d --delete prompt the user for files to preserve, deleting all others -I --isolate isolate each command-line parameter from one another; only return a match if the files are under different parameters -L --link-hard replace all duplicate files with hardlinks to the first file in each set of duplicates -m --summarize summarize duplicate file information -M --print-summarize print matches and summarize the duplicate file information at the end -N --no-prompt when used together with --delete , preserve the first file in each set of duplicates and delete the others without prompting the user -O --param-order parameter order preservation is more important than the chosen sort; this is particularly useful with the -N option to ensure that automatic deletion behaves in a controllable way -Q --quick skip byte-for-byte verification of duplicate pairs (use hashes only) -q --quiet hide progress indicator -r --recurse for every directory given follow subdirectories encountered within -S --size show size of duplicate files -s --symlinks follow symlinked directories -X --ext-filter=spec:info exclude/filter files based on specified criteria; see the filter format section -Z --soft-abort if the user aborts the program (as with CTRL-C), act on the matches that were found before the abort was received; the default behavior without -Z is to abort without taking any actions","title":"Options"},{"location":"learning/tools/jdupes/#filter-format","text":"jdupes -X filter[:value][size_suffix] Some filters take no value or multiple values. Filters that can take a numeric option generally support the size multipliers K / M / G / T / P / E , with or without an added iB or B . Multipliers are binary-style unless the -B suffix is used, which will use decimal multipliers. For example, 16k or 16kib = 16384; 16kb = 16000. Multipliers are case-insensitive. Filters have cumulative effects: jdupes -X size+:99 -X size-:101 will cause only files of exactly 100 bytes in size to be included. Extension matching is case-insensitive. Path substring matching is case-sensitive. Supported filters: size[+-=]:number[suffix] : match only if size is greater (+), less than (-), or equal to (=) the specified number; the \u00b1 and = specifiers can be combined, i.e. size+=:4K will only consider files with a size greater than or equal to 4 kilobytes (4096 bytes) noext:ext1[,ext2,...] : exclude files with certain extension(s), specified as a comma-separated list; do not use a leading dot onlyext:ext1[,ext2,...] : only include files with certain extension(s), specified as a comma-separated list; do not use a leading dot nostr:text_string : exclude all paths containing the substring text_string ; this scans the full file path, so it can be used to match directories, i.e. -X nostr:dir_name/ onlystr:text_string : require all paths to contain the substring text_string ; this scans the full file path, so it can be used to match directories, i.e. -X onlystr:dir_name/ newer:datetime : only include files newer than the specified date; use the date/time format YYYY-MM-DD HH:MM:SS ; time is optional older:datetime : only include files older than the specified date; use the date/time format YYYY-MM-DD HH:MM:SS ; time is optional","title":"Filter format"},{"location":"learning/tools/jdupes/#further-readings","text":"Jdupes' github page","title":"Further readings"},{"location":"learning/tools/jira/","text":"Jira \u00b6 # create a ticket curl https:// ${ COMPANY } .atlassian.net/rest/api/2/issue \\ -D - \\ -u ${ USER_EMAIL } : ${ API_TOKEN } \\ -H \"Content-Type: application/json\" \\ -X POST \\ --data '{ \"fields\": { \"project\": { \"key\": \"PROJECT_KEY\" }, \"summary\": \"REST ye merry gentlemen.\", \"description\": \"Creating of an issue using project keys and issue type names using the REST API\", \"issuetype\": { \"name\": \"Task\" } } }' Sources \u00b6 Creating JIRA issue using curl from command line","title":"Jira"},{"location":"learning/tools/jira/#jira","text":"# create a ticket curl https:// ${ COMPANY } .atlassian.net/rest/api/2/issue \\ -D - \\ -u ${ USER_EMAIL } : ${ API_TOKEN } \\ -H \"Content-Type: application/json\" \\ -X POST \\ --data '{ \"fields\": { \"project\": { \"key\": \"PROJECT_KEY\" }, \"summary\": \"REST ye merry gentlemen.\", \"description\": \"Creating of an issue using project keys and issue type names using the REST API\", \"issuetype\": { \"name\": \"Task\" } } }'","title":"Jira"},{"location":"learning/tools/jira/#sources","text":"Creating JIRA issue using curl from command line","title":"Sources"},{"location":"learning/tools/jmespath/","text":"JMESPath \u00b6 TL;DR \u00b6 # Filter elements in a list. az devops user list \\ --org https://dv.azure.com/organizationName \\ --query \"\\ items[? \\ startsWith(user.principalName, 'yourNameHere') && \\ \\! contains(accessLevel.licenseDisplayName, 'Test plans') \\ ].user.displayName\" Further readings \u00b6 Website","title":"JMESPath"},{"location":"learning/tools/jmespath/#jmespath","text":"","title":"JMESPath"},{"location":"learning/tools/jmespath/#tldr","text":"# Filter elements in a list. az devops user list \\ --org https://dv.azure.com/organizationName \\ --query \"\\ items[? \\ startsWith(user.principalName, 'yourNameHere') && \\ \\! contains(accessLevel.licenseDisplayName, 'Test plans') \\ ].user.displayName\"","title":"TL;DR"},{"location":"learning/tools/jmespath/#further-readings","text":"Website","title":"Further readings"},{"location":"learning/tools/jq/","text":"JQ \u00b6 TL;DR \u00b6 # Only list keys. jq 'keys' file.json # Sort all the keys. jq --sort-keys '.' input.json > output.json # Add a key. jq --arg REGION ${ AWS_REGION } '.spec.template.spec.containers[]?.env? += [{name: \"AWS_REGION\", value: $REGION}]' /tmp/service.kube.json # Delete a key. jq 'del(.items[].spec.clusterIP)' /tmp/service.kube.json # Change a value. jq '.extensionsGallery | .serviceUrl |= \"https://marketplace.visualstudio.com/_apis/public/gallery\"' \\ /usr/lib/code/product.json jq --arg NAMESPACE ${ NAMESPACE } '.spec.template.spec.containers[]?.env[]? |= {name: .name, value: (if .name == \"KUBERNETES_NAMESPACE\" then $NAMESPACE else .value end)}' /tmp/service.kube.json # Change multiple values at once. jq '.extensionsGallery | .serviceUrl = \"https://marketplace.visualstudio.com/_apis/public/gallery\" | .cacheUrl = \"https://vscode.blob.core.windows.net/gallery/index\" | .itemUrl = \"https://marketplace.visualstudio.com/items\"' \\ /usr/lib/code/product.json jq '.extensionsGallery + { serviceUrl: \"https://marketplace.visualstudio.com/_apis/public/gallery\", cacheUrl: \"https://vscode.blob.core.windows.net/gallery/index\", itemUrl: \"https://marketplace.visualstudio.com/items\" }' /usr/lib/code/product.json # Put specific keys on top. jq '.objects = [(.objects[] as $in | {type,name,id} + $in)]' prod/dataPipeline_deviceLocationConversion_prod.json # Convert Enpass' JSON export to a YAML file jq '.items[] | {title, fields} | .title + \":\", (.fields[] | select(.value != \"\") | \" \" + .label + \": \" + .value)' test.json -cr # Refactor a datapipeline definition. jq --sort-keys '.' datapipeline.json > /tmp/sorted.json \\ && jq '.objects = [(.objects[] as $in | {type,name,id} + $in | with_entries(select(.value != null)))]' \\ /tmp/sorted.json > /tmp/reordered.json \\ && mv /tmp/reordered.json datapipeline.json # Extract the value of elements with specific keys. kubectl get pods -o yaml \\ | yq -y ' .items[] | select(.metadata.name | test(\"^runner-.*\")) | select(.spec.tolerations[].key == \"component\" and .spec.tolerations[].value == \"big-runner\") | .spec.nodeSelector, .spec.tolerations' \\ - # Recursively find all the properties whose key is 'errors' whether it exists or not. # '..' unrolls the object, '?' checks for the value or returns null, and 'select(.)' is like a filter on truthy values. jq '[.. | .errors?[0] | select(.) ]' /tmp/helm.template.out.json # Find all images in a helm chart explicitly or implicitly using the tag 'latest'. helm template chartName \\ | yq -r ' .. | .image? | select(.) | select(.|test(\".*:.*\")|not), select(.|test(\".*:$\")), select(.|test(\".*:latest\"))' \\ - Further readings \u00b6 JQ recipes Sources \u00b6 Filter objects list with regex Select multiple conditions Change multiple values at once","title":"JQ"},{"location":"learning/tools/jq/#jq","text":"","title":"JQ"},{"location":"learning/tools/jq/#tldr","text":"# Only list keys. jq 'keys' file.json # Sort all the keys. jq --sort-keys '.' input.json > output.json # Add a key. jq --arg REGION ${ AWS_REGION } '.spec.template.spec.containers[]?.env? += [{name: \"AWS_REGION\", value: $REGION}]' /tmp/service.kube.json # Delete a key. jq 'del(.items[].spec.clusterIP)' /tmp/service.kube.json # Change a value. jq '.extensionsGallery | .serviceUrl |= \"https://marketplace.visualstudio.com/_apis/public/gallery\"' \\ /usr/lib/code/product.json jq --arg NAMESPACE ${ NAMESPACE } '.spec.template.spec.containers[]?.env[]? |= {name: .name, value: (if .name == \"KUBERNETES_NAMESPACE\" then $NAMESPACE else .value end)}' /tmp/service.kube.json # Change multiple values at once. jq '.extensionsGallery | .serviceUrl = \"https://marketplace.visualstudio.com/_apis/public/gallery\" | .cacheUrl = \"https://vscode.blob.core.windows.net/gallery/index\" | .itemUrl = \"https://marketplace.visualstudio.com/items\"' \\ /usr/lib/code/product.json jq '.extensionsGallery + { serviceUrl: \"https://marketplace.visualstudio.com/_apis/public/gallery\", cacheUrl: \"https://vscode.blob.core.windows.net/gallery/index\", itemUrl: \"https://marketplace.visualstudio.com/items\" }' /usr/lib/code/product.json # Put specific keys on top. jq '.objects = [(.objects[] as $in | {type,name,id} + $in)]' prod/dataPipeline_deviceLocationConversion_prod.json # Convert Enpass' JSON export to a YAML file jq '.items[] | {title, fields} | .title + \":\", (.fields[] | select(.value != \"\") | \" \" + .label + \": \" + .value)' test.json -cr # Refactor a datapipeline definition. jq --sort-keys '.' datapipeline.json > /tmp/sorted.json \\ && jq '.objects = [(.objects[] as $in | {type,name,id} + $in | with_entries(select(.value != null)))]' \\ /tmp/sorted.json > /tmp/reordered.json \\ && mv /tmp/reordered.json datapipeline.json # Extract the value of elements with specific keys. kubectl get pods -o yaml \\ | yq -y ' .items[] | select(.metadata.name | test(\"^runner-.*\")) | select(.spec.tolerations[].key == \"component\" and .spec.tolerations[].value == \"big-runner\") | .spec.nodeSelector, .spec.tolerations' \\ - # Recursively find all the properties whose key is 'errors' whether it exists or not. # '..' unrolls the object, '?' checks for the value or returns null, and 'select(.)' is like a filter on truthy values. jq '[.. | .errors?[0] | select(.) ]' /tmp/helm.template.out.json # Find all images in a helm chart explicitly or implicitly using the tag 'latest'. helm template chartName \\ | yq -r ' .. | .image? | select(.) | select(.|test(\".*:.*\")|not), select(.|test(\".*:$\")), select(.|test(\".*:latest\"))' \\ -","title":"TL;DR"},{"location":"learning/tools/jq/#further-readings","text":"JQ recipes","title":"Further readings"},{"location":"learning/tools/jq/#sources","text":"Filter objects list with regex Select multiple conditions Change multiple values at once","title":"Sources"},{"location":"learning/tools/jsonpath/","text":"JSONPath \u00b6 TL;DR \u00b6 # filter elements # only works on arrays, not on maps kubectl get serviceaccounts \\ -o jsonpath = \"{.items[?(@.metadata.name!='default')].metadata.name}\" Further readings \u00b6 JSONPath Syntax","title":"JSONPath"},{"location":"learning/tools/jsonpath/#jsonpath","text":"","title":"JSONPath"},{"location":"learning/tools/jsonpath/#tldr","text":"# filter elements # only works on arrays, not on maps kubectl get serviceaccounts \\ -o jsonpath = \"{.items[?(@.metadata.name!='default')].metadata.name}\"","title":"TL;DR"},{"location":"learning/tools/jsonpath/#further-readings","text":"JSONPath Syntax","title":"Further readings"},{"location":"learning/tools/kapp/","text":"Kapp \u00b6 TL;DR \u00b6 # Configurations picked up from a directory $ kapp deploy -a my-app -f ./examples/simple-app-example/config-1.yml # Can be used with helm charts, removing need for Tiller $ kapp -y deploy -a my-chart -f < ( helm template my-chart --values my-vals.yml ) # \u2026 and with kustomize $ kapp -y deploy -a my-app -f < ( kustomize build ./my-app ) # \u2026 or templated with ytt $ kapp -y deploy -a my-app -f < ( ytt -f ./examples/simple-app-example/config-1.yml ) Further readings \u00b6 Official website","title":"Kapp"},{"location":"learning/tools/kapp/#kapp","text":"","title":"Kapp"},{"location":"learning/tools/kapp/#tldr","text":"# Configurations picked up from a directory $ kapp deploy -a my-app -f ./examples/simple-app-example/config-1.yml # Can be used with helm charts, removing need for Tiller $ kapp -y deploy -a my-chart -f < ( helm template my-chart --values my-vals.yml ) # \u2026 and with kustomize $ kapp -y deploy -a my-app -f < ( kustomize build ./my-app ) # \u2026 or templated with ytt $ kapp -y deploy -a my-app -f < ( ytt -f ./examples/simple-app-example/config-1.yml )","title":"TL;DR"},{"location":"learning/tools/kapp/#further-readings","text":"Official website","title":"Further readings"},{"location":"learning/tools/kde/","text":"KDE \u00b6 TL;DR \u00b6 # Get from '~/.config/kinfocenterrc' the current value for the 'MenuBar' key in # the 'MainWindow' group. kreadconfig5 --file kinfocenterrc --group MainWindow --key MenuBar # Set into '~/.config/kdeglobals' a new value for the 'Show hidden files' key in # the 'KFileDialog Settings' group. kwriteconfig5 --file kdeglobals --group 'KFileDialog Settings' \\ --key 'Show hidden files' --type bool true Prioritize a WiFi network connection \u00b6 Plasma-nm lets you change a network's priority specifying a number in the network's General configuration tab. Higher numbers set a higher priority. Further readings \u00b6 KDE Configuration Files Sources \u00b6 Gsettings-like tools for KDE","title":"KDE"},{"location":"learning/tools/kde/#kde","text":"","title":"KDE"},{"location":"learning/tools/kde/#tldr","text":"# Get from '~/.config/kinfocenterrc' the current value for the 'MenuBar' key in # the 'MainWindow' group. kreadconfig5 --file kinfocenterrc --group MainWindow --key MenuBar # Set into '~/.config/kdeglobals' a new value for the 'Show hidden files' key in # the 'KFileDialog Settings' group. kwriteconfig5 --file kdeglobals --group 'KFileDialog Settings' \\ --key 'Show hidden files' --type bool true","title":"TL;DR"},{"location":"learning/tools/kde/#prioritize-a-wifi-network-connection","text":"Plasma-nm lets you change a network's priority specifying a number in the network's General configuration tab. Higher numbers set a higher priority.","title":"Prioritize a WiFi network connection"},{"location":"learning/tools/kde/#further-readings","text":"KDE Configuration Files","title":"Further readings"},{"location":"learning/tools/kde/#sources","text":"Gsettings-like tools for KDE","title":"Sources"},{"location":"learning/tools/keda/","text":"KEDA \u00b6 The Kubernetes-based Event Driven Auto-Scaler will automatically scale a resource in a Kubernetes cluster based on a scale trigger: KEDA will monitor the event source, and feed that data to Kubernetes to scale the resource out/in accordingly, leveraging standard Kubernetes components (e.g. HPA) and extending the existing functionality without overwriting or duplicating components. Any Kubernetes cluster >= 1.16.0 should work. Table of contents: How KEDA works Deployment Helm chart Manual deployment Usage ScaledObject ScaledJobs Authentication External Scalers Troubleshooting Access logging and telemetry Long running executions Manually uninstall everything Further readings Sources How KEDA works \u00b6 For details and updated information see KEDA's concepts page. Upon installation, KEDA creates the following custom resources to enable one to map an event source to a Deployment, StatefulSet, Custom Resource or Job for scaling: scaledobjects.keda.sh scaledjobs.keda.sh triggerauthentications.keda.sh ScaledObjects represent a mapping between an event source (e.g. Rabbit MQ) and any K8S resource (Deployment, StatefulSet or Custom) defining the /scale subresource; ScaledJobs specifically represent a mapping between an event source and a Kubernetes Job. TriggerAuthentication are referenced by ScaledObjects and ScaledJobs when they need to access authentication configurations or secrets to monitor the event source. KEDA also creates the following containers: keda-operator keda-operator-metrics-apiserver The operator acts as an agent which activates, regulates and deactivates the scaling of K8S resources defined in a ScaledObject based on the trigger events. The metrics apiserver exposes rich event data, like queue length or stream lag, to the Horizontal Pod Autoscaler to drive the scale out. It is then up to the resource to consume such events directly from the source. KEDA offers a wide range of triggers (A.K.A. scalers ) that can both detect if a resource should be activated or deactivated and feed custom metrics for a specific event source. The full list of scalers is available here . Deployment \u00b6 Helm chart \u00b6 # Installation. helm repo add kedacore https://kedacore.github.io/charts \\ && helm repo update kedacore \\ && helm upgrade -i keda kedacore/keda \\ --namespace keda --create-namespace # Uninstallation. helm uninstall keda --namespace keda \\ && kubectl delete namespace keda Manual deployment \u00b6 Use the YAML declaration (which includes the CRDs and all the other resources) available on the GitHub releases page: # Installation. kubectl apply -f https://github.com/kedacore/keda/releases/download/v2.0.0/keda-2.0.0.yaml # Uninstallation. kubectl delete -f https://github.com/kedacore/keda/releases/download/v2.0.0/keda-2.0.0.yaml One can also use the tools in the repository: git clone https://github.com/kedacore/keda cd keda VERSION = 2 .0.0 make deploy # installation VERSION = 2 .0.0 make undeploy # uninstallation Usage \u00b6 One can just add a resource to their deployment using the Custom Resource Definitions KEDA offers: ScaledObject for Deployments, StatefulSets and Custom Resources ScaledJob for Jobs ScaledObject \u00b6 For details and updated information see KEDA's Scaling Deployments, StatefulSets and Custom Resources page. The ScaledObject Custom Resource definition is what defines how KEDA should scale one's application and what the triggers (A.K.A. scalers) are. The full list of scalers is available here : apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : {{ scaledObject.name }} spec : scaleTargetRef : apiVersion : {{ targetResource.apiVersion }} # optional; defaults to 'apps/v1' kind : {{ targetResource.Kind }} # optional; defaults to 'Deployment' name : {{ targetResource.Name }} # mandatory; the target resource must reside in the same namespace as the ScaledObject envSourceContainerName : {{ container.name }} # optional; defaults to the target's '.spec.template.spec.containers[0]' field pollingInterval : 30 # optional; defaults to 30 seconds cooldownPeriod : 300 # optional; defaults to 300 seconds minReplicaCount : 0 # optional; defaults to 0 maxReplicaCount : 100 # optional; defaults to 100 advanced : # optional restoreToOriginalReplicaCount : false # optional; defaults to false horizontalPodAutoscalerConfig : # optional behavior : # optional; modifies the HPA's default scaling behavior scaleDown : stabilizationWindowSeconds : 300 policies : - type : Percent value : 100 periodSeconds : 15 triggers : [] # mandatory; list of the triggers (= scalers) which will scale the target resource Custom Resources are scaled the same way as Deployments and StatefulSets, as long as the target Custom Resource defines the /scale subresource . When a ScaledObject is already in place and one first creates the target resource, KEDA will immediately scale it to the value of the minReplicaCount specification and will then scale it up according to the triggers. The scaleTargetRef specification references the resource KEDA will scale up/down and setup an HPA for, based on the triggers defined in triggers . The resource referenced by name (and apiVersion and kind ) must reside in the same namespace as the ScaledObject. envSourceContainerName specifies the name of the container inside the target resource from which KEDA will retrieve the environment properties holding secrets etc. If not defined, KEDA will try to retrieve the environment properties from the first Container in the resource's definition. pollingInterval is the interval to check each trigger on. In a queue scenario, for example, KEDA will check the queueLength every pollingInterval seconds, and scale the resource up or down accordingly. cooldownPeriod sets how much time to wait after the last trigger reported active, before scaling the resource back to minReplicaCount . This only applies after a trigger occurs and when scaling down to a minReplicaCount value of 0: scaling from 1 to N replicas is handled by the Kubernetes Horizontal Pod Autoscaler. minReplicaCount is the minimum amount of replicas KEDA will scale the resource down to. If a non default value (> 0) is used, it will not be enforced, meaning one can manually scale the resource to 0 and KEDA will not scale it back up. However, KEDA will respect the value set there when scaling the resource afterwards. maxReplicaCount sets the maximum amount of replicas for the resource. This setting is passed to the HPA definition that KEDA will create for the target. restoreToOriginalReplicaCount specifies whether the target resource should be scaled back to original replicas count after the ScaledObject is deleted. The default behavior is to keep the replica count at the number it is set to at the moment of the ScaledObject's deletion. If running on Kubernetes v1.18+, the horizontalPodAutoscalerConfig.behavior field allows the HPA's scaling behavior to be configured feeding the values from this section directly to the HPA's behavior field. Example: apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : nextcloud namespace : nextcloud spec : scaleTargetRef : name : nextcloud minReplicaCount : 1 advanced : restoreToOriginalReplicaCount : true triggers : - type : prometheus metadata : serverAddress : http://prometheus-server.monitoring:9090 metricName : http_requests_total query : sum(rate(http_requests_total{deployment=\"nextcloud\"}[2m])) threshold : '10' ScaledJobs \u00b6 For details and updated information see KEDA's Scaling Jobs page. The ScaledJob Custom Resource definition is what defines how KEDA should scale a Job and what the triggers ( scalers ) are. The full list of scalers is available here . Instead of scaling up the number of replicas, KEDA will schedule a single Job for each detected event. For this, a ScaledJob is primarily used for long running executions or small tasks being able to run in parallel in massive spikes like processing queue messages: apiVersion : keda.sh/v1alpha1 kind : ScaledJob metadata : name : {{ scaledJob.name }} spec : jobTargetRef : parallelism : 1 # [max number of desired pods](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#controlling-parallelism) completions : 1 # [desired number of successfully finished pods](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#controlling-parallelism) activeDeadlineSeconds : 600 # specifies the duration in seconds relative to the startTime field that the job may be active before the system tries to terminate it; its value must be a positive integer backoffLimit : 6 # specifies the number of retries before marking this job failed; defaults to 6 template : # describes the [job template](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/) pollingInterval : 30 # optional; defaults to 30 seconds successfulJobsHistoryLimit : 5 # optional; how many completed jobs should be kept as history; defaults to 100 failedJobsHistoryLimit : 5 # optional; how many failed jobs should be kept as history; defaults to 100 envSourceContainerName : {{ container.name }} # optional; defaults to the target's '.spec.JobTargetRef.template.spec.containers[0]' field maxReplicaCount : 100 # optional; defaults to 100 scalingStrategy : strategy : \"custom\" # optional; which Scaling Strategy to use; defaults to 'default' customScalingQueueLengthDeduction : 1 # optional; a parameter to optimize custom ScalingStrategy. customScalingRunningJobPercentage : \"0.5\" # optional; a parameter to optimize custom ScalingStrategy. triggers : [] # list of the triggers (= scalers) which will spawn jobs pollingInterval is the interval in seconds KEDA will check each trigger on. successfulJobsHistoryLimit and failedJobsHistoryLimit specify how many completed and failed jobs should be kept, similarly to Jobs History Limits; it allows to learn what the outcome of the jobs are. The actual number of jobs could exceed the limit in a short time, but it is going to resolve in the cleanup period. Currently, the cleanup period is the same as the Polling interval. envSourceContainerName specifies the name of container in the target Job from which KEDA will retrieve the environment properties holding secrets etc. If not defined, KEDA will try to retrieve the environment properties from the first Container in the target resource's definition. maxReplicaCount is the max number of Job Pods to be in existence within a single polling period. If there are already some running Jobs others will be created only up to this numbers, or none if their number exceeds this value. scalingStrategy is one from default , custom , or accurate . For details and updated information see this PR . Example: apiVersion : v1 kind : Secret metadata : name : rabbitmq-consumer data : RabbitMqHost : <omitted> --- apiVersion : keda.sh/v1alpha1 kind : ScaledJob metadata : name : rabbitmq-consumer namespace : default spec : jobTargetRef : template : spec : containers : - name : rabbitmq-client image : tsuyoshiushio/rabbitmq-client:dev3 imagePullPolicy : Always command : [ \"receive\" , \"amqp://user:PASSWORD@rabbitmq.default.svc.cluster.local:5672\" , \"job\" ] envFrom : - secretRef : name : rabbitmq-consumer restartPolicy : Never backoffLimit : 4 pollingInterval : 10 maxReplicaCount : 30 successfulJobsHistoryLimit : 3 failedJobsHistoryLimit : 2 scalingStrategy : strategy : \"custom\" customScalingQueueLengthDeduction : 1 customScalingRunningJobPercentage : \"0.5\" triggers : - type : rabbitmq metadata : queueName : hello host : RabbitMqHost queueLength : '5' Authentication \u00b6 For details and updated information see KEDA's Authentication page. External Scalers \u00b6 For details and updated information see KEDA's External Scalers page. Troubleshooting \u00b6 Access logging and telemetry \u00b6 Use the logs for the keda operator or apiserver: kubectl logs --namespace keda keda-operator-8488964969-sqbxq kubectl logs --namespace keda keda-operator-metrics-apiserver-5b488bc7f6-8vbpl Long running executions \u00b6 There is at the moment of writing no way to control which of the replicas get terminated when a HPA decides to scale down a resource. This means the HPA may attempt to terminate a replica that is deep into processing a long execution (e.g. a 3 hour queue message). To handle this: leverage lifecycle hooks to delay termination use a Job to do the processing instead of a Deployment/StatefulSet/Custom Resource. Manually uninstall everything \u00b6 Just run the following: kubectl delete -f https://raw.githubusercontent.com/kedacore/keda/main/config/crd/bases/keda.sh_scaledobjects.yaml kubectl delete -f https://raw.githubusercontent.com/kedacore/keda/main/config/crd/bases/keda.sh_scaledjobs.yaml kubectl delete -f https://raw.githubusercontent.com/kedacore/keda/main/config/crd/bases/keda.sh_triggerauthentications.yaml and then delete the namespace. Further readings \u00b6 KEDA's concepts Authentication External Scalers Scaling Deployments, StatefulSets and Custom Resources Scaling Jobs The complete scalers list The project's website The project's FAQ s Sources \u00b6 KEDA: Event Driven and Serverless Containers in Kubernetes by Jeff Hollan, Microsoft The /scale subresource The ScaledObject specification","title":"KEDA <!-- omit in toc -->"},{"location":"learning/tools/keda/#keda","text":"The Kubernetes-based Event Driven Auto-Scaler will automatically scale a resource in a Kubernetes cluster based on a scale trigger: KEDA will monitor the event source, and feed that data to Kubernetes to scale the resource out/in accordingly, leveraging standard Kubernetes components (e.g. HPA) and extending the existing functionality without overwriting or duplicating components. Any Kubernetes cluster >= 1.16.0 should work. Table of contents: How KEDA works Deployment Helm chart Manual deployment Usage ScaledObject ScaledJobs Authentication External Scalers Troubleshooting Access logging and telemetry Long running executions Manually uninstall everything Further readings Sources","title":"KEDA "},{"location":"learning/tools/keda/#how-keda-works","text":"For details and updated information see KEDA's concepts page. Upon installation, KEDA creates the following custom resources to enable one to map an event source to a Deployment, StatefulSet, Custom Resource or Job for scaling: scaledobjects.keda.sh scaledjobs.keda.sh triggerauthentications.keda.sh ScaledObjects represent a mapping between an event source (e.g. Rabbit MQ) and any K8S resource (Deployment, StatefulSet or Custom) defining the /scale subresource; ScaledJobs specifically represent a mapping between an event source and a Kubernetes Job. TriggerAuthentication are referenced by ScaledObjects and ScaledJobs when they need to access authentication configurations or secrets to monitor the event source. KEDA also creates the following containers: keda-operator keda-operator-metrics-apiserver The operator acts as an agent which activates, regulates and deactivates the scaling of K8S resources defined in a ScaledObject based on the trigger events. The metrics apiserver exposes rich event data, like queue length or stream lag, to the Horizontal Pod Autoscaler to drive the scale out. It is then up to the resource to consume such events directly from the source. KEDA offers a wide range of triggers (A.K.A. scalers ) that can both detect if a resource should be activated or deactivated and feed custom metrics for a specific event source. The full list of scalers is available here .","title":"How KEDA works"},{"location":"learning/tools/keda/#deployment","text":"","title":"Deployment"},{"location":"learning/tools/keda/#helm-chart","text":"# Installation. helm repo add kedacore https://kedacore.github.io/charts \\ && helm repo update kedacore \\ && helm upgrade -i keda kedacore/keda \\ --namespace keda --create-namespace # Uninstallation. helm uninstall keda --namespace keda \\ && kubectl delete namespace keda","title":"Helm chart"},{"location":"learning/tools/keda/#manual-deployment","text":"Use the YAML declaration (which includes the CRDs and all the other resources) available on the GitHub releases page: # Installation. kubectl apply -f https://github.com/kedacore/keda/releases/download/v2.0.0/keda-2.0.0.yaml # Uninstallation. kubectl delete -f https://github.com/kedacore/keda/releases/download/v2.0.0/keda-2.0.0.yaml One can also use the tools in the repository: git clone https://github.com/kedacore/keda cd keda VERSION = 2 .0.0 make deploy # installation VERSION = 2 .0.0 make undeploy # uninstallation","title":"Manual deployment"},{"location":"learning/tools/keda/#usage","text":"One can just add a resource to their deployment using the Custom Resource Definitions KEDA offers: ScaledObject for Deployments, StatefulSets and Custom Resources ScaledJob for Jobs","title":"Usage"},{"location":"learning/tools/keda/#scaledobject","text":"For details and updated information see KEDA's Scaling Deployments, StatefulSets and Custom Resources page. The ScaledObject Custom Resource definition is what defines how KEDA should scale one's application and what the triggers (A.K.A. scalers) are. The full list of scalers is available here : apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : {{ scaledObject.name }} spec : scaleTargetRef : apiVersion : {{ targetResource.apiVersion }} # optional; defaults to 'apps/v1' kind : {{ targetResource.Kind }} # optional; defaults to 'Deployment' name : {{ targetResource.Name }} # mandatory; the target resource must reside in the same namespace as the ScaledObject envSourceContainerName : {{ container.name }} # optional; defaults to the target's '.spec.template.spec.containers[0]' field pollingInterval : 30 # optional; defaults to 30 seconds cooldownPeriod : 300 # optional; defaults to 300 seconds minReplicaCount : 0 # optional; defaults to 0 maxReplicaCount : 100 # optional; defaults to 100 advanced : # optional restoreToOriginalReplicaCount : false # optional; defaults to false horizontalPodAutoscalerConfig : # optional behavior : # optional; modifies the HPA's default scaling behavior scaleDown : stabilizationWindowSeconds : 300 policies : - type : Percent value : 100 periodSeconds : 15 triggers : [] # mandatory; list of the triggers (= scalers) which will scale the target resource Custom Resources are scaled the same way as Deployments and StatefulSets, as long as the target Custom Resource defines the /scale subresource . When a ScaledObject is already in place and one first creates the target resource, KEDA will immediately scale it to the value of the minReplicaCount specification and will then scale it up according to the triggers. The scaleTargetRef specification references the resource KEDA will scale up/down and setup an HPA for, based on the triggers defined in triggers . The resource referenced by name (and apiVersion and kind ) must reside in the same namespace as the ScaledObject. envSourceContainerName specifies the name of the container inside the target resource from which KEDA will retrieve the environment properties holding secrets etc. If not defined, KEDA will try to retrieve the environment properties from the first Container in the resource's definition. pollingInterval is the interval to check each trigger on. In a queue scenario, for example, KEDA will check the queueLength every pollingInterval seconds, and scale the resource up or down accordingly. cooldownPeriod sets how much time to wait after the last trigger reported active, before scaling the resource back to minReplicaCount . This only applies after a trigger occurs and when scaling down to a minReplicaCount value of 0: scaling from 1 to N replicas is handled by the Kubernetes Horizontal Pod Autoscaler. minReplicaCount is the minimum amount of replicas KEDA will scale the resource down to. If a non default value (> 0) is used, it will not be enforced, meaning one can manually scale the resource to 0 and KEDA will not scale it back up. However, KEDA will respect the value set there when scaling the resource afterwards. maxReplicaCount sets the maximum amount of replicas for the resource. This setting is passed to the HPA definition that KEDA will create for the target. restoreToOriginalReplicaCount specifies whether the target resource should be scaled back to original replicas count after the ScaledObject is deleted. The default behavior is to keep the replica count at the number it is set to at the moment of the ScaledObject's deletion. If running on Kubernetes v1.18+, the horizontalPodAutoscalerConfig.behavior field allows the HPA's scaling behavior to be configured feeding the values from this section directly to the HPA's behavior field. Example: apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : nextcloud namespace : nextcloud spec : scaleTargetRef : name : nextcloud minReplicaCount : 1 advanced : restoreToOriginalReplicaCount : true triggers : - type : prometheus metadata : serverAddress : http://prometheus-server.monitoring:9090 metricName : http_requests_total query : sum(rate(http_requests_total{deployment=\"nextcloud\"}[2m])) threshold : '10'","title":"ScaledObject"},{"location":"learning/tools/keda/#scaledjobs","text":"For details and updated information see KEDA's Scaling Jobs page. The ScaledJob Custom Resource definition is what defines how KEDA should scale a Job and what the triggers ( scalers ) are. The full list of scalers is available here . Instead of scaling up the number of replicas, KEDA will schedule a single Job for each detected event. For this, a ScaledJob is primarily used for long running executions or small tasks being able to run in parallel in massive spikes like processing queue messages: apiVersion : keda.sh/v1alpha1 kind : ScaledJob metadata : name : {{ scaledJob.name }} spec : jobTargetRef : parallelism : 1 # [max number of desired pods](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#controlling-parallelism) completions : 1 # [desired number of successfully finished pods](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#controlling-parallelism) activeDeadlineSeconds : 600 # specifies the duration in seconds relative to the startTime field that the job may be active before the system tries to terminate it; its value must be a positive integer backoffLimit : 6 # specifies the number of retries before marking this job failed; defaults to 6 template : # describes the [job template](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/) pollingInterval : 30 # optional; defaults to 30 seconds successfulJobsHistoryLimit : 5 # optional; how many completed jobs should be kept as history; defaults to 100 failedJobsHistoryLimit : 5 # optional; how many failed jobs should be kept as history; defaults to 100 envSourceContainerName : {{ container.name }} # optional; defaults to the target's '.spec.JobTargetRef.template.spec.containers[0]' field maxReplicaCount : 100 # optional; defaults to 100 scalingStrategy : strategy : \"custom\" # optional; which Scaling Strategy to use; defaults to 'default' customScalingQueueLengthDeduction : 1 # optional; a parameter to optimize custom ScalingStrategy. customScalingRunningJobPercentage : \"0.5\" # optional; a parameter to optimize custom ScalingStrategy. triggers : [] # list of the triggers (= scalers) which will spawn jobs pollingInterval is the interval in seconds KEDA will check each trigger on. successfulJobsHistoryLimit and failedJobsHistoryLimit specify how many completed and failed jobs should be kept, similarly to Jobs History Limits; it allows to learn what the outcome of the jobs are. The actual number of jobs could exceed the limit in a short time, but it is going to resolve in the cleanup period. Currently, the cleanup period is the same as the Polling interval. envSourceContainerName specifies the name of container in the target Job from which KEDA will retrieve the environment properties holding secrets etc. If not defined, KEDA will try to retrieve the environment properties from the first Container in the target resource's definition. maxReplicaCount is the max number of Job Pods to be in existence within a single polling period. If there are already some running Jobs others will be created only up to this numbers, or none if their number exceeds this value. scalingStrategy is one from default , custom , or accurate . For details and updated information see this PR . Example: apiVersion : v1 kind : Secret metadata : name : rabbitmq-consumer data : RabbitMqHost : <omitted> --- apiVersion : keda.sh/v1alpha1 kind : ScaledJob metadata : name : rabbitmq-consumer namespace : default spec : jobTargetRef : template : spec : containers : - name : rabbitmq-client image : tsuyoshiushio/rabbitmq-client:dev3 imagePullPolicy : Always command : [ \"receive\" , \"amqp://user:PASSWORD@rabbitmq.default.svc.cluster.local:5672\" , \"job\" ] envFrom : - secretRef : name : rabbitmq-consumer restartPolicy : Never backoffLimit : 4 pollingInterval : 10 maxReplicaCount : 30 successfulJobsHistoryLimit : 3 failedJobsHistoryLimit : 2 scalingStrategy : strategy : \"custom\" customScalingQueueLengthDeduction : 1 customScalingRunningJobPercentage : \"0.5\" triggers : - type : rabbitmq metadata : queueName : hello host : RabbitMqHost queueLength : '5'","title":"ScaledJobs"},{"location":"learning/tools/keda/#authentication","text":"For details and updated information see KEDA's Authentication page.","title":"Authentication"},{"location":"learning/tools/keda/#external-scalers","text":"For details and updated information see KEDA's External Scalers page.","title":"External Scalers"},{"location":"learning/tools/keda/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/keda/#access-logging-and-telemetry","text":"Use the logs for the keda operator or apiserver: kubectl logs --namespace keda keda-operator-8488964969-sqbxq kubectl logs --namespace keda keda-operator-metrics-apiserver-5b488bc7f6-8vbpl","title":"Access logging and telemetry"},{"location":"learning/tools/keda/#long-running-executions","text":"There is at the moment of writing no way to control which of the replicas get terminated when a HPA decides to scale down a resource. This means the HPA may attempt to terminate a replica that is deep into processing a long execution (e.g. a 3 hour queue message). To handle this: leverage lifecycle hooks to delay termination use a Job to do the processing instead of a Deployment/StatefulSet/Custom Resource.","title":"Long running executions"},{"location":"learning/tools/keda/#manually-uninstall-everything","text":"Just run the following: kubectl delete -f https://raw.githubusercontent.com/kedacore/keda/main/config/crd/bases/keda.sh_scaledobjects.yaml kubectl delete -f https://raw.githubusercontent.com/kedacore/keda/main/config/crd/bases/keda.sh_scaledjobs.yaml kubectl delete -f https://raw.githubusercontent.com/kedacore/keda/main/config/crd/bases/keda.sh_triggerauthentications.yaml and then delete the namespace.","title":"Manually uninstall everything"},{"location":"learning/tools/keda/#further-readings","text":"KEDA's concepts Authentication External Scalers Scaling Deployments, StatefulSets and Custom Resources Scaling Jobs The complete scalers list The project's website The project's FAQ s","title":"Further readings"},{"location":"learning/tools/keda/#sources","text":"KEDA: Event Driven and Serverless Containers in Kubernetes by Jeff Hollan, Microsoft The /scale subresource The ScaledObject specification","title":"Sources"},{"location":"learning/tools/keybase/","text":"Keybase \u00b6 TL;DR \u00b6 # Start the services. run_keybase run_keybase -fg # Authenticate your local service against the keybase server. keybase login keybase login --devicename \" $( hostname ) \" --paperkey 'paper key' -s KEYBASE_DEVICENAME = $( hostname ) KEYBASE_PAPERKEY = 'paper key' keybase login # Establish a temporary device. keybase oneshot keybase oneshot -u user --paperkey 'paper key' KEYBASE_USERNAME = 'user' KEYBASE_PAPERKEY = 'paper key' keybase oneshot # List git repositories. keybase git list # Run garbage collection on git repository. keybase git gc 'awesomerepo' # Enable LFS support for a repository. # Run it from the repository's root. keybase git lfs-config # Clone a repository with LFS-enabled files. git clone --no-checkout 'keybase://private/user/repo' \\ && cd 'repo' && keybase git lfs-config && cd - \\ && git -C 'repo' checkout -f HEAD # Import an existing repository in Keybase keybase git create 'repo' \\ && git clone --mirror 'https://github.com/user/repo' '/tmp/repo.git' \\ && git -C '/tmp/repo.git' push --mirror 'keybase://private/user/repo' # Run as root. KEYBASE_ALLOW_ROOT = '1' keybase oneshot # Run the service in a container. podman run -d --rm --name 'keybase' \\ -e KEYBASE_SERVICE = '1' \\ -e KEYBASE_USERNAME = 'user' \\ -e KEYBASE_PAPERKEY = 'paper key \u2026' \\ 'keybaseio/client:stable' # Execute commands using the containerized service. podman exec \\ --user keybase \\ keybase \\ keybase whoami Service execution \u00b6 run_keybase starts the Keybase service, KBFS and the GUI. If services are already running, they will be restarted. Options can also be controlled by setting the related environment variable to 1: Option Description Environment variable -a keep the GUI minimized in system tray after startup KEYBASE_AUTOSTART=1 -f do not start KBFS KEYBASE_NO_KBFS=1 -g do not start the gui KEYBASE_NO_GUI=1 -h print this help text - -k shut down all Keybase services KEYBASE_KILL=1 Import an existing repository in Keybase \u00b6 Use the import form in Keybase launches encrypted git , or: Create the remote repository: keybase git create dotfiles Copy the existing repository to a temporary directory: git clone --mirror https://github.com/user/dotfiles _tmp.git Push the contents of the old repository to the new one: git -C _tmp.git push --mirror keybase://private/user/dotfiles Run as root \u00b6 Keybase shouldn't be run as the root , and by default it will fail with a message explaining it. Under some circumnstances (like Docker or other containers) root can be the best or only option; run commands in concert with the KEYBASE_ALLOW_ROOT=1 environment variable to force the execution. Temporary devices \u00b6 Use keybase oneshot to establish a temporary device. The resulting process won't write credential information on the local storage disk nor it will make any changes to the user's sigchain; rather, it will hold the given paperkey in memory for as long as the corresponding keybase service process is running or until keybase logout is called; when this happens, it will disappear. keybase oneshot needs a username and a paperkey to work, either passed in via standard input, command-line flags, or environment variables: # Provide login information on the standard input. keybase oneshot # Use flags. keybase oneshot --username user --paperkey 'paper key' # Use environment variables KEYBASE_PAPERKEY = 'paper key' KEYBASE_USERNAME = 'user' keybase oneshot Exploding messages work in oneshot mode with the caveat that you cannot run multiple instances of such with the same paperkey at the same time as each instance will try to create ephemeral keys, but require a distinct paperkey to uniquely identify itself as a separate device. In addition, ephemeral keys are purged entirely when closing the oneshot session, and you will not be able to access any old ephemeral content when starting keybase up again. Further readings \u00b6 Website Linux guide Sources \u00b6 Keybase LFS support Keybase launches encrypted git How to use Keybase to encrypt files on Linux","title":"Keybase"},{"location":"learning/tools/keybase/#keybase","text":"","title":"Keybase"},{"location":"learning/tools/keybase/#tldr","text":"# Start the services. run_keybase run_keybase -fg # Authenticate your local service against the keybase server. keybase login keybase login --devicename \" $( hostname ) \" --paperkey 'paper key' -s KEYBASE_DEVICENAME = $( hostname ) KEYBASE_PAPERKEY = 'paper key' keybase login # Establish a temporary device. keybase oneshot keybase oneshot -u user --paperkey 'paper key' KEYBASE_USERNAME = 'user' KEYBASE_PAPERKEY = 'paper key' keybase oneshot # List git repositories. keybase git list # Run garbage collection on git repository. keybase git gc 'awesomerepo' # Enable LFS support for a repository. # Run it from the repository's root. keybase git lfs-config # Clone a repository with LFS-enabled files. git clone --no-checkout 'keybase://private/user/repo' \\ && cd 'repo' && keybase git lfs-config && cd - \\ && git -C 'repo' checkout -f HEAD # Import an existing repository in Keybase keybase git create 'repo' \\ && git clone --mirror 'https://github.com/user/repo' '/tmp/repo.git' \\ && git -C '/tmp/repo.git' push --mirror 'keybase://private/user/repo' # Run as root. KEYBASE_ALLOW_ROOT = '1' keybase oneshot # Run the service in a container. podman run -d --rm --name 'keybase' \\ -e KEYBASE_SERVICE = '1' \\ -e KEYBASE_USERNAME = 'user' \\ -e KEYBASE_PAPERKEY = 'paper key \u2026' \\ 'keybaseio/client:stable' # Execute commands using the containerized service. podman exec \\ --user keybase \\ keybase \\ keybase whoami","title":"TL;DR"},{"location":"learning/tools/keybase/#service-execution","text":"run_keybase starts the Keybase service, KBFS and the GUI. If services are already running, they will be restarted. Options can also be controlled by setting the related environment variable to 1: Option Description Environment variable -a keep the GUI minimized in system tray after startup KEYBASE_AUTOSTART=1 -f do not start KBFS KEYBASE_NO_KBFS=1 -g do not start the gui KEYBASE_NO_GUI=1 -h print this help text - -k shut down all Keybase services KEYBASE_KILL=1","title":"Service execution"},{"location":"learning/tools/keybase/#import-an-existing-repository-in-keybase","text":"Use the import form in Keybase launches encrypted git , or: Create the remote repository: keybase git create dotfiles Copy the existing repository to a temporary directory: git clone --mirror https://github.com/user/dotfiles _tmp.git Push the contents of the old repository to the new one: git -C _tmp.git push --mirror keybase://private/user/dotfiles","title":"Import an existing repository in Keybase"},{"location":"learning/tools/keybase/#run-as-root","text":"Keybase shouldn't be run as the root , and by default it will fail with a message explaining it. Under some circumnstances (like Docker or other containers) root can be the best or only option; run commands in concert with the KEYBASE_ALLOW_ROOT=1 environment variable to force the execution.","title":"Run as root"},{"location":"learning/tools/keybase/#temporary-devices","text":"Use keybase oneshot to establish a temporary device. The resulting process won't write credential information on the local storage disk nor it will make any changes to the user's sigchain; rather, it will hold the given paperkey in memory for as long as the corresponding keybase service process is running or until keybase logout is called; when this happens, it will disappear. keybase oneshot needs a username and a paperkey to work, either passed in via standard input, command-line flags, or environment variables: # Provide login information on the standard input. keybase oneshot # Use flags. keybase oneshot --username user --paperkey 'paper key' # Use environment variables KEYBASE_PAPERKEY = 'paper key' KEYBASE_USERNAME = 'user' keybase oneshot Exploding messages work in oneshot mode with the caveat that you cannot run multiple instances of such with the same paperkey at the same time as each instance will try to create ephemeral keys, but require a distinct paperkey to uniquely identify itself as a separate device. In addition, ephemeral keys are purged entirely when closing the oneshot session, and you will not be able to access any old ephemeral content when starting keybase up again.","title":"Temporary devices"},{"location":"learning/tools/keybase/#further-readings","text":"Website Linux guide","title":"Further readings"},{"location":"learning/tools/keybase/#sources","text":"Keybase LFS support Keybase launches encrypted git How to use Keybase to encrypt files on Linux","title":"Sources"},{"location":"learning/tools/keychron/","text":"Keychron \u00b6 set the keyboard to Windows mode using the side switch hold Fn + X + L for 4 seconds to set the function key row to fn mode ensure the hid_apple module is loaded sudo modprobe hid_apple # load at boot echo 'hid_apple' | sudo tee /etc/modules-load.d/keychron.conf configure the keyboard's fn mode : echo 0 | sudo tee /sys/module/hid_apple/parameters/fnmode # load at boot echo 'options hid_apple fnmode=0' | sudo tee /etc/modprobe.d/keychron.conf Further readings \u00b6 K8 keyboard user manual Sources \u00b6 Keychron fn keys in Linux Apple Keyboard on the Archlinux wiki","title":"Keychron"},{"location":"learning/tools/keychron/#keychron","text":"set the keyboard to Windows mode using the side switch hold Fn + X + L for 4 seconds to set the function key row to fn mode ensure the hid_apple module is loaded sudo modprobe hid_apple # load at boot echo 'hid_apple' | sudo tee /etc/modules-load.d/keychron.conf configure the keyboard's fn mode : echo 0 | sudo tee /sys/module/hid_apple/parameters/fnmode # load at boot echo 'options hid_apple fnmode=0' | sudo tee /etc/modprobe.d/keychron.conf","title":"Keychron"},{"location":"learning/tools/keychron/#further-readings","text":"K8 keyboard user manual","title":"Further readings"},{"location":"learning/tools/keychron/#sources","text":"Keychron fn keys in Linux Apple Keyboard on the Archlinux wiki","title":"Sources"},{"location":"learning/tools/kubectl/","text":"Kubectl \u00b6 Command line tool for communicating with a Kubernetes cluster's control plane using the Kubernetes API. Resource types are case in sensitive and can be specified in their singular, plural or abbreviated form for convenience: # The two commands below are equivalent. kubectl get deployment,replicasets,pods -A kubectl get deploy,rs,po -A Use kubectl api-resources to check out the available resources and their abbreviations. Multiple resources types can be specified together, but only one resource name is accepted at a time. Resource names are case sensitive and will filter the requested resources; use the -l , --selector option to get around filtering: kubectl get deployments,replicasets -A kubectl get pod etcd-minikube -n kube-system kubectl get pods -l app = nginx,tier = frontend Table of contents \u00b6 TL;DR Configuration Configure access to multiple clusters Create resources Output formatting Verbosity and debugging Further readings Sources TL;DR \u00b6 # Enable shell completion. source < ( kubectl completion bash ) echo \"[[ $commands [kubectl] ]] && source <(kubectl completion zsh)\" >> ~/.zshrc # Shot the merged configuration. kubectl config view # Get specific values from the configuration. kubectl config view -o jsonpath = '{.users[].name}' kubectl config view -o jsonpath = '{.users[*].name}' kubectl config view -o jsonpath = '{.users[?(@.name == \"e2e\")].user.password}' # Set configuration values. kubectl config set-context --current --namespace = keda kubectl config set-context gce --user = cluster-admin --namespace = foo kubectl config set-credentials \\ kubeuser/foo.kubernetes.com --username = kubeuser --password = kubepassword # Delete configuration values. kubectl config unset users.foo # Use multiple config files at once. # This will temporarily merge them in one big configuration file. KUBECONFIG = \"path/to/config1:path/to/configN\" # List contexts. kubectl config get-contexts kubectl config current-context # Set context as the default one. kubectl config use-context docker-desktop kubectl config use-context gce # Display addresses of the master and services. kubectl cluster-info # Dump the complete current cluster state. kubectl cluster-info dump kubectl cluster-info dump --output-directory = /path/to/cluster-state # List supported resources types along with their short name, API group, Kind, # and whether they are namespaced. kubectl api-resources kubectl api-resources --namespaced = true kubectl api-resources -o name kubectl api-resources -o wide kubectl api-resources --verbs = list,get # Show the documentation about resources or their fields. kubectl explain pods kubectl explain pods.spec.containers # List and filter resources. kubectl get pods kubectl get pod/coredns-845757d86-47np2 -n kube-system kubectl get namespaces,pods --show-labels kubectl get services -A -o wide kubectl get rs --sort-by = .metadata.name kubectl get pv --sort-by = .spec.capacity.storage --no-headers kubectl get po --sort-by = '.status.containerStatuses[0].restartCount' kubectl get events --sort-by .metadata.creationTimestamp kubectl get pods --field-selector = status.phase = Running kubectl get node -l = '!node-role.kubernetes.io/master' kubectl get replicasets -l 'environment in (prod, qa)' kubectl get deploy --selector 'tier,tier notin (frontend)' # Extract information from resources' definition. kubectl get deployment nginx -o yaml kubectl get cm kube-root-ca.crt -o jsonpath = '{.data.ca\\.crt}' kubectl get po -o = jsonpath = '{.items..metadata.name}' kubectl get po -l app = redis -o jsonpath = '{.items[*].metadata.labels.version}' kubectl get nodes \\ -o jsonpath = '{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' # List all fields under '.metadata' regardless of their name. kubectl get pods -A -o = custom-columns = 'DATA:metadata.*' # List images being run in a cluster. kubectl get po -A -o = custom-columns = 'DATA:spec.containers[*].image' kubectl get po -A -o = custom-columns = 'DATA:spec.containers[?(@.image!=\"k8s.gcr.io/coredns:1.6.2\")].image' # List all pods in status 'Shutdown'. kubectl get po -A \\ -o jsonpath = '{.items[?(@.status.reason==\"Shutdown\")].metadata.name}' # List ready nodes. kubectl get nodes \\ -o jsonpath = '{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ | grep \"Ready=True\" # List all secrets currently in use by a Pod. kubectl get pods -o json \\ | jq '.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name' \\ | grep -v null | sort | uniq # List the name of Pods belonging to a particular RC. SELECTOR = ${ $( kubectl get rc my-rc --output = json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"' ) %? } kubectl get pods -l = $SELECTOR \\ -o = jsonpath = '{.items..metadata.name}' # List the containerID of initContainers from all Pods. # Helpful when cleaning up stopped containers while avoiding the removal of # initContainers kubectl get pods --all-namespaces \\ -o jsonpath = '{range .items[*].status.initContainerStatuses[*]}{.containerID}{\"\\n\"}{end}' \\ | cut -d/ -f3 # Produce a period-delimited tree of all keys returned for nodes. # Helpful when trying to locate a specific key within a complex nested JSON # structure. kubectl get nodes -o json | jq -c 'path(..)|[.[]|tostring]|join(\".\")' # Show detailed information about resources. kubectl describe node pi kubectl describe deploy,rs,po -l app = redis # Create resources from manifests. kubectl apply -f manifest.yaml kubectl apply -f path/to/m1.yaml -f ./m2.yaml kubectl apply -f dir/ kubectl apply -f https://git.io/vPieo cat <<-EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: password: $(echo -n \"s33msi4\" | base64 -w0) username: $(echo -n \"jane\" | base64 -w0) EOF # Compare the current state of the cluster against the state it would be in if # a manifest was applied kubectl diff -f ./manifest.yaml # Start a Pod. kubectl run nginx --image nginx kubectl run busybox --rm -it --image = busybox -n keda -- sh # Start a Pod and write its specs into a file. kubectl run nginx --image = nginx --dry-run = client -o yaml > pod.yaml # Create a single instance deployment of 'nginx'. kubectl create deployment nginx --image = nginx # Start a Job using an existing Job as template kubectl create job backup-before-upgrade-13.6.2-to-13.9.2 \\ --from = cronjob.batch/backup -n gitlab # Wait for a pod to be 'ready'. kubectl wait --for 'condition=ready' --timeout 120s \\ pod -l 'app.kubernetes.io/component=controller' # Update the 'image' field of the 'www' containers from the 'frontend' # Deployment. # This starts a rolling update. kubectl set image deployment/frontend www = image:v2 # Show the history of resources, including the revision. kubectl rollout history deployment/frontend # Rollback resources to the latest previous version. kubectl rollout undo deployment/frontend # Rollback resources to a specific revision. kubectl rollout undo deployment/frontend --to-revision = 2 # Follow the rolling update status of the 'frontend' Deployment until its # completion. kubectl rollout status -w deployment/frontend # Start a rolling restart of the 'frontend' Deployment. kubectl rollout restart deployment/frontend # Replace a Pod based on the JSON passed into stdin. cat pod.json | kubectl replace -f - # Force replacement, deletion and recreation (in this order) of resources. # This Will cause a service outage. kubectl replace --force -f ./pod.json # Create a service for a replicated 'nginx'. # Set it to serve on port 80 and connect to the containers on port 8000. kubectl expose rc nginx --port = 80 --target-port = 8000 # Update a single-container Pod's image tag. kubectl get pod mypod -o yaml \\ | sed 's/\\(image: myimage\\):.*$/\\1:v4/' \\ | kubectl replace -f - # Add Labels to resources. kubectl label pods nginx custom-name = awesome # Add Annotations. kubectl annotate pods alpine icon-url = http://goo.gl/XXBTWq # Autoscale resources. kubectl autoscale deployment foo --min = 2 --max = 10 # Partially update resources. kubectl patch node k8s-node-1 -p '{\"spec\":{\"unschedulable\":true}}' # Update a container's image. # 'spec.containers[*].name' is required to specify the path of the merged key. kubectl patch pod valid-pod \\ -p '{\"spec\":{\"containers\": [{\"name\": \"kubernetes-serve-hostname\",\"image\":\"new image\"}]}}' # Update a container's image using a JSONPatch with positional arrays. kubectl patch pod valid-pod --type = 'json' \\ -p = '[{\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"new image\"}]' # Disable a Deployment's livenessProbe using a JSONPatch with positional arrays. kubectl patch deployment valid-deployment --type json \\ -p = '[{\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"}]' # Add a new element to a positional array. kubectl patch sa default --type = 'json' \\ -p = '[{\"op\": \"add\", \"path\": \"/secrets/1\", \"value\": {\"name\": \"whatever\" } }]' # Edit the service named docker-registry. kubectl edit svc/docker-registry KUBE_EDITOR = \"nano\" kubectl edit svc/docker-registry # Scale a replicaset named 'foo' to 3 kubectl scale --replicas = 3 rs/foo # Scale a resource specified in \"foo.yaml\" to 3 replicas. kubectl scale --replicas = 3 -f foo.yaml # If the Deployment named 'mysql''s current size is 2, scale it to 3. kubectl scale --current-replicas = 2 --replicas = 3 deployment/mysql # Scale multiple ReplicationControllers at once. kubectl scale --replicas = 5 rc/foo rc/bar rc/baz # Delete a Pod using the type and name specified in pod.json. kubectl delete -f ./pod.json # Delete Pods and Services named 'baz' and 'foo'. kubectl delete pod,service baz foo # Delete pods and services with Label name=myLabel. kubectl delete pods,services -l name = myLabel # Delete all pods and services in namespace my-ns. kubectl -n my-ns delete pod,svc --all # Delete all pods matching awk's pattern1 or pattern2. kubectl get pods --no-headers \\ | awk '/pattern1|pattern2/{print $1}' \\ | xargs -n1 kubectl delete pods # Delete non-default service accounts. kubectl get serviceaccounts \\ -o jsonpath = \"{.items[?(@.metadata.name!='default')].metadata.name}\" \\ | xargs -n1 kubectl delete serviceaccounts # Attach to a running container. kubectl attach my-pod -i # Run command in existing Pods. kubectl exec my-pod -- ls / kubectl exec my-pod -c my-container -- ls / # Show metrics for a given Pod and its containers. kubectl top pod busybox --containers # Get logs from resources. kubectl logs redis-0 kubectl logs -l name = myLabel kubectl logs my-pod -c my-container # Follow logs. kubectl logs -f my-pod kubectl logs -f my-pod -c my-container kubectl logs -f -l name = myLabel --all-containers # Get logs for a previous instantiation of a container. kubectl logs nginx --previous # Get the logs of the first Pod matching ID. kubectl logs $( kubectl get pods --no-headers | grep $ID | awk '{print $2}' ) # Verify user's permissions on the cluster. kubectl auth can-i create roles kubectl auth can-i list pods # Taint a Node. kubectl taint nodes node1 key1 = value1:NoSchedule # Taint all nodes in a certain nodepool (Azure AKS). kubectl get no -l \"agentpool=nodepool1\" -o jsonpath = '{.items[*].metadata.name}' | xargs -n1 -I {} -p kubectl taint nodes {} key1 = value1:NoSchedule # Remove a taint. # Notice the '-' sign at the end. kubectl taint nodes node1 key1 = value1:NoSchedule- # If a taint with that key and effect already exists, replace its value. kubectl taint nodes foo dedicated = special-user:NoSchedule # Mark Nodes as unschedulable. kubectl cordon my-node # Mark my-node as schedulable. kubectl uncordon my-node # Drain my-node in preparation for maintenance. kubectl drain my-node # Show metrics for a given node. kubectl top node my-node # Listen on port 5000 on the local machine and forward connections to port 6000 # of my-pod kubectl port-forward my-pod 5000 :6000 Configuration \u00b6 The configuration files are loaded as follows: If the --kubeconfig flag is set, then only that file is loaded; the flag may only be set once , and no merging takes place: kubectl config --kubeconfig config.local view If the $KUBECONFIG environment variable is set, then it is used as a list of paths following the normal path delimiting rules for your system; the files are merged: export KUBECONFIG = \"/tmp/config.local:.kube/config.prod\" When a value is modified, it is modified in the file that defines the stanza; when a value is created, it is created in the first existing file; if no file in the chain exist, then the last file in the list is created with the configuration. If none of the above happens, ~/.kube/config is used, and no merging takes place. The configuration file can be edited, or acted upon from the command line: # Show the merged configuration. kubectl config view KUBECONFIG = \"~/.kube/config:config.local\" kubectl config view # Show specific values only. kubectl config view -o jsonpath = '{.users[].name}' kubectl config view -o jsonpath = '{.users[?(@.name == \"e2e\")].user.password}' # Add a new user that supports basic auth. kubectl config set-credentials kubeuser/foo.kubernetes.com \\ --username = kubeuser --password = kubepassword # Delete user 'foo'. kubectl config unset users.foo # List the available contexts. kubectl config get-contexts # Display the current default context name. kubectl config current-context # Set the default context. kubectl config use-context minikube # Permanently setup specific contexts. kubectl config set-context --current --namespace = ggckad-s2 kubectl config set-context gce --user = cluster-admin --namespace = foo Configure access to multiple clusters \u00b6 See configure access to multiple clusters for details. Create resources \u00b6 The preferred way to create resources is to define them inside manifest s and then apply those: --- # file manifest.yaml --- apiVersion : v1 kind : Pod metadata : name : busybox-sleep spec : containers : - name : busybox image : busybox args : - sleep - \"1000000\" --- apiVersion : v1 kind : Pod metadata : name : busybox-sleep-less spec : containers : - name : busybox image : busybox args : - sleep - \"1000\" # Apply the manifest. kubectl apply -f manifest.yaml # Apply multiple manifests together. kubectl apply -f path/to/m1.yaml -f m2.yaml # Apply all manifests in a directory. kubectl apply -f ./dir # Apply a remote manifest. kubectl apply -f https://git.io/vPieo # Define a manifest using HEREDOC and apply it. cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: password: $(echo -n \"s33msi4\" | base64 -w0) username: $(echo -n \"jane\" | base64 -w0) EOF When subsequentially (re-)applying manifests, one can compare the current state of the cluster against the state it would be in if the manifest was applied: kubectl diff -f manifest.yaml Resources can also be created using default values or specifying them on the command line: # Start a Pod. kubectl run nginx --image nginx kubectl run busybox --rm -it --image = busybox -n keda -- sh # Start a Pod and write its specs into a file. kubectl run nginx --image = nginx --dry-run = client -o yaml > pod.yaml # Create a single instance deployment of 'nginx'. kubectl create deployment nginx --image = nginx # Start a Job using an existing Job as template kubectl create job backup-before-upgrade-13.6.2-to-13.9.2 \\ --from = cronjob.batch/backup -n gitlab Output formatting \u00b6 Add the -o , --output option to a command: Format Description -o=custom-columns=<spec> Print a table using a comma separated list of custom columns -o=custom-columns-file=<filename> Print a table using the custom columns template in the <filename> file -o=json Output a JSON formatted API object -o=jsonpath=<template> Print the fields defined in a jsonpath expression -o=jsonpath-file=<filename> Print the fields defined by the jsonpath expression in the <filename> file -o=name Print only the resource name and nothing else -o=wide Output in the plain-text format with any additional information, and for pods, the node name is included -o=yaml Output a YAML formatted API object Examples using -o=custom-columns : # Print all the container images running in the cluster. kubectl get pods -A -o = custom-columns = 'DATA:spec.containers[*].image' # As above, but exclude 'k8s.gcr.io/coredns:1.6.2' from the list. kubectl get pods -A \\ -o = custom-columns = 'DATA:spec.containers[?(@.image!=\"k8s.gcr.io/coredns:1.6.2\")].image' # Print all fields under 'metadata' regardless of their name kubectl get pods -A -o = custom-columns = 'DATA:metadata.*' Verbosity and debugging \u00b6 Verbosity is controlled through the -v flag, or --v followed by an integer representing the log level. General Kubernetes logging conventions and the associated log levels are described in the following table: Verbosity Description --v=0 Generally useful for this to always be visible to a cluster operator. --v=1 A reasonable default log level if you don't want verbosity. --v=2 Useful steady state information about the service and important log messages that may correlate to significant changes in the system. This is the recommended default log level for most systems. --v=3 Extended information about changes. --v=4 Debug level verbosity. --v=6 Display requested resources. --v=7 Display HTTP request headers. --v=8 Display HTTP request contents. --v=9 Display HTTP request contents without truncation of contents. Further readings \u00b6 Assigning Pods to Nodes Taints and Tolerations Commands reference Configure access to multiple clusters Sources \u00b6 Cheatsheet Run a single-instance stateful application Run a replicated stateful application Accessing an application on Kubernetes in Docker","title":"Kubectl <!-- omit in toc -->"},{"location":"learning/tools/kubectl/#kubectl","text":"Command line tool for communicating with a Kubernetes cluster's control plane using the Kubernetes API. Resource types are case in sensitive and can be specified in their singular, plural or abbreviated form for convenience: # The two commands below are equivalent. kubectl get deployment,replicasets,pods -A kubectl get deploy,rs,po -A Use kubectl api-resources to check out the available resources and their abbreviations. Multiple resources types can be specified together, but only one resource name is accepted at a time. Resource names are case sensitive and will filter the requested resources; use the -l , --selector option to get around filtering: kubectl get deployments,replicasets -A kubectl get pod etcd-minikube -n kube-system kubectl get pods -l app = nginx,tier = frontend","title":"Kubectl "},{"location":"learning/tools/kubectl/#table-of-contents","text":"TL;DR Configuration Configure access to multiple clusters Create resources Output formatting Verbosity and debugging Further readings Sources","title":"Table of contents"},{"location":"learning/tools/kubectl/#tldr","text":"# Enable shell completion. source < ( kubectl completion bash ) echo \"[[ $commands [kubectl] ]] && source <(kubectl completion zsh)\" >> ~/.zshrc # Shot the merged configuration. kubectl config view # Get specific values from the configuration. kubectl config view -o jsonpath = '{.users[].name}' kubectl config view -o jsonpath = '{.users[*].name}' kubectl config view -o jsonpath = '{.users[?(@.name == \"e2e\")].user.password}' # Set configuration values. kubectl config set-context --current --namespace = keda kubectl config set-context gce --user = cluster-admin --namespace = foo kubectl config set-credentials \\ kubeuser/foo.kubernetes.com --username = kubeuser --password = kubepassword # Delete configuration values. kubectl config unset users.foo # Use multiple config files at once. # This will temporarily merge them in one big configuration file. KUBECONFIG = \"path/to/config1:path/to/configN\" # List contexts. kubectl config get-contexts kubectl config current-context # Set context as the default one. kubectl config use-context docker-desktop kubectl config use-context gce # Display addresses of the master and services. kubectl cluster-info # Dump the complete current cluster state. kubectl cluster-info dump kubectl cluster-info dump --output-directory = /path/to/cluster-state # List supported resources types along with their short name, API group, Kind, # and whether they are namespaced. kubectl api-resources kubectl api-resources --namespaced = true kubectl api-resources -o name kubectl api-resources -o wide kubectl api-resources --verbs = list,get # Show the documentation about resources or their fields. kubectl explain pods kubectl explain pods.spec.containers # List and filter resources. kubectl get pods kubectl get pod/coredns-845757d86-47np2 -n kube-system kubectl get namespaces,pods --show-labels kubectl get services -A -o wide kubectl get rs --sort-by = .metadata.name kubectl get pv --sort-by = .spec.capacity.storage --no-headers kubectl get po --sort-by = '.status.containerStatuses[0].restartCount' kubectl get events --sort-by .metadata.creationTimestamp kubectl get pods --field-selector = status.phase = Running kubectl get node -l = '!node-role.kubernetes.io/master' kubectl get replicasets -l 'environment in (prod, qa)' kubectl get deploy --selector 'tier,tier notin (frontend)' # Extract information from resources' definition. kubectl get deployment nginx -o yaml kubectl get cm kube-root-ca.crt -o jsonpath = '{.data.ca\\.crt}' kubectl get po -o = jsonpath = '{.items..metadata.name}' kubectl get po -l app = redis -o jsonpath = '{.items[*].metadata.labels.version}' kubectl get nodes \\ -o jsonpath = '{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' # List all fields under '.metadata' regardless of their name. kubectl get pods -A -o = custom-columns = 'DATA:metadata.*' # List images being run in a cluster. kubectl get po -A -o = custom-columns = 'DATA:spec.containers[*].image' kubectl get po -A -o = custom-columns = 'DATA:spec.containers[?(@.image!=\"k8s.gcr.io/coredns:1.6.2\")].image' # List all pods in status 'Shutdown'. kubectl get po -A \\ -o jsonpath = '{.items[?(@.status.reason==\"Shutdown\")].metadata.name}' # List ready nodes. kubectl get nodes \\ -o jsonpath = '{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ | grep \"Ready=True\" # List all secrets currently in use by a Pod. kubectl get pods -o json \\ | jq '.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name' \\ | grep -v null | sort | uniq # List the name of Pods belonging to a particular RC. SELECTOR = ${ $( kubectl get rc my-rc --output = json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"' ) %? } kubectl get pods -l = $SELECTOR \\ -o = jsonpath = '{.items..metadata.name}' # List the containerID of initContainers from all Pods. # Helpful when cleaning up stopped containers while avoiding the removal of # initContainers kubectl get pods --all-namespaces \\ -o jsonpath = '{range .items[*].status.initContainerStatuses[*]}{.containerID}{\"\\n\"}{end}' \\ | cut -d/ -f3 # Produce a period-delimited tree of all keys returned for nodes. # Helpful when trying to locate a specific key within a complex nested JSON # structure. kubectl get nodes -o json | jq -c 'path(..)|[.[]|tostring]|join(\".\")' # Show detailed information about resources. kubectl describe node pi kubectl describe deploy,rs,po -l app = redis # Create resources from manifests. kubectl apply -f manifest.yaml kubectl apply -f path/to/m1.yaml -f ./m2.yaml kubectl apply -f dir/ kubectl apply -f https://git.io/vPieo cat <<-EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: password: $(echo -n \"s33msi4\" | base64 -w0) username: $(echo -n \"jane\" | base64 -w0) EOF # Compare the current state of the cluster against the state it would be in if # a manifest was applied kubectl diff -f ./manifest.yaml # Start a Pod. kubectl run nginx --image nginx kubectl run busybox --rm -it --image = busybox -n keda -- sh # Start a Pod and write its specs into a file. kubectl run nginx --image = nginx --dry-run = client -o yaml > pod.yaml # Create a single instance deployment of 'nginx'. kubectl create deployment nginx --image = nginx # Start a Job using an existing Job as template kubectl create job backup-before-upgrade-13.6.2-to-13.9.2 \\ --from = cronjob.batch/backup -n gitlab # Wait for a pod to be 'ready'. kubectl wait --for 'condition=ready' --timeout 120s \\ pod -l 'app.kubernetes.io/component=controller' # Update the 'image' field of the 'www' containers from the 'frontend' # Deployment. # This starts a rolling update. kubectl set image deployment/frontend www = image:v2 # Show the history of resources, including the revision. kubectl rollout history deployment/frontend # Rollback resources to the latest previous version. kubectl rollout undo deployment/frontend # Rollback resources to a specific revision. kubectl rollout undo deployment/frontend --to-revision = 2 # Follow the rolling update status of the 'frontend' Deployment until its # completion. kubectl rollout status -w deployment/frontend # Start a rolling restart of the 'frontend' Deployment. kubectl rollout restart deployment/frontend # Replace a Pod based on the JSON passed into stdin. cat pod.json | kubectl replace -f - # Force replacement, deletion and recreation (in this order) of resources. # This Will cause a service outage. kubectl replace --force -f ./pod.json # Create a service for a replicated 'nginx'. # Set it to serve on port 80 and connect to the containers on port 8000. kubectl expose rc nginx --port = 80 --target-port = 8000 # Update a single-container Pod's image tag. kubectl get pod mypod -o yaml \\ | sed 's/\\(image: myimage\\):.*$/\\1:v4/' \\ | kubectl replace -f - # Add Labels to resources. kubectl label pods nginx custom-name = awesome # Add Annotations. kubectl annotate pods alpine icon-url = http://goo.gl/XXBTWq # Autoscale resources. kubectl autoscale deployment foo --min = 2 --max = 10 # Partially update resources. kubectl patch node k8s-node-1 -p '{\"spec\":{\"unschedulable\":true}}' # Update a container's image. # 'spec.containers[*].name' is required to specify the path of the merged key. kubectl patch pod valid-pod \\ -p '{\"spec\":{\"containers\": [{\"name\": \"kubernetes-serve-hostname\",\"image\":\"new image\"}]}}' # Update a container's image using a JSONPatch with positional arrays. kubectl patch pod valid-pod --type = 'json' \\ -p = '[{\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"new image\"}]' # Disable a Deployment's livenessProbe using a JSONPatch with positional arrays. kubectl patch deployment valid-deployment --type json \\ -p = '[{\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"}]' # Add a new element to a positional array. kubectl patch sa default --type = 'json' \\ -p = '[{\"op\": \"add\", \"path\": \"/secrets/1\", \"value\": {\"name\": \"whatever\" } }]' # Edit the service named docker-registry. kubectl edit svc/docker-registry KUBE_EDITOR = \"nano\" kubectl edit svc/docker-registry # Scale a replicaset named 'foo' to 3 kubectl scale --replicas = 3 rs/foo # Scale a resource specified in \"foo.yaml\" to 3 replicas. kubectl scale --replicas = 3 -f foo.yaml # If the Deployment named 'mysql''s current size is 2, scale it to 3. kubectl scale --current-replicas = 2 --replicas = 3 deployment/mysql # Scale multiple ReplicationControllers at once. kubectl scale --replicas = 5 rc/foo rc/bar rc/baz # Delete a Pod using the type and name specified in pod.json. kubectl delete -f ./pod.json # Delete Pods and Services named 'baz' and 'foo'. kubectl delete pod,service baz foo # Delete pods and services with Label name=myLabel. kubectl delete pods,services -l name = myLabel # Delete all pods and services in namespace my-ns. kubectl -n my-ns delete pod,svc --all # Delete all pods matching awk's pattern1 or pattern2. kubectl get pods --no-headers \\ | awk '/pattern1|pattern2/{print $1}' \\ | xargs -n1 kubectl delete pods # Delete non-default service accounts. kubectl get serviceaccounts \\ -o jsonpath = \"{.items[?(@.metadata.name!='default')].metadata.name}\" \\ | xargs -n1 kubectl delete serviceaccounts # Attach to a running container. kubectl attach my-pod -i # Run command in existing Pods. kubectl exec my-pod -- ls / kubectl exec my-pod -c my-container -- ls / # Show metrics for a given Pod and its containers. kubectl top pod busybox --containers # Get logs from resources. kubectl logs redis-0 kubectl logs -l name = myLabel kubectl logs my-pod -c my-container # Follow logs. kubectl logs -f my-pod kubectl logs -f my-pod -c my-container kubectl logs -f -l name = myLabel --all-containers # Get logs for a previous instantiation of a container. kubectl logs nginx --previous # Get the logs of the first Pod matching ID. kubectl logs $( kubectl get pods --no-headers | grep $ID | awk '{print $2}' ) # Verify user's permissions on the cluster. kubectl auth can-i create roles kubectl auth can-i list pods # Taint a Node. kubectl taint nodes node1 key1 = value1:NoSchedule # Taint all nodes in a certain nodepool (Azure AKS). kubectl get no -l \"agentpool=nodepool1\" -o jsonpath = '{.items[*].metadata.name}' | xargs -n1 -I {} -p kubectl taint nodes {} key1 = value1:NoSchedule # Remove a taint. # Notice the '-' sign at the end. kubectl taint nodes node1 key1 = value1:NoSchedule- # If a taint with that key and effect already exists, replace its value. kubectl taint nodes foo dedicated = special-user:NoSchedule # Mark Nodes as unschedulable. kubectl cordon my-node # Mark my-node as schedulable. kubectl uncordon my-node # Drain my-node in preparation for maintenance. kubectl drain my-node # Show metrics for a given node. kubectl top node my-node # Listen on port 5000 on the local machine and forward connections to port 6000 # of my-pod kubectl port-forward my-pod 5000 :6000","title":"TL;DR"},{"location":"learning/tools/kubectl/#configuration","text":"The configuration files are loaded as follows: If the --kubeconfig flag is set, then only that file is loaded; the flag may only be set once , and no merging takes place: kubectl config --kubeconfig config.local view If the $KUBECONFIG environment variable is set, then it is used as a list of paths following the normal path delimiting rules for your system; the files are merged: export KUBECONFIG = \"/tmp/config.local:.kube/config.prod\" When a value is modified, it is modified in the file that defines the stanza; when a value is created, it is created in the first existing file; if no file in the chain exist, then the last file in the list is created with the configuration. If none of the above happens, ~/.kube/config is used, and no merging takes place. The configuration file can be edited, or acted upon from the command line: # Show the merged configuration. kubectl config view KUBECONFIG = \"~/.kube/config:config.local\" kubectl config view # Show specific values only. kubectl config view -o jsonpath = '{.users[].name}' kubectl config view -o jsonpath = '{.users[?(@.name == \"e2e\")].user.password}' # Add a new user that supports basic auth. kubectl config set-credentials kubeuser/foo.kubernetes.com \\ --username = kubeuser --password = kubepassword # Delete user 'foo'. kubectl config unset users.foo # List the available contexts. kubectl config get-contexts # Display the current default context name. kubectl config current-context # Set the default context. kubectl config use-context minikube # Permanently setup specific contexts. kubectl config set-context --current --namespace = ggckad-s2 kubectl config set-context gce --user = cluster-admin --namespace = foo","title":"Configuration"},{"location":"learning/tools/kubectl/#configure-access-to-multiple-clusters","text":"See configure access to multiple clusters for details.","title":"Configure access to multiple clusters"},{"location":"learning/tools/kubectl/#create-resources","text":"The preferred way to create resources is to define them inside manifest s and then apply those: --- # file manifest.yaml --- apiVersion : v1 kind : Pod metadata : name : busybox-sleep spec : containers : - name : busybox image : busybox args : - sleep - \"1000000\" --- apiVersion : v1 kind : Pod metadata : name : busybox-sleep-less spec : containers : - name : busybox image : busybox args : - sleep - \"1000\" # Apply the manifest. kubectl apply -f manifest.yaml # Apply multiple manifests together. kubectl apply -f path/to/m1.yaml -f m2.yaml # Apply all manifests in a directory. kubectl apply -f ./dir # Apply a remote manifest. kubectl apply -f https://git.io/vPieo # Define a manifest using HEREDOC and apply it. cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: password: $(echo -n \"s33msi4\" | base64 -w0) username: $(echo -n \"jane\" | base64 -w0) EOF When subsequentially (re-)applying manifests, one can compare the current state of the cluster against the state it would be in if the manifest was applied: kubectl diff -f manifest.yaml Resources can also be created using default values or specifying them on the command line: # Start a Pod. kubectl run nginx --image nginx kubectl run busybox --rm -it --image = busybox -n keda -- sh # Start a Pod and write its specs into a file. kubectl run nginx --image = nginx --dry-run = client -o yaml > pod.yaml # Create a single instance deployment of 'nginx'. kubectl create deployment nginx --image = nginx # Start a Job using an existing Job as template kubectl create job backup-before-upgrade-13.6.2-to-13.9.2 \\ --from = cronjob.batch/backup -n gitlab","title":"Create resources"},{"location":"learning/tools/kubectl/#output-formatting","text":"Add the -o , --output option to a command: Format Description -o=custom-columns=<spec> Print a table using a comma separated list of custom columns -o=custom-columns-file=<filename> Print a table using the custom columns template in the <filename> file -o=json Output a JSON formatted API object -o=jsonpath=<template> Print the fields defined in a jsonpath expression -o=jsonpath-file=<filename> Print the fields defined by the jsonpath expression in the <filename> file -o=name Print only the resource name and nothing else -o=wide Output in the plain-text format with any additional information, and for pods, the node name is included -o=yaml Output a YAML formatted API object Examples using -o=custom-columns : # Print all the container images running in the cluster. kubectl get pods -A -o = custom-columns = 'DATA:spec.containers[*].image' # As above, but exclude 'k8s.gcr.io/coredns:1.6.2' from the list. kubectl get pods -A \\ -o = custom-columns = 'DATA:spec.containers[?(@.image!=\"k8s.gcr.io/coredns:1.6.2\")].image' # Print all fields under 'metadata' regardless of their name kubectl get pods -A -o = custom-columns = 'DATA:metadata.*'","title":"Output formatting"},{"location":"learning/tools/kubectl/#verbosity-and-debugging","text":"Verbosity is controlled through the -v flag, or --v followed by an integer representing the log level. General Kubernetes logging conventions and the associated log levels are described in the following table: Verbosity Description --v=0 Generally useful for this to always be visible to a cluster operator. --v=1 A reasonable default log level if you don't want verbosity. --v=2 Useful steady state information about the service and important log messages that may correlate to significant changes in the system. This is the recommended default log level for most systems. --v=3 Extended information about changes. --v=4 Debug level verbosity. --v=6 Display requested resources. --v=7 Display HTTP request headers. --v=8 Display HTTP request contents. --v=9 Display HTTP request contents without truncation of contents.","title":"Verbosity and debugging"},{"location":"learning/tools/kubectl/#further-readings","text":"Assigning Pods to Nodes Taints and Tolerations Commands reference Configure access to multiple clusters","title":"Further readings"},{"location":"learning/tools/kubectl/#sources","text":"Cheatsheet Run a single-instance stateful application Run a replicated stateful application Accessing an application on Kubernetes in Docker","title":"Sources"},{"location":"learning/tools/kubeval/","text":"Kubeval \u00b6 Validates one or more Kubernetes configuration files. TL;DR \u00b6 $ kubeval my-invalid-rc.yaml || echo \"Validation failed\" > & 2 WARN - my-invalid-rc.yaml contains an invalid ReplicationController - spec.replicas: Invalid type. Expected: integer, given: string Validation failed Further readings \u00b6 Kubeval Validating kubernetes YAML files with kubeval","title":"Kubeval"},{"location":"learning/tools/kubeval/#kubeval","text":"Validates one or more Kubernetes configuration files.","title":"Kubeval"},{"location":"learning/tools/kubeval/#tldr","text":"$ kubeval my-invalid-rc.yaml || echo \"Validation failed\" > & 2 WARN - my-invalid-rc.yaml contains an invalid ReplicationController - spec.replicas: Invalid type. Expected: integer, given: string Validation failed","title":"TL;DR"},{"location":"learning/tools/kubeval/#further-readings","text":"Kubeval Validating kubernetes YAML files with kubeval","title":"Further readings"},{"location":"learning/tools/less/","text":"Less \u00b6 TL;DR \u00b6 # Search words *forwards* in the current document. :/keyword <ENTER> # Search words *backwards* in the current document. :?keyword <ENTER> # Toggle case insensitivity in searches. :-i <ENTER> Sources \u00b6","title":"Less"},{"location":"learning/tools/less/#less","text":"","title":"Less"},{"location":"learning/tools/less/#tldr","text":"# Search words *forwards* in the current document. :/keyword <ENTER> # Search words *backwards* in the current document. :?keyword <ENTER> # Toggle case insensitivity in searches. :-i <ENTER>","title":"TL;DR"},{"location":"learning/tools/less/#sources","text":"","title":"Sources"},{"location":"learning/tools/lint%20markdown%20files/","text":"Hot to lint markdown files \u00b6 See mdl .","title":"Hot to lint markdown files"},{"location":"learning/tools/lint%20markdown%20files/#hot-to-lint-markdown-files","text":"See mdl .","title":"Hot to lint markdown files"},{"location":"learning/tools/lm-sensors/","text":"LM-sensors \u00b6 TL;DR \u00b6 # Install. sudo dnf install lm_sensors # Detect and generate a list of kernel modules. # Needs to be executed prior of the next commands. sudo sensors-detect # Show the current readings of all sensors. sensors # Display sensor information in raw output. # Suitable for parsing. sensors -u # Show temperatures in degrees Fahrenheit. sensors -f sensors --fahrenheit Sources \u00b6 cheat.sh archlinux wiki How to Install lm Sensors on Linux","title":"LM-sensors"},{"location":"learning/tools/lm-sensors/#lm-sensors","text":"","title":"LM-sensors"},{"location":"learning/tools/lm-sensors/#tldr","text":"# Install. sudo dnf install lm_sensors # Detect and generate a list of kernel modules. # Needs to be executed prior of the next commands. sudo sensors-detect # Show the current readings of all sensors. sensors # Display sensor information in raw output. # Suitable for parsing. sensors -u # Show temperatures in degrees Fahrenheit. sensors -f sensors --fahrenheit","title":"TL;DR"},{"location":"learning/tools/lm-sensors/#sources","text":"cheat.sh archlinux wiki How to Install lm Sensors on Linux","title":"Sources"},{"location":"learning/tools/lower%20the%20power%20consumption%20on%20linux/","text":"Lower the power consumption on Linux \u00b6 echo '0' > '/proc/sys/kernel/nmi_watchdog' echo 'med_power_with_dipm' > '/sys/class/scsi_host/host0/link_power_management_policy' # Increase the virtual memory dirty writeback time to help aggregating disk I/O # together. This reduces spanned disk writes. # Value is in 1/100s of seconds. Default is 500 (5 seconds). echo 6000 > '/proc/sys/vm/dirty_writeback_centisecs' sudo sysctl vm.dirty_writeback_centisecs = 6000 Sources \u00b6 Arch Wiki's power management page","title":"Lower the power consumption on Linux"},{"location":"learning/tools/lower%20the%20power%20consumption%20on%20linux/#lower-the-power-consumption-on-linux","text":"echo '0' > '/proc/sys/kernel/nmi_watchdog' echo 'med_power_with_dipm' > '/sys/class/scsi_host/host0/link_power_management_policy' # Increase the virtual memory dirty writeback time to help aggregating disk I/O # together. This reduces spanned disk writes. # Value is in 1/100s of seconds. Default is 500 (5 seconds). echo 6000 > '/proc/sys/vm/dirty_writeback_centisecs' sudo sysctl vm.dirty_writeback_centisecs = 6000","title":"Lower the power consumption on Linux"},{"location":"learning/tools/lower%20the%20power%20consumption%20on%20linux/#sources","text":"Arch Wiki's power management page","title":"Sources"},{"location":"learning/tools/lxc/","text":"Linux Container Runtime \u00b6 TL;DR \u00b6 # install lxc apt-get install lxc snap install lxd # list available templates ls /usr/share/lxc/templates # create a new container # use the download template to choose from a list of distribution lxc-create --name container-name --template download # start a container lxc-start --name container-name lxc-start --name container-name --foreground lxc-start --name container-name --daemon --define CONFIGVAR = VALUE # stop a container lxc-stop --name container-name lxc-stop --name container-name --kill # destroy a container # needs the container to be stopped lxc-destroy --name container-name # get a container status lxc-info --name container-name # get the status of all containers lxc-ls --fancy # get a shell inside a container lxc-attach --name container-name # get config options on man man 5 lxc.container.conf man lxc.container.conf.5 man lxc.container.conf ( 5 ) Create new containers as unprivileged user \u00b6 # allow user vagrant to create up to 10 veth devices connected to the lxcbr0 bridge echo \"vagrant veth lxcbr0 10\" | sudo tee -a /etc/lxc/lxc-usernet Further readings \u00b6 LXC's website LXC's getting started guide","title":"Linux Container Runtime"},{"location":"learning/tools/lxc/#linux-container-runtime","text":"","title":"Linux Container Runtime"},{"location":"learning/tools/lxc/#tldr","text":"# install lxc apt-get install lxc snap install lxd # list available templates ls /usr/share/lxc/templates # create a new container # use the download template to choose from a list of distribution lxc-create --name container-name --template download # start a container lxc-start --name container-name lxc-start --name container-name --foreground lxc-start --name container-name --daemon --define CONFIGVAR = VALUE # stop a container lxc-stop --name container-name lxc-stop --name container-name --kill # destroy a container # needs the container to be stopped lxc-destroy --name container-name # get a container status lxc-info --name container-name # get the status of all containers lxc-ls --fancy # get a shell inside a container lxc-attach --name container-name # get config options on man man 5 lxc.container.conf man lxc.container.conf.5 man lxc.container.conf ( 5 )","title":"TL;DR"},{"location":"learning/tools/lxc/#create-new-containers-as-unprivileged-user","text":"# allow user vagrant to create up to 10 veth devices connected to the lxcbr0 bridge echo \"vagrant veth lxcbr0 10\" | sudo tee -a /etc/lxc/lxc-usernet","title":"Create new containers as unprivileged user"},{"location":"learning/tools/lxc/#further-readings","text":"LXC's website LXC's getting started guide","title":"Further readings"},{"location":"learning/tools/mac%20os%20x/","text":"Mac OS X \u00b6 TL;DR Xcode CLI tools Headless installation Removal Upgrade Hidden settings Resize an image from CLI Boot keys cheatsheet Update the OS from CLI Keychain access from CLI Use TouchID to authenticate in the terminal Fix iTerm2 Further readings Sources TL;DR \u00b6 # Install a .pkg file from CLI. # 'target' needs to be a device, not a path. installer -pkg /path/to/non-root-package.pkg -target CurrentUserHomeDirectory sudo installer -pkg /path/to/root-needed-package.pkg -target / # Install Xcode CLI tools. xcode-select --install # Show Xcode tools's path. xcode-select -p # Remove Xcode tools. sudo rm -rf $( xcode-select -p ) # List all available updates. softwareupdate --list --all # Install all recommended updates, agreeing to software license agreement # without interaction, and automatically restart if required. softwareupdate --install --recommended --restart --agree-to-license # Download (but not install) recommended updates. softwareupdate --download --recommended # Add a password to the default keychain. # The password needs to be left last. security add-generic-password -a johnny -s github -w 'b.good' # Add a password to the default keychain giving it some optional data. security add-generic-password -a johnny -s github -l work \\ -j 'my key for work' -w 'b.good' # Update passwords' value. security add-generic-password -a johnny -s github -l work -U -w 'new-pass' # Print passwords to stdout. security find-generic-password -w -a johnny -s github security find-generic-password -w -l work security find-generic-password -w -l work -s github # Delete a password from the default keychain. security delete-generic-password -a johnny -s github # Get the host's bonjour name. scutil --get LocalHostName /usr/libexec/PlistBuddy -c \"Print :System:Network:HostNames:LocalHostName\" \\ /Library/Preferences/SystemConfiguration/preferences.plist # Get the host's netbios name. defaults read /Library/Preferences/SystemConfiguration/com.apple.smb.server NetBIOSName /usr/libexec/PlistBuddy -c \"Print :NetBIOSName\" \\ /Library/Preferences/SystemConfiguration/com.apple.smb.server.plist # Get the host's computer name. scutil --get ComputerName /usr/libexec/PlistBuddy -c \"Print :System:System:ComputerName\" \\ /Library/Preferences/SystemConfiguration/preferences.plist Xcode CLI tools \u00b6 xcode-select --install The tools will be installed into /Library/Developer/CommandLineTools by default, with the binaries being available at $(xcode-select -p)/usr/bin/ . Headless installation \u00b6 # Force the `softwareupdate` utility to list the Command Line Tools. touch /tmp/.com.apple.dt.CommandLineTools.installondemand.in-progress # Get their label. CLI_TOOLS_LABEL = \" $( /usr/sbin/softwareupdate -l \\ | grep -B 1 -E 'Command Line Tools' \\ | awk -F '*' '/^ *\\\\*/ {print $2}' \\ | sed -e 's/^ *Label: //' -e 's/^ *//' \\ | sort -V \\ | tail -n1 ) \" # Install them. /usr/sbin/softwareupdate -i --agree-to-license \" $CLI_TOOLS_LABEL \" Removal \u00b6 sudo rm -rf $( xcode-select -p ) sudo rm -rf /Library/Developer/CommandLineTools Upgrade \u00b6 See How to update Xcode from command line for details. # Remove and reinstall. sudo rm -rf $( xcode-select -p ) xcode-select --install Hidden settings \u00b6 Note: once set something, you'll probably need to restart the dock with killall Dock # Show hidden apps indicators in the dock. defaults write com.apple.dock showhidden -bool TRUE # Reset changes to the dock. defaults delete com.apple.dock # Change the number of columns and rows in the springboard. defaults write com.apple.dock springboard-columns -int 9 defaults write com.apple.dock springboard-rows -int 7 # Reset changes to the launchpad. defaults delete com.apple.dock springboard-rows defaults delete com.apple.dock springboard-columns defaults write com.apple.dock ResetLaunchPad -bool TRUE # Force Finder to always display hidden files. defaults write com.apple.finder AppleShowAllFiles TRUE Resize an image from CLI \u00b6 # Retain ratio. # Save as different file. sips -Z 1000 -o resized.jpg IMG_20190527_013903.jpg Boot keys cheatsheet \u00b6 Only available on Intel based Macs. To use any of these key combinations, press and hold the keys immediately after pressing the power button to turn on your Mac, or after your Mac begins to restart. Keep holding until the described behavior occurs. Combination Behaviour \u2325 Option or Alt Start to Startup Manager , which allows you to choose other available startup disks or volumes. If your Mac is using a firmware password, you're prompted to enter the password \u2325 Option + \u2318 Command + P + R Reset the NVRAM or PRAM. If your Mac is using a firmware password, it ignores this key combination or starts up from Recovery \u21e7 Shift Start in safe mode. Disabled when using a firmware password \u2318 Command + R Start from the built-in Recovery system \u2325 Option + \u2318 Command + R or \u21e7 Shift + \u2325 Option + \u2318 Command + R Start from Recovery over the Internet. It installs different versions of macOS, depending on the key combination you use while starting up. If your Mac is using a firmware password, you're prompted to enter the password \u23cf Eject or F12 or mouse button or trackpad button Eject a removable media, such as an optical disc. Disabled when using a firmware password T Start in target disk mode. Disabled when using a firmware password \u2318 Command + V Start in verbose mode. Disabled when using a firmware password D Start to Apple Diagnostics \u2325 Option + D Start to Apple Diagnostics over the Internet. Disabled when using a firmware password N Start from a NetBoot server, if your Mac supports network startup volumes. Disabled when using a firmware password \u2325 Option + N Start from a NetBoot server and use the default boot image on it. Disabled when using a firmware password \u2318 Command + S Start in single-user mode. Disabled in macOS Mojave or later, or when using a firmware password Update the OS from CLI \u00b6 # List all available updates. softwareupdate --list --all # Install all recommended updates. # Agree to software license agreement without interaction. # Automatically restart if required. softwareupdate --install --recommended --restart --agree-to-license # Download (but not install) recommended updates. softwareupdate --download --recommended Keychain access from CLI \u00b6 Save a password with the following settings: user (a.k.a. account ): johnny password: b.good service name: github [optional] entry name (a.k.a. label ): work ; if not given, the service name will be used [optional] comment: my key for work ; if not given, it will be left blank The password's value needs to be given last . # Add the password to the default keychain. security add-generic-password -a johnny -s github -w 'b.good' # Also give it some optional data. security add-generic-password -a johnny -s github -l work \\ -j 'my key for work' -w 'b.good' # Update passwords' value. security add-generic-password -a johnny -s github -l work -U -w 'new-pass' # Print the above password to stdout. security find-generic-password -w -a johnny -s github security find-generic-password -w -l work security find-generic-password -w -l work -s github # Delete it. security delete-generic-password -a johnny -s github Use TouchID to authenticate in the terminal \u00b6 Add the pam_tid.so module as sufficient to /etc/pam.d/sudo : # sudo: auth account password session +auth sufficient pam_tid.so auth sufficient pam_smartcard.so auth required pam_opendirectory.so This file is normally read-only, so saving your changes may require you to force the save (e.g. vim will require the use of wq! when saving). Fix iTerm2 \u00b6 iTerm2 from version 3.2.8 comes with a reattach advanced feature which is incompatible with the addition of the pam_tid.so module alone. You can either: disable the feature: iTerm2 > Preferences > Advanced > (Goto the Session heading) > Allow sessions to survive logging out and back in install and enable the pam_reattach.so module as optional to /etc/pam.d/sudo : # pick one brew install pam-reattach sudo port install pam-reattach # sudo: auth account password session +auth optional /opt/local/lib/pam/pam_reattach.so ignore_ssh +auth sufficient pam_tid.so auth sufficient pam_smartcard.so auth required pam_opendirectory.so Note that when the module is not installed in /usr/lib/pam or /usr/local/lib/pam (e.g. on M1 Macs where Homebrew is installed in /opt/homebrew ), you must specify the full path to the module in the PAM service file. Further readings \u00b6 pam_reattach Sources \u00b6 Boot a Mac from USB Drive Mac startup key combinations Xcode Command Line Tools Installation FAQ How to update Xcode from command line Command line access to the Mac keychain Installing .pkg with terminal? Using Terminal to Find Your Mac's Network Name List of Xcode Command Line Tools Can Touch ID for the Mac Touch Bar authenticate sudo users and admin privileges?","title":"Mac OS X <!-- omit in toc -->"},{"location":"learning/tools/mac%20os%20x/#mac-os-x","text":"TL;DR Xcode CLI tools Headless installation Removal Upgrade Hidden settings Resize an image from CLI Boot keys cheatsheet Update the OS from CLI Keychain access from CLI Use TouchID to authenticate in the terminal Fix iTerm2 Further readings Sources","title":"Mac OS X "},{"location":"learning/tools/mac%20os%20x/#tldr","text":"# Install a .pkg file from CLI. # 'target' needs to be a device, not a path. installer -pkg /path/to/non-root-package.pkg -target CurrentUserHomeDirectory sudo installer -pkg /path/to/root-needed-package.pkg -target / # Install Xcode CLI tools. xcode-select --install # Show Xcode tools's path. xcode-select -p # Remove Xcode tools. sudo rm -rf $( xcode-select -p ) # List all available updates. softwareupdate --list --all # Install all recommended updates, agreeing to software license agreement # without interaction, and automatically restart if required. softwareupdate --install --recommended --restart --agree-to-license # Download (but not install) recommended updates. softwareupdate --download --recommended # Add a password to the default keychain. # The password needs to be left last. security add-generic-password -a johnny -s github -w 'b.good' # Add a password to the default keychain giving it some optional data. security add-generic-password -a johnny -s github -l work \\ -j 'my key for work' -w 'b.good' # Update passwords' value. security add-generic-password -a johnny -s github -l work -U -w 'new-pass' # Print passwords to stdout. security find-generic-password -w -a johnny -s github security find-generic-password -w -l work security find-generic-password -w -l work -s github # Delete a password from the default keychain. security delete-generic-password -a johnny -s github # Get the host's bonjour name. scutil --get LocalHostName /usr/libexec/PlistBuddy -c \"Print :System:Network:HostNames:LocalHostName\" \\ /Library/Preferences/SystemConfiguration/preferences.plist # Get the host's netbios name. defaults read /Library/Preferences/SystemConfiguration/com.apple.smb.server NetBIOSName /usr/libexec/PlistBuddy -c \"Print :NetBIOSName\" \\ /Library/Preferences/SystemConfiguration/com.apple.smb.server.plist # Get the host's computer name. scutil --get ComputerName /usr/libexec/PlistBuddy -c \"Print :System:System:ComputerName\" \\ /Library/Preferences/SystemConfiguration/preferences.plist","title":"TL;DR"},{"location":"learning/tools/mac%20os%20x/#xcode-cli-tools","text":"xcode-select --install The tools will be installed into /Library/Developer/CommandLineTools by default, with the binaries being available at $(xcode-select -p)/usr/bin/ .","title":"Xcode CLI tools"},{"location":"learning/tools/mac%20os%20x/#headless-installation","text":"# Force the `softwareupdate` utility to list the Command Line Tools. touch /tmp/.com.apple.dt.CommandLineTools.installondemand.in-progress # Get their label. CLI_TOOLS_LABEL = \" $( /usr/sbin/softwareupdate -l \\ | grep -B 1 -E 'Command Line Tools' \\ | awk -F '*' '/^ *\\\\*/ {print $2}' \\ | sed -e 's/^ *Label: //' -e 's/^ *//' \\ | sort -V \\ | tail -n1 ) \" # Install them. /usr/sbin/softwareupdate -i --agree-to-license \" $CLI_TOOLS_LABEL \"","title":"Headless installation"},{"location":"learning/tools/mac%20os%20x/#removal","text":"sudo rm -rf $( xcode-select -p ) sudo rm -rf /Library/Developer/CommandLineTools","title":"Removal"},{"location":"learning/tools/mac%20os%20x/#upgrade","text":"See How to update Xcode from command line for details. # Remove and reinstall. sudo rm -rf $( xcode-select -p ) xcode-select --install","title":"Upgrade"},{"location":"learning/tools/mac%20os%20x/#hidden-settings","text":"Note: once set something, you'll probably need to restart the dock with killall Dock # Show hidden apps indicators in the dock. defaults write com.apple.dock showhidden -bool TRUE # Reset changes to the dock. defaults delete com.apple.dock # Change the number of columns and rows in the springboard. defaults write com.apple.dock springboard-columns -int 9 defaults write com.apple.dock springboard-rows -int 7 # Reset changes to the launchpad. defaults delete com.apple.dock springboard-rows defaults delete com.apple.dock springboard-columns defaults write com.apple.dock ResetLaunchPad -bool TRUE # Force Finder to always display hidden files. defaults write com.apple.finder AppleShowAllFiles TRUE","title":"Hidden settings"},{"location":"learning/tools/mac%20os%20x/#resize-an-image-from-cli","text":"# Retain ratio. # Save as different file. sips -Z 1000 -o resized.jpg IMG_20190527_013903.jpg","title":"Resize an image from CLI"},{"location":"learning/tools/mac%20os%20x/#boot-keys-cheatsheet","text":"Only available on Intel based Macs. To use any of these key combinations, press and hold the keys immediately after pressing the power button to turn on your Mac, or after your Mac begins to restart. Keep holding until the described behavior occurs. Combination Behaviour \u2325 Option or Alt Start to Startup Manager , which allows you to choose other available startup disks or volumes. If your Mac is using a firmware password, you're prompted to enter the password \u2325 Option + \u2318 Command + P + R Reset the NVRAM or PRAM. If your Mac is using a firmware password, it ignores this key combination or starts up from Recovery \u21e7 Shift Start in safe mode. Disabled when using a firmware password \u2318 Command + R Start from the built-in Recovery system \u2325 Option + \u2318 Command + R or \u21e7 Shift + \u2325 Option + \u2318 Command + R Start from Recovery over the Internet. It installs different versions of macOS, depending on the key combination you use while starting up. If your Mac is using a firmware password, you're prompted to enter the password \u23cf Eject or F12 or mouse button or trackpad button Eject a removable media, such as an optical disc. Disabled when using a firmware password T Start in target disk mode. Disabled when using a firmware password \u2318 Command + V Start in verbose mode. Disabled when using a firmware password D Start to Apple Diagnostics \u2325 Option + D Start to Apple Diagnostics over the Internet. Disabled when using a firmware password N Start from a NetBoot server, if your Mac supports network startup volumes. Disabled when using a firmware password \u2325 Option + N Start from a NetBoot server and use the default boot image on it. Disabled when using a firmware password \u2318 Command + S Start in single-user mode. Disabled in macOS Mojave or later, or when using a firmware password","title":"Boot keys cheatsheet"},{"location":"learning/tools/mac%20os%20x/#update-the-os-from-cli","text":"# List all available updates. softwareupdate --list --all # Install all recommended updates. # Agree to software license agreement without interaction. # Automatically restart if required. softwareupdate --install --recommended --restart --agree-to-license # Download (but not install) recommended updates. softwareupdate --download --recommended","title":"Update the OS from CLI"},{"location":"learning/tools/mac%20os%20x/#keychain-access-from-cli","text":"Save a password with the following settings: user (a.k.a. account ): johnny password: b.good service name: github [optional] entry name (a.k.a. label ): work ; if not given, the service name will be used [optional] comment: my key for work ; if not given, it will be left blank The password's value needs to be given last . # Add the password to the default keychain. security add-generic-password -a johnny -s github -w 'b.good' # Also give it some optional data. security add-generic-password -a johnny -s github -l work \\ -j 'my key for work' -w 'b.good' # Update passwords' value. security add-generic-password -a johnny -s github -l work -U -w 'new-pass' # Print the above password to stdout. security find-generic-password -w -a johnny -s github security find-generic-password -w -l work security find-generic-password -w -l work -s github # Delete it. security delete-generic-password -a johnny -s github","title":"Keychain access from CLI"},{"location":"learning/tools/mac%20os%20x/#use-touchid-to-authenticate-in-the-terminal","text":"Add the pam_tid.so module as sufficient to /etc/pam.d/sudo : # sudo: auth account password session +auth sufficient pam_tid.so auth sufficient pam_smartcard.so auth required pam_opendirectory.so This file is normally read-only, so saving your changes may require you to force the save (e.g. vim will require the use of wq! when saving).","title":"Use TouchID to authenticate in the terminal"},{"location":"learning/tools/mac%20os%20x/#fix-iterm2","text":"iTerm2 from version 3.2.8 comes with a reattach advanced feature which is incompatible with the addition of the pam_tid.so module alone. You can either: disable the feature: iTerm2 > Preferences > Advanced > (Goto the Session heading) > Allow sessions to survive logging out and back in install and enable the pam_reattach.so module as optional to /etc/pam.d/sudo : # pick one brew install pam-reattach sudo port install pam-reattach # sudo: auth account password session +auth optional /opt/local/lib/pam/pam_reattach.so ignore_ssh +auth sufficient pam_tid.so auth sufficient pam_smartcard.so auth required pam_opendirectory.so Note that when the module is not installed in /usr/lib/pam or /usr/local/lib/pam (e.g. on M1 Macs where Homebrew is installed in /opt/homebrew ), you must specify the full path to the module in the PAM service file.","title":"Fix iTerm2"},{"location":"learning/tools/mac%20os%20x/#further-readings","text":"pam_reattach","title":"Further readings"},{"location":"learning/tools/mac%20os%20x/#sources","text":"Boot a Mac from USB Drive Mac startup key combinations Xcode Command Line Tools Installation FAQ How to update Xcode from command line Command line access to the Mac keychain Installing .pkg with terminal? Using Terminal to Find Your Mac's Network Name List of Xcode Command Line Tools Can Touch ID for the Mac Touch Bar authenticate sudo users and admin privileges?","title":"Sources"},{"location":"learning/tools/macports/","text":"Macports \u00b6 See the website for the installation instructions. Default ports install location is /opt/local . TL;DR \u00b6 # get help on a command port help install port help select # search for ports port search completion port search --name parallel # get info on a specific port port info zsh-completions # get a port's variants port variants k9s # install ports sudo port install zsh-completions apple-completion sudo port install nmap -subversion # use a variant sudo port install -d gettext # debug mode # list all installed ports port installed port echo installed # list all ports that have been explicitly installed by the user port echo requested # list all available ports port list port list nmap # limit to all versions of a package # list all files installed by a port # the port must be installed for this to work port contents py38-netaddr # list ports providing a file # the port must be installed for this to work port provides /opt/local/bin/envsubst # remove a port sudo port uninstall --follow-dependencies fzf # list available choices for a group port select --list python # show the current port selection port select --summary # Set a default version. # Symlinks the \"executable\"'s version to 'opt/local/bin/executable'. sudo port select --set postgresql postgresql12 sudo port select --set python3 python310 sudo port select --set virtualenv virtualenv310 # update macports itself to the latest version and sync the latest ports definitions sudo port selfupdate # deactivate an active port sudo port deactivate stow # activate an inactive port sudo port activate stow # list all outdated ports port echo outdated # upgrade a port sudo port upgrade tree # upgrade all outdated ports sudo port upgrade outdated # clean out all temporary assets of a port sudo port clean -f --all parallel # clean up leftovers sudo port reclaim # list all inactive ports # ports are deactivated when a newer version gets installed port echo inactive # remove all inactive ports sudo port uninstall inactive # list a port's dependencies port deps chezmoi # recursively list all ports depending on the given port port rdeps pcre # list the installed ports depending on the given port port dependents bzip2 # recursively list all the installed ports that depend on this port port rdependents libedit # view a port's notes if any are available # notes are displayed right after a port is installed # the port must be installed for this to work port notes postgres12 # get the path of a port within the ports tree port dir zlib # get the path of the tarball of a port # the port must be installed for this to work port location readline # get the path to a port's portfile port file openssl11 # get the path of the working directory for a port if it exists port work popt Further readings \u00b6 Website Official user guide Public ports database cheat.sh","title":"Macports"},{"location":"learning/tools/macports/#macports","text":"See the website for the installation instructions. Default ports install location is /opt/local .","title":"Macports"},{"location":"learning/tools/macports/#tldr","text":"# get help on a command port help install port help select # search for ports port search completion port search --name parallel # get info on a specific port port info zsh-completions # get a port's variants port variants k9s # install ports sudo port install zsh-completions apple-completion sudo port install nmap -subversion # use a variant sudo port install -d gettext # debug mode # list all installed ports port installed port echo installed # list all ports that have been explicitly installed by the user port echo requested # list all available ports port list port list nmap # limit to all versions of a package # list all files installed by a port # the port must be installed for this to work port contents py38-netaddr # list ports providing a file # the port must be installed for this to work port provides /opt/local/bin/envsubst # remove a port sudo port uninstall --follow-dependencies fzf # list available choices for a group port select --list python # show the current port selection port select --summary # Set a default version. # Symlinks the \"executable\"'s version to 'opt/local/bin/executable'. sudo port select --set postgresql postgresql12 sudo port select --set python3 python310 sudo port select --set virtualenv virtualenv310 # update macports itself to the latest version and sync the latest ports definitions sudo port selfupdate # deactivate an active port sudo port deactivate stow # activate an inactive port sudo port activate stow # list all outdated ports port echo outdated # upgrade a port sudo port upgrade tree # upgrade all outdated ports sudo port upgrade outdated # clean out all temporary assets of a port sudo port clean -f --all parallel # clean up leftovers sudo port reclaim # list all inactive ports # ports are deactivated when a newer version gets installed port echo inactive # remove all inactive ports sudo port uninstall inactive # list a port's dependencies port deps chezmoi # recursively list all ports depending on the given port port rdeps pcre # list the installed ports depending on the given port port dependents bzip2 # recursively list all the installed ports that depend on this port port rdependents libedit # view a port's notes if any are available # notes are displayed right after a port is installed # the port must be installed for this to work port notes postgres12 # get the path of a port within the ports tree port dir zlib # get the path of the tarball of a port # the port must be installed for this to work port location readline # get the path to a port's portfile port file openssl11 # get the path of the working directory for a port if it exists port work popt","title":"TL;DR"},{"location":"learning/tools/macports/#further-readings","text":"Website Official user guide Public ports database cheat.sh","title":"Further readings"},{"location":"learning/tools/magisk/","text":"Magisk \u00b6 TL;DR \u00b6 This procedure works for a OnePlus 5 phone. download the os' recovery-flashable zip file extract the boot.img file from the zip file open the Magisk app press the Install button on the Magisk card choose Select and Patch a File under Method select the boot image; the Magisk app will patch the image to [Internal Storage]/Download/magisk_patched_<random strings>.img copy the patched image to your computer using the file transfer mode or adb : adb pull /sdcard/Download/magisk_patched_<random strings>.img reboot the device to the bootloader (fastboot) flash the modified boot image: sudo fastboot flash boot path/to/modified/boot.img reboot to the system Further readings \u00b6 How to Install Magisk on your Android Phone","title":"Magisk"},{"location":"learning/tools/magisk/#magisk","text":"","title":"Magisk"},{"location":"learning/tools/magisk/#tldr","text":"This procedure works for a OnePlus 5 phone. download the os' recovery-flashable zip file extract the boot.img file from the zip file open the Magisk app press the Install button on the Magisk card choose Select and Patch a File under Method select the boot image; the Magisk app will patch the image to [Internal Storage]/Download/magisk_patched_<random strings>.img copy the patched image to your computer using the file transfer mode or adb : adb pull /sdcard/Download/magisk_patched_<random strings>.img reboot the device to the bootloader (fastboot) flash the modified boot image: sudo fastboot flash boot path/to/modified/boot.img reboot to the system","title":"TL;DR"},{"location":"learning/tools/magisk/#further-readings","text":"How to Install Magisk on your Android Phone","title":"Further readings"},{"location":"learning/tools/manjaro%20linux/","text":"Manjaro GNU/Linux \u00b6 Repositories \u00b6 To ensure continued stability and reliability, Manjaro uses its own dedicated software branches rather than relying on those provided by Arch: unstable : synced several times a day with Arch's package releases; only a subset of them are modified to suit Manjaro; people using this branch need to have the skills to get themselves out of trouble; thanks to the feedback from these users, many issues are caught and fixed at this level; the very latest software is be located here, and using this branch is usually safe but - in rare cases - may cause issues with your system testing : this branch is the second line of defense; users living in this branch refine the work done by users in the unstable branch by providing further feedback on the packages they receive as updates stable : packages which land here have gone through roughly a couple of weeks testing by users using the unstable and testing branches; these packages are usually free of any problems One can use the branch comparison tool to check in what branch a package is available. Printing \u00b6 pamac install manjaro-printer sudo gpasswd -a ${ USER } sys sudo systemctl enable --now cups.service # configure printers in ui pamac install system-config-printer Further readings \u00b6 The knowledge base about Arch Linux Branch comparison Switching branches Printing Sources \u00b6 Kde Plasma 5.23 not updating","title":"Manjaro GNU/Linux"},{"location":"learning/tools/manjaro%20linux/#manjaro-gnulinux","text":"","title":"Manjaro GNU/Linux"},{"location":"learning/tools/manjaro%20linux/#repositories","text":"To ensure continued stability and reliability, Manjaro uses its own dedicated software branches rather than relying on those provided by Arch: unstable : synced several times a day with Arch's package releases; only a subset of them are modified to suit Manjaro; people using this branch need to have the skills to get themselves out of trouble; thanks to the feedback from these users, many issues are caught and fixed at this level; the very latest software is be located here, and using this branch is usually safe but - in rare cases - may cause issues with your system testing : this branch is the second line of defense; users living in this branch refine the work done by users in the unstable branch by providing further feedback on the packages they receive as updates stable : packages which land here have gone through roughly a couple of weeks testing by users using the unstable and testing branches; these packages are usually free of any problems One can use the branch comparison tool to check in what branch a package is available.","title":"Repositories"},{"location":"learning/tools/manjaro%20linux/#printing","text":"pamac install manjaro-printer sudo gpasswd -a ${ USER } sys sudo systemctl enable --now cups.service # configure printers in ui pamac install system-config-printer","title":"Printing"},{"location":"learning/tools/manjaro%20linux/#further-readings","text":"The knowledge base about Arch Linux Branch comparison Switching branches Printing","title":"Further readings"},{"location":"learning/tools/manjaro%20linux/#sources","text":"Kde Plasma 5.23 not updating","title":"Sources"},{"location":"learning/tools/markdown/","text":"Markdown \u00b6 Troubleshooting \u00b6 Escape the backtick character \u00b6 Include a non-code formatted backtick by escaping it normally (with a \\ ). Render it in an inline code block using double backticks instead of single backticks. Alternatively, use a code block. This will wrap everything in a <pre> HTML tag. To do this, either indent 4 spaces to start a code block, or use fenced code blocks if supported. Sources \u00b6 Escaping backtick in Markdown","title":"Markdown"},{"location":"learning/tools/markdown/#markdown","text":"","title":"Markdown"},{"location":"learning/tools/markdown/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/markdown/#escape-the-backtick-character","text":"Include a non-code formatted backtick by escaping it normally (with a \\ ). Render it in an inline code block using double backticks instead of single backticks. Alternatively, use a code block. This will wrap everything in a <pre> HTML tag. To do this, either indent 4 spaces to start a code block, or use fenced code blocks if supported.","title":"Escape the backtick character"},{"location":"learning/tools/markdown/#sources","text":"Escaping backtick in Markdown","title":"Sources"},{"location":"learning/tools/mdl/","text":"Markdown lint tool \u00b6 Tool to check markdown files and flag style issues. TL;DR \u00b6 # Install. gem install mdl # Check files. mdl README.md mdl path/to/dir/ # Ignore a specific rule. mdl -r \"~MD013\" README.md Further readings \u00b6 GitHub","title":"Markdown lint tool"},{"location":"learning/tools/mdl/#markdown-lint-tool","text":"Tool to check markdown files and flag style issues.","title":"Markdown lint tool"},{"location":"learning/tools/mdl/#tldr","text":"# Install. gem install mdl # Check files. mdl README.md mdl path/to/dir/ # Ignore a specific rule. mdl -r \"~MD013\" README.md","title":"TL;DR"},{"location":"learning/tools/mdl/#further-readings","text":"GitHub","title":"Further readings"},{"location":"learning/tools/microos/","text":"OpenSUSE MicroOS \u00b6 TL;DR \u00b6 Every set of changes to the underlying system is executed on a new inactive snapshot, which will be the one the system will boot into on the next reboot. # Upgrade the system. sudo transactional-update dup pkcon update # Install a package. sudo transactional-update pkg install tlp ntfs-3g fuse-exfat nano pkcon install gnu_parallel # Get a shell on the next snapshot. # Lets you use zypper. sudo transactional-update shell sudo tukit execute bash Setting up MicroOS as a desktop OS \u00b6 See MicroOS Desktop for more and updated information. Further readings \u00b6 Flatpak Toolbox Sources \u00b6 MicroOS Desktop MicroOS Portal","title":"OpenSUSE MicroOS"},{"location":"learning/tools/microos/#opensuse-microos","text":"","title":"OpenSUSE MicroOS"},{"location":"learning/tools/microos/#tldr","text":"Every set of changes to the underlying system is executed on a new inactive snapshot, which will be the one the system will boot into on the next reboot. # Upgrade the system. sudo transactional-update dup pkcon update # Install a package. sudo transactional-update pkg install tlp ntfs-3g fuse-exfat nano pkcon install gnu_parallel # Get a shell on the next snapshot. # Lets you use zypper. sudo transactional-update shell sudo tukit execute bash","title":"TL;DR"},{"location":"learning/tools/microos/#setting-up-microos-as-a-desktop-os","text":"See MicroOS Desktop for more and updated information.","title":"Setting up MicroOS as a desktop OS"},{"location":"learning/tools/microos/#further-readings","text":"Flatpak Toolbox","title":"Further readings"},{"location":"learning/tools/microos/#sources","text":"MicroOS Desktop MicroOS Portal","title":"Sources"},{"location":"learning/tools/minikube/","text":"Minikube \u00b6 TL;DR \u00b6 # Install minikube. sudo pacman -S minikube brew install docker minikube # Shell completion. source < ( minikube completion $( basename $SHELL ) ) # Disable emojis in the commands. export MINIKUBE_IN_STYLE = false # Start the cluster. minikube start minikube start --cpus 4 --memory 8192 # Pause the cluster without impacting deployed applications minikube pause # Halt the cluster. minikube stop # Permanently increase the default memory limit. # Requires the cluster to restart. minikube config set memory 16384 # Browse the catalog of easily installable Kubernetes services. minikube addons list # Create a(nother) cluster running a specific Kubernetes version. minikube start -p old-k8s --kubernetes-version = v1.16.1 minikube config set kubernetes-version v1.16.15 && minikube start # Use a specific docker driver. minikube start --driver = docker minikube config set driver docker && minikube start # Disable new update notifications. minikube config set WantUpdateNotification false # Get IP and port of a service of type NodePort. minikube service --url nextcloud minikube service --url nextcloud --namespace nextcloud # Use the integrated kubectl command. minikube kubectl -- get pods # Log into the minikube environment (for debugging). minikube ssh # Delete all the clusters. minikube delete --all --purge Troubleshooting \u00b6 What happens if I use the LoadBalancer type with Services? \u00b6 On cloud providers that support load balancers, an external IP address would be provisioned to access the Service; on minikube, the LoadBalancer type makes the Service accessible through the minikube service command. Can I use custom certificates? \u00b6 Minikibe's certificates are available in the ~/.minikube/certs folder. Further readings \u00b6 Drivers Sources \u00b6 Accessing the services Getting started guide Cluster configuration Minikube's hello world The completion command The ssh command Use the docker driver How to use local docker images in Minikube How to use untrusted certs","title":"Minikube"},{"location":"learning/tools/minikube/#minikube","text":"","title":"Minikube"},{"location":"learning/tools/minikube/#tldr","text":"# Install minikube. sudo pacman -S minikube brew install docker minikube # Shell completion. source < ( minikube completion $( basename $SHELL ) ) # Disable emojis in the commands. export MINIKUBE_IN_STYLE = false # Start the cluster. minikube start minikube start --cpus 4 --memory 8192 # Pause the cluster without impacting deployed applications minikube pause # Halt the cluster. minikube stop # Permanently increase the default memory limit. # Requires the cluster to restart. minikube config set memory 16384 # Browse the catalog of easily installable Kubernetes services. minikube addons list # Create a(nother) cluster running a specific Kubernetes version. minikube start -p old-k8s --kubernetes-version = v1.16.1 minikube config set kubernetes-version v1.16.15 && minikube start # Use a specific docker driver. minikube start --driver = docker minikube config set driver docker && minikube start # Disable new update notifications. minikube config set WantUpdateNotification false # Get IP and port of a service of type NodePort. minikube service --url nextcloud minikube service --url nextcloud --namespace nextcloud # Use the integrated kubectl command. minikube kubectl -- get pods # Log into the minikube environment (for debugging). minikube ssh # Delete all the clusters. minikube delete --all --purge","title":"TL;DR"},{"location":"learning/tools/minikube/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/minikube/#what-happens-if-i-use-the-loadbalancer-type-with-services","text":"On cloud providers that support load balancers, an external IP address would be provisioned to access the Service; on minikube, the LoadBalancer type makes the Service accessible through the minikube service command.","title":"What happens if I use the LoadBalancer type with Services?"},{"location":"learning/tools/minikube/#can-i-use-custom-certificates","text":"Minikibe's certificates are available in the ~/.minikube/certs folder.","title":"Can I use custom certificates?"},{"location":"learning/tools/minikube/#further-readings","text":"Drivers","title":"Further readings"},{"location":"learning/tools/minikube/#sources","text":"Accessing the services Getting started guide Cluster configuration Minikube's hello world The completion command The ssh command Use the docker driver How to use local docker images in Minikube How to use untrusted certs","title":"Sources"},{"location":"learning/tools/mkpasswd/","text":"Debian's mkpasswd \u00b6 Crypts a given password using crypt(3). mkpasswd [ OPTIONS ] ... [ PASSWORD [ SALT ]] TL;DR \u00b6 # List available encrypting methods. mkpasswd -m -h # Return a hash of a specific method. mkpasswd -m 'nt' 'password' mkpasswd -m 'sha512crypt' 'password'","title":"Debian's mkpasswd"},{"location":"learning/tools/mkpasswd/#debians-mkpasswd","text":"Crypts a given password using crypt(3). mkpasswd [ OPTIONS ] ... [ PASSWORD [ SALT ]]","title":"Debian's mkpasswd"},{"location":"learning/tools/mkpasswd/#tldr","text":"# List available encrypting methods. mkpasswd -m -h # Return a hash of a specific method. mkpasswd -m 'nt' 'password' mkpasswd -m 'sha512crypt' 'password'","title":"TL;DR"},{"location":"learning/tools/mktemp/","text":"Mktemp \u00b6 Creates a unique temporary file or directory and returns the absolute path to it. TL;DR \u00b6 # create an empty temporary file mktemp # create an empty temporary directory mktemp -d # create an empty temporary file or directory with a random templated name # the Xs must be put at the end of the filename # the Xs specifies the templated parts and lenght in the file name mktemp /tmp/filenameXXX mktemp -d /tmp/dirname.XXX # create an empty temporary file or directory with a specified suffix (GNU only) mktemp --suffix \".txt\" # create an empty temporary file or directory with a specified prefix mktemp -t \"txt\" Further readings \u00b6 Man page","title":"Mktemp"},{"location":"learning/tools/mktemp/#mktemp","text":"Creates a unique temporary file or directory and returns the absolute path to it.","title":"Mktemp"},{"location":"learning/tools/mktemp/#tldr","text":"# create an empty temporary file mktemp # create an empty temporary directory mktemp -d # create an empty temporary file or directory with a random templated name # the Xs must be put at the end of the filename # the Xs specifies the templated parts and lenght in the file name mktemp /tmp/filenameXXX mktemp -d /tmp/dirname.XXX # create an empty temporary file or directory with a specified suffix (GNU only) mktemp --suffix \".txt\" # create an empty temporary file or directory with a specified prefix mktemp -t \"txt\"","title":"TL;DR"},{"location":"learning/tools/mktemp/#further-readings","text":"Man page","title":"Further readings"},{"location":"learning/tools/mount%20files%20as%20virtual%20file%20systems/","text":"Mount files as virtual file systems \u00b6 TL;DR \u00b6 # Create the file. truncate -s '10G' 'path/to/file' dd if = '/dev/zero' of = 'path/to/file' bs = 4MiB count = 250K status = 'progress' # Create the file system on such file. mkfs.ext4 'path/to/file' # Create the mount point. mkdir 'mount/point' # Mount the file system. # The 'loop' option is optional. sudo mount -t 'ext4' -o 'loop' 'path/to/file' 'mount/point' Prefer truncate to dd to let the file expand dynamically and be resized (both larger or smaller) without damaging data with losetup and resize2fs . Further readings \u00b6 dd truncate Sources \u00b6 How do I create a file and mount it as a filesystem?","title":"Mount files as virtual file systems"},{"location":"learning/tools/mount%20files%20as%20virtual%20file%20systems/#mount-files-as-virtual-file-systems","text":"","title":"Mount files as virtual file systems"},{"location":"learning/tools/mount%20files%20as%20virtual%20file%20systems/#tldr","text":"# Create the file. truncate -s '10G' 'path/to/file' dd if = '/dev/zero' of = 'path/to/file' bs = 4MiB count = 250K status = 'progress' # Create the file system on such file. mkfs.ext4 'path/to/file' # Create the mount point. mkdir 'mount/point' # Mount the file system. # The 'loop' option is optional. sudo mount -t 'ext4' -o 'loop' 'path/to/file' 'mount/point' Prefer truncate to dd to let the file expand dynamically and be resized (both larger or smaller) without damaging data with losetup and resize2fs .","title":"TL;DR"},{"location":"learning/tools/mount%20files%20as%20virtual%20file%20systems/#further-readings","text":"dd truncate","title":"Further readings"},{"location":"learning/tools/mount%20files%20as%20virtual%20file%20systems/#sources","text":"How do I create a file and mount it as a filesystem?","title":"Sources"},{"location":"learning/tools/mount%20samba%20shares%20from%20a%20unix%20client/","text":"Mount samba shares from a unix client \u00b6 TL;DR \u00b6 sudo mount -t cifs -o user = my-user //nas.local/shared_folder local_folder Further readings \u00b6 Mounting samba shares from a unix client","title":"Mount samba shares from a unix client"},{"location":"learning/tools/mount%20samba%20shares%20from%20a%20unix%20client/#mount-samba-shares-from-a-unix-client","text":"","title":"Mount samba shares from a unix client"},{"location":"learning/tools/mount%20samba%20shares%20from%20a%20unix%20client/#tldr","text":"sudo mount -t cifs -o user = my-user //nas.local/shared_folder local_folder","title":"TL;DR"},{"location":"learning/tools/mount%20samba%20shares%20from%20a%20unix%20client/#further-readings","text":"Mounting samba shares from a unix client","title":"Further readings"},{"location":"learning/tools/mount/","text":"Mount \u00b6 TL;DR \u00b6 # Mount a file system by its label. mount -L 'label' '/path/to/mount/point' # Mount a file system by its UUID. mount -U 'uuid' '/path/to/mount/point' # Manually Mount a SMB share. mount -t 'cifs' -o 'username=user_name' '//server/share_name' '/mount/point' # Mount a SMB share using an encrypted SMB 3.0 connection. mount -t 'cifs' -o 'username=DOMAIN\\Administrator,seal,vers=3.0' \\ '//server_name/share_name' '/mount/point' # Mount a NFS share mount -t 'nfs' 'server:/share_name' '/mount/point' mount -t 'nfs' -o 'nfsvers=3,nolock' 'server:/share_name' '/mount/point' Further readings \u00b6 Mount a disk partition using LABEL Mount a disk partition using UUID How to mount a .img file Manually Mounting an SMB Share Access denied by server while mounting NFS share","title":"Mount"},{"location":"learning/tools/mount/#mount","text":"","title":"Mount"},{"location":"learning/tools/mount/#tldr","text":"# Mount a file system by its label. mount -L 'label' '/path/to/mount/point' # Mount a file system by its UUID. mount -U 'uuid' '/path/to/mount/point' # Manually Mount a SMB share. mount -t 'cifs' -o 'username=user_name' '//server/share_name' '/mount/point' # Mount a SMB share using an encrypted SMB 3.0 connection. mount -t 'cifs' -o 'username=DOMAIN\\Administrator,seal,vers=3.0' \\ '//server_name/share_name' '/mount/point' # Mount a NFS share mount -t 'nfs' 'server:/share_name' '/mount/point' mount -t 'nfs' -o 'nfsvers=3,nolock' 'server:/share_name' '/mount/point'","title":"TL;DR"},{"location":"learning/tools/mount/#further-readings","text":"Mount a disk partition using LABEL Mount a disk partition using UUID How to mount a .img file Manually Mounting an SMB Share Access denied by server while mounting NFS share","title":"Further readings"},{"location":"learning/tools/multipass/","text":"Multipass \u00b6 TL;DR \u00b6 # Install. brew install --cask 'multipass' choco install 'multipass' sudo snap install 'multipass' # Find available VMs. multipass find # Launch a VM. multipass launch --name 'primary' multipass launch -c '2' -m '2G' -d '20G' -n 'my-test-vm' '21.10' multipass launch 'bionic' --name 'test-cloud-init' --cloud-init 'userdata.yaml' # List all VMs. multipass list # Launch a shell in the VM. multipass shell 'vm_name' # Stop started VMs. multipass stop 'vm_name' # Start stopped VMs. multipass start 'vm_name' # Delete stopped VMs. multipass delete my-test-vm # Clean up unused data. multipass purge Further readings \u00b6 Website Sources \u00b6 Use Linux Virtual Machines with Multipass","title":"Multipass"},{"location":"learning/tools/multipass/#multipass","text":"","title":"Multipass"},{"location":"learning/tools/multipass/#tldr","text":"# Install. brew install --cask 'multipass' choco install 'multipass' sudo snap install 'multipass' # Find available VMs. multipass find # Launch a VM. multipass launch --name 'primary' multipass launch -c '2' -m '2G' -d '20G' -n 'my-test-vm' '21.10' multipass launch 'bionic' --name 'test-cloud-init' --cloud-init 'userdata.yaml' # List all VMs. multipass list # Launch a shell in the VM. multipass shell 'vm_name' # Stop started VMs. multipass stop 'vm_name' # Start stopped VMs. multipass start 'vm_name' # Delete stopped VMs. multipass delete my-test-vm # Clean up unused data. multipass purge","title":"TL;DR"},{"location":"learning/tools/multipass/#further-readings","text":"Website","title":"Further readings"},{"location":"learning/tools/multipass/#sources","text":"Use Linux Virtual Machines with Multipass","title":"Sources"},{"location":"learning/tools/mysql/","text":"MySQL \u00b6 TL;DR \u00b6 # connect with user \"root\" on the local default socket # don't ask password and do not select db mysql # connect with user \"user\" on the local default socket # ask for a password and execute a command mysql -u user -p -e 'show databases;' # provide a password on the cli # put no spaces between -p and the password mysql -u ${ USERNAME } -h ${ HOST } -p ${ PASSWORD } ${ DATABASE } -- list the available databases SHOW DATABASES ; SHOW DATABASES LIKE 'open%' ; -- list tables in the pizza_store database use pizza_store ; show tables ; -- give permissions grant ALL on db . * to 'username' @ 'localhost' identified by 'password' ; grant ALL on db . * to 'username' @ '127.0.0.1' ; Further readings \u00b6 How to list tables in MySQL How to show databases in MySQL phpimap issue 1549","title":"MySQL"},{"location":"learning/tools/mysql/#mysql","text":"","title":"MySQL"},{"location":"learning/tools/mysql/#tldr","text":"# connect with user \"root\" on the local default socket # don't ask password and do not select db mysql # connect with user \"user\" on the local default socket # ask for a password and execute a command mysql -u user -p -e 'show databases;' # provide a password on the cli # put no spaces between -p and the password mysql -u ${ USERNAME } -h ${ HOST } -p ${ PASSWORD } ${ DATABASE } -- list the available databases SHOW DATABASES ; SHOW DATABASES LIKE 'open%' ; -- list tables in the pizza_store database use pizza_store ; show tables ; -- give permissions grant ALL on db . * to 'username' @ 'localhost' identified by 'password' ; grant ALL on db . * to 'username' @ '127.0.0.1' ;","title":"TL;DR"},{"location":"learning/tools/mysql/#further-readings","text":"How to list tables in MySQL How to show databases in MySQL phpimap issue 1549","title":"Further readings"},{"location":"learning/tools/nc/","text":"Netcat \u00b6 -N : close the network socket when finished; not available in nmap's netcat -l : bind to the port and listen for incoming connections (server mode) -n : do not resolve hostnames via DNS -p : specify the source port to use -t : use telnet negotiation -u : use UDP -v : set verbosity level; can be used several times -w=SECS : timeout for connects and final net reads, in seconds -z : zero-I/O mode, exit once connected TL;DR \u00b6 # Check ports on hosts. nc -Nnvz 192 .168.0.81 22 nc -Nvz host.name 443 # List hosts with a specific port open. parallel -j 0 \"nc -Nnvz -w 2 192.168.0.{} 22 2>&1\" ::: { 2 ..254 } \\ | grep -v \"timed out\" # Wait for a host to be up. until nc -Nvz -w 3 pi.lan 22 ; do sleep 3 ; done","title":"Netcat"},{"location":"learning/tools/nc/#netcat","text":"-N : close the network socket when finished; not available in nmap's netcat -l : bind to the port and listen for incoming connections (server mode) -n : do not resolve hostnames via DNS -p : specify the source port to use -t : use telnet negotiation -u : use UDP -v : set verbosity level; can be used several times -w=SECS : timeout for connects and final net reads, in seconds -z : zero-I/O mode, exit once connected","title":"Netcat"},{"location":"learning/tools/nc/#tldr","text":"# Check ports on hosts. nc -Nnvz 192 .168.0.81 22 nc -Nvz host.name 443 # List hosts with a specific port open. parallel -j 0 \"nc -Nnvz -w 2 192.168.0.{} 22 2>&1\" ::: { 2 ..254 } \\ | grep -v \"timed out\" # Wait for a host to be up. until nc -Nvz -w 3 pi.lan 22 ; do sleep 3 ; done","title":"TL;DR"},{"location":"learning/tools/needs-restarting/","text":"Needs-restarting \u00b6 needs-restarting -r returns 1 if a reboot is needed, and 0 if it is not. TL;DR \u00b6 # Install. sudo dnf install dnf-utils sudo yum install yum-utils # Check if a full reboot is required. sudo needs-restarting -r # Show what services need to be restarted. sudo needs-restarting -s $ sudo needs-restarting The following running processes use deleted files: PID | PPID | UID | User | Command | Service -----+------+------+------------+-----------------------------------+---------------- 731 | 1 | 488 | avahi | avahi-daemon | avahi-daemon 736 | 1 | 490 | messagebus | dbus-daemon | dbus \u2026 6260 | 1756 | 1000 | mek | kdeinit5 | You may wish to restart these processes. See 'man zypper' for information about the meaning of values in the above table. Core libraries or services have been updated. Reboot is required to ensure that your system benefits from these updates. Sources \u00b6 Automatic Reboot on Kernel Update","title":"Needs-restarting"},{"location":"learning/tools/needs-restarting/#needs-restarting","text":"needs-restarting -r returns 1 if a reboot is needed, and 0 if it is not.","title":"Needs-restarting"},{"location":"learning/tools/needs-restarting/#tldr","text":"# Install. sudo dnf install dnf-utils sudo yum install yum-utils # Check if a full reboot is required. sudo needs-restarting -r # Show what services need to be restarted. sudo needs-restarting -s $ sudo needs-restarting The following running processes use deleted files: PID | PPID | UID | User | Command | Service -----+------+------+------------+-----------------------------------+---------------- 731 | 1 | 488 | avahi | avahi-daemon | avahi-daemon 736 | 1 | 490 | messagebus | dbus-daemon | dbus \u2026 6260 | 1756 | 1000 | mek | kdeinit5 | You may wish to restart these processes. See 'man zypper' for information about the meaning of values in the above table. Core libraries or services have been updated. Reboot is required to ensure that your system benefits from these updates.","title":"TL;DR"},{"location":"learning/tools/needs-restarting/#sources","text":"Automatic Reboot on Kernel Update","title":"Sources"},{"location":"learning/tools/netrc/","text":"The .netrc file \u00b6 Specifies automatic login information for the ftp and rexec commands. It is located under a user's home directory ( ~/.netrc ) and must be owned either by the user executing the command or by the root user. If the .netrc file contains a login password, the file's permissions must be set to 600 (read and write by its owner only). Format \u00b6 The file can contain the following entries separated by spaces, tabs, or new lines: machine hostname : this begins the definition of the automatic login process for the specified hostname ; all the following entries, up to a new machine entry or the end of the file, will apply to hostname default : like machine , but matches any hostname; there can be only 1 in the whole file and it is considered the last entry (entries following it will be ignored) login username : the full domain user name used for authentication; if found the automatic login process initiates a login with the specified username , else it will fail password password : the password to use for authentication; it must be set at the remote host and must be present in .netrc , otherwise the process will fail and the user is prompted for a new value passwords in this fields cannot contain spaces The two formats below are equivalent: machine example.com login daniel password qwerty machine host1.austin.century.com login fred password bluebonnet machine example.com login daniel password qwerty machine host1.austin.century.com login fred password bluebonnet Further readings \u00b6 netrc","title":"The `.netrc` file"},{"location":"learning/tools/netrc/#the-netrc-file","text":"Specifies automatic login information for the ftp and rexec commands. It is located under a user's home directory ( ~/.netrc ) and must be owned either by the user executing the command or by the root user. If the .netrc file contains a login password, the file's permissions must be set to 600 (read and write by its owner only).","title":"The .netrc file"},{"location":"learning/tools/netrc/#format","text":"The file can contain the following entries separated by spaces, tabs, or new lines: machine hostname : this begins the definition of the automatic login process for the specified hostname ; all the following entries, up to a new machine entry or the end of the file, will apply to hostname default : like machine , but matches any hostname; there can be only 1 in the whole file and it is considered the last entry (entries following it will be ignored) login username : the full domain user name used for authentication; if found the automatic login process initiates a login with the specified username , else it will fail password password : the password to use for authentication; it must be set at the remote host and must be present in .netrc , otherwise the process will fail and the user is prompted for a new value passwords in this fields cannot contain spaces The two formats below are equivalent: machine example.com login daniel password qwerty machine host1.austin.century.com login fred password bluebonnet machine example.com login daniel password qwerty machine host1.austin.century.com login fred password bluebonnet","title":"Format"},{"location":"learning/tools/netrc/#further-readings","text":"netrc","title":"Further readings"},{"location":"learning/tools/network%20manager/","text":"Network Manager \u00b6 TL;DR \u00b6 # Get all settings of a connection. nmcli connection show 'Wired connection 1' # Change the autoconnect priority setting of a connection. # Higher numbers set a higher priority. nmcli connection modify 'it hurts when ip' connection.autoconnect-priority 1 # Start the TUI. nmtui Sources \u00b6 nm-settings","title":"Network Manager"},{"location":"learning/tools/network%20manager/#network-manager","text":"","title":"Network Manager"},{"location":"learning/tools/network%20manager/#tldr","text":"# Get all settings of a connection. nmcli connection show 'Wired connection 1' # Change the autoconnect priority setting of a connection. # Higher numbers set a higher priority. nmcli connection modify 'it hurts when ip' connection.autoconnect-priority 1 # Start the TUI. nmtui","title":"TL;DR"},{"location":"learning/tools/network%20manager/#sources","text":"nm-settings","title":"Sources"},{"location":"learning/tools/newman/","text":"Newman \u00b6 CLI Collection runner for Postman. TL;DR Further readings TL;DR \u00b6 brew install newman Further readings \u00b6 Postman Running Collections on the command line with Newman","title":"Newman <!-- omit in toc -->"},{"location":"learning/tools/newman/#newman","text":"CLI Collection runner for Postman. TL;DR Further readings","title":"Newman "},{"location":"learning/tools/newman/#tldr","text":"brew install newman","title":"TL;DR"},{"location":"learning/tools/newman/#further-readings","text":"Postman Running Collections on the command line with Newman","title":"Further readings"},{"location":"learning/tools/nmap/","text":"Nmap \u00b6 TL;DR \u00b6 # scan all 65535 ports on a host nmap -p- 192 .168.1.1 # scan a single port on a subnet nmap -p 22 192 .168.0.0/24 # detect a host's os nmap -O 192 .168.0.1 Further readings \u00b6 Cheatsheet OS detection","title":"Nmap"},{"location":"learning/tools/nmap/#nmap","text":"","title":"Nmap"},{"location":"learning/tools/nmap/#tldr","text":"# scan all 65535 ports on a host nmap -p- 192 .168.1.1 # scan a single port on a subnet nmap -p 22 192 .168.0.0/24 # detect a host's os nmap -O 192 .168.0.1","title":"TL;DR"},{"location":"learning/tools/nmap/#further-readings","text":"Cheatsheet OS detection","title":"Further readings"},{"location":"learning/tools/nvme-cli/","text":"Nvme-cli \u00b6 TL;DR \u00b6 # Installation. sudo apt install nvme-cli # List available devices. sudo nvme list # Show data about devices. sudo nvme smart-log /dev/nvme0 Sources \u00b6 How to check CPU temperature on Ubuntu Linux","title":"Nvme-cli"},{"location":"learning/tools/nvme-cli/#nvme-cli","text":"","title":"Nvme-cli"},{"location":"learning/tools/nvme-cli/#tldr","text":"# Installation. sudo apt install nvme-cli # List available devices. sudo nvme list # Show data about devices. sudo nvme smart-log /dev/nvme0","title":"TL;DR"},{"location":"learning/tools/nvme-cli/#sources","text":"How to check CPU temperature on Ubuntu Linux","title":"Sources"},{"location":"learning/tools/openssl/","text":"OpenSSL \u00b6 TL;DR Troubleshooting Code 20: unable to get local issuer certificate Code 21: unable to verify the first certificate Sources TL;DR \u00b6 # Check a certificate and return information about it. openssl x509 -in 'certificate.crt' -text -noout # Check a key and verify its consistency. openssl rsa -in 'file.key' -check # Verify a CSR and print the data given in input during creation. openssl req -in 'request.csr' -text -noout -verify # Check a PKCS#12 file (.p12 or .pfx). openssl pkcs12 -info -in 'keyStore.p12' # Check a MD5 hash of the public key to ensure it matches with the one in a CSR # or private key. openssl x509 -noout -modulus -in 'certificate.crt' | openssl md5 openssl rsa -noout -modulus -in 'private.key' | openssl md5 openssl req -noout -modulus -in 'request.csr' | openssl md5 # Check an SSL connection. # All the certificates (including the intermediate ones) should be displayed. # CA certificates bundle on Linux: /etc/ssl/certs/ca-certificates.crt. # '-servername' used to specify a domain for multi-domain servers. openssl s_client -connect 'fqdn:port' -servername 'host-fqdn' -showcerts openssl \u2026 -CAfile 'ca/certificates/bundle.crt' openssl \u2026 -CApath '/etc/ssl/certs' # Generate a password-protected self-signed certificate. openssl req -x509 \\ -sha256 -newkey 'rsa:4096' -keyout 'private.key' \\ -subj '/C=US/ST=Oregon/L=Portland/O=Company Name/OU=Org/CN=www.example.com' \\ -out 'certificate.pem' -days '365' # Generate a new non-protected signing request. openssl req -new \\ -config 'domain.conf' \\ -sha256 -newkey 'rsa:2048' -nodes -keyout 'domain.key' \\ -days '365' -out 'domain.req.pem' # Generate a Certificate Signing Request for an existing private key. openssl req -new -key 'private.key' -out 'request.csr' # Generate a Certificate Signing Request from an existing certificate and key. openssl x509 -x509toreq \\ -in 'certificate.crt' -out 'request.csr' -signkey 'private.key' # Remove password protection from a key. openssl rsa -in 'protected.key' -out 'unprotected.key' # Convert a DER-formatted file (.crt .cer .der) to the PEM format. openssl x509 -inform 'der' -in 'certificate.cer' -out 'certificate.pem' # Convert a PEM file to the DER format. openssl x509 -outform 'der' -in 'certificate.pem' -out 'certificate.der' # Convert a PKCS#12 file (.pfx .p12) with private key and certificates to PEM. # Add -nocerts to output only the private key. # Add -nokeys to output only the certificates. openssl pkcs12 -in 'keyStore.pfx' -out 'keyStore.pem' -nodes # Convert a PEM certificate file and a private key to PKCS#12 (.pfx .p12). openssl pkcs12 -export -out 'certificate.pfx' \\ -inkey 'privateKey.key' -in 'certificate.crt' -certfile # Verify a certificate chain. # If a certificate is its own issuer, it is assumed to be the root CA. # This means the root CA needs to be self signed for 'verify' to work. openssl verify -CAfile 'RootCert.pem' -untrusted 'Intermediate.pem' 'UserCert.pem' # Create bundles. cat 'server.crt' 'intermediate1.crt' 'intermediateN.crt' 'rootca.crt' Troubleshooting \u00b6 Code 20: unable to get local issuer certificate \u00b6 An openssl s_client -connect attempt fails with this error message: CONNECTED(00000003) depth=0 C = US, CN = server.fqdn verify error:num=20:unable to get local issuer certificate verify return:1 depth=0 C = US, CN = server.fqdn verify error:num=21:unable to verify the first certificate verify return:1 --- \u2026 SSL-Session: \u2026 Verify return code: 21 (unable to verify the first certificate) --- closed See also OpenSSL unable to verify the first certificate for Experian URL and Verify certificate chain with OpenSSL . One or more certificates in the certificate chain is not valid, self-signed or simply was not provided by either the server or the client (if a client certificate is needed). This could also mean that the root certificate is not in the local database of trusted root certificates, which could have been not given to, or queried by, OpenSSL. A well configured server sends the entire certificate chain during the handshake, therefore providing all the necessary intermediate certificates; servers for which the connection fails might be providing only the end entity certificate. OpenSSL is not capable of getting missing intermediate certificates on-the-fly, so a s_client -connect attempt could fail where a full-fledge browser, able to discover certificates, would succeed on the same URL. You can: either make the server send the entire certificate chain or pass the missing certificates to OpenSSL as client-side parameters using the '-CApath' or '-CAfile' options. Code 21: unable to verify the first certificate \u00b6 The certificate chain is broken. This error is somewhat generic, and a previous error message might be telling more about the problem. See code 20 . Sources \u00b6 OpenSSL commands to check and verify your SSL certificate, key and CSR How to generate a self-signed SSL certificate using OpenSSL The most common OpenSSL commands OpenSSL unable to verify the first certificate for Experian URL Verify certificate chain with OpenSSL","title":"OpenSSL"},{"location":"learning/tools/openssl/#openssl","text":"TL;DR Troubleshooting Code 20: unable to get local issuer certificate Code 21: unable to verify the first certificate Sources","title":"OpenSSL"},{"location":"learning/tools/openssl/#tldr","text":"# Check a certificate and return information about it. openssl x509 -in 'certificate.crt' -text -noout # Check a key and verify its consistency. openssl rsa -in 'file.key' -check # Verify a CSR and print the data given in input during creation. openssl req -in 'request.csr' -text -noout -verify # Check a PKCS#12 file (.p12 or .pfx). openssl pkcs12 -info -in 'keyStore.p12' # Check a MD5 hash of the public key to ensure it matches with the one in a CSR # or private key. openssl x509 -noout -modulus -in 'certificate.crt' | openssl md5 openssl rsa -noout -modulus -in 'private.key' | openssl md5 openssl req -noout -modulus -in 'request.csr' | openssl md5 # Check an SSL connection. # All the certificates (including the intermediate ones) should be displayed. # CA certificates bundle on Linux: /etc/ssl/certs/ca-certificates.crt. # '-servername' used to specify a domain for multi-domain servers. openssl s_client -connect 'fqdn:port' -servername 'host-fqdn' -showcerts openssl \u2026 -CAfile 'ca/certificates/bundle.crt' openssl \u2026 -CApath '/etc/ssl/certs' # Generate a password-protected self-signed certificate. openssl req -x509 \\ -sha256 -newkey 'rsa:4096' -keyout 'private.key' \\ -subj '/C=US/ST=Oregon/L=Portland/O=Company Name/OU=Org/CN=www.example.com' \\ -out 'certificate.pem' -days '365' # Generate a new non-protected signing request. openssl req -new \\ -config 'domain.conf' \\ -sha256 -newkey 'rsa:2048' -nodes -keyout 'domain.key' \\ -days '365' -out 'domain.req.pem' # Generate a Certificate Signing Request for an existing private key. openssl req -new -key 'private.key' -out 'request.csr' # Generate a Certificate Signing Request from an existing certificate and key. openssl x509 -x509toreq \\ -in 'certificate.crt' -out 'request.csr' -signkey 'private.key' # Remove password protection from a key. openssl rsa -in 'protected.key' -out 'unprotected.key' # Convert a DER-formatted file (.crt .cer .der) to the PEM format. openssl x509 -inform 'der' -in 'certificate.cer' -out 'certificate.pem' # Convert a PEM file to the DER format. openssl x509 -outform 'der' -in 'certificate.pem' -out 'certificate.der' # Convert a PKCS#12 file (.pfx .p12) with private key and certificates to PEM. # Add -nocerts to output only the private key. # Add -nokeys to output only the certificates. openssl pkcs12 -in 'keyStore.pfx' -out 'keyStore.pem' -nodes # Convert a PEM certificate file and a private key to PKCS#12 (.pfx .p12). openssl pkcs12 -export -out 'certificate.pfx' \\ -inkey 'privateKey.key' -in 'certificate.crt' -certfile # Verify a certificate chain. # If a certificate is its own issuer, it is assumed to be the root CA. # This means the root CA needs to be self signed for 'verify' to work. openssl verify -CAfile 'RootCert.pem' -untrusted 'Intermediate.pem' 'UserCert.pem' # Create bundles. cat 'server.crt' 'intermediate1.crt' 'intermediateN.crt' 'rootca.crt'","title":"TL;DR"},{"location":"learning/tools/openssl/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/openssl/#code-20-unable-to-get-local-issuer-certificate","text":"An openssl s_client -connect attempt fails with this error message: CONNECTED(00000003) depth=0 C = US, CN = server.fqdn verify error:num=20:unable to get local issuer certificate verify return:1 depth=0 C = US, CN = server.fqdn verify error:num=21:unable to verify the first certificate verify return:1 --- \u2026 SSL-Session: \u2026 Verify return code: 21 (unable to verify the first certificate) --- closed See also OpenSSL unable to verify the first certificate for Experian URL and Verify certificate chain with OpenSSL . One or more certificates in the certificate chain is not valid, self-signed or simply was not provided by either the server or the client (if a client certificate is needed). This could also mean that the root certificate is not in the local database of trusted root certificates, which could have been not given to, or queried by, OpenSSL. A well configured server sends the entire certificate chain during the handshake, therefore providing all the necessary intermediate certificates; servers for which the connection fails might be providing only the end entity certificate. OpenSSL is not capable of getting missing intermediate certificates on-the-fly, so a s_client -connect attempt could fail where a full-fledge browser, able to discover certificates, would succeed on the same URL. You can: either make the server send the entire certificate chain or pass the missing certificates to OpenSSL as client-side parameters using the '-CApath' or '-CAfile' options.","title":"Code 20: unable to get local issuer certificate"},{"location":"learning/tools/openssl/#code-21-unable-to-verify-the-first-certificate","text":"The certificate chain is broken. This error is somewhat generic, and a previous error message might be telling more about the problem. See code 20 .","title":"Code 21: unable to verify the first certificate"},{"location":"learning/tools/openssl/#sources","text":"OpenSSL commands to check and verify your SSL certificate, key and CSR How to generate a self-signed SSL certificate using OpenSSL The most common OpenSSL commands OpenSSL unable to verify the first certificate for Experian URL Verify certificate chain with OpenSSL","title":"Sources"},{"location":"learning/tools/opensuse%20linux/","text":"OpenSUSE \u00b6 Enable Bluetooth pairing on boot \u00b6 enable the bluetooth service on boot install bluez-auto-enable-devices ; this will create the configuration file /etc/bluetooth/main.conf Also see specific settings in the Bluetooth KB. Enable SSH access from outside after installation \u00b6 Open port 22 on the firewall: using Yast: open Yast2 > Firewall make sure your interfaces are appointed to the External zone check ssh is in the Allowed services column and add it to the list if not save the configuration and exit (make sure the firewall is reloaded on exit) using firewall-cmd on the command line: sudo firewall-cmd --add-port = 22 /tcp --permanent Start the SSH daemon: using Yast: open Yast2 > System services and enable SSHD using systemctl on the command line: sudo systemctl enable --now sshd.service Raspberry Pi \u00b6 Install the OS from another computer capable of reading and writing SD cards. Given /dev/sdb being a SD card, use the following: curl -C - -L -o opensuse.raw.xz http://download.opensuse.org/ports/aarch64/tumbleweed/appliances/openSUSE-Tumbleweed-ARM-JeOS-raspberrypi.aarch64.raw.xz xzcat opensuse.raw.xz \\ | sudo dd bs = 4M of = /dev/sdb iflag = fullblock oflag = direct status = progress \\ && sync Insert the SD card in the Raspberry Pi and power it on. The network is configured to get an IP address on eth0 using DHCP. Connect using SSH and login using root:linux . Firmware update from a running system \u00b6 # Check for an updated firmware. sudo rpi-eeprom-update # Install the new version and reboot. sudo rpi-eeprom-update -a && sync && reboot Rollback from a bootable snapshot \u00b6 Do as follows: boot the system in GRUB's boot menu, choose Bootable snapshots select the snapshot you want to boot into; the list of snapshots is listed by date, the most recent snapshot being listed first log in to the system carefully check whether everything works as expected You cannot write to any directory that is part of the snapshot. Data you write to other directories will not get lost, regardless of what you do next. Depending on whether you want to perform the rollback or not, choose your next step: if the system is in a state where you do not want to do a rollback, reboot and boot again into a different snapshot, or start the rescue system. to perform the rollback, run sudo snapper rollback and reboot On the boot screen, choose the default boot entry to reboot into the reinstated system. A snapshot of the file system status before the rollback is created, and the default subvolume for root will be replaced with a fresh read-write snapshot. Further readings \u00b6 Bluetooth Firewalld Systemd System Recovery and Snapshot Management with Snapper Sources \u00b6 OpenSSH basics Bluetooth on boot Raspberry Pi4","title":"OpenSUSE"},{"location":"learning/tools/opensuse%20linux/#opensuse","text":"","title":"OpenSUSE"},{"location":"learning/tools/opensuse%20linux/#enable-bluetooth-pairing-on-boot","text":"enable the bluetooth service on boot install bluez-auto-enable-devices ; this will create the configuration file /etc/bluetooth/main.conf Also see specific settings in the Bluetooth KB.","title":"Enable Bluetooth pairing on boot"},{"location":"learning/tools/opensuse%20linux/#enable-ssh-access-from-outside-after-installation","text":"Open port 22 on the firewall: using Yast: open Yast2 > Firewall make sure your interfaces are appointed to the External zone check ssh is in the Allowed services column and add it to the list if not save the configuration and exit (make sure the firewall is reloaded on exit) using firewall-cmd on the command line: sudo firewall-cmd --add-port = 22 /tcp --permanent Start the SSH daemon: using Yast: open Yast2 > System services and enable SSHD using systemctl on the command line: sudo systemctl enable --now sshd.service","title":"Enable SSH access from outside after installation"},{"location":"learning/tools/opensuse%20linux/#raspberry-pi","text":"Install the OS from another computer capable of reading and writing SD cards. Given /dev/sdb being a SD card, use the following: curl -C - -L -o opensuse.raw.xz http://download.opensuse.org/ports/aarch64/tumbleweed/appliances/openSUSE-Tumbleweed-ARM-JeOS-raspberrypi.aarch64.raw.xz xzcat opensuse.raw.xz \\ | sudo dd bs = 4M of = /dev/sdb iflag = fullblock oflag = direct status = progress \\ && sync Insert the SD card in the Raspberry Pi and power it on. The network is configured to get an IP address on eth0 using DHCP. Connect using SSH and login using root:linux .","title":"Raspberry Pi"},{"location":"learning/tools/opensuse%20linux/#firmware-update-from-a-running-system","text":"# Check for an updated firmware. sudo rpi-eeprom-update # Install the new version and reboot. sudo rpi-eeprom-update -a && sync && reboot","title":"Firmware update from a running system"},{"location":"learning/tools/opensuse%20linux/#rollback-from-a-bootable-snapshot","text":"Do as follows: boot the system in GRUB's boot menu, choose Bootable snapshots select the snapshot you want to boot into; the list of snapshots is listed by date, the most recent snapshot being listed first log in to the system carefully check whether everything works as expected You cannot write to any directory that is part of the snapshot. Data you write to other directories will not get lost, regardless of what you do next. Depending on whether you want to perform the rollback or not, choose your next step: if the system is in a state where you do not want to do a rollback, reboot and boot again into a different snapshot, or start the rescue system. to perform the rollback, run sudo snapper rollback and reboot On the boot screen, choose the default boot entry to reboot into the reinstated system. A snapshot of the file system status before the rollback is created, and the default subvolume for root will be replaced with a fresh read-write snapshot.","title":"Rollback from a bootable snapshot"},{"location":"learning/tools/opensuse%20linux/#further-readings","text":"Bluetooth Firewalld Systemd System Recovery and Snapshot Management with Snapper","title":"Further readings"},{"location":"learning/tools/opensuse%20linux/#sources","text":"OpenSSH basics Bluetooth on boot Raspberry Pi4","title":"Sources"},{"location":"learning/tools/opkg/","text":"Opkg \u00b6 TL;DR \u00b6 # update the list of available packages opkg update # list available/installed/upgradable packages opkg list opkg list-installed opkg list-upgradable # install one or more packages opkg install zram-swap opkg install http://downloads.openwrt.org/snapshots/trunk/ar71xx/packages/hiawatha_7.7-2_ar71xx.ipk opkg install /tmp/hiawatha_7.7-2_ar71xx.ipk # remove one or more packages opkg remove youtube-dl # upgrade all installed packages opkg upgrade # upgrade one or more specific packages opkg upgrade vim yubico-pam # display informations for a specific package opkg info python3-dns # list packages providing a file opkg search /usr/bin/vim # list user modified configuration files opkg list-changed-conffiles # list dependencies of a package opkg depends dropbear Further readings \u00b6 Opkg package manager","title":"Opkg"},{"location":"learning/tools/opkg/#opkg","text":"","title":"Opkg"},{"location":"learning/tools/opkg/#tldr","text":"# update the list of available packages opkg update # list available/installed/upgradable packages opkg list opkg list-installed opkg list-upgradable # install one or more packages opkg install zram-swap opkg install http://downloads.openwrt.org/snapshots/trunk/ar71xx/packages/hiawatha_7.7-2_ar71xx.ipk opkg install /tmp/hiawatha_7.7-2_ar71xx.ipk # remove one or more packages opkg remove youtube-dl # upgrade all installed packages opkg upgrade # upgrade one or more specific packages opkg upgrade vim yubico-pam # display informations for a specific package opkg info python3-dns # list packages providing a file opkg search /usr/bin/vim # list user modified configuration files opkg list-changed-conffiles # list dependencies of a package opkg depends dropbear","title":"TL;DR"},{"location":"learning/tools/opkg/#further-readings","text":"Opkg package manager","title":"Further readings"},{"location":"learning/tools/pacman/","text":"Pacman \u00b6 Useful options: --asdeps --asexplicit --needed --unneeded TL;DR \u00b6 # search an installed package pacman --query --search ddc # list all explicitly installed packages pacman --query --explicit # set a package as explicitly (manually) installed pacman --database --asexplicit dkms # set a package as installed as dependency (automatically installed) pacman --database --asdeps autoconf # install zsh unsupervisioned (useful in scrips) pacman --noconfirm \\ --sync --needed --noprogressbar --quiet --refresh \\ fzf zsh-completions # completely remove virtualbox-guest-utils-nox unsupervisioned (useful in scrips) pacman --noconfirm \\ --remove --nosave --noprogressbar --quiet --recursive --unneeded \\ virtualbox-guest-utils-nox Further readings \u00b6 Prevent pacman from reinstalling packages that were already installed","title":"Pacman"},{"location":"learning/tools/pacman/#pacman","text":"Useful options: --asdeps --asexplicit --needed --unneeded","title":"Pacman"},{"location":"learning/tools/pacman/#tldr","text":"# search an installed package pacman --query --search ddc # list all explicitly installed packages pacman --query --explicit # set a package as explicitly (manually) installed pacman --database --asexplicit dkms # set a package as installed as dependency (automatically installed) pacman --database --asdeps autoconf # install zsh unsupervisioned (useful in scrips) pacman --noconfirm \\ --sync --needed --noprogressbar --quiet --refresh \\ fzf zsh-completions # completely remove virtualbox-guest-utils-nox unsupervisioned (useful in scrips) pacman --noconfirm \\ --remove --nosave --noprogressbar --quiet --recursive --unneeded \\ virtualbox-guest-utils-nox","title":"TL;DR"},{"location":"learning/tools/pacman/#further-readings","text":"Prevent pacman from reinstalling packages that were already installed","title":"Further readings"},{"location":"learning/tools/pamac/","text":"Pamac \u00b6 TL;DR \u00b6 # check if updates are available (in aur too) pamac checkupdates --aur # delete all unneded packages and their dependencies from the system pamac remove --no-save --orphans --unneeded Further readings \u00b6 Manjaro's Wiki","title":"Pamac"},{"location":"learning/tools/pamac/#pamac","text":"","title":"Pamac"},{"location":"learning/tools/pamac/#tldr","text":"# check if updates are available (in aur too) pamac checkupdates --aur # delete all unneded packages and their dependencies from the system pamac remove --no-save --orphans --unneeded","title":"TL;DR"},{"location":"learning/tools/pamac/#further-readings","text":"Manjaro's Wiki","title":"Further readings"},{"location":"learning/tools/parallel/","text":"GNU Parallel \u00b6 TL;DR \u00b6 # group output (--group) # fill up cpu threads (--jobs 100%) # use newline as delimiter for the arguments in input # simulate and print to output the command that would have been executed find . -type f \\ | parallel --group --jobs 0 --delimiter '\\n' --dry-run clamscan {} # get the exit status of all subjobs (--joblog $outfile) # use all the threads you can (--jobs 0), hammering the cpu find . -type d -name .git -exec dirname \"{}\" + \\ | parallel --group --jobs 0 --tagstring { / } --joblog - \\ 'git -C {} pull --recurse-submodules' # inject istio to all deployments in a namespace in (GNU) parallel kubectl get deployments -o jsonpath = '{.items[*].metadata.name}' \\ | parallel --group --jobs 0 'kubectl -n ${NAMESPACE:-default} apply -f \\ <(istioctl kube-inject -f \\ <(kubectl get deployments,services {} -o json))' # given a list of namespaces get pods and their nodes parallel --group --jobs 100 % --tag \\ \"kubectl --context $KUBE_CONTEXT --namespace {} get pods --output json \\ | jq -r '.items[] | .metadata.name + \\\"\\t\\\" + .spec.nodeName' -\" \\ ::: \" ${ NAMESPACES } \" \\ | column -t Further readings \u00b6 GNU Parallel's man page GNU Parallel's tutorial Obtaining exit status values from GNU parallel","title":"GNU Parallel"},{"location":"learning/tools/parallel/#gnu-parallel","text":"","title":"GNU Parallel"},{"location":"learning/tools/parallel/#tldr","text":"# group output (--group) # fill up cpu threads (--jobs 100%) # use newline as delimiter for the arguments in input # simulate and print to output the command that would have been executed find . -type f \\ | parallel --group --jobs 0 --delimiter '\\n' --dry-run clamscan {} # get the exit status of all subjobs (--joblog $outfile) # use all the threads you can (--jobs 0), hammering the cpu find . -type d -name .git -exec dirname \"{}\" + \\ | parallel --group --jobs 0 --tagstring { / } --joblog - \\ 'git -C {} pull --recurse-submodules' # inject istio to all deployments in a namespace in (GNU) parallel kubectl get deployments -o jsonpath = '{.items[*].metadata.name}' \\ | parallel --group --jobs 0 'kubectl -n ${NAMESPACE:-default} apply -f \\ <(istioctl kube-inject -f \\ <(kubectl get deployments,services {} -o json))' # given a list of namespaces get pods and their nodes parallel --group --jobs 100 % --tag \\ \"kubectl --context $KUBE_CONTEXT --namespace {} get pods --output json \\ | jq -r '.items[] | .metadata.name + \\\"\\t\\\" + .spec.nodeName' -\" \\ ::: \" ${ NAMESPACES } \" \\ | column -t","title":"TL;DR"},{"location":"learning/tools/parallel/#further-readings","text":"GNU Parallel's man page GNU Parallel's tutorial Obtaining exit status values from GNU parallel","title":"Further readings"},{"location":"learning/tools/pdfgrep/","text":"PDFGrep \u00b6","title":"PDFGrep"},{"location":"learning/tools/pdfgrep/#pdfgrep","text":"","title":"PDFGrep"},{"location":"learning/tools/pdftk/","text":"PDFtk \u00b6 TL;DR \u00b6 # combine multiple files pdftk file1.pdf file2.pdf file3.pdf cat output newfile.pdf # rotate a file pdftk file.pdf cat 1 -endleft output newfile.pdf Combine multiple files \u00b6 pdftk file1.pdf file2.pdf file3.pdf cat output newfile.pdf where: file{1..3}.pdf are the input file cat is the operation on the files output is the operation after the read newfile.pdf is the new file with the result Rotate a file \u00b6 pdftk file.pdf cat 1 -endleft output newfile.pdf where: file.pdf is the input file cat is the operation on the file 1-end is the range of pages on which execute the rotation left is the direction of the rotation output is the operation after the read newfile.pdf is the new file with the result Further readings \u00b6 Combine multiple PDF files with PDFTK Lossless rotation of PDF files with ImageMagick","title":"PDFtk"},{"location":"learning/tools/pdftk/#pdftk","text":"","title":"PDFtk"},{"location":"learning/tools/pdftk/#tldr","text":"# combine multiple files pdftk file1.pdf file2.pdf file3.pdf cat output newfile.pdf # rotate a file pdftk file.pdf cat 1 -endleft output newfile.pdf","title":"TL;DR"},{"location":"learning/tools/pdftk/#combine-multiple-files","text":"pdftk file1.pdf file2.pdf file3.pdf cat output newfile.pdf where: file{1..3}.pdf are the input file cat is the operation on the files output is the operation after the read newfile.pdf is the new file with the result","title":"Combine multiple files"},{"location":"learning/tools/pdftk/#rotate-a-file","text":"pdftk file.pdf cat 1 -endleft output newfile.pdf where: file.pdf is the input file cat is the operation on the file 1-end is the range of pages on which execute the rotation left is the direction of the rotation output is the operation after the read newfile.pdf is the new file with the result","title":"Rotate a file"},{"location":"learning/tools/pdftk/#further-readings","text":"Combine multiple PDF files with PDFTK Lossless rotation of PDF files with ImageMagick","title":"Further readings"},{"location":"learning/tools/pi-hole/","text":"Pi-hole \u00b6 TL;DR \u00b6 # One-step automated install. curl -sSL 'https://install.pi-hole.net' | bash # Update Graviton's DB. pihole -g # Check when Graviton's DB has been updated. stat /etc/pihole/gravity.db Further readings \u00b6 Pi-hole's repository","title":"Pi-hole"},{"location":"learning/tools/pi-hole/#pi-hole","text":"","title":"Pi-hole"},{"location":"learning/tools/pi-hole/#tldr","text":"# One-step automated install. curl -sSL 'https://install.pi-hole.net' | bash # Update Graviton's DB. pihole -g # Check when Graviton's DB has been updated. stat /etc/pihole/gravity.db","title":"TL;DR"},{"location":"learning/tools/pi-hole/#further-readings","text":"Pi-hole's repository","title":"Further readings"},{"location":"learning/tools/pkexec/","text":"Pkexec \u00b6 Allows an authorized user to execute a command as another user. If a username is not specified, the command will be executed as root . TL;DR \u00b6 pkexec systemctl hibernate Further readings \u00b6 Man page","title":"Pkexec"},{"location":"learning/tools/pkexec/#pkexec","text":"Allows an authorized user to execute a command as another user. If a username is not specified, the command will be executed as root .","title":"Pkexec"},{"location":"learning/tools/pkexec/#tldr","text":"pkexec systemctl hibernate","title":"TL;DR"},{"location":"learning/tools/pkexec/#further-readings","text":"Man page","title":"Further readings"},{"location":"learning/tools/pkgutil/","text":"Pkgutil \u00b6 Query and manipulate Mac OS X Installer packages and receipts. pkgutil reads and manipulates Mac OS X Installer flat packages, and provides access to the \"receipt\" database used by the Installer. Options are processed first, and affect the operation of all commands. Multiple commands are performed sequentially in the given order. TL;DR \u00b6 # list the package id of all installed packages pkgutil --pkgs pkgutil --packages --volume / # verify the cryptographic signature of a package pkgutil --check-signature path/to/filename.pkg # list all the files provided by an installed package given its id pkgutil --files com.microsoft.Word # extract the contents of a package into a directory pkgutil --expand-full path/to/filename.pkg path/to/directory # find what package provides a file pkgutil --file-info Bitwarden.app/Contents/MacOS/Bitwarden Further readings \u00b6 cheat.sh","title":"Pkgutil"},{"location":"learning/tools/pkgutil/#pkgutil","text":"Query and manipulate Mac OS X Installer packages and receipts. pkgutil reads and manipulates Mac OS X Installer flat packages, and provides access to the \"receipt\" database used by the Installer. Options are processed first, and affect the operation of all commands. Multiple commands are performed sequentially in the given order.","title":"Pkgutil"},{"location":"learning/tools/pkgutil/#tldr","text":"# list the package id of all installed packages pkgutil --pkgs pkgutil --packages --volume / # verify the cryptographic signature of a package pkgutil --check-signature path/to/filename.pkg # list all the files provided by an installed package given its id pkgutil --files com.microsoft.Word # extract the contents of a package into a directory pkgutil --expand-full path/to/filename.pkg path/to/directory # find what package provides a file pkgutil --file-info Bitwarden.app/Contents/MacOS/Bitwarden","title":"TL;DR"},{"location":"learning/tools/pkgutil/#further-readings","text":"cheat.sh","title":"Further readings"},{"location":"learning/tools/portage/","text":"Portage \u00b6 TL;DR \u00b6 # System update. sudo emerge --sync sudo emerge --depclean --ask sudo emerge -qv --update --deep --newuse --with-bdeps = y -a @world # Show what portage features are currently active. portageq envvar FEATURES | xargs -n1 Sources \u00b6 /etc/portage Portage","title":"Portage"},{"location":"learning/tools/portage/#portage","text":"","title":"Portage"},{"location":"learning/tools/portage/#tldr","text":"# System update. sudo emerge --sync sudo emerge --depclean --ask sudo emerge -qv --update --deep --newuse --with-bdeps = y -a @world # Show what portage features are currently active. portageq envvar FEATURES | xargs -n1","title":"TL;DR"},{"location":"learning/tools/portage/#sources","text":"/etc/portage Portage","title":"Sources"},{"location":"learning/tools/postgresql/","text":"PostgreSQL \u00b6 TL;DR \u00b6 # connect to a server psql --host \" ${ HOSTNAME } \" --port \" ${ PORT :- 5432 } \" \" ${ DATABASENAME :- root } \" \" ${ USERNAME :- root } \" Further readings \u00b6","title":"PostgreSQL"},{"location":"learning/tools/postgresql/#postgresql","text":"","title":"PostgreSQL"},{"location":"learning/tools/postgresql/#tldr","text":"# connect to a server psql --host \" ${ HOSTNAME } \" --port \" ${ PORT :- 5432 } \" \" ${ DATABASENAME :- root } \" \" ${ USERNAME :- root } \"","title":"TL;DR"},{"location":"learning/tools/postgresql/#further-readings","text":"","title":"Further readings"},{"location":"learning/tools/postman/","text":"Postman \u00b6 API platform for building and using APIs. Further readings Further readings \u00b6 Website Documentation Insomnia , an alternative to Postman Newman , CLI Collection runner for Postman","title":"Postman <!-- omit in toc -->"},{"location":"learning/tools/postman/#postman","text":"API platform for building and using APIs. Further readings","title":"Postman "},{"location":"learning/tools/postman/#further-readings","text":"Website Documentation Insomnia , an alternative to Postman Newman , CLI Collection runner for Postman","title":"Further readings"},{"location":"learning/tools/powershell/","text":"Windows PowerShell \u00b6 TL;DR Further readings Sources TL;DR \u00b6 # Calculate the hash of a file. CertUtil -hashfile path / to / file sha256 # Get super user privileges. powershell Start-Process powershell -Verb runAs # Assign values to variables. $variableName = 'value' $response = Invoke-WebRequest -Uri 'https://jsonplaceholder.typicode.com/users' # Print the value of the PATH environment variable. $env:PATH Write-Output $env:PATH Write-Host $env:PATH # Pipe the output of a command into another. $users = $response | ConvertFrom-Json $response | ConvertFrom-Json | Select-Object -Property username , email # Access Objects' properties via dot-notation. $users . id ( Invoke-WebRequest -Uri 'https://jsonplaceholder.typicode.com/users' ). Content # Show selected Objects' properties. $users | Select-Object -Property id , username , email $users | select -Property id , username , email # Show selected Objects' properties (expanded). # Dot-notation automatically expands the output. $users | Select-Object -Expand id , username , email $users | select -Expand id , username , email # Filter Objects' values. $users | Where-Object -Property id -EQ 10 $users | where { $_ . id -eq 10 } $users | where {( $_ . id -eq 10 ) -or ( $_ . id -lt 3 )} # Split a command on multiple lines. Invoke-WebRequest ` -Uri 'https://jsonplaceholder.typicode.com/users' ` -UseBasicParsing ` | select -Expand Content ` | ConvertFrom-Json ` | Select-Object -Property id , username , email # Filter out nodes name and their issues from K8S nodes' command output. # Both contructions do the same operations and have the same output. kubectl get nodes -o json ` | ConvertFrom-Json ` | Select-Object -ExpandProperty items ` | Select-Object -Property ` @{ l = \"Node\" ; e ={ $_ . metadata . name }},` @{ l = \"Issues\" ; e ={ $_ . status . conditions ` | Where-Object { ( $_ . status -ne \"False\" ) -and ( $_ . type -ne \"Ready\" ) } ` | Select-Object -ExpandProperty type }} ( kubectl get nodes -o json | ConvertFrom-Json ). items ` | select @{ l = \"Node\" ; e ={ $_ . metadata . name }},@{ l = \"Issues\" ; e ={` ( $_ . status . conditions | where {( $_ . status -ne \"False\" ) -and ( $_ . type -ne \"Ready\" )}). type ` }} Further readings \u00b6 How to print environment variables to the console in PowerShell? Running PowerShell as Administrator with the Command Line [Multiline Command] Sources \u00b6 Working with JSON data in PowerShell JSON file to table Retrieve JSON object by field value Select-Object of multiple properties Multiple -and -or in PowerShell Where-Object statement","title":"Windows PowerShell <!-- omit in toc -->"},{"location":"learning/tools/powershell/#windows-powershell","text":"TL;DR Further readings Sources","title":"Windows PowerShell "},{"location":"learning/tools/powershell/#tldr","text":"# Calculate the hash of a file. CertUtil -hashfile path / to / file sha256 # Get super user privileges. powershell Start-Process powershell -Verb runAs # Assign values to variables. $variableName = 'value' $response = Invoke-WebRequest -Uri 'https://jsonplaceholder.typicode.com/users' # Print the value of the PATH environment variable. $env:PATH Write-Output $env:PATH Write-Host $env:PATH # Pipe the output of a command into another. $users = $response | ConvertFrom-Json $response | ConvertFrom-Json | Select-Object -Property username , email # Access Objects' properties via dot-notation. $users . id ( Invoke-WebRequest -Uri 'https://jsonplaceholder.typicode.com/users' ). Content # Show selected Objects' properties. $users | Select-Object -Property id , username , email $users | select -Property id , username , email # Show selected Objects' properties (expanded). # Dot-notation automatically expands the output. $users | Select-Object -Expand id , username , email $users | select -Expand id , username , email # Filter Objects' values. $users | Where-Object -Property id -EQ 10 $users | where { $_ . id -eq 10 } $users | where {( $_ . id -eq 10 ) -or ( $_ . id -lt 3 )} # Split a command on multiple lines. Invoke-WebRequest ` -Uri 'https://jsonplaceholder.typicode.com/users' ` -UseBasicParsing ` | select -Expand Content ` | ConvertFrom-Json ` | Select-Object -Property id , username , email # Filter out nodes name and their issues from K8S nodes' command output. # Both contructions do the same operations and have the same output. kubectl get nodes -o json ` | ConvertFrom-Json ` | Select-Object -ExpandProperty items ` | Select-Object -Property ` @{ l = \"Node\" ; e ={ $_ . metadata . name }},` @{ l = \"Issues\" ; e ={ $_ . status . conditions ` | Where-Object { ( $_ . status -ne \"False\" ) -and ( $_ . type -ne \"Ready\" ) } ` | Select-Object -ExpandProperty type }} ( kubectl get nodes -o json | ConvertFrom-Json ). items ` | select @{ l = \"Node\" ; e ={ $_ . metadata . name }},@{ l = \"Issues\" ; e ={` ( $_ . status . conditions | where {( $_ . status -ne \"False\" ) -and ( $_ . type -ne \"Ready\" )}). type ` }}","title":"TL;DR"},{"location":"learning/tools/powershell/#further-readings","text":"How to print environment variables to the console in PowerShell? Running PowerShell as Administrator with the Command Line [Multiline Command]","title":"Further readings"},{"location":"learning/tools/powershell/#sources","text":"Working with JSON data in PowerShell JSON file to table Retrieve JSON object by field value Select-Object of multiple properties Multiple -and -or in PowerShell Where-Object statement","title":"Sources"},{"location":"learning/tools/pre-commit/","text":"Pre-commit \u00b6 TL;DR \u00b6 # Generate a very basic configuration. pre-commit sample-config > .pre-commit-config.yaml # Manually run checks. pre-commit run --all-files pre-commit run ansible-lint --files ansible/ # Automatically run checks at every commit. pre-commit install # Update all hooks to the latest version. pre-commit autoupdate # Skip check on commit. SKIP = flake8 git commit -m \"foo\" --- # File .pre-commit-config.yaml # See https://pre-commit.com for more information # See https://pre-commit.com/hooks.html for more hooks # See https://github.com/pre-commit/identify/blob/main/identify/extensions.py for the list of file types by extension repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v4.2.0 hooks : - id : trailing-whitespace args : - --markdown-linebreak-ext=md # ignore markdown's line break - id : end-of-file-fixer - id : check-yaml - id : check-added-large-files - repo : https://github.com/markdownlint/markdownlint rev : v0.11.0 hooks : - id : markdownlint types : [ markdown ] # limit target types args : - -r \"~MD013\" # ignore line-length rule - repo : https://github.com/ansible-community/ansible-lint rev : v6.0.2 hooks : - id : ansible-lint name : ansilint # use an alias Troubleshooting \u00b6 Some files are skipped during a run \u00b6 Check they are tracked (have been add ed to the repository). Further readings \u00b6 Pre-commit's website List of supported hooks","title":"Pre-commit"},{"location":"learning/tools/pre-commit/#pre-commit","text":"","title":"Pre-commit"},{"location":"learning/tools/pre-commit/#tldr","text":"# Generate a very basic configuration. pre-commit sample-config > .pre-commit-config.yaml # Manually run checks. pre-commit run --all-files pre-commit run ansible-lint --files ansible/ # Automatically run checks at every commit. pre-commit install # Update all hooks to the latest version. pre-commit autoupdate # Skip check on commit. SKIP = flake8 git commit -m \"foo\" --- # File .pre-commit-config.yaml # See https://pre-commit.com for more information # See https://pre-commit.com/hooks.html for more hooks # See https://github.com/pre-commit/identify/blob/main/identify/extensions.py for the list of file types by extension repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v4.2.0 hooks : - id : trailing-whitespace args : - --markdown-linebreak-ext=md # ignore markdown's line break - id : end-of-file-fixer - id : check-yaml - id : check-added-large-files - repo : https://github.com/markdownlint/markdownlint rev : v0.11.0 hooks : - id : markdownlint types : [ markdown ] # limit target types args : - -r \"~MD013\" # ignore line-length rule - repo : https://github.com/ansible-community/ansible-lint rev : v6.0.2 hooks : - id : ansible-lint name : ansilint # use an alias","title":"TL;DR"},{"location":"learning/tools/pre-commit/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/pre-commit/#some-files-are-skipped-during-a-run","text":"Check they are tracked (have been add ed to the repository).","title":"Some files are skipped during a run"},{"location":"learning/tools/pre-commit/#further-readings","text":"Pre-commit's website List of supported hooks","title":"Further readings"},{"location":"learning/tools/print%20a%20compressed%20stream%20to%20stdout/","text":"Print a compressed stream to stdout \u00b6 cat file.zip | zcat cat file.zip | busybox unzip -p - cat file.gz | gunzip -c - curl 'https://example.com/some.zip' | bsdtar -xOf - Sources \u00b6 Unzip from stdin to stdout","title":"Print a compressed stream to stdout"},{"location":"learning/tools/print%20a%20compressed%20stream%20to%20stdout/#print-a-compressed-stream-to-stdout","text":"cat file.zip | zcat cat file.zip | busybox unzip -p - cat file.gz | gunzip -c - curl 'https://example.com/some.zip' | bsdtar -xOf -","title":"Print a compressed stream to stdout"},{"location":"learning/tools/print%20a%20compressed%20stream%20to%20stdout/#sources","text":"Unzip from stdin to stdout","title":"Sources"},{"location":"learning/tools/python/","text":"Python \u00b6 String formatting \u00b6 # f-strings f \"Hello, { name } . You are { age } .\" F \" { name . lower () } is funny.\" Web servers \u00b6 Flask \u00b6 request.args gets query arguments request.form gets POST arguments from flask import request , jsonify @app . route ( '/get/questions/' , methods = [ 'GET' , 'POST' , 'DELETE' , 'PATCH' ]) def question (): if request . method == 'GET' : start = request . args . get ( 'start' , default = 0 , type = int ) limit_url = request . args . get ( 'limit' , default = 20 , type = int ) data = [ doc for doc in questions ] return jsonify ( isError = False , message = \"Success\" , statusCode = 200 , data = data ), 200 if request . method == 'POST' : question = request . form . get ( 'question' ) topics = request . form . get ( 'topics' ) return jsonify ( isError = True , message = \"Conflict\" , statusCode = 409 , data = data ), 409 WSGI server \u00b6 You can use waitress : from flask import Flask app = Flask ( __name__ ) @app . route ( \"/\" ) def index (): return \"<h1>Hello!</h1>\" if __name__ == \"__main__\" : from waitress import serve serve ( app , host = \"0.0.0.0\" , port = 8080 ) pip install flask waitress python hello.py Maintenance \u00b6 # generate a list of all outdated packages pip list --outdated # upgrade all packages (oneliner) pip install --requirement < ( pip freeze | sed 's/==/>=/' ) --upgrade # remove orphaned dependencies # after installation of pip-autoremove pip-autoremove # upgrade onboard pip on mac os x pip3 install --user --upgrade pip echo 'export PATH=\"${HOME}/Library/Python/3.8/bin:${PATH}\"' >> ${ HOME } /.zprofile Further readings \u00b6 flask at first run: do not use the development server in a production environment f-strings data types flask example with POST multi-value query parameters with flask How To Update All Python Packages invl/pip-autoremove","title":"Python"},{"location":"learning/tools/python/#python","text":"","title":"Python"},{"location":"learning/tools/python/#string-formatting","text":"# f-strings f \"Hello, { name } . You are { age } .\" F \" { name . lower () } is funny.\"","title":"String formatting"},{"location":"learning/tools/python/#web-servers","text":"","title":"Web servers"},{"location":"learning/tools/python/#flask","text":"request.args gets query arguments request.form gets POST arguments from flask import request , jsonify @app . route ( '/get/questions/' , methods = [ 'GET' , 'POST' , 'DELETE' , 'PATCH' ]) def question (): if request . method == 'GET' : start = request . args . get ( 'start' , default = 0 , type = int ) limit_url = request . args . get ( 'limit' , default = 20 , type = int ) data = [ doc for doc in questions ] return jsonify ( isError = False , message = \"Success\" , statusCode = 200 , data = data ), 200 if request . method == 'POST' : question = request . form . get ( 'question' ) topics = request . form . get ( 'topics' ) return jsonify ( isError = True , message = \"Conflict\" , statusCode = 409 , data = data ), 409","title":"Flask"},{"location":"learning/tools/python/#wsgi-server","text":"You can use waitress : from flask import Flask app = Flask ( __name__ ) @app . route ( \"/\" ) def index (): return \"<h1>Hello!</h1>\" if __name__ == \"__main__\" : from waitress import serve serve ( app , host = \"0.0.0.0\" , port = 8080 ) pip install flask waitress python hello.py","title":"WSGI server"},{"location":"learning/tools/python/#maintenance","text":"# generate a list of all outdated packages pip list --outdated # upgrade all packages (oneliner) pip install --requirement < ( pip freeze | sed 's/==/>=/' ) --upgrade # remove orphaned dependencies # after installation of pip-autoremove pip-autoremove # upgrade onboard pip on mac os x pip3 install --user --upgrade pip echo 'export PATH=\"${HOME}/Library/Python/3.8/bin:${PATH}\"' >> ${ HOME } /.zprofile","title":"Maintenance"},{"location":"learning/tools/python/#further-readings","text":"flask at first run: do not use the development server in a production environment f-strings data types flask example with POST multi-value query parameters with flask How To Update All Python Packages invl/pip-autoremove","title":"Further readings"},{"location":"learning/tools/raspberry%20pi%20os/","text":"Raspberry Pi OS \u00b6 Store files on the SD even when the overlay file system is active Swap Run containers Kernel containerization features Firewall settings Sources Store files on the SD even when the overlay file system is active \u00b6 The files just need to be stored on a different file system from / . You can partition the SD and use that, or create a file and mount it as a virtual file system: truncate -s '6G' 'file' mkfs.ext4 'file' mkdir 'mount/point' sudo mount -t 'ext4' -o 'loop' 'file' 'mount/point' sudo chown 'user' : 'group' 'mount/point' touch 'mount/point/new-file' Swap \u00b6 Disable the swap file. sudo systemctl disable --now 'dphys-swapfile' Run containers \u00b6 enable the kernel's containerization feature disable swap if kubernetes is involved, set up the firewall to use the legacy configuration Kernel containerization features \u00b6 Enable containerization features in the kernel to be able to run containers as intended. Add the following properties at the end of the line in /boot/cmdline.txt : cgroup_enable = cpuset cgroup_enable = memory cgroup_memory = 1 sed -i '/cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1/!s/\\s*$/ cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1&/' /boot/cmdline.txt Firewall settings \u00b6 Switch Debian firewall to use the legacy configuration: update-alternatives --set iptables /usr/sbin/iptables-legacy update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy Sources \u00b6 The k3s project page The Build your very own self-hosting platform with Raspberry Pi and Kubernetes series of articles Run Kubernetes on a Raspberry Pi with k3s Project's issue 2067","title":"Raspberry Pi OS"},{"location":"learning/tools/raspberry%20pi%20os/#raspberry-pi-os","text":"Store files on the SD even when the overlay file system is active Swap Run containers Kernel containerization features Firewall settings Sources","title":"Raspberry Pi OS"},{"location":"learning/tools/raspberry%20pi%20os/#store-files-on-the-sd-even-when-the-overlay-file-system-is-active","text":"The files just need to be stored on a different file system from / . You can partition the SD and use that, or create a file and mount it as a virtual file system: truncate -s '6G' 'file' mkfs.ext4 'file' mkdir 'mount/point' sudo mount -t 'ext4' -o 'loop' 'file' 'mount/point' sudo chown 'user' : 'group' 'mount/point' touch 'mount/point/new-file'","title":"Store files on the SD even when the overlay file system is active"},{"location":"learning/tools/raspberry%20pi%20os/#swap","text":"Disable the swap file. sudo systemctl disable --now 'dphys-swapfile'","title":"Swap"},{"location":"learning/tools/raspberry%20pi%20os/#run-containers","text":"enable the kernel's containerization feature disable swap if kubernetes is involved, set up the firewall to use the legacy configuration","title":"Run containers"},{"location":"learning/tools/raspberry%20pi%20os/#kernel-containerization-features","text":"Enable containerization features in the kernel to be able to run containers as intended. Add the following properties at the end of the line in /boot/cmdline.txt : cgroup_enable = cpuset cgroup_enable = memory cgroup_memory = 1 sed -i '/cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1/!s/\\s*$/ cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1&/' /boot/cmdline.txt","title":"Kernel containerization features"},{"location":"learning/tools/raspberry%20pi%20os/#firewall-settings","text":"Switch Debian firewall to use the legacy configuration: update-alternatives --set iptables /usr/sbin/iptables-legacy update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy","title":"Firewall settings"},{"location":"learning/tools/raspberry%20pi%20os/#sources","text":"The k3s project page The Build your very own self-hosting platform with Raspberry Pi and Kubernetes series of articles Run Kubernetes on a Raspberry Pi with k3s Project's issue 2067","title":"Sources"},{"location":"learning/tools/redis/","text":"Redis \u00b6 TL;DR \u00b6 # debug the server redis-cli -h \" ${ HOST } \" -p \" ${ PORT } \" --user \" ${ USERNAME } \" --askpass MONITOR # execute the commands from the master's container on kubernetes kubectl exec redis-0 -- redis-cli MONITOR","title":"Redis"},{"location":"learning/tools/redis/#redis","text":"","title":"Redis"},{"location":"learning/tools/redis/#tldr","text":"# debug the server redis-cli -h \" ${ HOST } \" -p \" ${ PORT } \" --user \" ${ USERNAME } \" --askpass MONITOR # execute the commands from the master's container on kubernetes kubectl exec redis-0 -- redis-cli MONITOR","title":"TL;DR"},{"location":"learning/tools/reload%20a%20user%27s%20group%20assignments%20without%20logging%20out/","text":"Reload a user's group assignments without logging out \u00b6 TL;DR \u00b6 exec su -l $USER Further readings \u00b6 Reload a Linux user's group assignments without logging out","title":"Reload a user's group assignments without logging out"},{"location":"learning/tools/reload%20a%20user%27s%20group%20assignments%20without%20logging%20out/#reload-a-users-group-assignments-without-logging-out","text":"","title":"Reload a user's group assignments without logging out"},{"location":"learning/tools/reload%20a%20user%27s%20group%20assignments%20without%20logging%20out/#tldr","text":"exec su -l $USER","title":"TL;DR"},{"location":"learning/tools/reload%20a%20user%27s%20group%20assignments%20without%20logging%20out/#further-readings","text":"Reload a Linux user's group assignments without logging out","title":"Further readings"},{"location":"learning/tools/rename/","text":"rename \u00b6 Note: this page refers to the command from the util-linux package on Linux or Homebrew's rename package. TL;DR \u00b6 # Show what would change without changing anything (dry run). rename -vn foo bar * rename -nv 's/^(\\d{4}-\\d{2}-\\d{2}) (.*)$/$1 addition $2/' 'file' Sources \u00b6 cheat.sh","title":"`rename`"},{"location":"learning/tools/rename/#rename","text":"Note: this page refers to the command from the util-linux package on Linux or Homebrew's rename package.","title":"rename"},{"location":"learning/tools/rename/#tldr","text":"# Show what would change without changing anything (dry run). rename -vn foo bar * rename -nv 's/^(\\d{4}-\\d{2}-\\d{2}) (.*)$/$1 addition $2/' 'file'","title":"TL;DR"},{"location":"learning/tools/rename/#sources","text":"cheat.sh","title":"Sources"},{"location":"learning/tools/resize%20an%20image%20from%20cli/","text":"Resize images using the CLI \u00b6 Leverages convert from imagemagick . convert input.jpg -adaptive-resize 50 % output.jpg # Scale down all images in a folder. ls -1 | xargs -I {} convert {} -adaptive-resize 50 % {} _scaled.jpg Further readings imagemagick","title":"Resize images using the CLI"},{"location":"learning/tools/resize%20an%20image%20from%20cli/#resize-images-using-the-cli","text":"Leverages convert from imagemagick . convert input.jpg -adaptive-resize 50 % output.jpg # Scale down all images in a folder. ls -1 | xargs -I {} convert {} -adaptive-resize 50 % {} _scaled.jpg Further readings imagemagick","title":"Resize images using the CLI"},{"location":"learning/tools/retrieve%20disk%20information/","text":"Retrieve disk information \u00b6 TL;DR \u00b6 lshw -class disk smartctl -i /dev/sda hdparm -i /dev/sda hwinfo --disk ls /dev/disk/by-id Further readings \u00b6 Retrieve disk info from the command line","title":"Retrieve disk information"},{"location":"learning/tools/retrieve%20disk%20information/#retrieve-disk-information","text":"","title":"Retrieve disk information"},{"location":"learning/tools/retrieve%20disk%20information/#tldr","text":"lshw -class disk smartctl -i /dev/sda hdparm -i /dev/sda hwinfo --disk ls /dev/disk/by-id","title":"TL;DR"},{"location":"learning/tools/retrieve%20disk%20information/#further-readings","text":"Retrieve disk info from the command line","title":"Further readings"},{"location":"learning/tools/rotate%20a%20pdf%20file/","text":"Rotate a PDF file \u00b6 Further readings \u00b6 pdftk","title":"Rotate a PDF file"},{"location":"learning/tools/rotate%20a%20pdf%20file/#rotate-a-pdf-file","text":"","title":"Rotate a PDF file"},{"location":"learning/tools/rotate%20a%20pdf%20file/#further-readings","text":"pdftk","title":"Further readings"},{"location":"learning/tools/rpm-ostree/","text":"rpm-ostree \u00b6 Name Base distribution Kinoite Fedora KDE Silverblue Fedora Workstation TL;DR \u00b6 Changes to the base layer are executed in a new bootable filesystem root. This means that the system must be rebooted after a package has been layered. # Check for available upgrades. rpm-ostree upgrade --check # Upgrade the system. rpm-ostree upgrade # Install packages. rpm-ostree install kmod-nvidia xorg-x11-drv-nvidia # Override packages. rpm-ostree override replace \\ local/path/to/podman-3.1.2-1.fc34.x86_64.rpm \\ https://kojipkgs.fedoraproject.org/packages/podman/3.1.2/1.fc34/x86_64/podman-plugins-3.1.2-1.fc34.x86_64.rpm # Remove packages. # Packages will still exist in the undelying base layer, but will not appear # in the booted root. rpm-ostree override remove nano # Rollback. rpm-ostree rollback # Make changes to the kernel's boot arguments. rpm-ostree kargs \\ --append = rd.driver.blacklist = nouveau \\ --append = modprobe.blacklist = nouveau \\ --append = nvidia-drm.modeset = 1 # Preview changes on the current filesystem. rpm-ostree ex apply-live Package layering \u00b6 Package layering works by modifying your Silverblue installation by extending the packages from which Silverblue is composed. Using package layering creates a new deployment , or bootable filesystem root which does not affect your current root. This means that the system must be rebooted after a package has been layered. If you don't want to reboot your system to switch to the new deployment, you can use rpm-ostree ex apply-live to update the current filesystem and be able to see the changes from the new deployment. It's generally expected that you use package layering sparingly, and use flatpak s and toolbox . Further readings \u00b6 rpm-ostree Fedora Silverblue user guide Flatpak Toolbox","title":"rpm-ostree"},{"location":"learning/tools/rpm-ostree/#rpm-ostree","text":"Name Base distribution Kinoite Fedora KDE Silverblue Fedora Workstation","title":"rpm-ostree"},{"location":"learning/tools/rpm-ostree/#tldr","text":"Changes to the base layer are executed in a new bootable filesystem root. This means that the system must be rebooted after a package has been layered. # Check for available upgrades. rpm-ostree upgrade --check # Upgrade the system. rpm-ostree upgrade # Install packages. rpm-ostree install kmod-nvidia xorg-x11-drv-nvidia # Override packages. rpm-ostree override replace \\ local/path/to/podman-3.1.2-1.fc34.x86_64.rpm \\ https://kojipkgs.fedoraproject.org/packages/podman/3.1.2/1.fc34/x86_64/podman-plugins-3.1.2-1.fc34.x86_64.rpm # Remove packages. # Packages will still exist in the undelying base layer, but will not appear # in the booted root. rpm-ostree override remove nano # Rollback. rpm-ostree rollback # Make changes to the kernel's boot arguments. rpm-ostree kargs \\ --append = rd.driver.blacklist = nouveau \\ --append = modprobe.blacklist = nouveau \\ --append = nvidia-drm.modeset = 1 # Preview changes on the current filesystem. rpm-ostree ex apply-live","title":"TL;DR"},{"location":"learning/tools/rpm-ostree/#package-layering","text":"Package layering works by modifying your Silverblue installation by extending the packages from which Silverblue is composed. Using package layering creates a new deployment , or bootable filesystem root which does not affect your current root. This means that the system must be rebooted after a package has been layered. If you don't want to reboot your system to switch to the new deployment, you can use rpm-ostree ex apply-live to update the current filesystem and be able to see the changes from the new deployment. It's generally expected that you use package layering sparingly, and use flatpak s and toolbox .","title":"Package layering"},{"location":"learning/tools/rpm-ostree/#further-readings","text":"rpm-ostree Fedora Silverblue user guide Flatpak Toolbox","title":"Further readings"},{"location":"learning/tools/rpm/","text":"The RPM package manager \u00b6 TL;DR \u00b6 # list all installed packages rpm --query --all # list files installed by a package rpm --query --list package # find the package owning a file rpm --query --file /usr/bin/file Further readings \u00b6 How can I list all files which have been installed by an ZYpp/Zypper package?","title":"The RPM package manager"},{"location":"learning/tools/rpm/#the-rpm-package-manager","text":"","title":"The RPM package manager"},{"location":"learning/tools/rpm/#tldr","text":"# list all installed packages rpm --query --all # list files installed by a package rpm --query --list package # find the package owning a file rpm --query --file /usr/bin/file","title":"TL;DR"},{"location":"learning/tools/rpm/#further-readings","text":"How can I list all files which have been installed by an ZYpp/Zypper package?","title":"Further readings"},{"location":"learning/tools/rsync/","text":"rsync \u00b6 TL;DR Explored options Sources TL;DR \u00b6 # Synchronize 2 files. rsync 'source/file' 'destination/file' rsync 'source/file' 'username@host:/destination/file' # Synchronize the content of 2 or more directories. rsync -r 'source/dir/' 'destination/dir/' rsync -r 'source/dir/1/' 'source/dir/n/' 'destination/dir/' # Synchronize directories **and** their contents to a destination. rsync -r 'source/dir/1' 'source/dir/n' 'destination/dir' # Delete files at the destination that to not exist at the source. rsync \u2026 --delete # Just show what would change at the destination. rsync -vv \u2026 --dry-run # Copy targets in archive mode if they don't already exist. rsync -av --ignore-existing 'source/file' 'destination/file' rsync -av --ignore-existing 'source/dir/1' 'source/dir/n/' 'destination/dir/' # Exclude files from the sync. rsync \u2026 --exclude \"*.DS_Store\" --exclude \"._*\" rsync \u2026 --filter \"merge filter.txt\" # Copy local files to a folder in the user's remote home over SSH on port 1234. # Compress the data during transfer. rsync 'source/file' 'username@host:destination/file' -ze 'ssh -p 1234' # Copy a remote directory to the local host. # Show total progress and be more verbose. rsync -avv --info = 'progress2' 'username@host:/source/dir/' 'destination/dir/' # Backup items changing at the destination. rsync \u2026 -b --suffix = \".backup_ $( date + '%F' ) \" rsync \u2026 -b --backup-dir = \"changes_ $( date + '%F' ) \" # Resume a sync. rsync \u2026 --partial --append rsync \u2026 -P --append-verify # Limit the transfer's bandwidth. rsync \u2026 --bwlimit = 1200 rsync \u2026 --bwlimit = 5m # Execute multiple syncs to a single destination. ls -1 'source/dir' \\ | xargs -I {} -P $( nproc ) -t \\ rsync -a --info = 'progress2' --dry-run \\ source/dir/ {} / 'username@host:/destination/dir/' find 'source/dir' -maxdepth 1 -mindepth 1 -type d \\ | xargs -I {} -P $( nproc ) -t \\ rsync -AXahvz --chown = 'user' \\ --partial --append-verify \\ --info = 'progress2' --no-i-r --no-motd \\ {} / 'username@host:/destination/dir/' # Sync a directory from/to a Synology NAS. rsync -AHPXazv --append-verify --no-motd 'source/dir/' 'synology.lan:/shared/folder/' --dry-run rsync -AHPXazv --append-verify --no-motd --rsh ssh --exclude \"#*\" --exclude \"@*\" 'user@synology.lan:/shared/folder/' 'destination/dir/' --delete --dry-run rsync -AHPazv --append-verify --no-motd --exclude \"#*\" --exclude \"@*\" 'source/dir/' 'user@synology.lan:/shared/folder/' --delete --dry-run rsync -AXaz --append-verify --chown = 'user' --fake-super --info = 'progress2' --no-i-r --no-motd --partial -e \"ssh -i /home/user/.ssh/id_ed25519 -o UserKnownHostsFile=/home/user/.ssh/known_hosts\" 'source/dir/' 'user@synology.lan:/shared/folder/' -n Explored options \u00b6 Long format Short format Description -P same as --partial --progress --acls -A preserve ACLs; implies --perms --append-verify like --append , but use the data already there to check the items --archive -a archive mode, equals -rlptgoD ; does not imply -H , -A , nor -X --backup-dir=DIR use the specified directory to backup changing items --backup -b backup items changing at the destination; see also --suffix and --backup-dir --bwlimit=RATE limit the socket's I/O bandwidth to RATE ; with no suffix, the value will be in KBPS --checksum -c skip files basing on checksum instead of modify time and size --chown=USER:GROUP simple username/groupname mapping --compress -z compress file data during the transfer --crtimes only available on Mac OS X --delete-during --del set the receiver to delete files during the transfer --delete delete items at the destination that don't exist in the source --dry-run -n perform a trial run with no changes made --exclude=PATTERN exclude files matching PATTERN --executability -E preserve executability --fake-super store/recover privileged attrs using xattrs --filter=RULE -f add a file-filtering RULE --hard-links -H preserve hard links --human-readable -h output numbers in a human-readable format --ignore-existing skip updating files that already exist at the destination --info=FLAGS fine-grained informational verbosity; the progress2 value is available since version 3.1.0 --links -l copy symlinks as symlinks --no-inc-recursive --no-i-r scan all directories on startup instead of incrementally --no-motd suppress daemon-mode MOTD --no-OPTION turn off an implied OPTION (e.g. --no-D ) --partial keep partially transferred files --progress show progress for each file during transfer --protect-args -s no space-splitting; wildcard chars only --prune-empty-dirs -m prune empty directory chains from file-list --recursive -r recurse into directories --remove-source-files set the sender to remove synchronized files; it does not remove directories --rsh=COMMAND -e specify the remote shell to use (with options, e.g. ssh -p 1234 ) --sparse -S turn sequences of nulls into sparse blocks --stats give some file-transfer stats --suffix=SUFFIX suffix for backups; defaults to ~ --update -u skip files that are newer on the receiver --verbose -v increase verbosity once for each copy of this switch --xattrs -X preserve extended attributes Sources \u00b6 cheat.sh Showing total progress in rsync: is it possible?","title":"rsync"},{"location":"learning/tools/rsync/#rsync","text":"TL;DR Explored options Sources","title":"rsync"},{"location":"learning/tools/rsync/#tldr","text":"# Synchronize 2 files. rsync 'source/file' 'destination/file' rsync 'source/file' 'username@host:/destination/file' # Synchronize the content of 2 or more directories. rsync -r 'source/dir/' 'destination/dir/' rsync -r 'source/dir/1/' 'source/dir/n/' 'destination/dir/' # Synchronize directories **and** their contents to a destination. rsync -r 'source/dir/1' 'source/dir/n' 'destination/dir' # Delete files at the destination that to not exist at the source. rsync \u2026 --delete # Just show what would change at the destination. rsync -vv \u2026 --dry-run # Copy targets in archive mode if they don't already exist. rsync -av --ignore-existing 'source/file' 'destination/file' rsync -av --ignore-existing 'source/dir/1' 'source/dir/n/' 'destination/dir/' # Exclude files from the sync. rsync \u2026 --exclude \"*.DS_Store\" --exclude \"._*\" rsync \u2026 --filter \"merge filter.txt\" # Copy local files to a folder in the user's remote home over SSH on port 1234. # Compress the data during transfer. rsync 'source/file' 'username@host:destination/file' -ze 'ssh -p 1234' # Copy a remote directory to the local host. # Show total progress and be more verbose. rsync -avv --info = 'progress2' 'username@host:/source/dir/' 'destination/dir/' # Backup items changing at the destination. rsync \u2026 -b --suffix = \".backup_ $( date + '%F' ) \" rsync \u2026 -b --backup-dir = \"changes_ $( date + '%F' ) \" # Resume a sync. rsync \u2026 --partial --append rsync \u2026 -P --append-verify # Limit the transfer's bandwidth. rsync \u2026 --bwlimit = 1200 rsync \u2026 --bwlimit = 5m # Execute multiple syncs to a single destination. ls -1 'source/dir' \\ | xargs -I {} -P $( nproc ) -t \\ rsync -a --info = 'progress2' --dry-run \\ source/dir/ {} / 'username@host:/destination/dir/' find 'source/dir' -maxdepth 1 -mindepth 1 -type d \\ | xargs -I {} -P $( nproc ) -t \\ rsync -AXahvz --chown = 'user' \\ --partial --append-verify \\ --info = 'progress2' --no-i-r --no-motd \\ {} / 'username@host:/destination/dir/' # Sync a directory from/to a Synology NAS. rsync -AHPXazv --append-verify --no-motd 'source/dir/' 'synology.lan:/shared/folder/' --dry-run rsync -AHPXazv --append-verify --no-motd --rsh ssh --exclude \"#*\" --exclude \"@*\" 'user@synology.lan:/shared/folder/' 'destination/dir/' --delete --dry-run rsync -AHPazv --append-verify --no-motd --exclude \"#*\" --exclude \"@*\" 'source/dir/' 'user@synology.lan:/shared/folder/' --delete --dry-run rsync -AXaz --append-verify --chown = 'user' --fake-super --info = 'progress2' --no-i-r --no-motd --partial -e \"ssh -i /home/user/.ssh/id_ed25519 -o UserKnownHostsFile=/home/user/.ssh/known_hosts\" 'source/dir/' 'user@synology.lan:/shared/folder/' -n","title":"TL;DR"},{"location":"learning/tools/rsync/#explored-options","text":"Long format Short format Description -P same as --partial --progress --acls -A preserve ACLs; implies --perms --append-verify like --append , but use the data already there to check the items --archive -a archive mode, equals -rlptgoD ; does not imply -H , -A , nor -X --backup-dir=DIR use the specified directory to backup changing items --backup -b backup items changing at the destination; see also --suffix and --backup-dir --bwlimit=RATE limit the socket's I/O bandwidth to RATE ; with no suffix, the value will be in KBPS --checksum -c skip files basing on checksum instead of modify time and size --chown=USER:GROUP simple username/groupname mapping --compress -z compress file data during the transfer --crtimes only available on Mac OS X --delete-during --del set the receiver to delete files during the transfer --delete delete items at the destination that don't exist in the source --dry-run -n perform a trial run with no changes made --exclude=PATTERN exclude files matching PATTERN --executability -E preserve executability --fake-super store/recover privileged attrs using xattrs --filter=RULE -f add a file-filtering RULE --hard-links -H preserve hard links --human-readable -h output numbers in a human-readable format --ignore-existing skip updating files that already exist at the destination --info=FLAGS fine-grained informational verbosity; the progress2 value is available since version 3.1.0 --links -l copy symlinks as symlinks --no-inc-recursive --no-i-r scan all directories on startup instead of incrementally --no-motd suppress daemon-mode MOTD --no-OPTION turn off an implied OPTION (e.g. --no-D ) --partial keep partially transferred files --progress show progress for each file during transfer --protect-args -s no space-splitting; wildcard chars only --prune-empty-dirs -m prune empty directory chains from file-list --recursive -r recurse into directories --remove-source-files set the sender to remove synchronized files; it does not remove directories --rsh=COMMAND -e specify the remote shell to use (with options, e.g. ssh -p 1234 ) --sparse -S turn sequences of nulls into sparse blocks --stats give some file-transfer stats --suffix=SUFFIX suffix for backups; defaults to ~ --update -u skip files that are newer on the receiver --verbose -v increase verbosity once for each copy of this switch --xattrs -X preserve extended attributes","title":"Explored options"},{"location":"learning/tools/rsync/#sources","text":"cheat.sh Showing total progress in rsync: is it possible?","title":"Sources"},{"location":"learning/tools/save%20the%20current%20picture%20of%20the%20day/","text":"Save the current picture of the day \u00b6 If using KDE \u00b6 Check the contents of $HOME/.cache/plasma_engine_potd . Sources \u00b6 Reddit","title":"Save the current picture of the day"},{"location":"learning/tools/save%20the%20current%20picture%20of%20the%20day/#save-the-current-picture-of-the-day","text":"","title":"Save the current picture of the day"},{"location":"learning/tools/save%20the%20current%20picture%20of%20the%20day/#if-using-kde","text":"Check the contents of $HOME/.cache/plasma_engine_potd .","title":"If using KDE"},{"location":"learning/tools/save%20the%20current%20picture%20of%20the%20day/#sources","text":"Reddit","title":"Sources"},{"location":"learning/tools/sddm/","text":"SDDM \u00b6 TL;DR \u00b6 # open a new window for every monitor you have connected and show a preview of the theme sddm-greeter --test-mode --theme /usr/share/sddm/themes/breeze Further readings \u00b6 The SDDM article on the Archlinux wiki","title":"SDDM"},{"location":"learning/tools/sddm/#sddm","text":"","title":"SDDM"},{"location":"learning/tools/sddm/#tldr","text":"# open a new window for every monitor you have connected and show a preview of the theme sddm-greeter --test-mode --theme /usr/share/sddm/themes/breeze","title":"TL;DR"},{"location":"learning/tools/sddm/#further-readings","text":"The SDDM article on the Archlinux wiki","title":"Further readings"},{"location":"learning/tools/sed/","text":"SED \u00b6 TL;DR \u00b6 # Delete lines matching \"OAM\" from a file. # Overwrite the source file with the changes. sed '/OAM/d' -i .bash_history # Show changed fstab entries. # Don't save the changes. sed /etc/fstab \\ -e \"s|#.*\\s*/boot\\s*.*|/dev/sda1 /boot vfat defaults 0 0|\" \\ -e \"s|#.*\\s*ext4\\s*.*|/dev/sda2 / btrfs compress-force=zstd 0 0|\" \\ -e '/#.*\\s*swap\\s*.*/d' Character classes and bracket expressions \u00b6 Class Description [[:alnum:]] alphanumeric characters [[:alpha:]] and [[:digit:]] ; this is the same as [0-9A-Za-z] in the C locale and ASCII character [[:alpha:]] alphabetic characters [[:lower:]] and [[:upper:]] ; this is the same as [A-Za-z] in the C locale and ASCII character encoding [[:blank:]] blank characters space and tab [[:cntrl:]] control characters; in ASCII these characters have octal codes 000 through 037 and 177 (DEL), in other character sets these are the equivalent characters, if any [[:digit:]] digits 0 to 9 [[:graph:]] graphical characters [[:alnum:]] and [[:punct:]] [[:lower:]] lower-case letters a to z in the C locale and ASCII character encoding [[:print:]] printable characters [[:alnum:]] , [[:punct:]] and space [[:punct:]] punctuation characters ! , \" , # , $ , % , & , ' , ( , ) , * , + , , , - , . , / , : , ; , < , = , > , ? , @ , [ , \\ , ] , ^ , _ , ` , { , \\| , } and ~ in the C locale and ASCII character encoding [[:space:]] space characters tab , newline , vertical tab , form feed , carriage return and space in the C locale [[:upper:]] upper-case letters A to Z in the C locale and ASCII character encoding [[:xdigit:]] hexadecimal digits 0 to 9 , A to F and a to f Further readings \u00b6 GNU SED Online Tester Character Classes and Bracket Expressions","title":"SED"},{"location":"learning/tools/sed/#sed","text":"","title":"SED"},{"location":"learning/tools/sed/#tldr","text":"# Delete lines matching \"OAM\" from a file. # Overwrite the source file with the changes. sed '/OAM/d' -i .bash_history # Show changed fstab entries. # Don't save the changes. sed /etc/fstab \\ -e \"s|#.*\\s*/boot\\s*.*|/dev/sda1 /boot vfat defaults 0 0|\" \\ -e \"s|#.*\\s*ext4\\s*.*|/dev/sda2 / btrfs compress-force=zstd 0 0|\" \\ -e '/#.*\\s*swap\\s*.*/d'","title":"TL;DR"},{"location":"learning/tools/sed/#character-classes-and-bracket-expressions","text":"Class Description [[:alnum:]] alphanumeric characters [[:alpha:]] and [[:digit:]] ; this is the same as [0-9A-Za-z] in the C locale and ASCII character [[:alpha:]] alphabetic characters [[:lower:]] and [[:upper:]] ; this is the same as [A-Za-z] in the C locale and ASCII character encoding [[:blank:]] blank characters space and tab [[:cntrl:]] control characters; in ASCII these characters have octal codes 000 through 037 and 177 (DEL), in other character sets these are the equivalent characters, if any [[:digit:]] digits 0 to 9 [[:graph:]] graphical characters [[:alnum:]] and [[:punct:]] [[:lower:]] lower-case letters a to z in the C locale and ASCII character encoding [[:print:]] printable characters [[:alnum:]] , [[:punct:]] and space [[:punct:]] punctuation characters ! , \" , # , $ , % , & , ' , ( , ) , * , + , , , - , . , / , : , ; , < , = , > , ? , @ , [ , \\ , ] , ^ , _ , ` , { , \\| , } and ~ in the C locale and ASCII character encoding [[:space:]] space characters tab , newline , vertical tab , form feed , carriage return and space in the C locale [[:upper:]] upper-case letters A to Z in the C locale and ASCII character encoding [[:xdigit:]] hexadecimal digits 0 to 9 , A to F and a to f","title":"Character classes and bracket expressions"},{"location":"learning/tools/sed/#further-readings","text":"GNU SED Online Tester Character Classes and Bracket Expressions","title":"Further readings"},{"location":"learning/tools/send%20an%20email%20from%20cli/","text":"Send an email from CLI \u00b6 TL;DR \u00b6 mail -s \"Subject\" recipient@mail.server echo \"\" | mail -a attachment.file -s \"Subject\" recipient@mail.server # send larger files cat file.txt | mail -s \"Subject\" recipient@mail.server # make \"email-safe\" the contents of a file uuencode file.txt | mail -s \"Subject\" recipient@mail.server Further readings \u00b6 linux mail command examples uuencode","title":"Send an email from CLI"},{"location":"learning/tools/send%20an%20email%20from%20cli/#send-an-email-from-cli","text":"","title":"Send an email from CLI"},{"location":"learning/tools/send%20an%20email%20from%20cli/#tldr","text":"mail -s \"Subject\" recipient@mail.server echo \"\" | mail -a attachment.file -s \"Subject\" recipient@mail.server # send larger files cat file.txt | mail -s \"Subject\" recipient@mail.server # make \"email-safe\" the contents of a file uuencode file.txt | mail -s \"Subject\" recipient@mail.server","title":"TL;DR"},{"location":"learning/tools/send%20an%20email%20from%20cli/#further-readings","text":"linux mail command examples uuencode","title":"Further readings"},{"location":"learning/tools/set%20the%20ondemand%20cpu%20governor%20to%20not%20rise%20the%20frequencies%20for%20niced%20load/","text":"Set the ondemand CPU governor to not rise the frequencies for niced loads \u00b6 TL:DR \u00b6 sudo cpupower frequency-set --governor ondemand echo 1 | sudo tee /sys/devices/system/cpu/cpufreq/ondemand/ignore_nice_load # set this on boot echo \"w /sys/devices/system/cpu/cpufreq/ondemand/ignore_nice_load - - - - 1\" | sudo tee /etc/tmpfiles.d/ondemand-ignore-nice.conf Further readings \u00b6 Cpufreq Laptop overheating and battery duration reduction","title":"Set the _ondemand_ CPU governor to not rise the frequencies for niced loads"},{"location":"learning/tools/set%20the%20ondemand%20cpu%20governor%20to%20not%20rise%20the%20frequencies%20for%20niced%20load/#set-the-ondemand-cpu-governor-to-not-rise-the-frequencies-for-niced-loads","text":"","title":"Set the ondemand CPU governor to not rise the frequencies for niced loads"},{"location":"learning/tools/set%20the%20ondemand%20cpu%20governor%20to%20not%20rise%20the%20frequencies%20for%20niced%20load/#tldr","text":"sudo cpupower frequency-set --governor ondemand echo 1 | sudo tee /sys/devices/system/cpu/cpufreq/ondemand/ignore_nice_load # set this on boot echo \"w /sys/devices/system/cpu/cpufreq/ondemand/ignore_nice_load - - - - 1\" | sudo tee /etc/tmpfiles.d/ondemand-ignore-nice.conf","title":"TL:DR"},{"location":"learning/tools/set%20the%20ondemand%20cpu%20governor%20to%20not%20rise%20the%20frequencies%20for%20niced%20load/#further-readings","text":"Cpufreq Laptop overheating and battery duration reduction","title":"Further readings"},{"location":"learning/tools/shellcheck/","text":"ShellCheck \u00b6 Gives warnings and suggestions about bash / sh shell scripts. TL;DR \u00b6 shellcheck /path/to/script.sh Further readings \u00b6 Website Github","title":"ShellCheck"},{"location":"learning/tools/shellcheck/#shellcheck","text":"Gives warnings and suggestions about bash / sh shell scripts.","title":"ShellCheck"},{"location":"learning/tools/shellcheck/#tldr","text":"shellcheck /path/to/script.sh","title":"TL;DR"},{"location":"learning/tools/shellcheck/#further-readings","text":"Website Github","title":"Further readings"},{"location":"learning/tools/shred/","text":"Shred \u00b6 TL;DR \u00b6 shred --force --remove --verbose --zero file other-file","title":"Shred"},{"location":"learning/tools/shred/#shred","text":"","title":"Shred"},{"location":"learning/tools/shred/#tldr","text":"shred --force --remove --verbose --zero file other-file","title":"TL;DR"},{"location":"learning/tools/shuf/","text":"Shuf \u00b6 TL;DR \u00b6 # randomize the order of lines in a file and output the result shuf filename # only output the first 5 entries of the result shuf -n 5 filename # write output to another file shuf filename -o output_filename # generate random numbers in range 1-10: shuf -i 1 -10","title":"Shuf"},{"location":"learning/tools/shuf/#shuf","text":"","title":"Shuf"},{"location":"learning/tools/shuf/#tldr","text":"# randomize the order of lines in a file and output the result shuf filename # only output the first 5 entries of the result shuf -n 5 filename # write output to another file shuf filename -o output_filename # generate random numbers in range 1-10: shuf -i 1 -10","title":"TL;DR"},{"location":"learning/tools/snap/","text":"Snap \u00b6 TL;DR \u00b6 # Find snaps. snap find chezmoi snap find --private boincstats-js snap search vscode # View detailed information about snaps. snap info snapd # Download snaps and their assertions without installing them snap download constellation # Install snaps. sudo snap install karuta sudo snap install code-tray --channel = beta snap ack foo.assert && snap install foo.snap snap install --dangerous foo.snap snap install --devmode foo snap install --classic foo # List installed snaps. snap list snap list --all # Manually update snaps. sudo snap refresh sudo snap refresh awdur sudo snap refresh mist --channel = beta # Revert snaps to a prior version. sudo snap revert widl-nan sudo snap revert bunyan --revision 5 # Remove snaps. sudo snap remove runjs # Remove all old revisions of all installed snaps. snap list --all \\ | grep disabled | awk '{print $3, $1}' \\ | xargs -I {} -t sh -c \"sudo snap remove --purge --revision {}\" # Log in/out to/from snap. sudo snap login snap logout # View transaction logs. snap changes snap change 123 # Watch transactions. snap watch 123 # Abort transactions. snap abort 123 # View available snap interfaces. snap interfaces # Connect a plug to the ubuntu core slot. snap connect foo:camera :camera # Disconnect a plug from the ubuntu core slot. snap disconnect foo:camera # Disable snaps. snap disable foo # Enable snaps. snap enable foo # Set a snap's properties. snap set foo bar = 10 # Read a snap's current properties. snap get foo bar Manage revisions \u00b6 # List installed snaps with all their revisions. snap list --all # Remove all old revisions of all installed snaps. snap list --all \\ | grep disabled | awk '{print $3, $1}' \\ | xargs -I {} -t sh -c \"sudo snap remove --purge --revision {}\" Further readings \u00b6 cheat.sh Managing Ubuntu snaps","title":"Snap"},{"location":"learning/tools/snap/#snap","text":"","title":"Snap"},{"location":"learning/tools/snap/#tldr","text":"# Find snaps. snap find chezmoi snap find --private boincstats-js snap search vscode # View detailed information about snaps. snap info snapd # Download snaps and their assertions without installing them snap download constellation # Install snaps. sudo snap install karuta sudo snap install code-tray --channel = beta snap ack foo.assert && snap install foo.snap snap install --dangerous foo.snap snap install --devmode foo snap install --classic foo # List installed snaps. snap list snap list --all # Manually update snaps. sudo snap refresh sudo snap refresh awdur sudo snap refresh mist --channel = beta # Revert snaps to a prior version. sudo snap revert widl-nan sudo snap revert bunyan --revision 5 # Remove snaps. sudo snap remove runjs # Remove all old revisions of all installed snaps. snap list --all \\ | grep disabled | awk '{print $3, $1}' \\ | xargs -I {} -t sh -c \"sudo snap remove --purge --revision {}\" # Log in/out to/from snap. sudo snap login snap logout # View transaction logs. snap changes snap change 123 # Watch transactions. snap watch 123 # Abort transactions. snap abort 123 # View available snap interfaces. snap interfaces # Connect a plug to the ubuntu core slot. snap connect foo:camera :camera # Disconnect a plug from the ubuntu core slot. snap disconnect foo:camera # Disable snaps. snap disable foo # Enable snaps. snap enable foo # Set a snap's properties. snap set foo bar = 10 # Read a snap's current properties. snap get foo bar","title":"TL;DR"},{"location":"learning/tools/snap/#manage-revisions","text":"# List installed snaps with all their revisions. snap list --all # Remove all old revisions of all installed snaps. snap list --all \\ | grep disabled | awk '{print $3, $1}' \\ | xargs -I {} -t sh -c \"sudo snap remove --purge --revision {}\"","title":"Manage revisions"},{"location":"learning/tools/snap/#further-readings","text":"cheat.sh Managing Ubuntu snaps","title":"Further readings"},{"location":"learning/tools/snapper/","text":"Snapper \u00b6 TL;DR \u00b6 # list existing configurations snapper list-config # list existing snapshots snapper list # create a manual standalone snapshot snapper --config root create --type single --description \"manual checkpoint\" --userdata \"important=yes\" --read-only # rollback to snapshot 0 snapper rollback 0 # delete one or more snapshots snapper delete 5 snapper delete --sync { 7 ..9 } # compare 2 snapshots snapper status 0 ..6 snapper diff 6 ..21 Further readings \u00b6 Arch Wiki","title":"Snapper"},{"location":"learning/tools/snapper/#snapper","text":"","title":"Snapper"},{"location":"learning/tools/snapper/#tldr","text":"# list existing configurations snapper list-config # list existing snapshots snapper list # create a manual standalone snapshot snapper --config root create --type single --description \"manual checkpoint\" --userdata \"important=yes\" --read-only # rollback to snapshot 0 snapper rollback 0 # delete one or more snapshots snapper delete 5 snapper delete --sync { 7 ..9 } # compare 2 snapshots snapper status 0 ..6 snapper diff 6 ..21","title":"TL;DR"},{"location":"learning/tools/snapper/#further-readings","text":"Arch Wiki","title":"Further readings"},{"location":"learning/tools/sort/","text":"Sort \u00b6 TL;DR \u00b6 # Sort given lines. sort path/to/file # Sort lines in reverse. sort -r path/to/file # Sort lines numerically. sort -n path/to/file # Sort lines and remove duplicates. sort -u path/to/file # Sort by the value in the last field. awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt \\ | sort -n -t '|' | awk -F '|' '{print $NF}' Sort by the value in the last field \u00b6 copy the last field (column) of each line at the beginning of each of the lines with a different delimiter: awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt sort on the 1 st field specifing the delimiter to be the character above: awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt | sort -n -t '|' discard the first field awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt | sort -n -t '|' | awk -F '|' '{print $NF}' awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt | sort -n -t '|' | awk -F '|' '{print $2}' awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt | sort -n -t '|' | cut -d '|' -f 2 Sources \u00b6 Sort a file in Unix based on the last field","title":"Sort"},{"location":"learning/tools/sort/#sort","text":"","title":"Sort"},{"location":"learning/tools/sort/#tldr","text":"# Sort given lines. sort path/to/file # Sort lines in reverse. sort -r path/to/file # Sort lines numerically. sort -n path/to/file # Sort lines and remove duplicates. sort -u path/to/file # Sort by the value in the last field. awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt \\ | sort -n -t '|' | awk -F '|' '{print $NF}'","title":"TL;DR"},{"location":"learning/tools/sort/#sort-by-the-value-in-the-last-field","text":"copy the last field (column) of each line at the beginning of each of the lines with a different delimiter: awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt sort on the 1 st field specifing the delimiter to be the character above: awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt | sort -n -t '|' discard the first field awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt | sort -n -t '|' | awk -F '|' '{print $NF}' awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt | sort -n -t '|' | awk -F '|' '{print $2}' awk 'BEGIN {FS=\",\"; OFS=\"|\"} {print $NF,$0}' file.txt | sort -n -t '|' | cut -d '|' -f 2","title":"Sort by the value in the last field"},{"location":"learning/tools/sort/#sources","text":"Sort a file in Unix based on the last field","title":"Sources"},{"location":"learning/tools/split/","text":"Split \u00b6 TL;DR \u00b6 # Break the 'home.tar.bz2' archive file into small blocks. # Each block up to 10MB (10\\*1000\\*1000) in size. # Prefix each chunk with 'home.tar.bz2.part'. split -b 10M home.tar.bz2 \"home.tar.bz2.part\" # Break the 'logs.tgz' file into 2M (2\\*1024\\*1024) bytes blocks. # Number them in the suffix. split -b 2M -d logs.tgz \"logs.tgz.\" Sources \u00b6 split large tar into multiple files of certain size create a tar archive split into blocks of a maximum size","title":"Split"},{"location":"learning/tools/split/#split","text":"","title":"Split"},{"location":"learning/tools/split/#tldr","text":"# Break the 'home.tar.bz2' archive file into small blocks. # Each block up to 10MB (10\\*1000\\*1000) in size. # Prefix each chunk with 'home.tar.bz2.part'. split -b 10M home.tar.bz2 \"home.tar.bz2.part\" # Break the 'logs.tgz' file into 2M (2\\*1024\\*1024) bytes blocks. # Number them in the suffix. split -b 2M -d logs.tgz \"logs.tgz.\"","title":"TL;DR"},{"location":"learning/tools/split/#sources","text":"split large tar into multiple files of certain size create a tar archive split into blocks of a maximum size","title":"Sources"},{"location":"learning/tools/sponge/","text":"Sponge \u00b6 TL;DR \u00b6 # installation brew install sponge # or moreutils # append file content to the source file cat path/to/file | sponge -a path/to/other/file # remove all lines starting with \"#\" in a file grep -v '^{{#}}' path/to/file | sponge path/to/other/file Further readings \u00b6 mankier man page tldr live demo page","title":"Sponge"},{"location":"learning/tools/sponge/#sponge","text":"","title":"Sponge"},{"location":"learning/tools/sponge/#tldr","text":"# installation brew install sponge # or moreutils # append file content to the source file cat path/to/file | sponge -a path/to/other/file # remove all lines starting with \"#\" in a file grep -v '^{{#}}' path/to/file | sponge path/to/other/file","title":"TL;DR"},{"location":"learning/tools/sponge/#further-readings","text":"mankier man page tldr live demo page","title":"Further readings"},{"location":"learning/tools/ssh/","text":"SSH \u00b6 TL;DR Key Management SSHFS Installation Configuration Further readings Sources TL;DR \u00b6 # Load keys from '~/.ssh' and add them to the agent. eval ` ssh-agent ` && ssh-add # Create new keys. ssh-keygen -t rsa -b 4096 ssh-keygen -t dsa ssh-keygen -t ecdsa -b 521 ssh-keygen -t ed25519 -f ~/.ssh/keys/id_ed25519 -C test@winzoz # Remove elements from the known hosts list. ssh-keygen -R \"pi4.lan\" ssh-keygen -R 192 .168.1.237 -f .ssh/known_hosts ssh-keygen -R \"raspberrypi.lan\" -f \" ${ HOME } /.ssh/known_hosts\" # Change the password of a key. ssh-keygen -f ~/.ssh/id_rsa -p # Mount a remote folder. sshfs nas.lan:/mnt/data Data -o auto_cache,reconnect,defer_permissions,noappledouble,volname = Data # List keys added to the agent by fingerprint. ssh-add -l ssh-add -L # full key in OpenSSH format # Authorize keys for passwordless access. ssh-copy-id -i ~/.ssh/id_rsa.pub user@nas.lan Key Management \u00b6 Create a new key: ssh-keygen -t rsa -b 4096 ssh-keygen -t dsa ssh-keygen -t ecdsa -b 521 ssh-keygen -t ed25519 -f .ssh/id_ed25519 -C test@winzoz Generating public/private ed25519 key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in C:\\Users\\test/.ssh/id_ed25519. Your public key has been saved in C:\\Users\\test/.ssh/id_ed25519.pub. The key fingerprint is: SHA256:lFrpPyqTy0d30TfnN0QRY678LnyCzmvMDbl1Qj2/U/w test@winzoz The key's randomart image is: +--[ED25519 256]--+ | +o.o++| | ==*O| | . .X*| | o . +=| | S S +..==| | . .+..*E| | + ...o| | .+ .o = | | =+ .o .| +----[SHA256]-----+ Remove a host from the list of known hosts: ssh-keygen -R \"pi4.lan\" ssh-keygen -R 192 .168.1.237 -f .ssh/known_hosts ssh-keygen -R \"raspberrypi.lan\" -f \".ssh/known_hosts\" Host pi4.lan found: line 5 /home/mek/.ssh/known_hosts updated. Original contents retained as /home/mek/.ssh/known_hosts.old Change password of a key file ssh-keygen -f ~/.ssh/id_rsa -p SSHFS \u00b6 Options: auto_cache enables caching based on modification times; reconnect reconnects to the server; defer_permissions works around the issue where certain shares may mount properly, but cause permissions denied errors when accessed (caused by how Mac OS X's Finder translates and interprets permissions; noappledouble prevents Mac OS X to write .DS_Store files on the remote file system; volname defines the name to use for the volume. Usage: sshfs -o $OPTIONS_LIST $HOST : $REMOTE_PATH $LOCAL_PATH sshfs user@nas.lan:/mnt/data Data -o auto_cache,reconnect,defer_permissions,noappledouble,volname = Data Installation \u00b6 # Mac OS X requires `macports`, since `brew` does not offer 'sshfs' anymore sudo port install sshfs Configuration \u00b6 When connecting to a host, the SSH client will use settings: from the command line, from the user's ~/.ssh/config file, from the /etc/ssh/ssh_config file In a first-come-first-served way. Settings should hence appear from the most specific to the most generic: Host targaryen HostName targaryen.example.com User john Port 2322 IdentityFile ~/.ssh/targaryen.key LogLevel INFO Compression yes Host *ell user oberyn sendenv BE_SASSY StrictHostKeyChecking no Host * !martell LogLevel INFO StrictHostKeyChecking accept-new UserKnownHostsFile /dev/null Host * User root Compression yes SendEnv -LC_* -LANG* # Append domains to a hostname before attempting to check if they exist. CanonicalizeHostname yes CanonicalDomains xxx.auckland.ac.nz yyy.auckland.ac.nz Host *.xxx.auckland.ac.nz User user_xxx Host *.yyy.auckland.ac.nz User user_yyy # Keep a connection open for 30s and reuse it when possible. # Save the above pipe in a safe directory, and use a hash of different data to # identify it. # source: https://www.cyberciti.biz/faq/linux-unix-reuse-openssh-connection/ ControlMaster auto ControlPath ~/.ssh/control-%C ControlPersist 30s Further readings \u00b6 ssh-agent Sources \u00b6 Use SSHFS to mount a remote directory as a volume on OSX Using the SSH config file How to list keys added to ssh-agent with ssh-add? Multiple similar entries in ssh config How to enable SSH access using a GPG key for authentication How to perform hostname canonicalization How to reuse SSH connection to speed up remote login process using multiplexing","title":"SSH"},{"location":"learning/tools/ssh/#ssh","text":"TL;DR Key Management SSHFS Installation Configuration Further readings Sources","title":"SSH"},{"location":"learning/tools/ssh/#tldr","text":"# Load keys from '~/.ssh' and add them to the agent. eval ` ssh-agent ` && ssh-add # Create new keys. ssh-keygen -t rsa -b 4096 ssh-keygen -t dsa ssh-keygen -t ecdsa -b 521 ssh-keygen -t ed25519 -f ~/.ssh/keys/id_ed25519 -C test@winzoz # Remove elements from the known hosts list. ssh-keygen -R \"pi4.lan\" ssh-keygen -R 192 .168.1.237 -f .ssh/known_hosts ssh-keygen -R \"raspberrypi.lan\" -f \" ${ HOME } /.ssh/known_hosts\" # Change the password of a key. ssh-keygen -f ~/.ssh/id_rsa -p # Mount a remote folder. sshfs nas.lan:/mnt/data Data -o auto_cache,reconnect,defer_permissions,noappledouble,volname = Data # List keys added to the agent by fingerprint. ssh-add -l ssh-add -L # full key in OpenSSH format # Authorize keys for passwordless access. ssh-copy-id -i ~/.ssh/id_rsa.pub user@nas.lan","title":"TL;DR"},{"location":"learning/tools/ssh/#key-management","text":"Create a new key: ssh-keygen -t rsa -b 4096 ssh-keygen -t dsa ssh-keygen -t ecdsa -b 521 ssh-keygen -t ed25519 -f .ssh/id_ed25519 -C test@winzoz Generating public/private ed25519 key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in C:\\Users\\test/.ssh/id_ed25519. Your public key has been saved in C:\\Users\\test/.ssh/id_ed25519.pub. The key fingerprint is: SHA256:lFrpPyqTy0d30TfnN0QRY678LnyCzmvMDbl1Qj2/U/w test@winzoz The key's randomart image is: +--[ED25519 256]--+ | +o.o++| | ==*O| | . .X*| | o . +=| | S S +..==| | . .+..*E| | + ...o| | .+ .o = | | =+ .o .| +----[SHA256]-----+ Remove a host from the list of known hosts: ssh-keygen -R \"pi4.lan\" ssh-keygen -R 192 .168.1.237 -f .ssh/known_hosts ssh-keygen -R \"raspberrypi.lan\" -f \".ssh/known_hosts\" Host pi4.lan found: line 5 /home/mek/.ssh/known_hosts updated. Original contents retained as /home/mek/.ssh/known_hosts.old Change password of a key file ssh-keygen -f ~/.ssh/id_rsa -p","title":"Key Management"},{"location":"learning/tools/ssh/#sshfs","text":"Options: auto_cache enables caching based on modification times; reconnect reconnects to the server; defer_permissions works around the issue where certain shares may mount properly, but cause permissions denied errors when accessed (caused by how Mac OS X's Finder translates and interprets permissions; noappledouble prevents Mac OS X to write .DS_Store files on the remote file system; volname defines the name to use for the volume. Usage: sshfs -o $OPTIONS_LIST $HOST : $REMOTE_PATH $LOCAL_PATH sshfs user@nas.lan:/mnt/data Data -o auto_cache,reconnect,defer_permissions,noappledouble,volname = Data","title":"SSHFS"},{"location":"learning/tools/ssh/#installation","text":"# Mac OS X requires `macports`, since `brew` does not offer 'sshfs' anymore sudo port install sshfs","title":"Installation"},{"location":"learning/tools/ssh/#configuration","text":"When connecting to a host, the SSH client will use settings: from the command line, from the user's ~/.ssh/config file, from the /etc/ssh/ssh_config file In a first-come-first-served way. Settings should hence appear from the most specific to the most generic: Host targaryen HostName targaryen.example.com User john Port 2322 IdentityFile ~/.ssh/targaryen.key LogLevel INFO Compression yes Host *ell user oberyn sendenv BE_SASSY StrictHostKeyChecking no Host * !martell LogLevel INFO StrictHostKeyChecking accept-new UserKnownHostsFile /dev/null Host * User root Compression yes SendEnv -LC_* -LANG* # Append domains to a hostname before attempting to check if they exist. CanonicalizeHostname yes CanonicalDomains xxx.auckland.ac.nz yyy.auckland.ac.nz Host *.xxx.auckland.ac.nz User user_xxx Host *.yyy.auckland.ac.nz User user_yyy # Keep a connection open for 30s and reuse it when possible. # Save the above pipe in a safe directory, and use a hash of different data to # identify it. # source: https://www.cyberciti.biz/faq/linux-unix-reuse-openssh-connection/ ControlMaster auto ControlPath ~/.ssh/control-%C ControlPersist 30s","title":"Configuration"},{"location":"learning/tools/ssh/#further-readings","text":"ssh-agent","title":"Further readings"},{"location":"learning/tools/ssh/#sources","text":"Use SSHFS to mount a remote directory as a volume on OSX Using the SSH config file How to list keys added to ssh-agent with ssh-add? Multiple similar entries in ssh config How to enable SSH access using a GPG key for authentication How to perform hostname canonicalization How to reuse SSH connection to speed up remote login process using multiplexing","title":"Sources"},{"location":"learning/tools/stow/","text":"GNU Stow \u00b6 TL;DR \u00b6 # Stow files from packages in the current directory. stow --dotfiles package1 packageN # Simulate to stow files from packages in ~/.dotfiles to $HOME. $ stow --dotfiles --dir $HOME /.dotfiles --target $HOME vim screen --simulate --verbose Further readings \u00b6 Website","title":"GNU Stow"},{"location":"learning/tools/stow/#gnu-stow","text":"","title":"GNU Stow"},{"location":"learning/tools/stow/#tldr","text":"# Stow files from packages in the current directory. stow --dotfiles package1 packageN # Simulate to stow files from packages in ~/.dotfiles to $HOME. $ stow --dotfiles --dir $HOME /.dotfiles --target $HOME vim screen --simulate --verbose","title":"TL;DR"},{"location":"learning/tools/stow/#further-readings","text":"Website","title":"Further readings"},{"location":"learning/tools/sudo/","text":"Sudo \u00b6 Avoid modifying the sudoers files manually and execute visudo instead; it will check the syntax on save, preventing you from screwing up the file. TL;DR \u00b6 # Make changes to a sudoers file. visudo visudo -f path/to/file # Check the syntax of a sudoers file. visudo -c path/to/file Drop privileges \u00b6 # Invalidate the user's cached credentials. sudo -k # Ignore the user's cached credentials for the given command only. sudo -k ls Restrict permissions a little \u00b6 # file /etc/sudoers.d/user Cmnd_Alias UPGRADE_CMND = /usr/bin/apt update, /usr/bin/apt list --upgradable, /usr/bin/apt upgrade Cmnd_Alias SHUTDOWN_CMND = /sbin/shutdown user ALL =( ALL:ALL ) NOPASSWD: SHUTDOWN_CMND, UPGRADE_CMND Avoid providing a password \u00b6 # file /etc/sudoers.d/user user ALL =( ALL:ALL ) NOPASSWD: ALL Execute commands as a specific user \u00b6 Invoke a login shell using the -i, --login option. When one does not specify a command a login shell prompt is returned, otherwise the output of the command is returned: % whoami root % sudo -i -u user $ whoami user % sudo -i -u user whoami user Troubleshooting \u00b6 I modified a sudoers file manually, messed it up, and now I cannot use sudo anymore \u00b6 Should you see something similar to this when using sudo : $ sudo visudo >>> /etc/sudoers: syntax error near line 28 <<< sudo: parse error in /etc/sudoers near line 28 sudo: no valid sudoers sources found, quitting try using another access method like PolicyKit and fix the file up: pkexec visudo -f /etc/sudoers.d/user Sources \u00b6 How to modify an invalid sudoers file sudo as another user with their environment sudo: Drop root privileges","title":"Sudo"},{"location":"learning/tools/sudo/#sudo","text":"Avoid modifying the sudoers files manually and execute visudo instead; it will check the syntax on save, preventing you from screwing up the file.","title":"Sudo"},{"location":"learning/tools/sudo/#tldr","text":"# Make changes to a sudoers file. visudo visudo -f path/to/file # Check the syntax of a sudoers file. visudo -c path/to/file","title":"TL;DR"},{"location":"learning/tools/sudo/#drop-privileges","text":"# Invalidate the user's cached credentials. sudo -k # Ignore the user's cached credentials for the given command only. sudo -k ls","title":"Drop privileges"},{"location":"learning/tools/sudo/#restrict-permissions-a-little","text":"# file /etc/sudoers.d/user Cmnd_Alias UPGRADE_CMND = /usr/bin/apt update, /usr/bin/apt list --upgradable, /usr/bin/apt upgrade Cmnd_Alias SHUTDOWN_CMND = /sbin/shutdown user ALL =( ALL:ALL ) NOPASSWD: SHUTDOWN_CMND, UPGRADE_CMND","title":"Restrict permissions a little"},{"location":"learning/tools/sudo/#avoid-providing-a-password","text":"# file /etc/sudoers.d/user user ALL =( ALL:ALL ) NOPASSWD: ALL","title":"Avoid providing a password"},{"location":"learning/tools/sudo/#execute-commands-as-a-specific-user","text":"Invoke a login shell using the -i, --login option. When one does not specify a command a login shell prompt is returned, otherwise the output of the command is returned: % whoami root % sudo -i -u user $ whoami user % sudo -i -u user whoami user","title":"Execute commands as a specific user"},{"location":"learning/tools/sudo/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/sudo/#i-modified-a-sudoers-file-manually-messed-it-up-and-now-i-cannot-use-sudo-anymore","text":"Should you see something similar to this when using sudo : $ sudo visudo >>> /etc/sudoers: syntax error near line 28 <<< sudo: parse error in /etc/sudoers near line 28 sudo: no valid sudoers sources found, quitting try using another access method like PolicyKit and fix the file up: pkexec visudo -f /etc/sudoers.d/user","title":"I modified a sudoers file manually, messed it up, and now I cannot use sudo anymore"},{"location":"learning/tools/sudo/#sources","text":"How to modify an invalid sudoers file sudo as another user with their environment sudo: Drop root privileges","title":"Sources"},{"location":"learning/tools/swap/","text":"Swap \u00b6 TL;DR \u00b6 # show the swap usage swapon --show free -h # enable or disable a swap partition or file sudo swapon /root/swapfile sudo swapoff LABEL = swap sudo swapoff /dev/sda2 # enable or disable *all* swap partition or file sudo swapon -a sudo swapoff --all # check what processes are swapping # see the \"si\" (swap in) and \"so\" (swap out) columns vmstat vmstat --wide 1 Swappiness \u00b6 # change the current value sudo sysctl vm.swappiness = 10 sudo sysctl -w vm/swappiness = 5 # persistent configuration echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf echo 'vm.swappiness = 5' | sudo tee -a /etc/sysctl.d/99-swappiness.conf Swapfile \u00b6 # add a swapfile sudo fallocate -l 1G /swapfile # or sudo dd if=/dev/zero of=/swapfile bs=1024 count=1048576 sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile echo '/swapfile swap swap defaults 0 0' | sudo tee -a /etc/fstab # remove a swapfile sudo swapoff -v /swapfile sudo sed -i.bak '/\\/swapfile/d' /etc/fstab sudo rm /swapfile Further readings \u00b6 create a linux swap file How to reload sysctl.conf variables on Linux How to empty swap if there is free RAM","title":"Swap"},{"location":"learning/tools/swap/#swap","text":"","title":"Swap"},{"location":"learning/tools/swap/#tldr","text":"# show the swap usage swapon --show free -h # enable or disable a swap partition or file sudo swapon /root/swapfile sudo swapoff LABEL = swap sudo swapoff /dev/sda2 # enable or disable *all* swap partition or file sudo swapon -a sudo swapoff --all # check what processes are swapping # see the \"si\" (swap in) and \"so\" (swap out) columns vmstat vmstat --wide 1","title":"TL;DR"},{"location":"learning/tools/swap/#swappiness","text":"# change the current value sudo sysctl vm.swappiness = 10 sudo sysctl -w vm/swappiness = 5 # persistent configuration echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf echo 'vm.swappiness = 5' | sudo tee -a /etc/sysctl.d/99-swappiness.conf","title":"Swappiness"},{"location":"learning/tools/swap/#swapfile","text":"# add a swapfile sudo fallocate -l 1G /swapfile # or sudo dd if=/dev/zero of=/swapfile bs=1024 count=1048576 sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile echo '/swapfile swap swap defaults 0 0' | sudo tee -a /etc/fstab # remove a swapfile sudo swapoff -v /swapfile sudo sed -i.bak '/\\/swapfile/d' /etc/fstab sudo rm /swapfile","title":"Swapfile"},{"location":"learning/tools/swap/#further-readings","text":"create a linux swap file How to reload sysctl.conf variables on Linux How to empty swap if there is free RAM","title":"Further readings"},{"location":"learning/tools/sync/","text":"Sync \u00b6 # Flush all cached file data of the current user only. sync # Flush all pending write operations on all disks and mounted file systems. sudo sync # Flush all pending write operations on given files only to disk. sync path/to/first/file path/to/second/file # Flush all pending write operations on all files in a directory, recursively. sync path/to/directory # Flush only the file data and its minimal metadata to disk. sync -d path/to/file # Flush all pending write operations on only the filesystem on mounted partition # '/dev/sdc1'. sudo sync /dev/sdc1 # Flush all pending write operations on all mounted filesystem from '/dev/sdb'. sudo sync /dev/sdb # Flush all pending write operations on the entire file system which contains # '/var/log/syslog'. sudo sync -f /var/log/syslog Sources \u00b6 cheat.sh Linux sync command","title":"Sync"},{"location":"learning/tools/sync/#sync","text":"# Flush all cached file data of the current user only. sync # Flush all pending write operations on all disks and mounted file systems. sudo sync # Flush all pending write operations on given files only to disk. sync path/to/first/file path/to/second/file # Flush all pending write operations on all files in a directory, recursively. sync path/to/directory # Flush only the file data and its minimal metadata to disk. sync -d path/to/file # Flush all pending write operations on only the filesystem on mounted partition # '/dev/sdc1'. sudo sync /dev/sdc1 # Flush all pending write operations on all mounted filesystem from '/dev/sdb'. sudo sync /dev/sdb # Flush all pending write operations on the entire file system which contains # '/var/log/syslog'. sudo sync -f /var/log/syslog","title":"Sync"},{"location":"learning/tools/sync/#sources","text":"cheat.sh Linux sync command","title":"Sources"},{"location":"learning/tools/synology%20dsm/","text":"Synology DiskStation Manager \u00b6 System's shared folders \u00b6 Automatically created by services or packages. Cannot be changed/removed manually if their creator is still active or installed. /volumeX \u251c\u2500\u2500 docker # data container for the Docker service, created by it upon installation \u251c\u2500\u2500 homes # all users' home directories, created by the SSH service upon activation \u251c\u2500\u2500 music # created by the Media Server package upon installation \u251c\u2500\u2500 NetBackup # created by the rsync service upon activation \u251c\u2500\u2500 photo # created by the Media Server package upon installation \u2514\u2500\u2500 video # created by the Media Server package upon installation Rsync \u00b6 Requirements: the rsync service is enabled under Control Panel > File Services > rsync the user has the right permissions for the shared folder under either Control Panel > Shared Folders > Shared Folder edit window > Permissions , or Control Panel > User & Group > User or Group edit window > Permissions Examples: # From a shared folder on a NAS to a local one. # Use the SSH port defined in the NAS settings. rsync \\ \"user@nas:/volume1/shared_folder/\" \\ \"path/to/local/folder/\" \\ --archive --copy-links --protect-args \\ --acls --xattrs --fake-super \\ --partial --append-verify --sparse \\ --progress -vv --no-inc-recursive \\ --compress --no-motd --rsh = 'ssh -p12345' \\ --exclude \"@eaDir\" --exclude \"#recycle\" \\ --delete --dry-run # Sync all snapshotted data to a folder. find /volume1/@sharesnap/shared_folder \\ -maxdepth 1 -mindepth 1 \\ -type d | xargs -I {} -n 1 -t \\ rsync \\ -AXahvz --chown = user --info = progress2 \\ --append-verify --partial \\ --no-inc-recursive --no-motd \\ {} / \\ /volume2/destination/folder/ Snapshots \u00b6 Use the Snapshot Replication package available in the Package Center for better control and automation. Gotchas: when the Make snapshot visible option in a shared folder's settings in Snapshot Replication is ticked: the #snapshot folder is created in the shared folder's root directory the default snapshots directory for that shared folder is mounted on it in read only mode: /dev/mapper/cachedev_0 on /volume1/Data/#snapshot type btrfs (ro,nodev,relatime,ssd,synoacl,space_cache=v2,auto_reclaim_space,metadata_ratio=50,block_group_cache_tree,subvolid=266,subvol=/@syno/@sharesnap/Data) Data deduplication \u00b6 Requirements: docker needs to be installed from the package manager, as it is simpler (and safer?) to run a container than installing duperemove or jdupes and all their dependencies on the machine Remove duplicated files with jdupes \u00b6 Examples: # `sudo` is only needed if the user has no privileges to run `docker` commands. sudo docker run \\ -it --init --rm --name jdupes \\ -v \"/volume/shared_folder1:/data1\" \\ -v \"/volume/shared_folder2:/data2\" \\ ghcr.io/jbruchon/jdupes:latest \\ -drOZ \\ -X 'nostr:@eaDir' -X 'nostr:#recycle' \\ \"/data1\" \"/data2\" Deduplicate blocks in a volume with duperemove \u00b6 Gotchas: duperemove 's container needs to be run in privileged mode ( --privileged ) due to it taking actions on the disk the container might fail on very large datasets, usually due to Out Of Memory (OOM) issues; to avoid this: offload the hashes from RAM using a hash file ( --hashfile \"/volume1/NetBackup/duperemove.tmp\" ) use smaller datasets where possible, like a shared folder and just one of its snapshots instead of all of them duperemove can dedupe blocks only if acting on folders in a rw mount; when deduplicating snapshots, use their rw mount path /@syno/@sharesnap/shared_folder instead of their ro version /volumeN/shared_folder/#snapshot Examples: # small/medium dataset # 2 folders in a shared folder sudo docker run --privileged \\ --rm --name duperemove \\ --mount \"type=bind,source=/volume1/Data,target=/sharedfolder\" \\ michelecereda/duperemove:0.11.2 \\ -Adhr \\ \"/sharedfolder/folder1\" \"/sharedfolder/folder2\" # large dataset # 1 shared folder and all its snapshots sudo docker run --privileged \\ --rm --name duperemove \\ --mount \"type=bind,source=/volume1,target=/volume1\" \\ michelecereda/duperemove:0.11.2 \\ -Adhr \\ --hashfile \"/volume1/NetBackup/duperemove.tmp\" \\ \"/volume1/Data\" \"/volume1/@sharesnap/Data\" Use keybase \u00b6 Just use a containerized service and execute commands with it: # Run the service. docker run -d --name 'keybase' \\ -e KEYBASE_SERVICE = '1' \\ -e KEYBASE_USERNAME = 'user' \\ -e KEYBASE_PAPERKEY = 'paper key' \\ 'keybaseio/client:stable' # Execute commands using the containerized service. docker exec \\ --user 'keybase' \\ keybase \\ keybase whoami Manage git repositories with a containerized keybase instance \u00b6 See the readme for michelecereda/keybaseio-client . Ask for a feature to be implemented \u00b6 Use the online feature request form . Posting a request on the community site will not work. Further readings \u00b6 CLI Administrator Guide for Synology NAS Sources \u00b6 Configuring deduplication block on the Synology","title":"Synology DiskStation Manager"},{"location":"learning/tools/synology%20dsm/#synology-diskstation-manager","text":"","title":"Synology DiskStation Manager"},{"location":"learning/tools/synology%20dsm/#systems-shared-folders","text":"Automatically created by services or packages. Cannot be changed/removed manually if their creator is still active or installed. /volumeX \u251c\u2500\u2500 docker # data container for the Docker service, created by it upon installation \u251c\u2500\u2500 homes # all users' home directories, created by the SSH service upon activation \u251c\u2500\u2500 music # created by the Media Server package upon installation \u251c\u2500\u2500 NetBackup # created by the rsync service upon activation \u251c\u2500\u2500 photo # created by the Media Server package upon installation \u2514\u2500\u2500 video # created by the Media Server package upon installation","title":"System's shared folders"},{"location":"learning/tools/synology%20dsm/#rsync","text":"Requirements: the rsync service is enabled under Control Panel > File Services > rsync the user has the right permissions for the shared folder under either Control Panel > Shared Folders > Shared Folder edit window > Permissions , or Control Panel > User & Group > User or Group edit window > Permissions Examples: # From a shared folder on a NAS to a local one. # Use the SSH port defined in the NAS settings. rsync \\ \"user@nas:/volume1/shared_folder/\" \\ \"path/to/local/folder/\" \\ --archive --copy-links --protect-args \\ --acls --xattrs --fake-super \\ --partial --append-verify --sparse \\ --progress -vv --no-inc-recursive \\ --compress --no-motd --rsh = 'ssh -p12345' \\ --exclude \"@eaDir\" --exclude \"#recycle\" \\ --delete --dry-run # Sync all snapshotted data to a folder. find /volume1/@sharesnap/shared_folder \\ -maxdepth 1 -mindepth 1 \\ -type d | xargs -I {} -n 1 -t \\ rsync \\ -AXahvz --chown = user --info = progress2 \\ --append-verify --partial \\ --no-inc-recursive --no-motd \\ {} / \\ /volume2/destination/folder/","title":"Rsync"},{"location":"learning/tools/synology%20dsm/#snapshots","text":"Use the Snapshot Replication package available in the Package Center for better control and automation. Gotchas: when the Make snapshot visible option in a shared folder's settings in Snapshot Replication is ticked: the #snapshot folder is created in the shared folder's root directory the default snapshots directory for that shared folder is mounted on it in read only mode: /dev/mapper/cachedev_0 on /volume1/Data/#snapshot type btrfs (ro,nodev,relatime,ssd,synoacl,space_cache=v2,auto_reclaim_space,metadata_ratio=50,block_group_cache_tree,subvolid=266,subvol=/@syno/@sharesnap/Data)","title":"Snapshots"},{"location":"learning/tools/synology%20dsm/#data-deduplication","text":"Requirements: docker needs to be installed from the package manager, as it is simpler (and safer?) to run a container than installing duperemove or jdupes and all their dependencies on the machine","title":"Data deduplication"},{"location":"learning/tools/synology%20dsm/#remove-duplicated-files-with-jdupes","text":"Examples: # `sudo` is only needed if the user has no privileges to run `docker` commands. sudo docker run \\ -it --init --rm --name jdupes \\ -v \"/volume/shared_folder1:/data1\" \\ -v \"/volume/shared_folder2:/data2\" \\ ghcr.io/jbruchon/jdupes:latest \\ -drOZ \\ -X 'nostr:@eaDir' -X 'nostr:#recycle' \\ \"/data1\" \"/data2\"","title":"Remove duplicated files with jdupes"},{"location":"learning/tools/synology%20dsm/#deduplicate-blocks-in-a-volume-with-duperemove","text":"Gotchas: duperemove 's container needs to be run in privileged mode ( --privileged ) due to it taking actions on the disk the container might fail on very large datasets, usually due to Out Of Memory (OOM) issues; to avoid this: offload the hashes from RAM using a hash file ( --hashfile \"/volume1/NetBackup/duperemove.tmp\" ) use smaller datasets where possible, like a shared folder and just one of its snapshots instead of all of them duperemove can dedupe blocks only if acting on folders in a rw mount; when deduplicating snapshots, use their rw mount path /@syno/@sharesnap/shared_folder instead of their ro version /volumeN/shared_folder/#snapshot Examples: # small/medium dataset # 2 folders in a shared folder sudo docker run --privileged \\ --rm --name duperemove \\ --mount \"type=bind,source=/volume1/Data,target=/sharedfolder\" \\ michelecereda/duperemove:0.11.2 \\ -Adhr \\ \"/sharedfolder/folder1\" \"/sharedfolder/folder2\" # large dataset # 1 shared folder and all its snapshots sudo docker run --privileged \\ --rm --name duperemove \\ --mount \"type=bind,source=/volume1,target=/volume1\" \\ michelecereda/duperemove:0.11.2 \\ -Adhr \\ --hashfile \"/volume1/NetBackup/duperemove.tmp\" \\ \"/volume1/Data\" \"/volume1/@sharesnap/Data\"","title":"Deduplicate blocks in a volume with duperemove"},{"location":"learning/tools/synology%20dsm/#use-keybase","text":"Just use a containerized service and execute commands with it: # Run the service. docker run -d --name 'keybase' \\ -e KEYBASE_SERVICE = '1' \\ -e KEYBASE_USERNAME = 'user' \\ -e KEYBASE_PAPERKEY = 'paper key' \\ 'keybaseio/client:stable' # Execute commands using the containerized service. docker exec \\ --user 'keybase' \\ keybase \\ keybase whoami","title":"Use keybase"},{"location":"learning/tools/synology%20dsm/#manage-git-repositories-with-a-containerized-keybase-instance","text":"See the readme for michelecereda/keybaseio-client .","title":"Manage git repositories with a containerized keybase instance"},{"location":"learning/tools/synology%20dsm/#ask-for-a-feature-to-be-implemented","text":"Use the online feature request form . Posting a request on the community site will not work.","title":"Ask for a feature to be implemented"},{"location":"learning/tools/synology%20dsm/#further-readings","text":"CLI Administrator Guide for Synology NAS","title":"Further readings"},{"location":"learning/tools/synology%20dsm/#sources","text":"Configuring deduplication block on the Synology","title":"Sources"},{"location":"learning/tools/sysctl/","text":"Sysctl \u00b6 Default configuration files locations: /run/sysctl.d/*.conf /etc/sysctl.d/*.conf /usr/local/lib/sysctl.d/*.conf /usr/lib/sysctl.d/*.conf /lib/sysctl.d/*.conf /etc/sysctl.conf TL;DR \u00b6 # Show the value of a single setting. sysctl kernel.ostype sysctl vm.swappiness # Show the values of all settings. sysctl -a # Change the current value of a setting. sudo sysctl vm.swappiness = 10 sudo sysctl -w net.ipv4.ip_forward = 1 # Reload settings from specific configuration files. sudo sysctl -p sudo sysctl -p /etc/sysctl.d/99-swappiness.conf # Reload settings from the default configuration files locations. sudo sysctl --system # Set up persistent settings. echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf echo 'vm.swappiness = 5' | sudo tee -a /etc/sysctl.d/99-swappiness.conf Further readings \u00b6 How to reload sysctl.conf variables on Linux Documentation for /proc/sys","title":"Sysctl"},{"location":"learning/tools/sysctl/#sysctl","text":"Default configuration files locations: /run/sysctl.d/*.conf /etc/sysctl.d/*.conf /usr/local/lib/sysctl.d/*.conf /usr/lib/sysctl.d/*.conf /lib/sysctl.d/*.conf /etc/sysctl.conf","title":"Sysctl"},{"location":"learning/tools/sysctl/#tldr","text":"# Show the value of a single setting. sysctl kernel.ostype sysctl vm.swappiness # Show the values of all settings. sysctl -a # Change the current value of a setting. sudo sysctl vm.swappiness = 10 sudo sysctl -w net.ipv4.ip_forward = 1 # Reload settings from specific configuration files. sudo sysctl -p sudo sysctl -p /etc/sysctl.d/99-swappiness.conf # Reload settings from the default configuration files locations. sudo sysctl --system # Set up persistent settings. echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf echo 'vm.swappiness = 5' | sudo tee -a /etc/sysctl.d/99-swappiness.conf","title":"TL;DR"},{"location":"learning/tools/sysctl/#further-readings","text":"How to reload sysctl.conf variables on Linux Documentation for /proc/sys","title":"Further readings"},{"location":"learning/tools/sysstat/","text":"sysstat \u00b6 TL;DR Installation example Further readings Sources TL;DR \u00b6 # Get all the available stats. sudo sar -P ALL Installation example \u00b6 sudo apt install sysstat sudo yum install sysstat sudo sed -i.bak 's/ENABLED=\"false\"/ENABLED=\"true\"/' /etc/default/sysstat sudo systemctl enable --now sysstat.service sudo systemctl restart sysstat.service # Wait some time (15-20 mins) for some data to be collected. sudo sar -P ALL Example output: Linux 5.10.17-v8+ (raspberrypi) 07/08/21 _aarch64_ (4 CPU) 10:24:12 LINUX RESTART (4 CPU) 10:25:01 CPU %user %nice %system %iowait %steal %idle 10:35:01 all 0.01 68.97 0.30 0.00 0.00 30.72 10:35:01 0 0.00 91.70 0.20 0.00 0.00 8.09 10:35:01 1 0.00 91.79 0.04 0.00 0.00 8.16 10:35:01 2 0.00 91.96 0.04 0.00 0.00 8.00 10:35:01 3 0.03 0.30 0.91 0.00 0.00 98.76 10:35:01 CPU %user %nice %system %iowait %steal %idle 10:45:01 all 0.09 68.96 0.31 0.00 0.00 30.65 10:45:01 0 0.00 91.88 0.12 0.00 0.00 8.00 10:45:01 1 0.00 54.26 0.49 0.00 0.00 45.25 10:45:01 2 0.00 91.74 0.04 0.00 0.00 8.22 10:45:01 3 0.35 37.90 0.57 0.00 0.00 61.17 Average: CPU %user %nice %system %iowait %steal %idle Average: all 0.05 68.97 0.30 0.00 0.00 30.68 Average: 0 0.00 91.79 0.16 0.00 0.00 8.05 Average: 1 0.00 73.03 0.27 0.00 0.00 26.70 Average: 2 0.00 91.85 0.04 0.00 0.00 8.11 Average: 3 0.19 19.11 0.74 0.00 0.00 79.96 Further readings \u00b6 tutorial Sources \u00b6 webpage github page","title":"sysstat"},{"location":"learning/tools/sysstat/#sysstat","text":"TL;DR Installation example Further readings Sources","title":"sysstat"},{"location":"learning/tools/sysstat/#tldr","text":"# Get all the available stats. sudo sar -P ALL","title":"TL;DR"},{"location":"learning/tools/sysstat/#installation-example","text":"sudo apt install sysstat sudo yum install sysstat sudo sed -i.bak 's/ENABLED=\"false\"/ENABLED=\"true\"/' /etc/default/sysstat sudo systemctl enable --now sysstat.service sudo systemctl restart sysstat.service # Wait some time (15-20 mins) for some data to be collected. sudo sar -P ALL Example output: Linux 5.10.17-v8+ (raspberrypi) 07/08/21 _aarch64_ (4 CPU) 10:24:12 LINUX RESTART (4 CPU) 10:25:01 CPU %user %nice %system %iowait %steal %idle 10:35:01 all 0.01 68.97 0.30 0.00 0.00 30.72 10:35:01 0 0.00 91.70 0.20 0.00 0.00 8.09 10:35:01 1 0.00 91.79 0.04 0.00 0.00 8.16 10:35:01 2 0.00 91.96 0.04 0.00 0.00 8.00 10:35:01 3 0.03 0.30 0.91 0.00 0.00 98.76 10:35:01 CPU %user %nice %system %iowait %steal %idle 10:45:01 all 0.09 68.96 0.31 0.00 0.00 30.65 10:45:01 0 0.00 91.88 0.12 0.00 0.00 8.00 10:45:01 1 0.00 54.26 0.49 0.00 0.00 45.25 10:45:01 2 0.00 91.74 0.04 0.00 0.00 8.22 10:45:01 3 0.35 37.90 0.57 0.00 0.00 61.17 Average: CPU %user %nice %system %iowait %steal %idle Average: all 0.05 68.97 0.30 0.00 0.00 30.68 Average: 0 0.00 91.79 0.16 0.00 0.00 8.05 Average: 1 0.00 73.03 0.27 0.00 0.00 26.70 Average: 2 0.00 91.85 0.04 0.00 0.00 8.11 Average: 3 0.19 19.11 0.74 0.00 0.00 79.96","title":"Installation example"},{"location":"learning/tools/sysstat/#further-readings","text":"tutorial","title":"Further readings"},{"location":"learning/tools/sysstat/#sources","text":"webpage github page","title":"Sources"},{"location":"learning/tools/systemd/","text":"Systemd \u00b6 TL;DR \u00b6 # List all available units. systemctl list-unit-files # List failed units only. systemctl list-units --state = failed # Start services. sudo systemctl start adb.service systemctl --user start keybase.service # Restart services. sudo systemctl restart bluetooth.service systemctl --user restart davmail.service # Stop services. sudo systemctl stop cups.service systemctl --user stop davmail.service # Enable services on boot. sudo systemctl enable sshd.service sudo systemctl enable --now docker.service systemctl --user enable --now davmail.service # Disable services from boot. sudo systemctl disable clamav-freshclam.service sudo systemctl disable --now gdm.service systemctl --user disable --now davmail.service # Suspend the system. # Saves the state to RAM only. systemctl suspend # Hibernate the system. # Saves the state to disk only. systemctl hibernate # Suspend the system in hybrid mode. # Saves the state to *both* RAM *and* disk. systemctl hybrid-sleep # Suspend the system, then hibernate after some time. # Saves the state to RAM initially, and if not interrupted within the specified # delay then wake up using an RTC alarm and hibernate. # Specify such delay in HibernateDelaySec in systemd-sleep.conf(5). systemctl suspend-then-hibernate # Show log entries. journalctl journalctl -f journalctl -n 20 journalctl -o json-pretty journalctl --no-pager journalctl --utc # Show what boots the system has logs about. journalctl --list-boots # Display logs from specific boots only. # Persistent logging needs to be enabled. journalctl -b journalctl -b -3 # Display logs in a specific time window journalctl --since yesterday journalctl --since \"2015-01-10 17:15:00\" journalctl --since 09 :00 --until \"1 hour ago\" journalctl --since \"2015-01-10\" --until \"2015-01-11 03:00\" # Filter logs by unit. journalctl -u nginx.service journalctl -u nginx.service -u php-fpm.service --since today # Filter logs by process, user id or group id. journalctl _PID = 8088 journalctl _UID = 33 --since today journalctl -F _GID # Filter logs by path. journalctl /usr/bin/bash # Display kernel logs only. # Works like `dmesg`. journalctl -k journalctl -k -b -5 # Filter logs by priority. journalctl -p err -b # Truncate the output. journalctl --no-full # Print everything. journalctl -a # Show current logs disk usage. journalctl --disk-usage # Delete old logs. sudo journalctl --vacuum-size = 1G sudo journalctl --vacuum-time = 1years # List available timezones. timedatectl list-timezones # Set timezones. sudo timedatectl set-timezone UTC sudo timedatectl set-timezone Europe/Dublin # Set the time. sudo timedatectl set-time 15 :58:30 sudo timedatectl set-time '2015-11-20 16:14:50' # Set the hardware clock to UTC. timedatectl set-local-rtc 0 # Set the hardware clock to local timezone. timedatectl set-local-rtc 1 # Set automatic time sync. sudo timedatectl set-ntp true sudo timedatectl set-ntp false # Check the time and timezones state. timedatectl status # Show the current hostname state. hostnamectl hostnamectl --pretty status hostnamectl --static status # Set hostnames. hostnamectl set-hostname staticky --static hostnamectl set-hostname prettiky --pretty User services \u00b6 User's service files should be placed into ~/.config/systemd/user : cat > \" ${ HOME } /.config/systemd/user/davmail.service\" <<EOF [Unit] Description=Davmail [Service] ExecStart=/usr/bin/davmail -notray [Install] WantedBy=default.target EOF and can be acted upon as normal using systemctl 's --user switch systemctl --user enable --now davmail.service systemctl --user status davmail.service Keep past boots record (persistent logging) \u00b6 Edit the journal configuration file and set the following option: # file /etc/systemd/journald.conf [Journal] Storage = persistent Resolved \u00b6 Disable systemd-resolved \u00b6 disable and stop the systemd-resolved service: sudo systemctl disable --now systemd-resolved.service set NetworkManager to use the default DNS resolution. # file /etc/NetworkManager/NetworkManager.conf [main] dns = default delete /etc/resolv.conf : sudo unlink /etc/resolv.conf restart NetworkManager sudo service network-manager restart Sources \u00b6 How to disable systemd-resolved in Ubuntu What are the systemctl options to list all failed units? How To Use Journalctl to View and Manipulate Systemd Logs How to Set Time, Timezone and Synchronize System Clock Using timedatectl Command How to Set Hostname Using Hostnamectl Command? Suspend and hibernate","title":"Systemd"},{"location":"learning/tools/systemd/#systemd","text":"","title":"Systemd"},{"location":"learning/tools/systemd/#tldr","text":"# List all available units. systemctl list-unit-files # List failed units only. systemctl list-units --state = failed # Start services. sudo systemctl start adb.service systemctl --user start keybase.service # Restart services. sudo systemctl restart bluetooth.service systemctl --user restart davmail.service # Stop services. sudo systemctl stop cups.service systemctl --user stop davmail.service # Enable services on boot. sudo systemctl enable sshd.service sudo systemctl enable --now docker.service systemctl --user enable --now davmail.service # Disable services from boot. sudo systemctl disable clamav-freshclam.service sudo systemctl disable --now gdm.service systemctl --user disable --now davmail.service # Suspend the system. # Saves the state to RAM only. systemctl suspend # Hibernate the system. # Saves the state to disk only. systemctl hibernate # Suspend the system in hybrid mode. # Saves the state to *both* RAM *and* disk. systemctl hybrid-sleep # Suspend the system, then hibernate after some time. # Saves the state to RAM initially, and if not interrupted within the specified # delay then wake up using an RTC alarm and hibernate. # Specify such delay in HibernateDelaySec in systemd-sleep.conf(5). systemctl suspend-then-hibernate # Show log entries. journalctl journalctl -f journalctl -n 20 journalctl -o json-pretty journalctl --no-pager journalctl --utc # Show what boots the system has logs about. journalctl --list-boots # Display logs from specific boots only. # Persistent logging needs to be enabled. journalctl -b journalctl -b -3 # Display logs in a specific time window journalctl --since yesterday journalctl --since \"2015-01-10 17:15:00\" journalctl --since 09 :00 --until \"1 hour ago\" journalctl --since \"2015-01-10\" --until \"2015-01-11 03:00\" # Filter logs by unit. journalctl -u nginx.service journalctl -u nginx.service -u php-fpm.service --since today # Filter logs by process, user id or group id. journalctl _PID = 8088 journalctl _UID = 33 --since today journalctl -F _GID # Filter logs by path. journalctl /usr/bin/bash # Display kernel logs only. # Works like `dmesg`. journalctl -k journalctl -k -b -5 # Filter logs by priority. journalctl -p err -b # Truncate the output. journalctl --no-full # Print everything. journalctl -a # Show current logs disk usage. journalctl --disk-usage # Delete old logs. sudo journalctl --vacuum-size = 1G sudo journalctl --vacuum-time = 1years # List available timezones. timedatectl list-timezones # Set timezones. sudo timedatectl set-timezone UTC sudo timedatectl set-timezone Europe/Dublin # Set the time. sudo timedatectl set-time 15 :58:30 sudo timedatectl set-time '2015-11-20 16:14:50' # Set the hardware clock to UTC. timedatectl set-local-rtc 0 # Set the hardware clock to local timezone. timedatectl set-local-rtc 1 # Set automatic time sync. sudo timedatectl set-ntp true sudo timedatectl set-ntp false # Check the time and timezones state. timedatectl status # Show the current hostname state. hostnamectl hostnamectl --pretty status hostnamectl --static status # Set hostnames. hostnamectl set-hostname staticky --static hostnamectl set-hostname prettiky --pretty","title":"TL;DR"},{"location":"learning/tools/systemd/#user-services","text":"User's service files should be placed into ~/.config/systemd/user : cat > \" ${ HOME } /.config/systemd/user/davmail.service\" <<EOF [Unit] Description=Davmail [Service] ExecStart=/usr/bin/davmail -notray [Install] WantedBy=default.target EOF and can be acted upon as normal using systemctl 's --user switch systemctl --user enable --now davmail.service systemctl --user status davmail.service","title":"User services"},{"location":"learning/tools/systemd/#keep-past-boots-record-persistent-logging","text":"Edit the journal configuration file and set the following option: # file /etc/systemd/journald.conf [Journal] Storage = persistent","title":"Keep past boots record (persistent logging)"},{"location":"learning/tools/systemd/#resolved","text":"","title":"Resolved"},{"location":"learning/tools/systemd/#disable-systemd-resolved","text":"disable and stop the systemd-resolved service: sudo systemctl disable --now systemd-resolved.service set NetworkManager to use the default DNS resolution. # file /etc/NetworkManager/NetworkManager.conf [main] dns = default delete /etc/resolv.conf : sudo unlink /etc/resolv.conf restart NetworkManager sudo service network-manager restart","title":"Disable systemd-resolved"},{"location":"learning/tools/systemd/#sources","text":"How to disable systemd-resolved in Ubuntu What are the systemctl options to list all failed units? How To Use Journalctl to View and Manipulate Systemd Logs How to Set Time, Timezone and Synchronize System Clock Using timedatectl Command How to Set Hostname Using Hostnamectl Command? Suspend and hibernate","title":"Sources"},{"location":"learning/tools/tar/","text":"Tar \u00b6 TL;DR \u00b6 # create an archive tar czvf directory.tar.gz directory tar capvf archive.tar.bz2 directory1 directory2 file # list the content of an archive tar tf archive.tar tar tf archive.tar member # extract an archive tar xpf archive.tar tar xapf archive.tar.gz tar xjpf archive.tar.bz2 file Interesting switches \u00b6 short long description -a --auto-compress use archive suffix to determine the compression program -c --create create a new archive; directories are archived recursively, unless the --no-recursion option is given -C --directory DIR change to DIR before performing any operations; this option affects all options that follow -f --file FILE use archive file or device FILE; if not given, tar will first examine the environment variable TAPE and default to the compiled-in default -r --append append files to the end of an archive -t --list list the contents of an archive; arguments are optional, but when given they specify the names of the members to list Further readings \u00b6 how to compress and extract files using the tar command on linux how to create tar gz file in linux using command line","title":"Tar"},{"location":"learning/tools/tar/#tar","text":"","title":"Tar"},{"location":"learning/tools/tar/#tldr","text":"# create an archive tar czvf directory.tar.gz directory tar capvf archive.tar.bz2 directory1 directory2 file # list the content of an archive tar tf archive.tar tar tf archive.tar member # extract an archive tar xpf archive.tar tar xapf archive.tar.gz tar xjpf archive.tar.bz2 file","title":"TL;DR"},{"location":"learning/tools/tar/#interesting-switches","text":"short long description -a --auto-compress use archive suffix to determine the compression program -c --create create a new archive; directories are archived recursively, unless the --no-recursion option is given -C --directory DIR change to DIR before performing any operations; this option affects all options that follow -f --file FILE use archive file or device FILE; if not given, tar will first examine the environment variable TAPE and default to the compiled-in default -r --append append files to the end of an archive -t --list list the contents of an archive; arguments are optional, but when given they specify the names of the members to list","title":"Interesting switches"},{"location":"learning/tools/tar/#further-readings","text":"how to compress and extract files using the tar command on linux how to create tar gz file in linux using command line","title":"Further readings"},{"location":"learning/tools/terraform/","text":"Terraform \u00b6 TL;DR Modules Useful internal variables Versioning Troubleshooting count vs for_each Conditional creation of a resource Force the recreation of specific resources Error: at least 1 \"features\" blocks are required Add/subtract time Export the contents of a tfvars file as shell variables Further readings Sources TL;DR \u00b6 # Initialization. terraform init terraform init -reconfigure # Validate files. terraform validate # Show what would be done. terraform plan terraform plan -state 'path/to/file.tfstate' -var-file 'path/to/var.tfvars' terraform plan -out 'path/to/file.tfstate' -parallelism '50' # Make the changes. terraform apply terraform apply -auto-approve -backup -parallelism '25' 'path/to/plan.tfstate' # Destroy everything. # `destroy` is an alias of `apply -destroy` and is being deprecated. terraform destroy terraform apply -destroy # Unlock a state file. terraform force-unlock 'lock_id' # Format files. terraform fmt terraform fmt -check -diff -recursive # Create a dependency graph. # Requires `dot` from 'graphviz' for image generation. terraform graph terraform graph | dot -Tsvg > 'graph.svg' # Show an existing resource. terraform state show 'packet_device.worker' terraform state show 'packet_device.worker[\"example\"]' terraform state show 'module.foo.packet_device.worker' # Recursively update all modules. # `get` is being deprecated in favour of `init` terraform get -update -no-color Modules \u00b6 Include a module in the configuration with the module keyword: module \"remote_vpc_module\" { # module settings source = \"terraform-aws-modules/vpc/aws\" # required version = \"2.21.0\" # module variables \u2026 } module \"local_vpc_module\" { # module settings source = \"./modules/aws_vpc\" # required # module variables \u2026 } Run terraform init or terraform get to install the modules. Modules are installed in the .terraform/modules directory inside the configuration's working directory; local modules are symlinked from there. When terraform processes a module block, that block will inherit the provider from the enclosing configuration. A module's output can be accessed from the configuration that calls the module through the syntax module.$moduleName.$outputName . Module outputs are read-only attributes. Useful internal variables \u00b6 Name Description path.root filesystem path of the root module of the configuration path.module filesystem path of the module where the expression is placed path.cwd filesystem path of the current working directory terraform.workspace name of the currently selected workspace Versioning \u00b6 Use a string literal containing one or more conditions separated by commas: version = \">= 1.2.0, < 2.0.0\" version = \"~> 1.3, < 1.9.5\" Each condition must consist of an operator and a version number. The available operators are as follow: Operator Description = or not present Specify the exact version number. It cannot be combined with other conditions. != Exclude the exact version number. > , >= , < , <= Compare the available versions against the one specified and allow those for which the comparison is true. ~> Allow only the rightmost version component to be incremented. Troubleshooting \u00b6 count vs for_each \u00b6 count creates an unordered list of objects, while for_each creates a map. count is sensitive to any changes in the list order and this means that if for some reason order of the list is changed terraform will force the replacement of all resources for which the index in the list has changed: variable \"my_list\" { - default = [\"first\", \"second\", \"third\"] + default = [\"zeroth\", \"first\", \"second\", \"third\"] } Terraform will perform the following actions: # null_resource.default[0] must be replaced -/+ resource \"null_resource\" \"default\" { ~ id = \"4074861383382414527\" -> (known after apply) ~ triggers = { # forces replacement \"list_index\" = \"0\" ~ \"list_value\" = \"first\" -> \"zeroth\" } } \u2026 # null_resource.default[3] will be created + resource \"null_resource\" \"default\" { + id = (known after apply) + triggers = { + \"list_index\" = \"3\" + \"list_value\" = \"third\" } } Conditional creation of a resource \u00b6 You can conditionally create one or more resources. There are 2 ways to do this: with count : resource \"cloudflare_record\" \"record\" { count = var.cloudflare_enabled ? 1 : 0 \u2026 } with for_each : resource \"cloudflare_record\" \"record\" { for_each = length(var.cloudflare_records_map) > 0 ? var.cloudflare_records_map : {} \u2026 } Mind the type of object in the line, and the gotchas for each method. Force the recreation of specific resources \u00b6 Use the -replace=resource_path option during a plan or apply : terraform apply -replace = aws_instance.example # aws_instance.example will be replaced, as requested -/+ resource \"aws_instance\" \"example\" { \u2026 } Error: at least 1 \"features\" blocks are required \u00b6 The azurerm provider needs to be configured with at least the following lines: provider \"azurerm\" { features {} } Add/subtract time \u00b6 Instead of using the timeadd() function, it is advisable to use the time_offset resource: resource \"time_offset\" \"one_year_from_now\" { offset_years = 1 } resource \"azurerm_key_vault_key\" \"key\" { expiration_date = time_offset.one_year_from_now.rfc3339 \u2026 } Export the contents of a tfvars file as shell variables \u00b6 # As normal shell variables. eval \"export $( sed -E 's/[[:blank:]]*//g' file.tfvars ) \" # As TF shell variables (TF_VAR_*). eval \"export $( sed -E 's/([[:graph:]]+)[[:blank:]]*=[[:blank:]]*([[:graph:]]+)/TF_VAR_\\1=\\2/' file.tfvars ) \" Further readings \u00b6 CLI Documentation Providers best practices Version constraints References to Named Values Environment Variables Forcing Re-creation of Resources Sources \u00b6 for_each vs count Azure Provider Conditional creation of a resource based on a variable in .tfvars","title":"Terraform"},{"location":"learning/tools/terraform/#terraform","text":"TL;DR Modules Useful internal variables Versioning Troubleshooting count vs for_each Conditional creation of a resource Force the recreation of specific resources Error: at least 1 \"features\" blocks are required Add/subtract time Export the contents of a tfvars file as shell variables Further readings Sources","title":"Terraform"},{"location":"learning/tools/terraform/#tldr","text":"# Initialization. terraform init terraform init -reconfigure # Validate files. terraform validate # Show what would be done. terraform plan terraform plan -state 'path/to/file.tfstate' -var-file 'path/to/var.tfvars' terraform plan -out 'path/to/file.tfstate' -parallelism '50' # Make the changes. terraform apply terraform apply -auto-approve -backup -parallelism '25' 'path/to/plan.tfstate' # Destroy everything. # `destroy` is an alias of `apply -destroy` and is being deprecated. terraform destroy terraform apply -destroy # Unlock a state file. terraform force-unlock 'lock_id' # Format files. terraform fmt terraform fmt -check -diff -recursive # Create a dependency graph. # Requires `dot` from 'graphviz' for image generation. terraform graph terraform graph | dot -Tsvg > 'graph.svg' # Show an existing resource. terraform state show 'packet_device.worker' terraform state show 'packet_device.worker[\"example\"]' terraform state show 'module.foo.packet_device.worker' # Recursively update all modules. # `get` is being deprecated in favour of `init` terraform get -update -no-color","title":"TL;DR"},{"location":"learning/tools/terraform/#modules","text":"Include a module in the configuration with the module keyword: module \"remote_vpc_module\" { # module settings source = \"terraform-aws-modules/vpc/aws\" # required version = \"2.21.0\" # module variables \u2026 } module \"local_vpc_module\" { # module settings source = \"./modules/aws_vpc\" # required # module variables \u2026 } Run terraform init or terraform get to install the modules. Modules are installed in the .terraform/modules directory inside the configuration's working directory; local modules are symlinked from there. When terraform processes a module block, that block will inherit the provider from the enclosing configuration. A module's output can be accessed from the configuration that calls the module through the syntax module.$moduleName.$outputName . Module outputs are read-only attributes.","title":"Modules"},{"location":"learning/tools/terraform/#useful-internal-variables","text":"Name Description path.root filesystem path of the root module of the configuration path.module filesystem path of the module where the expression is placed path.cwd filesystem path of the current working directory terraform.workspace name of the currently selected workspace","title":"Useful internal variables"},{"location":"learning/tools/terraform/#versioning","text":"Use a string literal containing one or more conditions separated by commas: version = \">= 1.2.0, < 2.0.0\" version = \"~> 1.3, < 1.9.5\" Each condition must consist of an operator and a version number. The available operators are as follow: Operator Description = or not present Specify the exact version number. It cannot be combined with other conditions. != Exclude the exact version number. > , >= , < , <= Compare the available versions against the one specified and allow those for which the comparison is true. ~> Allow only the rightmost version component to be incremented.","title":"Versioning"},{"location":"learning/tools/terraform/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"learning/tools/terraform/#count-vs-for_each","text":"count creates an unordered list of objects, while for_each creates a map. count is sensitive to any changes in the list order and this means that if for some reason order of the list is changed terraform will force the replacement of all resources for which the index in the list has changed: variable \"my_list\" { - default = [\"first\", \"second\", \"third\"] + default = [\"zeroth\", \"first\", \"second\", \"third\"] } Terraform will perform the following actions: # null_resource.default[0] must be replaced -/+ resource \"null_resource\" \"default\" { ~ id = \"4074861383382414527\" -> (known after apply) ~ triggers = { # forces replacement \"list_index\" = \"0\" ~ \"list_value\" = \"first\" -> \"zeroth\" } } \u2026 # null_resource.default[3] will be created + resource \"null_resource\" \"default\" { + id = (known after apply) + triggers = { + \"list_index\" = \"3\" + \"list_value\" = \"third\" } }","title":"count vs for_each"},{"location":"learning/tools/terraform/#conditional-creation-of-a-resource","text":"You can conditionally create one or more resources. There are 2 ways to do this: with count : resource \"cloudflare_record\" \"record\" { count = var.cloudflare_enabled ? 1 : 0 \u2026 } with for_each : resource \"cloudflare_record\" \"record\" { for_each = length(var.cloudflare_records_map) > 0 ? var.cloudflare_records_map : {} \u2026 } Mind the type of object in the line, and the gotchas for each method.","title":"Conditional creation of a resource"},{"location":"learning/tools/terraform/#force-the-recreation-of-specific-resources","text":"Use the -replace=resource_path option during a plan or apply : terraform apply -replace = aws_instance.example # aws_instance.example will be replaced, as requested -/+ resource \"aws_instance\" \"example\" { \u2026 }","title":"Force the recreation of specific resources"},{"location":"learning/tools/terraform/#error-at-least-1-features-blocks-are-required","text":"The azurerm provider needs to be configured with at least the following lines: provider \"azurerm\" { features {} }","title":"Error: at least 1 \"features\" blocks are required"},{"location":"learning/tools/terraform/#addsubtract-time","text":"Instead of using the timeadd() function, it is advisable to use the time_offset resource: resource \"time_offset\" \"one_year_from_now\" { offset_years = 1 } resource \"azurerm_key_vault_key\" \"key\" { expiration_date = time_offset.one_year_from_now.rfc3339 \u2026 }","title":"Add/subtract time"},{"location":"learning/tools/terraform/#export-the-contents-of-a-tfvars-file-as-shell-variables","text":"# As normal shell variables. eval \"export $( sed -E 's/[[:blank:]]*//g' file.tfvars ) \" # As TF shell variables (TF_VAR_*). eval \"export $( sed -E 's/([[:graph:]]+)[[:blank:]]*=[[:blank:]]*([[:graph:]]+)/TF_VAR_\\1=\\2/' file.tfvars ) \"","title":"Export the contents of a tfvars file as shell variables"},{"location":"learning/tools/terraform/#further-readings","text":"CLI Documentation Providers best practices Version constraints References to Named Values Environment Variables Forcing Re-creation of Resources","title":"Further readings"},{"location":"learning/tools/terraform/#sources","text":"for_each vs count Azure Provider Conditional creation of a resource based on a variable in .tfvars","title":"Sources"},{"location":"learning/tools/timemachine/","text":"TimeMachine \u00b6 TL;DR \u00b6 # follow logs log stream --style syslog --predicate 'senderImagePath contains[cd] \"TimeMachine\"' --info --debug # add or set a destination sudo tmutil setdestination Follow logs \u00b6 use stream to keep watching \"tail style\" use --predicate to filter out relevant logs add --style syslog to watch them in a syslog style","title":"TimeMachine"},{"location":"learning/tools/timemachine/#timemachine","text":"","title":"TimeMachine"},{"location":"learning/tools/timemachine/#tldr","text":"# follow logs log stream --style syslog --predicate 'senderImagePath contains[cd] \"TimeMachine\"' --info --debug # add or set a destination sudo tmutil setdestination","title":"TL;DR"},{"location":"learning/tools/timemachine/#follow-logs","text":"use stream to keep watching \"tail style\" use --predicate to filter out relevant logs add --style syslog to watch them in a syslog style","title":"Follow logs"},{"location":"learning/tools/tldr/","text":"TL;DR \u00b6 pip3 install tldr # official python client brew install tealdeer # rust client sudo port install tldr-cpp-client # c++ client Further readings \u00b6 website live demo project's github page","title":"TL;DR"},{"location":"learning/tools/tldr/#tldr","text":"pip3 install tldr # official python client brew install tealdeer # rust client sudo port install tldr-cpp-client # c++ client","title":"TL;DR"},{"location":"learning/tools/tldr/#further-readings","text":"website live demo project's github page","title":"Further readings"},{"location":"learning/tools/toolbox/","text":"Toolbox \u00b6 Runs on top of Podman . TL;DR \u00b6 # List locally available images and containers. toolbox list toolbox list -c # Download an OCI image and create a container from it. toolbox create toolbox create -d rhel -r 8 .1 toolbox create -i registry.fedoraproject.org/fedora-toolbox:35 fedora # Run a command inside the container without entering it. toolbox run ls -la toolbox run -d rhel -r 8 .1 uptime toolbox-run -c fedora cat /etc/os-release # Get a shell inside the container. toolbox enter toolbox enter -d fedora -r f35 toolbox enter rhel # Remove a container. toolbox rm fedora-toolbox-35 # Remove an image. toolbox rmi fedora-toolbox:35 toolbox rmi -af Further readings \u00b6 GitHub page Podman Sources \u00b6 Fedora Silverblue","title":"Toolbox"},{"location":"learning/tools/toolbox/#toolbox","text":"Runs on top of Podman .","title":"Toolbox"},{"location":"learning/tools/toolbox/#tldr","text":"# List locally available images and containers. toolbox list toolbox list -c # Download an OCI image and create a container from it. toolbox create toolbox create -d rhel -r 8 .1 toolbox create -i registry.fedoraproject.org/fedora-toolbox:35 fedora # Run a command inside the container without entering it. toolbox run ls -la toolbox run -d rhel -r 8 .1 uptime toolbox-run -c fedora cat /etc/os-release # Get a shell inside the container. toolbox enter toolbox enter -d fedora -r f35 toolbox enter rhel # Remove a container. toolbox rm fedora-toolbox-35 # Remove an image. toolbox rmi fedora-toolbox:35 toolbox rmi -af","title":"TL;DR"},{"location":"learning/tools/toolbox/#further-readings","text":"GitHub page Podman","title":"Further readings"},{"location":"learning/tools/toolbox/#sources","text":"Fedora Silverblue","title":"Sources"},{"location":"learning/tools/trap/","text":"Trap \u00b6 TL;DR \u00b6 # Run a command or function on exit, kill or error. trap \"rm -f $tempfile \" EXIT SIGTERM ERR trap function -name EXIT SIGTERM ERR # Disable CTRL-C trap \"\" SIGINT # Re-enable CTRL-C trap - SIGINT Sources \u00b6 Using Bash traps in your scripts The Bash trap command","title":"Trap"},{"location":"learning/tools/trap/#trap","text":"","title":"Trap"},{"location":"learning/tools/trap/#tldr","text":"# Run a command or function on exit, kill or error. trap \"rm -f $tempfile \" EXIT SIGTERM ERR trap function -name EXIT SIGTERM ERR # Disable CTRL-C trap \"\" SIGINT # Re-enable CTRL-C trap - SIGINT","title":"TL;DR"},{"location":"learning/tools/trap/#sources","text":"Using Bash traps in your scripts The Bash trap command","title":"Sources"},{"location":"learning/tools/truncate/","text":"truncate \u00b6 Shrink or extend the size of a file to the specified size. TL;DR \u00b6 # Empty the contents of files. truncate -s 0 'file' # Set the size of an existing file. # if the file does not exist, create it anew of the specified size. truncate -s 100 'file' truncate --size 5k 'file' truncate --size 10G 'file' # Extend a file's size by 50 MiB and fill it with holes. # Holes read as zero bytes. truncate --size +50M 'file' # Shrink a file by 2 GiB. # Removes data from the end of file. truncate --size -2G 'file' # Empty the file's content, but do not create it if existing. truncate --no-create --size 0 'file' Further readings \u00b6 GNU's documentation Sources \u00b6 cheat.sh","title":"truncate"},{"location":"learning/tools/truncate/#truncate","text":"Shrink or extend the size of a file to the specified size.","title":"truncate"},{"location":"learning/tools/truncate/#tldr","text":"# Empty the contents of files. truncate -s 0 'file' # Set the size of an existing file. # if the file does not exist, create it anew of the specified size. truncate -s 100 'file' truncate --size 5k 'file' truncate --size 10G 'file' # Extend a file's size by 50 MiB and fill it with holes. # Holes read as zero bytes. truncate --size +50M 'file' # Shrink a file by 2 GiB. # Removes data from the end of file. truncate --size -2G 'file' # Empty the file's content, but do not create it if existing. truncate --no-create --size 0 'file'","title":"TL;DR"},{"location":"learning/tools/truncate/#further-readings","text":"GNU's documentation","title":"Further readings"},{"location":"learning/tools/truncate/#sources","text":"cheat.sh","title":"Sources"},{"location":"learning/tools/turris/","text":"Turris OS \u00b6 TL;DR \u00b6 # Get LEDs intensity. rainbow get intensity # Set LEDs intensity. rainbow intensity 50 rainbow intensity 100 rainbow intensity 0 # Gracefully shutdown the device. poweroff LED diodes settings \u00b6 A permanent change of color can be set in the UCI configuration file /etc/config/rainbow . The rainbow utility allows one to change the color and set the status of each diode individually. The setting are disable (off), enable (on) or auto ; auto leaves the control of the diodes to the hardware, like blinking during data transfer and so on. Automatic overnight dimming \u00b6 Should you want to see the state of individual devices during day but not to be dazzled by the diodes in the night, you can automatically adjust the intensity of LEDs using a cronjob. Create a text file in the /etc/cron.d directory: # File /etc/cron.d/rainbow_night. # Set the light intensity to the second lowest degree every day at 11 PM and set # it back to maximum every day at 7 AM. MAILTO=\"\" # avoid automatic logging of the output 0 23 * * * root rainbow intensity 5 0 7 * * * root rainbow intensity 100 Containerized pi-hole \u00b6 Requires the lxc package to be installed. See Installing pi-hole on Turris Omnia , Install Pi-hole and Pi-Hole on Turris Omnia for details. In Turris OS: # Create the LXC container. lxc-create --name pi-hole --template debian lxc-create --name pi-hole --template download --dist Ubuntu --release Focal --arch armv7l --server repo.turris.cz/lxc # Start it. lxc-start --name pi-hole # Check it's running correctly. lxc-info --name pi-hole # Get a shell to it. lxc-attach --name pi-hole In the container: # Set the correct hostname. hostnamectl set-hostname pi-hole # Install pi-hole. DEBIAN_FRONTEND = noninteractive apt-get install --assume-yes ca-certificates curl curl -sSL https://install.pi-hole.net | bash # Follow the guided procedure. Again in Turris OS: # Configure pi-hole's static IP lease. uci add dhcp host uci set dhcp.@host [ -1 ]= host uci set dhcp.@host [ -1 ] .name = pi-hole uci set dhcp.@host [ -1 ] .mac = ` grep hwaddr /srv/lxc/pi-hole/config | sed 's/.*= //' ` uci set dhcp.@host [ -1 ] .ip = 192 .168.111.2 # Distribute pi-hole as primary DNS. uci set dhcp.lan.dhcp_option = '6,192.168.111.2' uci add_list dhcp.lan.dns = ` lxc-info --name pi-hole | grep \"IP.* f[cd]\" | sed \"s/IP: *//\" ` # Apply the new configuration. /etc/init.d/odhcpd restart /etc/init.d/dnsmasq restart Factory reset \u00b6 Keep pressed the reset button on the back panel and wait for LEDs to indicate the number of the desired mode, then release the reset button. The LEDs are used as a counter, with the number of lid LEDs (regardless of the color) indicating the reset mode the router will reboot into. The LEDs will transition from green to red, and when the last LED turns red the next LED will light up and the counter is incremented. When the counter reaches 12 (the total number of LEDs), it will start again from 1. When the reset button is released, the LED counter will blink three times to confirm the selected reset mode. If the selected mode is different from the required one, just press the reset button again and start the mode selection process again. Available reset modes are: 1 LED: standard (re)boot 2 LEDs: rollback to latest snapshot 3 LEDs: rollback to factory reset 4 LEDs: re-flash the router from a flash drive 5 LEDs: enable an insecure SSH on 192.168.1.1 (Omnia 2019 and newer) 6 LEDs: re-flash from the Internet (Omnia 2019 and newer) 7 LEDs: start a rescue shell Tip: release the reset button immediately after the required number of LEDs starts shining (regardless of the color). Do not unnecessarily prolong holding the reset button when the last LED is lit. By doing this you decrease a chance of accidentally transitioning to the next mode at the same moment when the button is released. After the selected mode indication is performed, all LEDs will turn blue for a moment and then a light wave indicates the start the first stage boot during which LEDs turn green. Warning: when LEDs turn red, it means that some highly sensitive operation is in process and data may be corrupted if it is interrupted. Try not to reset router during the process or you might end up with corrupted filesystem. That one can be fixed using mode 4 but with complete data loss. Further readings \u00b6 Led settings on the wiki opkg Sources \u00b6 Turris official documentation Turris wiki Install Pi-hole Pi-Hole on Turris Omnia Installing pi-hole on Turris Omnia Factory reset on Turris Omnia","title":"Turris OS"},{"location":"learning/tools/turris/#turris-os","text":"","title":"Turris OS"},{"location":"learning/tools/turris/#tldr","text":"# Get LEDs intensity. rainbow get intensity # Set LEDs intensity. rainbow intensity 50 rainbow intensity 100 rainbow intensity 0 # Gracefully shutdown the device. poweroff","title":"TL;DR"},{"location":"learning/tools/turris/#led-diodes-settings","text":"A permanent change of color can be set in the UCI configuration file /etc/config/rainbow . The rainbow utility allows one to change the color and set the status of each diode individually. The setting are disable (off), enable (on) or auto ; auto leaves the control of the diodes to the hardware, like blinking during data transfer and so on.","title":"LED diodes settings"},{"location":"learning/tools/turris/#automatic-overnight-dimming","text":"Should you want to see the state of individual devices during day but not to be dazzled by the diodes in the night, you can automatically adjust the intensity of LEDs using a cronjob. Create a text file in the /etc/cron.d directory: # File /etc/cron.d/rainbow_night. # Set the light intensity to the second lowest degree every day at 11 PM and set # it back to maximum every day at 7 AM. MAILTO=\"\" # avoid automatic logging of the output 0 23 * * * root rainbow intensity 5 0 7 * * * root rainbow intensity 100","title":"Automatic overnight dimming"},{"location":"learning/tools/turris/#containerized-pi-hole","text":"Requires the lxc package to be installed. See Installing pi-hole on Turris Omnia , Install Pi-hole and Pi-Hole on Turris Omnia for details. In Turris OS: # Create the LXC container. lxc-create --name pi-hole --template debian lxc-create --name pi-hole --template download --dist Ubuntu --release Focal --arch armv7l --server repo.turris.cz/lxc # Start it. lxc-start --name pi-hole # Check it's running correctly. lxc-info --name pi-hole # Get a shell to it. lxc-attach --name pi-hole In the container: # Set the correct hostname. hostnamectl set-hostname pi-hole # Install pi-hole. DEBIAN_FRONTEND = noninteractive apt-get install --assume-yes ca-certificates curl curl -sSL https://install.pi-hole.net | bash # Follow the guided procedure. Again in Turris OS: # Configure pi-hole's static IP lease. uci add dhcp host uci set dhcp.@host [ -1 ]= host uci set dhcp.@host [ -1 ] .name = pi-hole uci set dhcp.@host [ -1 ] .mac = ` grep hwaddr /srv/lxc/pi-hole/config | sed 's/.*= //' ` uci set dhcp.@host [ -1 ] .ip = 192 .168.111.2 # Distribute pi-hole as primary DNS. uci set dhcp.lan.dhcp_option = '6,192.168.111.2' uci add_list dhcp.lan.dns = ` lxc-info --name pi-hole | grep \"IP.* f[cd]\" | sed \"s/IP: *//\" ` # Apply the new configuration. /etc/init.d/odhcpd restart /etc/init.d/dnsmasq restart","title":"Containerized pi-hole"},{"location":"learning/tools/turris/#factory-reset","text":"Keep pressed the reset button on the back panel and wait for LEDs to indicate the number of the desired mode, then release the reset button. The LEDs are used as a counter, with the number of lid LEDs (regardless of the color) indicating the reset mode the router will reboot into. The LEDs will transition from green to red, and when the last LED turns red the next LED will light up and the counter is incremented. When the counter reaches 12 (the total number of LEDs), it will start again from 1. When the reset button is released, the LED counter will blink three times to confirm the selected reset mode. If the selected mode is different from the required one, just press the reset button again and start the mode selection process again. Available reset modes are: 1 LED: standard (re)boot 2 LEDs: rollback to latest snapshot 3 LEDs: rollback to factory reset 4 LEDs: re-flash the router from a flash drive 5 LEDs: enable an insecure SSH on 192.168.1.1 (Omnia 2019 and newer) 6 LEDs: re-flash from the Internet (Omnia 2019 and newer) 7 LEDs: start a rescue shell Tip: release the reset button immediately after the required number of LEDs starts shining (regardless of the color). Do not unnecessarily prolong holding the reset button when the last LED is lit. By doing this you decrease a chance of accidentally transitioning to the next mode at the same moment when the button is released. After the selected mode indication is performed, all LEDs will turn blue for a moment and then a light wave indicates the start the first stage boot during which LEDs turn green. Warning: when LEDs turn red, it means that some highly sensitive operation is in process and data may be corrupted if it is interrupted. Try not to reset router during the process or you might end up with corrupted filesystem. That one can be fixed using mode 4 but with complete data loss.","title":"Factory reset"},{"location":"learning/tools/turris/#further-readings","text":"Led settings on the wiki opkg","title":"Further readings"},{"location":"learning/tools/turris/#sources","text":"Turris official documentation Turris wiki Install Pi-hole Pi-Hole on Turris Omnia Installing pi-hole on Turris Omnia Factory reset on Turris Omnia","title":"Sources"},{"location":"learning/tools/uniq/","text":"Uniq \u00b6 TL;DR \u00b6 # Print all but duplicates lines. uniq path/to/file # Only print unique lines. uniq -u path/to/file # Only print repeating lines. uniq -d path/to/file # Count unique lines only. uniq -cu path/to/file # Count duplicated lines only. uniq -cd path/to/file","title":"Uniq"},{"location":"learning/tools/uniq/#uniq","text":"","title":"Uniq"},{"location":"learning/tools/uniq/#tldr","text":"# Print all but duplicates lines. uniq path/to/file # Only print unique lines. uniq -u path/to/file # Only print repeating lines. uniq -d path/to/file # Count unique lines only. uniq -cu path/to/file # Count duplicated lines only. uniq -cd path/to/file","title":"TL;DR"},{"location":"learning/tools/useradd/","text":"Useradd \u00b6 Create a new user. TL;DR \u00b6 # Create a new user. sudo useradd username sudo useradd -p encryptedPassword username # Create a new user with the specified user ID. sudo useradd --uid id username # Create a new user with the specified expiration date. sudo useradd -e 2022 -10-10 username # Create a new user with the specified shell. sudo useradd --shell path/to/shell username # Create a new user belonging to additional groups. sudo useradd --groups group1,group2,... username # Create a new user with or without its default home directory. sudo useradd --create-home username sudo useradd --no-create-home username # Create a new user with the home directory filled by template directory files. sudo useradd --skel path/to/template_directory --create-home username # Create a new system user without the home directory. sudo useradd --system username Sources \u00b6 cheat.sh","title":"Useradd"},{"location":"learning/tools/useradd/#useradd","text":"Create a new user.","title":"Useradd"},{"location":"learning/tools/useradd/#tldr","text":"# Create a new user. sudo useradd username sudo useradd -p encryptedPassword username # Create a new user with the specified user ID. sudo useradd --uid id username # Create a new user with the specified expiration date. sudo useradd -e 2022 -10-10 username # Create a new user with the specified shell. sudo useradd --shell path/to/shell username # Create a new user belonging to additional groups. sudo useradd --groups group1,group2,... username # Create a new user with or without its default home directory. sudo useradd --create-home username sudo useradd --no-create-home username # Create a new user with the home directory filled by template directory files. sudo useradd --skel path/to/template_directory --create-home username # Create a new system user without the home directory. sudo useradd --system username","title":"TL;DR"},{"location":"learning/tools/useradd/#sources","text":"cheat.sh","title":"Sources"},{"location":"learning/tools/userdel/","text":"Useradd \u00b6 Delete a user account and its related files. TL;DR \u00b6 # Remove a user. sudo userdel username # Remove a user in other root directory. sudo userdel --root path/to/other/root username # Remove a user along with the home directory and mail spool. sudo userdel --remove username Sources \u00b6 cheat.sh","title":"Useradd"},{"location":"learning/tools/userdel/#useradd","text":"Delete a user account and its related files.","title":"Useradd"},{"location":"learning/tools/userdel/#tldr","text":"# Remove a user. sudo userdel username # Remove a user in other root directory. sudo userdel --root path/to/other/root username # Remove a user along with the home directory and mail spool. sudo userdel --remove username","title":"TL;DR"},{"location":"learning/tools/userdel/#sources","text":"cheat.sh","title":"Sources"},{"location":"learning/tools/usermod/","text":"Usermod \u00b6 TL;DR \u00b6 # Change a user's primary group. sudo usermod -g docker bob # Add/remove a user to/from supplementary groups. sudo usermod -aG wheel carly sudo usermod --append --groups kvm,video,audio alice sudo usermod -rG sudo,admin eve # Change a user's login name sudo usermod --login to-stephen from-micha # Change a user's ID. sudo usermod --uid 1001 hugo # Change a user's shell. sudo usermod --shell /usr/bin/zsh rick # Change a user's password. sudo usermod -p encryptedPassword john # Lock/unlock a user. sudo usermod -L damian sudo usermod -U luke # Change a user's home directory. sudo usermod --move-home --home path/to/new_home lonny Sources \u00b6 cheat.sh","title":"Usermod"},{"location":"learning/tools/usermod/#usermod","text":"","title":"Usermod"},{"location":"learning/tools/usermod/#tldr","text":"# Change a user's primary group. sudo usermod -g docker bob # Add/remove a user to/from supplementary groups. sudo usermod -aG wheel carly sudo usermod --append --groups kvm,video,audio alice sudo usermod -rG sudo,admin eve # Change a user's login name sudo usermod --login to-stephen from-micha # Change a user's ID. sudo usermod --uid 1001 hugo # Change a user's shell. sudo usermod --shell /usr/bin/zsh rick # Change a user's password. sudo usermod -p encryptedPassword john # Lock/unlock a user. sudo usermod -L damian sudo usermod -U luke # Change a user's home directory. sudo usermod --move-home --home path/to/new_home lonny","title":"TL;DR"},{"location":"learning/tools/usermod/#sources","text":"cheat.sh","title":"Sources"},{"location":"learning/tools/vagrant/","text":"Vagrant \u00b6 TL;DR \u00b6 # start a vm from a box vagrant up vagrant up --provider libvirt # connect to a started vm vagrant ssh # print the ssh config snippet to connect to the vm vagrant ssh-config # (re)provision a vm vagrant provision vagrant up --provision # add a box vagrant add archlinux/archlinux vagrant add debian/testing64 --provider virtualbox # list downloaded boxes vagrant box list # list outdated boxes vagrant box outdated # update a box vagrant box update vagrant box update --box generic/gentoo # remove a box vagrant box remove archlinux/archlinux # destroy a machine vagrant destroy vagrant destroy --force # install autocomplete vagrant autocomplete install --bash vagrant autocomplete install --zsh # install a plugin vagrant plugin install vagrant-disksize Usage \u00b6 All commands need to be run from the vm's folder. Install Vagrant. Optionally, create a folder to keep all files in order and move into it: mkdir test-vm cd $_ Create a configuration: vagrant init archlinux/archlinux Start the vm: vagrant up # re-provision the vm after startup vagrant up --provision Connect to the vm: vagrant ssh Boxes management \u00b6 vagrant box add archlinux/archlinux vagrant box add archlinux/archlinux --provider virtualbox vagrant box list vagrant box update vagrant box update --box generic/gentoo Install shell's autocomplete \u00b6 $ vagrant autocomplete install --bash Autocomplete installed at paths: - /home/user/.bashrc $ vagrant autocomplete install --zsh Autocomplete installed at paths: - /home/user/.zshrc Customize a box \u00b6 Vagrant . configure ( \"2\" ) do | config | config . vm . box = \"archlinux/archlinux\" config . vm . provider \"virtualbox\" do | vb | # Vagrant can call any VBoxManage command prior to booting the machine. # Multiple customize directives will be executed in order. vb . customize [ \"modifyvm\" , :id , \"--vram\" , \"64\" ] vb . customize [ \"modifyvm\" , :id , \"--graphicscontroller\" , \"vmsvga\" ] vb . customize [ \"modifyvm\" , :id , \"--cpuexecutioncap\" , \"50\" ] # Some settings have convenience shortcuts. vb . name = \"xfce4 latest\" vb . cpus = 2 vb . memory = \"2048\" vb . default_nic_type = \"82543GC\" vb . gui = true # Skip the guest additions check. vb . check_guest_additions = false end Use environment variables in the provisioning script \u00b6 Add the variables as argument of the config.vm.provision key: Vagrant . configure ( \"2\" ) do | config | config . vm . provision :shell do | shell | shell . env = { \"STATIC\" => \"set-in-config\" , \"FORWARDED\" => ENV [ 'HOST_VAR' ] , } shell . inline = <<- SHELL printenv STATIC FORWARDED sudo -u vagrant --preserve-env=STATIC,FORWARDED printenv STATIC FORWARDED SHELL end end Specify the disk size \u00b6 Install the vagrant-disksize plugin: vagrant plugin install vagrant-disksize then set it up: vagrant . configure ( '2' ) do | config | config . disksize . size = '50GB' end Reboot after provisioning \u00b6 Add one of the following to the box's Vagrantfile: config . vm . provision \"shell\" , reboot : true config . vm . provision :shell do | shell | shell . privileged = true shell . reboot = true end Further readings \u00b6 getting started how to set vagrant virtualbox video memory Pass environment variables to vagrant shell provisioner Tips & Tricks Multi-Machine how to specify the disk size How do I reboot a Vagrant guest from a provisioner?","title":"Vagrant"},{"location":"learning/tools/vagrant/#vagrant","text":"","title":"Vagrant"},{"location":"learning/tools/vagrant/#tldr","text":"# start a vm from a box vagrant up vagrant up --provider libvirt # connect to a started vm vagrant ssh # print the ssh config snippet to connect to the vm vagrant ssh-config # (re)provision a vm vagrant provision vagrant up --provision # add a box vagrant add archlinux/archlinux vagrant add debian/testing64 --provider virtualbox # list downloaded boxes vagrant box list # list outdated boxes vagrant box outdated # update a box vagrant box update vagrant box update --box generic/gentoo # remove a box vagrant box remove archlinux/archlinux # destroy a machine vagrant destroy vagrant destroy --force # install autocomplete vagrant autocomplete install --bash vagrant autocomplete install --zsh # install a plugin vagrant plugin install vagrant-disksize","title":"TL;DR"},{"location":"learning/tools/vagrant/#usage","text":"All commands need to be run from the vm's folder. Install Vagrant. Optionally, create a folder to keep all files in order and move into it: mkdir test-vm cd $_ Create a configuration: vagrant init archlinux/archlinux Start the vm: vagrant up # re-provision the vm after startup vagrant up --provision Connect to the vm: vagrant ssh","title":"Usage"},{"location":"learning/tools/vagrant/#boxes-management","text":"vagrant box add archlinux/archlinux vagrant box add archlinux/archlinux --provider virtualbox vagrant box list vagrant box update vagrant box update --box generic/gentoo","title":"Boxes management"},{"location":"learning/tools/vagrant/#install-shells-autocomplete","text":"$ vagrant autocomplete install --bash Autocomplete installed at paths: - /home/user/.bashrc $ vagrant autocomplete install --zsh Autocomplete installed at paths: - /home/user/.zshrc","title":"Install shell's autocomplete"},{"location":"learning/tools/vagrant/#customize-a-box","text":"Vagrant . configure ( \"2\" ) do | config | config . vm . box = \"archlinux/archlinux\" config . vm . provider \"virtualbox\" do | vb | # Vagrant can call any VBoxManage command prior to booting the machine. # Multiple customize directives will be executed in order. vb . customize [ \"modifyvm\" , :id , \"--vram\" , \"64\" ] vb . customize [ \"modifyvm\" , :id , \"--graphicscontroller\" , \"vmsvga\" ] vb . customize [ \"modifyvm\" , :id , \"--cpuexecutioncap\" , \"50\" ] # Some settings have convenience shortcuts. vb . name = \"xfce4 latest\" vb . cpus = 2 vb . memory = \"2048\" vb . default_nic_type = \"82543GC\" vb . gui = true # Skip the guest additions check. vb . check_guest_additions = false end","title":"Customize a box"},{"location":"learning/tools/vagrant/#use-environment-variables-in-the-provisioning-script","text":"Add the variables as argument of the config.vm.provision key: Vagrant . configure ( \"2\" ) do | config | config . vm . provision :shell do | shell | shell . env = { \"STATIC\" => \"set-in-config\" , \"FORWARDED\" => ENV [ 'HOST_VAR' ] , } shell . inline = <<- SHELL printenv STATIC FORWARDED sudo -u vagrant --preserve-env=STATIC,FORWARDED printenv STATIC FORWARDED SHELL end end","title":"Use environment variables in the provisioning script"},{"location":"learning/tools/vagrant/#specify-the-disk-size","text":"Install the vagrant-disksize plugin: vagrant plugin install vagrant-disksize then set it up: vagrant . configure ( '2' ) do | config | config . disksize . size = '50GB' end","title":"Specify the disk size"},{"location":"learning/tools/vagrant/#reboot-after-provisioning","text":"Add one of the following to the box's Vagrantfile: config . vm . provision \"shell\" , reboot : true config . vm . provision :shell do | shell | shell . privileged = true shell . reboot = true end","title":"Reboot after provisioning"},{"location":"learning/tools/vagrant/#further-readings","text":"getting started how to set vagrant virtualbox video memory Pass environment variables to vagrant shell provisioner Tips & Tricks Multi-Machine how to specify the disk size How do I reboot a Vagrant guest from a provisioner?","title":"Further readings"},{"location":"learning/tools/upgrade%20a%20ps4%27s%20hdd/","text":"Upgrade a PS4's HDD \u00b6 Back up the current system data and saves to an external drive; shut down the PS4 completely; upgrade the HDD ; install the system software on the new drive; restore the system data and saves from the external drive. Back up the existing data on an external USB storage device \u00b6 To back up data you need an FAT32 or exFAT-formatted USB drive with at least the storage space of the PS4 backup file. If you don't have enough space to back up everything on your device, you can choose not to back up application data. It's important to sync your Trophies first, as they are not included in the backup data: go to Trophies , press the OPTIONS button, and select Sync Trophies With PSN . Insert the USB drive into your PS4 console; go to Settings > System > Back Up and Restore ; select Back Up ; confirm which data you'd like to back up. It is important to back up saved data to avoid losing any game progress. Customize the backup file name and select Back Up ; this will restart the console and start the backup process; remove the USB drive once the console has been started up normally again. Upgrade the HDD \u00b6 This procedure has been tested on a PS4 Pro. Other models have different procedures. Place the console upside-down on a flat surface and remove the HDD bay cover; remove from the right side first. You may see a sticker covering the HDD bay cover. It's safe to remove this, and it will not affect the warranty. Remove the screw holding the tray in place and and pull the HDD mounting bracket to remove it; Remove the HDD from the mounting bracket and insert its replacement; screw the screws back in, being careful not to over-tighten them; reinsert the HDD mounting bracket and put the crew back in to hold it in place; re-attach the HDD cover. Reinstall the system software \u00b6 This will delete all of the data on your PS4 console. This process is often referred to as a \"factory\" reset, or \"hard\" reset. Using another device: create a folder named PS4 on a USB drive formatted as FAT32; inside that folder, create another folder named UPDATE ; download the full installation file from the system software download page ; the system software installation file must be for a version that is >= to the firmware currently installed on the console. copy the installation file in the UPDATE folder created before; the file must be named PS4UPDATE.PUP . Plug the USB drive containing the file into the PS4; start the console in Safe Mode pressing and hold the power button, and releasing it after the second beep; select Safe Mode's option 7: Initialize PS4 (Reinstall System Software) ; confirm at the prompts. If the PS4 does not recognize the file, check that the folder names and file name are correct. Enter the folder names and file name using uppercase letters. Restore backed up data from an external USB storage device to the PlayStation 4 \u00b6 When restoring data, your PS4 will erase all the data currently saved on your console. This can't be undone, even if you cancel the restore operation. Make sure you don't erase any important data by mistake. Erased data can't be restored. Go to Settings > System > Back Up and Restore ; insert the USB drive that contains the backup into your PS4; select Restore PS4 ; select the backup file you'd like to restore; confirm to restore. Please note, users who have never signed in to PlayStation\u2122Network (PSN) can restore saved data to only the original PS4 console that was backed up. To restore saved data to another PS4 console, you must sign in to PSN before backing up data. Sources \u00b6 PS4: upgrade HDD PS4: External hard drive support How to back up and restore PS4 console data","title":"Upgrade a PS4's HDD"},{"location":"learning/tools/upgrade%20a%20ps4%27s%20hdd/#upgrade-a-ps4s-hdd","text":"Back up the current system data and saves to an external drive; shut down the PS4 completely; upgrade the HDD ; install the system software on the new drive; restore the system data and saves from the external drive.","title":"Upgrade a PS4's HDD"},{"location":"learning/tools/upgrade%20a%20ps4%27s%20hdd/#back-up-the-existing-data-on-an-external-usb-storage-device","text":"To back up data you need an FAT32 or exFAT-formatted USB drive with at least the storage space of the PS4 backup file. If you don't have enough space to back up everything on your device, you can choose not to back up application data. It's important to sync your Trophies first, as they are not included in the backup data: go to Trophies , press the OPTIONS button, and select Sync Trophies With PSN . Insert the USB drive into your PS4 console; go to Settings > System > Back Up and Restore ; select Back Up ; confirm which data you'd like to back up. It is important to back up saved data to avoid losing any game progress. Customize the backup file name and select Back Up ; this will restart the console and start the backup process; remove the USB drive once the console has been started up normally again.","title":"Back up the existing data on an external USB storage device"},{"location":"learning/tools/upgrade%20a%20ps4%27s%20hdd/#upgrade-the-hdd","text":"This procedure has been tested on a PS4 Pro. Other models have different procedures. Place the console upside-down on a flat surface and remove the HDD bay cover; remove from the right side first. You may see a sticker covering the HDD bay cover. It's safe to remove this, and it will not affect the warranty. Remove the screw holding the tray in place and and pull the HDD mounting bracket to remove it; Remove the HDD from the mounting bracket and insert its replacement; screw the screws back in, being careful not to over-tighten them; reinsert the HDD mounting bracket and put the crew back in to hold it in place; re-attach the HDD cover.","title":"Upgrade the HDD"},{"location":"learning/tools/upgrade%20a%20ps4%27s%20hdd/#reinstall-the-system-software","text":"This will delete all of the data on your PS4 console. This process is often referred to as a \"factory\" reset, or \"hard\" reset. Using another device: create a folder named PS4 on a USB drive formatted as FAT32; inside that folder, create another folder named UPDATE ; download the full installation file from the system software download page ; the system software installation file must be for a version that is >= to the firmware currently installed on the console. copy the installation file in the UPDATE folder created before; the file must be named PS4UPDATE.PUP . Plug the USB drive containing the file into the PS4; start the console in Safe Mode pressing and hold the power button, and releasing it after the second beep; select Safe Mode's option 7: Initialize PS4 (Reinstall System Software) ; confirm at the prompts. If the PS4 does not recognize the file, check that the folder names and file name are correct. Enter the folder names and file name using uppercase letters.","title":"Reinstall the system software"},{"location":"learning/tools/upgrade%20a%20ps4%27s%20hdd/#restore-backed-up-data-from-an-external-usb-storage-device-to-the-playstation-4","text":"When restoring data, your PS4 will erase all the data currently saved on your console. This can't be undone, even if you cancel the restore operation. Make sure you don't erase any important data by mistake. Erased data can't be restored. Go to Settings > System > Back Up and Restore ; insert the USB drive that contains the backup into your PS4; select Restore PS4 ; select the backup file you'd like to restore; confirm to restore. Please note, users who have never signed in to PlayStation\u2122Network (PSN) can restore saved data to only the original PS4 console that was backed up. To restore saved data to another PS4 console, you must sign in to PSN before backing up data.","title":"Restore backed up data from an external USB storage device to the PlayStation 4"},{"location":"learning/tools/upgrade%20a%20ps4%27s%20hdd/#sources","text":"PS4: upgrade HDD PS4: External hard drive support How to back up and restore PS4 console data","title":"Sources"},{"location":"server/","text":"Server Details \u00b6 Linux Package Management Basics DNS Basics","title":"Server Details"},{"location":"server/#server-details","text":"Linux Package Management Basics DNS Basics","title":"Server Details"},{"location":"server/arm/","text":"Amlogic S905W Installation Download Ubuntu ARM 64 Bit OS check for focal current image.xz package On Windows use Etcher to update the image after extraction into SD Card. On Linux, go to /boot/dtb/meson-gxl-s905w-p281.dtb and copy the file name. Use the above file name and update file /boot/extlinux/extlinux.conf as per instruction Amlogic S905W Installation. Copy uboot file at root named u-boot-s905x-s912 as u-boot.ext Configure wireless network details # Use su to edit the below files. /etc/network/interfaces # Add or uncomment the lines allow-hotplug wlan0 iface wlan0 inet manual wpa-roam /etc/wpa_supplicant/wpa_supplicant.conf post-up ifdown eth0 iface default inet dhcp /etc/wpa_supplicant/wpa_supplicant.conf # Add below contents and update Wifi details ctrl_interface = DIR = /var/run/wpa_supplicant GROUP = netdev update_config = 1 network ={ ssid = \"YOUR_SSID_HERE\" psk = \"YOUR_SECRET_PASSPHRASE_HERE\" id_str = \"SOME_DESCRIPTIVE_NAME\" } Extract SD Card and insert inside Tx device to boot from new SD Card. After boot, check the IP address assigned in Router DHCP Client list SSH into Tx device IP as ssh root@IP , default passw0rd is 1234. Create new Sudo user and reset passwd. Update OS, type armbian-config in the terminal to configure the device. System \u2192 Firmware \u2192 Update System and then reboot sudo apt-update if that fails for libc-bin package, force pin the version and install it. sudo apt install libc6 = 2 .31-0ubuntu9.2 libc-bin = 2 .31-0ubuntu9.2 sudo apt-get update -y sudo apt-get upgrade -y sudo apt-get autoremove -y && sudo apt-get clean -y Update NAND frequency in case EEMC (internal 2GB RAM and 8GB ROM) is not detected. Use this guide . # You need to unpack your dtb file into dts via device-tree-compile tool. sudo apt-get install device-tree-compiler # decompile the dtb file dtc -I dtb -O dts -o meson-gxl-s905w-p281.dts meson-gxl-s905w-p281.dtb # Edit using sudo in BOOT/dtb/amlogic searching for \"mmc@74000\" block max-frequency = <0x5f5e100> ; 5f5e100 in hex = 100000000 in dec edit it to 0x2faf080, 50000000 in dec # compile with dtc -I dts -O dtb -o meson-gxl-s905w-p281.dtb meson-gxl-s905w-p281.dts # Delete the dts file # Remove SD card and inside into Tx device Get system diagonastics armbianmonitor -u , which is stored in html. Copy Boot directory to internal EEMC chip using ./install-aml.sh present in the root home directory. Remove the SD card and use the Tx device henceforth. After reboot usig EEMC, armbian-config \u2013 System and 3 rd Party Software, Install Full Firmware Packages. Use armbian-config to set static IP to Tx device nmcli con show sudo nmtui # Use UI to update IP Product Specs of Amlogic S905W Android TV Box \u00b6 Model NO. M18 CPU Amlogic S905W 64bits Quad-Core Cortex-A53@1.5GHz GPU Penta-core ARM\u00ae Mali\u2122-450 RAM 1G ROM 8G WIFI Wi-Fi 802.11 b/g/n Memory Card Support TF card, up to 32GB HOST USB 2.0 Operating System Android 7.1 Bluetooth No BT for 1G+8G Product Size 100 100 10mm","title":"Arm"},{"location":"server/arm/#product-specs-of-amlogic-s905w-android-tv-box","text":"Model NO. M18 CPU Amlogic S905W 64bits Quad-Core Cortex-A53@1.5GHz GPU Penta-core ARM\u00ae Mali\u2122-450 RAM 1G ROM 8G WIFI Wi-Fi 802.11 b/g/n Memory Card Support TF card, up to 32GB HOST USB 2.0 Operating System Android 7.1 Bluetooth No BT for 1G+8G Product Size 100 100 10mm","title":"Product Specs of Amlogic S905W Android TV Box"},{"location":"server/install/","text":"Installation \u00b6 Centos \u00b6 Identifying Version Download the latest version of Centos from centos.org Creating Live USB Format a USB as NTFS. Download Fedora LiveUSB Creator or read the supported software for Centos in the downloads page. See the instructions to write the ISO into the USB Setting Up PC Format the HDD (optional) Ensure - Advanced Option, Legacy USB Support is Enabled In Boot priority, USB is the first drive Ensure the Boot option has Boot from USB selected above HDD Configuring Centos Full Installation Steps Create Custom partitions for /boot, /, /var, /tmp, /home Initial Server Setup Update Centos with latest patches yum update -y && yum upgrade -y yum clean all # To clean downloaded packages in /var/cache/yum Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Adding Non Root User Add and Delete Users Creating Volume Groups Securing Server Security based on articles mentioned here or here or DigitalOcean -keygen -t rsa -b 4096 It will create two files: id_rsa and id_rsa.pub in the ~/.ssh directory. When you add a passphrase to your SSH key, it encrypts the private key using 128-bit AES so that the private key is useless without the passphrase to decrypt it. cat ~/.ssh/id_rsa.pub For example, if we had a Linux VM named myserver with a user azureuser, we could use the following command to install the public key file and authorize the user with the key: ssh-copy-id -i ~/.ssh/id_rsa.pub azureuser@myserver Connect with SSH ssh jim@137.117.101.249 Try executing a few Linux commands ls -la / to show the root of the disk ps -l to show all the running processes dmesg to list all the kernel messages lsblk to list all the block devices - here you will see your drives Initialize data disks Any additional drives you create from scratch need to be initialized and formatted. dmesg | grep SCSI - list all the messages from the kernel for SCSI devices. Use fdisk to initialize the drive We can use the following command to create a new primary partition: (echo n; echo p; echo 1; echo ; echo ; echo w) | sudo fdisk /dev/sdc We need to write a file system to the partition with the mkfs command. sudo mkfs -t ext4 /dev/sdc1 We need to mount the drive to the file system. sudo mkdir /data & sudo mount /dev/sdc1 /data Always make sure to lock down ports used for administrative access. An even better approach is to create a VPN to link the virtual network to your private network and only allow RDP or SSH requests from that address range. You can also change the port used by SSH to be something other than the default. Keep in mind that changing ports is not sufficient to stop attacks. It simply makes it a little harder to discover.","title":"Installation"},{"location":"server/install/#installation","text":"","title":"Installation"},{"location":"server/install/#centos","text":"Identifying Version Download the latest version of Centos from centos.org Creating Live USB Format a USB as NTFS. Download Fedora LiveUSB Creator or read the supported software for Centos in the downloads page. See the instructions to write the ISO into the USB Setting Up PC Format the HDD (optional) Ensure - Advanced Option, Legacy USB Support is Enabled In Boot priority, USB is the first drive Ensure the Boot option has Boot from USB selected above HDD Configuring Centos Full Installation Steps Create Custom partitions for /boot, /, /var, /tmp, /home Initial Server Setup Update Centos with latest patches yum update -y && yum upgrade -y yum clean all # To clean downloaded packages in /var/cache/yum Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Adding Non Root User Add and Delete Users Creating Volume Groups Securing Server Security based on articles mentioned here or here or DigitalOcean -keygen -t rsa -b 4096 It will create two files: id_rsa and id_rsa.pub in the ~/.ssh directory. When you add a passphrase to your SSH key, it encrypts the private key using 128-bit AES so that the private key is useless without the passphrase to decrypt it. cat ~/.ssh/id_rsa.pub For example, if we had a Linux VM named myserver with a user azureuser, we could use the following command to install the public key file and authorize the user with the key: ssh-copy-id -i ~/.ssh/id_rsa.pub azureuser@myserver Connect with SSH ssh jim@137.117.101.249 Try executing a few Linux commands ls -la / to show the root of the disk ps -l to show all the running processes dmesg to list all the kernel messages lsblk to list all the block devices - here you will see your drives Initialize data disks Any additional drives you create from scratch need to be initialized and formatted. dmesg | grep SCSI - list all the messages from the kernel for SCSI devices. Use fdisk to initialize the drive We can use the following command to create a new primary partition: (echo n; echo p; echo 1; echo ; echo ; echo w) | sudo fdisk /dev/sdc We need to write a file system to the partition with the mkfs command. sudo mkfs -t ext4 /dev/sdc1 We need to mount the drive to the file system. sudo mkdir /data & sudo mount /dev/sdc1 /data Always make sure to lock down ports used for administrative access. An even better approach is to create a VPN to link the virtual network to your private network and only allow RDP or SSH requests from that address range. You can also change the port used by SSH to be something other than the default. Keep in mind that changing ports is not sufficient to stop attacks. It simply makes it a little harder to discover.","title":"Centos"},{"location":"server/mobile/","text":"Converting Android Device Into Linux Server \u00b6 Centos \u00b6 Pre-requisites Identify an Android mobile phone Factory reset to remove any unwanted apps and data Remove any apps or services from the device to free up space and memory Install Termux and AnLinux from Playstore Installation Start AnLinux and search for the distro. Select Centos Copy the installation command Open Termux terminal and paste the command Begin installation Access Mobile Configure Static Ip to Mobile Add Ip address and remote-hostname to Laptop's hosts file. Install SSH on Termux apt install openssh Start the ssh server sshd . This will also generate the private and public keys. Test the service locally ssh localhost -p 8022 . Accept the public key. ssh-copy-id copies the local-host\u2019s public key to the remote-host\u2019s authorized_keys file. ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-hostname> -p 8022 . Enter remote user password to complete transaction. Login from Laptop ssh <hostname> -p 8022 . Enter the passphrase. Install Python Configuring Centos Update Centos with latest patches yum install update -y && yum install upgrade -y Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Init Centos yum -y install initscripts & yum clean all Change Root Password passwd root Install Sudo yum install sudo -y The difference between wheel and sudo . In CentOS, a user belonging to the wheel group can execute su and directly ascend to root. Meanwhile, a sudo user would have use the sudo su first. Essentially, there is no real difference except for the syntax used to become root, and users belonging to both groups can use the sudo command. * Adding Non Root User bmmnb Add and Delete Users Securing Server Security based on articles mentioned here or here or DigitalOcean Configures a Hostname Reconfigures the Timezone Updates the entire System Creates a New Admin user so you can manage your server safely without the need of doing remote connections with root. Helps user Generate Secure RSA Keys, so that remote access to your server is done exclusive from your local pc and no Conventional password Configures, Optimize and secures the SSH Server (Some Settings Following CIS Benchmark) Configures IPTABLES Rules to protect the server from common attacks Disables unused FileSystems and Network protocols Protects the server against Brute Force attacks by installing a configuring fail2ban Installs and Configure Artillery as a Honeypot, Monitoring, Blocking and Alerting tool Installs PortSentry Installs RootKit Hunter Secures Root Home and Grub Configuration Files Installs Unhide to help Detect Malicious Hidden Processes Installs Tiger, A Security Auditing and Intrusion Prevention system Restrict Access to Apache Config Files Disables Compilers Creates Daily Cron job for System Updates Kernel Hardening via sysctl configuration File (Tweaked) /tmp Directory Hardening PSAD IDS installation Enables Process Accounting Enables Unattended Upgrades MOTD and Banners for Unauthorized access Disables USB Support for Improved Security (Optional) Configures a Restrictive Default UMASK Configures and enables Auditd Configures Auditd rules following CIS Benchmark Sysstat install ArpWatch install Additional Hardening steps following CIS Benchmark Secures Cron Automates the process of setting a GRUB Bootloader Password Secures Boot Settings Sets Secure File Permissions for Critical System Files","title":"Mobile"},{"location":"server/mobile/#converting-android-device-into-linux-server","text":"","title":"Converting Android Device Into Linux Server"},{"location":"server/mobile/#centos","text":"Pre-requisites Identify an Android mobile phone Factory reset to remove any unwanted apps and data Remove any apps or services from the device to free up space and memory Install Termux and AnLinux from Playstore Installation Start AnLinux and search for the distro. Select Centos Copy the installation command Open Termux terminal and paste the command Begin installation Access Mobile Configure Static Ip to Mobile Add Ip address and remote-hostname to Laptop's hosts file. Install SSH on Termux apt install openssh Start the ssh server sshd . This will also generate the private and public keys. Test the service locally ssh localhost -p 8022 . Accept the public key. ssh-copy-id copies the local-host\u2019s public key to the remote-host\u2019s authorized_keys file. ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-hostname> -p 8022 . Enter remote user password to complete transaction. Login from Laptop ssh <hostname> -p 8022 . Enter the passphrase. Install Python Configuring Centos Update Centos with latest patches yum install update -y && yum install upgrade -y Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Init Centos yum -y install initscripts & yum clean all Change Root Password passwd root Install Sudo yum install sudo -y The difference between wheel and sudo . In CentOS, a user belonging to the wheel group can execute su and directly ascend to root. Meanwhile, a sudo user would have use the sudo su first. Essentially, there is no real difference except for the syntax used to become root, and users belonging to both groups can use the sudo command. * Adding Non Root User bmmnb Add and Delete Users Securing Server Security based on articles mentioned here or here or DigitalOcean Configures a Hostname Reconfigures the Timezone Updates the entire System Creates a New Admin user so you can manage your server safely without the need of doing remote connections with root. Helps user Generate Secure RSA Keys, so that remote access to your server is done exclusive from your local pc and no Conventional password Configures, Optimize and secures the SSH Server (Some Settings Following CIS Benchmark) Configures IPTABLES Rules to protect the server from common attacks Disables unused FileSystems and Network protocols Protects the server against Brute Force attacks by installing a configuring fail2ban Installs and Configure Artillery as a Honeypot, Monitoring, Blocking and Alerting tool Installs PortSentry Installs RootKit Hunter Secures Root Home and Grub Configuration Files Installs Unhide to help Detect Malicious Hidden Processes Installs Tiger, A Security Auditing and Intrusion Prevention system Restrict Access to Apache Config Files Disables Compilers Creates Daily Cron job for System Updates Kernel Hardening via sysctl configuration File (Tweaked) /tmp Directory Hardening PSAD IDS installation Enables Process Accounting Enables Unattended Upgrades MOTD and Banners for Unauthorized access Disables USB Support for Improved Security (Optional) Configures a Restrictive Default UMASK Configures and enables Auditd Configures Auditd rules following CIS Benchmark Sysstat install ArpWatch install Additional Hardening steps following CIS Benchmark Secures Cron Automates the process of setting a GRUB Bootloader Password Secures Boot Settings Sets Secure File Permissions for Critical System Files","title":"Centos"},{"location":"server/proxy/","text":"Nginx Reverse Proxy Setup","title":"Proxy"},{"location":"server/service/","text":"Create a systemd unit file for starting the application: \u00b6 Example service file can be found here: $ wget https://gist.githubusercontent.com/Artemmkin/ce82397cfc69d912df9cd648a8d69bec/raw/7193a36c9661c6b90e7e482d256865f085a853f2/raddit.service Move it to the systemd directory $ sudo mv raddit.service /etc/systemd/system/raddit.service Now start the application and enable autostart: $ sudo systemctl start raddit $ sudo systemctl enable raddit Verify that it's running: $ sudo systemctl status raddit TFTP Server \u00b6 TFTP service is used to distribute network packages at boot time. # Install packages sudo apt install xinetd tftpd tftp # Verify the tftp service configuration vi /etc/xinetd.d/tdtftp # Create the directory where bin packages will be stored mkdir -p /tftpboot # Update file permissions so anyone can read and write to this directory sudo chmod -R 777 /tftpboot sudo chown -R nobody /tftpboot # Restart the service systemctl restart xinetd.service # Verify if service is working by opening tftp client tftp get <filename> # Downloads file to the current directory on the client # quit to exit the tftp session","title":"Create a systemd unit file for starting the application:"},{"location":"server/service/#create-a-systemd-unit-file-for-starting-the-application","text":"Example service file can be found here: $ wget https://gist.githubusercontent.com/Artemmkin/ce82397cfc69d912df9cd648a8d69bec/raw/7193a36c9661c6b90e7e482d256865f085a853f2/raddit.service Move it to the systemd directory $ sudo mv raddit.service /etc/systemd/system/raddit.service Now start the application and enable autostart: $ sudo systemctl start raddit $ sudo systemctl enable raddit Verify that it's running: $ sudo systemctl status raddit","title":"Create a systemd unit file for starting the application:"},{"location":"server/service/#tftp-server","text":"TFTP service is used to distribute network packages at boot time. # Install packages sudo apt install xinetd tftpd tftp # Verify the tftp service configuration vi /etc/xinetd.d/tdtftp # Create the directory where bin packages will be stored mkdir -p /tftpboot # Update file permissions so anyone can read and write to this directory sudo chmod -R 777 /tftpboot sudo chown -R nobody /tftpboot # Restart the service systemctl restart xinetd.service # Verify if service is working by opening tftp client tftp get <filename> # Downloads file to the current directory on the client # quit to exit the tftp session","title":"TFTP Server"},{"location":"server/volume-groups/","text":"Configuration \u00b6 Centos \u00b6 Prerequisites Install EPEL Packages yum install epel-release Install ntfs-3g and fuse packages to support NTFS drives yum install ntfs-3g fuse -y Format the new NTFS filestsystem to xfs mkfs.xfs -f /dev/sda Identifying Additional HDD Run blkid to verify if HDD is detected. On detection of HDD, follow the below steps to configure as XFS file system. * Run fdisk -l to list the devices, partitions and volume groups * Scan your hard drive for Bad Sectors or Bad Blocks 1. Partitioning HDD (Optional and for Primary HDD) * On identifying the device(HDD), say /dev/sda as the target device to partition, run fdisk /dev/sda * These steps below are not for secondary HDD as partitioning will not utilize the entire space * Option m to list the operations, Option p to print the current partitions * Note the start and end block ids of the current partitions * Option d to delete the current partitions, select the partition to delete 2 * Option n to create new partition, select only 1 partition for the HDD, select default start and end block id, it should match the one printed above * Option t to select the type of partition, followed by 8e to select as Linux LVM * Format the new partition to xfs mkfs.xfs -f /dev/sda1 as only one partition was created * Mount the new partition to test mount -t xfs /dev/sda1 /data * Make entry in /etc/fstab to make it permanent * Verify the mounted partition by running df -Th to see free and used space Securing Drive and adding Redundancy Software RAID and LVM For the partitions that are needed as Raid 1, use fdsik /dev/sdb3 ,Press \u2018t\u2019 to change the partition type. Press \u2018fd\u2019 to choose Linux Raid Auto. Install Raid 1 on /dev/sdb3 and /dev/sda1 using mdadm --create /dev/data --level=1 --raid-devices=2 /dev/sdb3 /dev/sda1 Encrypting Raid devices Volume Group Setup Logical Volume Manager (LVM) is a software-based RAID-like system that lets you create \"pools\" of storage and add hard drive space to those pools as needed. * LVM Basic * Create, expand, and encrypt storage pools Volume Group Lifecycle Migrate Data from one LV to another Identify the current Physical Volume and Volume Group mapping pvscan or pvs List the partitions and volumes in the Volume Groups vgdisplay centos-digital -v Logical volume and device mapping lvs -o+devices List all devices and device mapping lsscsi Before Reducing logical volume, move to another drive Format /dev/sdb3 to a physical volume: pvcreate /dev/sdb3 Give /dev/sdb3 to the volume group centos-digital: vgextend centos-digital /dev/sdb3 Migrate our logical volume Users from /dev/sdb2 to /dev/sdb3, pvmove -n Users /dev/sdb2 /dev/sdb3 Verify if the data is migrated lsblk Reduce your logical volume to make place for your new partition lvresize -L 25G /dev/centos-digital/home Use fdisk to reduce the size of /dev/sdb2 to 80G as total for volume group centos-digital was 75G (yes, a little bit bigger as we used in lvresize!) Use fdisk to create a new LVM partition, /dev/sdb3, with the new available space (it will be around 400G). Check if the kernel could automatically detect the partition change. See if /proc/partition matches the new state (thus, /dev/sdb3 is visible). If not, you need to reboot. (Probably it will.) Make /dev/sdb2 to be as big again pvresize /dev/sdb2","title":"Configuration"},{"location":"server/volume-groups/#configuration","text":"","title":"Configuration"},{"location":"server/volume-groups/#centos","text":"Prerequisites Install EPEL Packages yum install epel-release Install ntfs-3g and fuse packages to support NTFS drives yum install ntfs-3g fuse -y Format the new NTFS filestsystem to xfs mkfs.xfs -f /dev/sda Identifying Additional HDD Run blkid to verify if HDD is detected. On detection of HDD, follow the below steps to configure as XFS file system. * Run fdisk -l to list the devices, partitions and volume groups * Scan your hard drive for Bad Sectors or Bad Blocks 1. Partitioning HDD (Optional and for Primary HDD) * On identifying the device(HDD), say /dev/sda as the target device to partition, run fdisk /dev/sda * These steps below are not for secondary HDD as partitioning will not utilize the entire space * Option m to list the operations, Option p to print the current partitions * Note the start and end block ids of the current partitions * Option d to delete the current partitions, select the partition to delete 2 * Option n to create new partition, select only 1 partition for the HDD, select default start and end block id, it should match the one printed above * Option t to select the type of partition, followed by 8e to select as Linux LVM * Format the new partition to xfs mkfs.xfs -f /dev/sda1 as only one partition was created * Mount the new partition to test mount -t xfs /dev/sda1 /data * Make entry in /etc/fstab to make it permanent * Verify the mounted partition by running df -Th to see free and used space Securing Drive and adding Redundancy Software RAID and LVM For the partitions that are needed as Raid 1, use fdsik /dev/sdb3 ,Press \u2018t\u2019 to change the partition type. Press \u2018fd\u2019 to choose Linux Raid Auto. Install Raid 1 on /dev/sdb3 and /dev/sda1 using mdadm --create /dev/data --level=1 --raid-devices=2 /dev/sdb3 /dev/sda1 Encrypting Raid devices Volume Group Setup Logical Volume Manager (LVM) is a software-based RAID-like system that lets you create \"pools\" of storage and add hard drive space to those pools as needed. * LVM Basic * Create, expand, and encrypt storage pools Volume Group Lifecycle Migrate Data from one LV to another Identify the current Physical Volume and Volume Group mapping pvscan or pvs List the partitions and volumes in the Volume Groups vgdisplay centos-digital -v Logical volume and device mapping lvs -o+devices List all devices and device mapping lsscsi Before Reducing logical volume, move to another drive Format /dev/sdb3 to a physical volume: pvcreate /dev/sdb3 Give /dev/sdb3 to the volume group centos-digital: vgextend centos-digital /dev/sdb3 Migrate our logical volume Users from /dev/sdb2 to /dev/sdb3, pvmove -n Users /dev/sdb2 /dev/sdb3 Verify if the data is migrated lsblk Reduce your logical volume to make place for your new partition lvresize -L 25G /dev/centos-digital/home Use fdisk to reduce the size of /dev/sdb2 to 80G as total for volume group centos-digital was 75G (yes, a little bit bigger as we used in lvresize!) Use fdisk to create a new LVM partition, /dev/sdb3, with the new available space (it will be around 400G). Check if the kernel could automatically detect the partition change. See if /proc/partition matches the new state (thus, /dev/sdb3 is visible). If not, you need to reboot. (Probably it will.) Make /dev/sdb2 to be as big again pvresize /dev/sdb2","title":"Centos"}]}