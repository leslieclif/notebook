{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Leslie's Notebook \u00b6 Install mkdocs using command pip install mkdocs Topics \u00b6 Server - Setup basic Linux server Kubernetes - Setup Kubernetes IDE - Tips and Tricks","title":"Installation"},{"location":"#welcome-to-leslies-notebook","text":"Install mkdocs using command pip install mkdocs","title":"Welcome to Leslie's Notebook"},{"location":"#topics","text":"Server - Setup basic Linux server Kubernetes - Setup Kubernetes IDE - Tips and Tricks","title":"Topics"},{"location":"developer/","text":"Windows \u00b6 Update WSL2 first (by default WLS1 is enabled) Install Ubuntu from Microsoft Stores Install Visual Studio Code Update Linux packages sudo apt update sudo apt -y upgrade To find the home directory in Ubuntu explorer.exe . Install Windows Terminal for Miscrosoft Store Install Menlo font (from Powerlevel10k site) To test the terminal color output, run this code in the terminal for code in { 30 ..37 } ; do \\ echo -en \"\\e[ ${ code } m\" '\\\\e[' \" $code \" 'm' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;1m\" '\\\\e[' \" $code \" ';1m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;3m\" '\\\\e[' \" $code \" ';3m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;4m\" '\\\\e[' \" $code \" ';4m' \"\\e[0m\" ; \\ echo -e \" \\e[ $(( code+60 )) m\" '\\\\e[' \" $(( code+60 )) \" 'm' \"\\e[0m\" ; \\ done Generate SSH keys ssh-keygen -t rsa -b 4096 -f ~/.ssh/raddit-user -C raddit-user ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user Using the .ssh config files (~/.ssh/config) # 1. Generate public & private ssh keys: ` ssh-keygen -t rsa ` # Type in a name which will be put in `~/.ssh` directory # 2. To bypass password prompt, you should add the `foo.pub` file to the `authorized_keys` file on the # server's `~/.ssh` directory. You can do a pipe via ssh: ` cat mykey.pub | ssh myuser@mysite.com -p 123 'cat >> .ssh/authorized_keys' ` # 3. Add the publickey name to the `~/.ssh/config` file like this: Host bitbucket.org IdentityFile ~/.ssh/myprivatekeyfile # the leading spaces are important! Port 123 # 4. Verify and then SSH into the remote server. To check if your config is right type: `ssh -T git@github.com` ssh root@mysite.com or ssh mysite.com # if you setup the User setting in config Ubuntu \u00b6 Use bootable USB created using ventoy Press F12 at startup and select the bootable USB, select Ubuntu is image to begin installation Configure linux partitions as encrypted Add swap partion instead of efi as we will dual boot into same system. Part 2 Change root password. sudo passwd root Move Windows above Ubuntu in boot menu. Use Grub Customizer Part 3 Backup and restore data using rsync. Install programs using dotfiles. Adding SSH Keys to servers SSH Client Config Edit setings on the new terminal to make Ubuntu as the default terminal. Also set the fontFace and https://www.the-digital-life.com/en/awesome-wsl-wsl2-terminal/ - Create Sudo User - Securing Sudoers #sudo visudo /etc/sudoers.d/leslie leslie ALL =( ALL:ALL ) NOPASSWD: /usr/bin/docker, /usr/sbin/reboot, /usr/sbin/shutdown, /usr/bin/apt-get, /usr/local/bin/docker-compose Switching remote URLs from HTTPS to SSH \u00b6 List your existing remotes in order to get the name of the remote you want to change. $ git remote -v > origin https://github.com/USERNAME/REPOSITORY.git ( fetch ) > origin https://github.com/USERNAME/REPOSITORY.git ( push ) Change your remote's URL from HTTPS to SSH with the git remote set-url command. $ git remote set-url origin git@github.com:USERNAME/REPOSITORY.git git remote set-url origin git@github.com :leslieclif/notebook.git Inspirational dotfile repos \u00b6 https://www.freecodecamp.org/news/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/ Dotfiles Intial Automation Tmux and Otherconfig nickjj/dotfiles https://github.com/jieverson/dotfiles-win/blob/master/install.sh Bashrc Automation \u00b6 https://victoria.dev/blog/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/ https://victoria.dev/blog/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/ Vagrant setup \u00b6 https://www.techdrabble.com/ansible/36-install-ansible-molecule-vagrant-on-windows-wsl Tmux \u00b6 Every Hacker should have a great terminal | TMUX - Medium \u00b6 Tmux Basics Tmux Config VSCode \u00b6 Key Shortcuts Mastering Terminal","title":"Developer Setup"},{"location":"developer/#windows","text":"Update WSL2 first (by default WLS1 is enabled) Install Ubuntu from Microsoft Stores Install Visual Studio Code Update Linux packages sudo apt update sudo apt -y upgrade To find the home directory in Ubuntu explorer.exe . Install Windows Terminal for Miscrosoft Store Install Menlo font (from Powerlevel10k site) To test the terminal color output, run this code in the terminal for code in { 30 ..37 } ; do \\ echo -en \"\\e[ ${ code } m\" '\\\\e[' \" $code \" 'm' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;1m\" '\\\\e[' \" $code \" ';1m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;3m\" '\\\\e[' \" $code \" ';3m' \"\\e[0m\" ; \\ echo -en \" \\e[ $code ;4m\" '\\\\e[' \" $code \" ';4m' \"\\e[0m\" ; \\ echo -e \" \\e[ $(( code+60 )) m\" '\\\\e[' \" $(( code+60 )) \" 'm' \"\\e[0m\" ; \\ done Generate SSH keys ssh-keygen -t rsa -b 4096 -f ~/.ssh/raddit-user -C raddit-user ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user Using the .ssh config files (~/.ssh/config) # 1. Generate public & private ssh keys: ` ssh-keygen -t rsa ` # Type in a name which will be put in `~/.ssh` directory # 2. To bypass password prompt, you should add the `foo.pub` file to the `authorized_keys` file on the # server's `~/.ssh` directory. You can do a pipe via ssh: ` cat mykey.pub | ssh myuser@mysite.com -p 123 'cat >> .ssh/authorized_keys' ` # 3. Add the publickey name to the `~/.ssh/config` file like this: Host bitbucket.org IdentityFile ~/.ssh/myprivatekeyfile # the leading spaces are important! Port 123 # 4. Verify and then SSH into the remote server. To check if your config is right type: `ssh -T git@github.com` ssh root@mysite.com or ssh mysite.com # if you setup the User setting in config","title":"Windows"},{"location":"developer/#ubuntu","text":"Use bootable USB created using ventoy Press F12 at startup and select the bootable USB, select Ubuntu is image to begin installation Configure linux partitions as encrypted Add swap partion instead of efi as we will dual boot into same system. Part 2 Change root password. sudo passwd root Move Windows above Ubuntu in boot menu. Use Grub Customizer Part 3 Backup and restore data using rsync. Install programs using dotfiles. Adding SSH Keys to servers SSH Client Config Edit setings on the new terminal to make Ubuntu as the default terminal. Also set the fontFace and https://www.the-digital-life.com/en/awesome-wsl-wsl2-terminal/ - Create Sudo User - Securing Sudoers #sudo visudo /etc/sudoers.d/leslie leslie ALL =( ALL:ALL ) NOPASSWD: /usr/bin/docker, /usr/sbin/reboot, /usr/sbin/shutdown, /usr/bin/apt-get, /usr/local/bin/docker-compose","title":"Ubuntu"},{"location":"developer/#switching-remote-urls-from-https-to-ssh","text":"List your existing remotes in order to get the name of the remote you want to change. $ git remote -v > origin https://github.com/USERNAME/REPOSITORY.git ( fetch ) > origin https://github.com/USERNAME/REPOSITORY.git ( push ) Change your remote's URL from HTTPS to SSH with the git remote set-url command. $ git remote set-url origin git@github.com:USERNAME/REPOSITORY.git git remote set-url origin git@github.com :leslieclif/notebook.git","title":"Switching remote URLs from HTTPS to SSH"},{"location":"developer/#inspirational-dotfile-repos","text":"https://www.freecodecamp.org/news/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/ Dotfiles Intial Automation Tmux and Otherconfig nickjj/dotfiles https://github.com/jieverson/dotfiles-win/blob/master/install.sh","title":"Inspirational dotfile repos"},{"location":"developer/#bashrc-automation","text":"https://victoria.dev/blog/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/ https://victoria.dev/blog/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/","title":"Bashrc Automation"},{"location":"developer/#vagrant-setup","text":"https://www.techdrabble.com/ansible/36-install-ansible-molecule-vagrant-on-windows-wsl","title":"Vagrant setup"},{"location":"developer/#tmux","text":"","title":"Tmux"},{"location":"developer/#every-hacker-should-have-a-great-terminal--tmux---medium","text":"Tmux Basics Tmux Config","title":"Every Hacker should have a great terminal | TMUX - Medium"},{"location":"developer/#vscode","text":"Key Shortcuts Mastering Terminal","title":"VSCode"},{"location":"devops/","text":"Developer Skills Roadmap Devops Skills Roadmap Devops Skillset Search for Autopilot and Technology to find for automation scripts or patterns","title":"Index"},{"location":"ide/","text":"IDE Tips and Tricks \u00b6 VS Code \u00b6 Intellij \u00b6 How to open multiple modules in the same workspace? Open the first module / project. Select File -> Project Structure -> Modules. Select Copy (icon) and open Browse your local directory and select the project you would like to add. Module name will resolve automatically. When you want to reopen to project with multiple sub-projects, in order to avoid re-doing steps as described above, just go to File -> Open Recent -> 'Your Big Project'. IDE Shortcuts Option Description Ctrl + Space Code Completion Ctrl + Shift + / Comments selected lines of code Ctrl + N Navigate to any class by typing the name in the editor Ctrl + B Jump to declaration","title":"IDE Tips and Tricks"},{"location":"ide/#ide-tips-and-tricks","text":"","title":"IDE Tips and Tricks"},{"location":"ide/#vs-code","text":"","title":"VS Code"},{"location":"ide/#intellij","text":"How to open multiple modules in the same workspace? Open the first module / project. Select File -> Project Structure -> Modules. Select Copy (icon) and open Browse your local directory and select the project you would like to add. Module name will resolve automatically. When you want to reopen to project with multiple sub-projects, in order to avoid re-doing steps as described above, just go to File -> Open Recent -> 'Your Big Project'. IDE Shortcuts Option Description Ctrl + Space Code Completion Ctrl + Shift + / Comments selected lines of code Ctrl + N Navigate to any class by typing the name in the editor Ctrl + B Jump to declaration","title":"Intellij"},{"location":"ide/markdown/","text":"General Syntax \u00b6 MarkDown Cheat Sheet Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Horizontal Rule ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks Emphasis Bold **rendered as bold text** Italics _rendered as italicized text_ BlockQuote > Blockquotes Nested BlockQuote > First Level >> Second Level >>> Third Level List Unordered * or + or - Ordered 1. Code Block ` or [```html] (3 backticks and follwed by the language) Links Inline Links [Text](http://text.io) Link titles [Text](https://github.com/site/ \"Visit Site!\") Images ![Minion](http://octodex.github.com/images/minion.png) Using Mkdocs formatting \u00b6 Tabbed Data Inline Examples Output p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} , p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} . Markdown $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ , \\( p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } \\) . Adding Tips Inline Configuration This is an example of a tip. Make the paragragh tabbed inline with the heading Adding Danger Reminder This is a call to action Adding Note Note Adding notes Adding Summary Summary This is to sumarize the information New Information New 7.1 New Info Adding Note Collapsible tab Click Me! Thanks! Success Collapsible tab Success Content. Warning Collapsible tab Warning Content. Adding Settings Gear Basic Software Setup sudo apt install Adding multi-level collapisble tabs Details must contain a blank line before they start. Use ??? to start a details block or ???+ if you want to start a details block whose default state is 'open'. Follow the start of the block with an optional class or classes (separated with spaces) and the summary contained in quotes. Content is placed below the header and must be indented. Open styled details Nested details! And more content again. Adding checklist inside summary Tasklist eggs bread milk Adding strikethrough and subscript Tilde Tilde is syntactically built around the ~ character. It adds support for inserting sub scripts and adds an easy way to place text in a < del > tag. Showing Critic changes Critic Added CSS changes in extra.css to activate Critic change higlights. This is deleted This is added Showing Emojis Inline Code Highlighting InlineHilite utilizes the following syntax to insert inline highlighted code: `:::language mycode` or `#!language mycode` . Inline Highlighted Code Example Here is some code: import pymdownx ; pymdownx . __version__ . The mock shebang will be treated like text here: #!js var test = 0; . Marking words Mark Example mark me Preserve Tab spaces ============================================================ T Tp Sp D Dp S D7 T ------------------------------------------------------------ A F#m Bm E C#m D E7 A A# Gm Cm F Dm D# F7 A# B\u266d Gm Cm F Dm E\u266dm F7 B\u266d Showing Line Number in code import foo.bar import car - Highlighting specific line numbers \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz Highlighting line range and specific lines import foo import boo.baz import foo.bar.baz class Foo : def __init__ ( self ): self . foo = None self . bar = None self . baz = None","title":"Markdown"},{"location":"ide/markdown/#general-syntax","text":"MarkDown Cheat Sheet Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Horizontal Rule ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks Emphasis Bold **rendered as bold text** Italics _rendered as italicized text_ BlockQuote > Blockquotes Nested BlockQuote > First Level >> Second Level >>> Third Level List Unordered * or + or - Ordered 1. Code Block ` or [```html] (3 backticks and follwed by the language) Links Inline Links [Text](http://text.io) Link titles [Text](https://github.com/site/ \"Visit Site!\") Images ![Minion](http://octodex.github.com/images/minion.png)","title":"General Syntax"},{"location":"ide/markdown/#using-mkdocs-formatting","text":"Tabbed Data Inline Examples Output p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} , p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} . Markdown $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ , \\( p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } \\) . Adding Tips Inline Configuration This is an example of a tip. Make the paragragh tabbed inline with the heading Adding Danger Reminder This is a call to action Adding Note Note Adding notes Adding Summary Summary This is to sumarize the information New Information New 7.1 New Info Adding Note Collapsible tab Click Me! Thanks! Success Collapsible tab Success Content. Warning Collapsible tab Warning Content. Adding Settings Gear Basic Software Setup sudo apt install Adding multi-level collapisble tabs Details must contain a blank line before they start. Use ??? to start a details block or ???+ if you want to start a details block whose default state is 'open'. Follow the start of the block with an optional class or classes (separated with spaces) and the summary contained in quotes. Content is placed below the header and must be indented. Open styled details Nested details! And more content again. Adding checklist inside summary Tasklist eggs bread milk Adding strikethrough and subscript Tilde Tilde is syntactically built around the ~ character. It adds support for inserting sub scripts and adds an easy way to place text in a < del > tag. Showing Critic changes Critic Added CSS changes in extra.css to activate Critic change higlights. This is deleted This is added Showing Emojis Inline Code Highlighting InlineHilite utilizes the following syntax to insert inline highlighted code: `:::language mycode` or `#!language mycode` . Inline Highlighted Code Example Here is some code: import pymdownx ; pymdownx . __version__ . The mock shebang will be treated like text here: #!js var test = 0; . Marking words Mark Example mark me Preserve Tab spaces ============================================================ T Tp Sp D Dp S D7 T ------------------------------------------------------------ A F#m Bm E C#m D E7 A A# Gm Cm F Dm D# F7 A# B\u266d Gm Cm F Dm E\u266dm F7 B\u266d Showing Line Number in code import foo.bar import car - Highlighting specific line numbers \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz Highlighting line range and specific lines import foo import boo.baz import foo.bar.baz class Foo : def __init__ ( self ): self . foo = None self . bar = None self . baz = None","title":"Using Mkdocs formatting"},{"location":"k8s/","text":"Kubernetes \u00b6","title":"Kubernetes"},{"location":"k8s/#kubernetes","text":"","title":"Kubernetes"},{"location":"k8s/install/","text":"Installing K8s \u00b6 Kubeadm \u00b6 Kops \u00b6","title":"Installation"},{"location":"k8s/install/#installing-k8s","text":"","title":"Installing K8s"},{"location":"k8s/install/#kubeadm","text":"","title":"Kubeadm"},{"location":"k8s/install/#kops","text":"","title":"Kops"},{"location":"learning/ansible/","text":"Introduction \u00b6 Ansible 101 Ansible Cheat Sheet Ansible Tips and Tricks DO Practise examples & Explaination & Tutorials YAML Spec Ref Card Full Application on Cloud Documentation on cmdline \u00b6 # System outputs the man page for debug module ansible-doc debug Setup Server \u00b6 Default Configuration \u00b6 ansible.cfg and hosts files are present inside /etc/ansible Testing ansible on Ubuntu WSL ansible localhost -m ping Enabling SSH on the VM \u00b6 If you need SSH enabled on the system, follow the below steps: Ensure the /etc/apt/sources.list file has been updated as per above Run the command: apt-get update Run the command: apt-get install openssh-server Run the command: service sshd start ssh-keygen -t rsa -C \"ansible\" #OR # Generate an SSH key pair for future connections to the VM instances (run the command exactly as it is): ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user -P \"\" #Add the SSH private key to the ssh-agent: ssh-add ~/.ssh/ansible-user #Verify that the key was added to the ssh-agent: $ ssh-add -l Access VM over SSH \u00b6 ssh vagrant@127.0.0.1 -p 2222 -i ~/.ssh/insecure_private_key Copy files recursively from local desktop to remote server \u00b6 scp -r ./scripts vagrant@127.0.0.1:/home/vagrant -p 2222 -i ~/.ssh/insecure_private_key Target Docker containers for Ansible controller \u00b6 The Docker file used to create the ubuntu-ssh-enabled Docker image is located here. Issues installing Ansible and its dependencies \u00b6 Once the Debian VM is up and running make the following changes to the /etc/apt/sources.list file to get the Ansible installation working right. deb http://security.debian.org/ jessie/updates main contrib deb-src http://security.debian.org/ jessie/updates main contrib deb http://ftp.debian.org/debian/ jessie-updates main contrib deb-src http://ftp.debian.org/debian/ jessie-updates main contrib deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main deb http://ftp.de.debian.org/debian sid main Ansible Directory Structure as per Best Practises \u00b6 This is the directory layout of this repository with explanation. production.ini # inventory file for production stage development.ini # inventory file for development stage test.ini # inventory file for test stage vpass # ansible-vault password file # This file should not be committed into the repository # therefore file is ignored by git group_vars/ all/ # variables under this directory belongs all the groups apt.yml # ansible-apt role variable file for all groups webservers/ # here we assign variables to webservers groups apt.yml # Each file will correspond to a role i.e. apt.yml nginx.yml # \"\" postgresql/ # here we assign variables to postgresql groups postgresql.yml # Each file will correspond to a role i.e. postgresql postgresql-password.yml # Encrypted password file plays/ ansible.cfg # Ansible.cfg file that holds all ansible config webservers.yml # playbook for webserver tier postgresql.yml # playbook for postgresql tier roles/ roles_requirements.yml# All the information about the roles external/ # All the roles that are in git or ansible galaxy # Roles that are in roles_requirements.yml file will be downloaded into this directory internal/ # All the roles that are not public scripts/ setup/ # All the setup files for updating roles and ansible dependencies WebApp Installation Instructions for Centos 7 \u00b6 Install Python Pip and dependencies on Centos 7 \u00b6 sudo yum install -y epel-release python python-pip sudo pip install flask flask-mysql If you come across a certification validation error while running the above command, please use the below command. sudo pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org flask flask-mysql Install MySQL Server on Centos 7 \u00b6 wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm sudo yum update sudo yum -y install mysql-server sudo service mysql start The complete playbook to get the same workin on CentOS is here: https://github.com/kodekloudhub/simple_web_application Executing Ansible Playbook \u00b6 Launching Ansible situational commands \u00b6 # To check the inventory file ansible-inventory --list -y # Test Connection ansible all -m ping -u root # Ask for Sudo password ansible all -m ping --ask-pass # Using a specific SSH private key and a user ansible -m ping hosts --private-key = ~/.ssh/keys/id_rsa -u centos # Check the disk usage of all servers ansible all -a \"df -h\" -u root # Check the time of `uptime` each host in a group **servers** ansible servers -a \"uptime\" -u root # Specify multiple hosts by separating their names with colons ansible server1:server2 -m ping -u root # Get system dat in json format of target ansible target1 -i myhosts -m setup --private-key = ~/.ssh/ansible-user -u root # Filter json output ansible target1 -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root ansible all -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root Launching Ansible Playbook situational commands \u00b6 ansible-playbook -i myhosts site.yml # Ask for Sudo password ansible-playbook myplaybook.yml --ask-become-pass # Or use the -K option ansible-playbook -i inventory myplaybook.yml -u sammy -K # Execute a play without making any changes to the remote servers ansible-playbook myplaybook.yml --list-tasks # List all hosts that would be affected by a play ansible-playbook myplaybook.yml --list-hosts ansible-playbook -i myhosts playbooks/atmo_playbook.yml --user atmouser # Passing variables which executing playbooks ansible-playbook playbooks/atmo_playbook.yml -e \"ATMOUSERNAME=atmouser\" ansible host01 -i myhosts -m copy -a \"src=test.txt dest=/tmp/\" ansible host01 -i myhosts -m file -a \"dest=/tmp/test mode=644 state=directory\" ansible host01 -i myhosts -m apt -a \"name=sudo state=latest\" ansible host01 -i myhosts -m shell -a \"echo $TERM \" ansible host01 -i myhosts -m command -a \"mkdir folder1\" # Run playbook on one host ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1\" # Run playbook on multiple hosts ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1,host2\" # Flush Ansible memory f pevious runs ansible-playbook playbooks/PLAYBOOK_NAME.yml --flush-cache # Dry run mode ansible-playbook playbooks/PLAYBOOK_NAME.yml --check # Starts playbook execution from an intermediate task, task name should match ansible-playbook myplaybook.yml --start-at-task = \"Set Up Nginx\" # Increasing debug verbosity ansible-playbook myplaybook.yml -v ansible-playbook myplaybook.yml -vvvv Launching Ansible Vault situational commands \u00b6 # Create new encrypted file, enter password ansible-vault encrypt credentials.yml # View the contents of encrypted file ansible-vault view credentials.yml # Edit the encrypted file ansible-vault edit credentials.yml # Permanently decrypt the file ansible-vault decrypt credentials.yml # Creating multiple vaults per env like dev, prod # create a new vault ID named dev that uses prompt as password source. # Prompt will ask you to enter a password, or a valid path to a password file. ansible-vault create --vault-id dev@prompt credentials_dev.yml ansible-vault create --vault-id prod@prompt credentials_prod.yml # Editing , Decrypting multiple vaults ansible-vault edit credentials_dev.yml --vault-id dev@prompt # Using Password file when using 3rd party automation ansible-vault create --vault-id dev@path/to/passfile credentials_dev.yml # Running playbooks with encrypted password ansible-playbook myplaybook.yml --ask-vault-pass # Passing password file ansible-playbook myplaybook.yml --vault-password-file my_vault_password.py # Passing multi env password ansible-playbook myplaybook.yml --vault-id dev@prompt ansible-playbook myplaybook.yml --vault-id dev@vault_password.py --vault-id test@prompt --vault-id ci@prompt Ansible Playbook Examples \u00b6 Install latest software version - hosts : host01 --- become : true tasks : - name : ensure latest sysstat is installed apt : name : sysstat state : latest Install software on all hosts --- - name : install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name : apache2 update_cache : yes state : latest Copy file only when it does not exists --- - hosts : host01 tasks : - stat : path : /home/ubuntu/folder1/something.j2 register : st - name : Template to copy file template : src : ./something.j2 dest : /home/ubuntu/folder1/something.j2 mode : '0644' when : st.stat.exists == False Add users using Loops # Looping - name : add several users user : name : \"{{ item }}\" state : present groups : \"developer\" with_items : - raj - david - john - lauren Using Looping with debug # show all the hosts in the inventory - debug : msg : \"{{ item }}\" with_items : - \"{{ groups['all'] }}\" # show all the hosts in the current play - debug : msg : \"{{ item }}\" with_items : - \"{{ play_hosts }}\" Conditionals # This Playbook will add Java Packages to different systems (handling Ubuntu/Debian OS) - name : debian | ubuntu | add java ppa repo apt_repository : repo=ppa:webupd8team/java state=present become : yes when : ansible_distribution == 'Ubuntu' - name : debian | ensure the webupd8 launchpad apt repository is present apt_repository : repo=\"{{ item }} http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" update_cache=yes state=present with_items : - deb - deb-src become : yes when : ansible_distribution == 'Debian' Full Play --- - name : install software hosts : host01 sudo : yes tasks : - name : Update and upgrade apt packages apt : upgrade : dist update_cache : yes cache_valid_time : 86400 - name : install software apt : name : \"{{item}}\" update_cache : yes state : installed with_items : - nginx - postgresql - postgresql-contrib - libpq-dev - python-psycopg2 - name : Ensure the Nginx service is running service : name : nginx state : started enabled : yes - name : Ensure the PostgreSQL service is running service : name : postgresql state : started enabled : yes #file: vars.yml --- var : 20 #file: playbook.yml --- - hosts : all vars_files : - vars.yml tasks : - debug : msg=\"Variable 'var' is set to {{ var }}\" #host variables - Variables are defined inline for individual host [ group1 ] host1 http_port=80 host2 http_port=303 #group variables - Variables are applied to entire group of hosts [ group1 : vars ] ntp_server= example.com proxy=proxy.example.com Full Play - Deploying an Nginx static site on Ubuntu # playbook.yml --- - hosts : all become : yes vars : server_name : \"{{ ansible_default_ipv4.address }}\" document_root : /var/www app_root : html_demo_site-main tasks : - name : Update apt cache and install Nginx apt : name : nginx state : latest update_cache : yes - name : Copy website files to the server's document root copy : src : \"{{ app_root }}\" dest : \"{{ document_root }}\" mode : preserve - name : Apply Nginx template template : src : files/nginx.conf.j2 dest : /etc/nginx/sites-available/default notify : Restart Nginx - name : Enable new site file : src : /etc/nginx/sites-available/default dest : /etc/nginx/sites-enabled/default state : link notify : Restart Nginx - name : Allow all access to tcp port 80 ufw : rule : allow port : '80' proto : tcp handlers : - name : Restart Nginx service : name : nginx state : restarted # Copy the static files and unzip to folder root curl -L https://github.com/do-community/html_demo_site/archive/refs/heads/main.zip -o html_demo.zip # files/nginx.conf.j2 server { listen 80; root {{ document_root }}/{{ app_root }}; index index.html index.htm; server_name {{ server_name }}; location / { default_type \"text/html\"; try_files $uri.html $uri $uri/ =404; } } # Executing the playbook with sammy user and prompting for password ansible-playbook -i inventory playbook.yml -u sammy -K Using ansible system variables \u00b6 Whenever you run Playbook, Ansible by default collects information (facts) about each host like host IP address, CPU type, disk space, operating system information etc. ansible host01 -i myhosts -m setup Consider you need the IP address of all the servers in you web group using 'group' variable { % for host in groups.web % } server {{ host.inventory_hostname }} {{ host.ansible_default_ipv4.address }} :8080 { % endfor % } Get a list of all the variables associated with the current host with the help of hostvars and inventory_hostname variables. --- - name : built-in variables hosts : all tasks : - debug : var=hostvars[inventory_hostname] Using register variables # register variable stores the output, after executing command module, in contents variable # stdout is used to access string content of register variable --- - name : check registered variable for emptiness hosts : all tasks : - name : list contents of the directory in the host command : ls /home/ubuntu register : contents - name : check dir is empty debug : msg=\"Directory is empty\" when : contents.stdout == \"\" - name : check dir has contents debug : msg=\"Directory is not empty\" when : contents.stdout != \"\" Variable Precedence => Command Line > Playbook > Facts > Roles CLI: While running the playbook in Command Line redefine the variable # Passing runtime values in plays ansible-playbook -i myhosts test.yml --extra-vars \"ansible_bios_version=Ansible\" Async \u00b6 async - How long to run the task poll - How frequently to check the task status. Default is 10 seconds async_status - Check status of an async task - name : Deploy a mysql DB hosts : db_server roles : - python - mysql_db - name : Deploy a Web Server hosts : web_server roles : - python - flask_web # Below task will run the async in parallel as poll is 0 and register the output - name : Monitor Web Application for 6 Minutes hosts : web_server command : /opt/monitor_webapp.py async : 360 poll : 0 register : webapp_result - name : Monitor Database for 6 Minutes hosts : db_server command : /opt/monitor_database.py async : 360 poll : 0 register : database_result # To avoid job from completing, async_status can be used to poll all async jobs have completed - name : Check status of async task async_status : jid={{ webapp_result.ansible_job_id }} register : job_result until : job_result.finished retries : 30 Deployment Strategy and Forks \u00b6 Serial - Default: All tasks are run after the previous once completes Free: Once the task completes in a host, it continues next execution without waiting for other hosts Batch: Based on serial, but takes action on multiple host (Rolling Updates) Forks: Deployment on multiple servers # Runs playbook on 2 servers at a time - name : Deploy a web application hosts : app_servers serial : 2 vars : db_name : employee_db db_user : db_user db_password : Passw0rd tasks : - name : Install dependencies - name : Install MySQL database - name : Start Mysql Service - name : Create Application Database - name : Create Application DB User - name : Install Python Flask dependencies - name : Copy web-server code - name : Start web-application # Deploy based on random rolling strategy name : Deploy a web application hosts : app_servers serial : - 2 - 3 - 5 # Deploy based on percentage name : Deploy a web application hosts : app_servers serial : \"20%\" # Deploy based on completion name : Deploy a web application hosts : app_servers strategy : free Error Handling \u00b6 Playbook Error Handling We would like Ansible to stop execution of the entire playbook if a single server was to fail. # To fail playbook on any failure and stop processing on all servers name : Deploy a web application hosts : app_servers any_errors_fatal : true # This will stop all processing # To avoid failure of playbook due to an insignificant task name : Deploy a web application hosts : app_servers tasks : - mail : to : devops@abc.com subject : Server Deployed! body : Webserver is live! ignore_errors : yes # Add this to ignore task failure - command : cat /var/log/server.log register : command_output failed_when : \"'ERROR' in command_output.stdout\" # Conditional failure of task Jinja2 Templating \u00b6 Templating: A process a generating dynamic content or expressions String Manipulation - Filters # Substitution The name is {{ my_name }} # Uppercase The name is {{ my_name | upper }} # Lowercase The name is {{ my_name | lower }} # Titlecase The name is {{ my_name | title }} # Replace The name is {{ my_name | replace(\"Bond\", \"Bourne\") }} # Default value The name is {{ first_name | default(\"James\") }} {{ my_name }} Filters - List and Set # Min {{ [ 1 , 2 , 3 ] | min }} => 1 # Max {{ [1,2,3] | min }} => 3 # Unique {{ [1,2,3,2] | unique }} => 1,2,3 # Union {{ [1,2,3,4] | union([4,5]) }} => 1,2,3,4,5 # Intersect {{ [1,2,3,4] | intersect([4,5]) }} => 4 {{ 100 | random }} => generates random number between 1 to 100 # Join {{ [\"The\",\"name\",\"is\",\"Bond\"] | join(\" \")}} => The name is Bond Filters - File {{ \"/etc/hosts\" | basename }} => hosts Lookups \u00b6 Lookups : To get data from another source on the system # Credentials File csv Hostname,Password web_server,Passw0rd db_server,Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('csvfile', 'web_server file=/tmp/credentials.csv delimiter=,') }}\" => Passw0rd # Credentials File ini [ web_server ] password = Passw0rd [ db_server ] password = Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('ini', 'password section=web_server file=/tmp/credentials.ini') }}\" => Passw0rd Tags \u00b6 Tags are names pinned on individual tasks, roles or an entire play, that allows you to run or skip parts of your Playbook. Tags can help you while testing certain parts of your Playbook. # tag.yml --- - name : Play1-install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name=apache2 update_cache=yes state=latest - name : displaying \"hello world\" debug : msg=\"hello world\" tags : - tag1 - name : Play2-install nginx hosts : all sudo : yes tags : - tag2 tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : debug module displays message in control machine debug : msg=\"have a good day\" tags : - mymessage - name : shell module displays message in host machine. shell : echo \"yet another task\" tags : - mymessage Executing above play using tags # displays the list of tasks in the Playbook ansible-playbook -i myhosts tag.yml --list-tasks # displays only tags in your Playbook ansible-playbook -i myhosts tag.yml --list-tags # executes only certain tasks which are tagged as tag1 and mymessage ansible-playbook -i myhosts tag.yml --tags \"tag1,mymessage\" Includes (Outdated after 2.0) \u00b6 Ansible gives you the flexibility of organizing your tasks through include keyword, that introduces more abstraction and make your Playbook more easily maintainable, reusable and powerful. --- - name : testing includes hosts : all sudo : yes tasks : - include : apache.yml - include : content.yml - include : create_folder.yml - include : content.yml - include : nginx.yml # apache.yml will not have hosts & tasks but nginx.yml as a separate play will have tasks and can run independently #nginx.yml --- - name : installing nginx hosts : all sudo : yes tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : displaying message debug : msg=\"yayy!! nginx installed\" Roles \u00b6 A Role is completely self contained or encapsulated and completely reusable # cat ansible.cfg [ defaults ] host_key_checking=False inventory = /etc/ansible/myhosts log_path = /home/scrapbook/tutorial/output.txt roles_path = /home/scrapbook/tutorial/roles/ # master-playbook.yml --- - name : my first role in ansible hosts : all sudo : yes roles : - sample_role - sample_role2 # sample_role/tasks/main.yml --- - include : nginx.yml - include : copy-template.yml - include : copy-static.yml # sample_role/tasks/nginx.yml --- - name : Installs nginx apt : pkg=nginx state=installed update_cache=true notify : - start nginx # sample_role/handlers/main.yml --- - name : start nginx service : name=nginx state=started # sample_role/tasks/copy-template.yml --- - name : sample template - x template : src : template-file.j2 dest : /home/ubuntu/copy-template-file.j2 with_items : var_x # sample_role/vars/main.yml var_x : - 'variable x' var_y : - 'variable y' # sample_role/tasks/copy-static.yml # Ensure some-file.txt is present under files folder of the role --- - name : Copy a file copy : src=some-file.txt dest=/home/ubuntu/file1.txt Executing the play # Let us run the master_playbook and check the output: ansible-playbook -i myhosts master_playbook.yml Ansible Galaxy \u00b6 ansible-galaxy is command line tool for scaffolding the creation of directory structure needed for organizing your code ansible-galaxy init sample_role ansible-galaxy useful commands \u00b6 Install a Role from Ansible Galaxy To use others role, visit https://galaxy.ansible.com/ and search for the role that will achieve your goal. Goal: Install Apache in the host machines. # Ensure that roles_path are defined in ansible.cfg for a role to successfully install. # Here, apache is role name and geerlingguy is name of the user in GitHub who created the role. ansible-galaxy install geerlingguy.apache # Forcefully Recreate Role ansible-galaxy init geerlingguy.apache --force # Listing Installed Roles ansible-galaxy list # Remove an Installed Role ansible-galaxy remove geerlingguy.apache Environment Variables \u00b6 Ansible recommends maintaining inventory file for each environment, instead of keeping all your hosts in a single inventory. Each environment directory has one inventory file (hosts) and group_vars directory.","title":"Ansible"},{"location":"learning/ansible/#introduction","text":"Ansible 101 Ansible Cheat Sheet Ansible Tips and Tricks DO Practise examples & Explaination & Tutorials YAML Spec Ref Card Full Application on Cloud","title":"Introduction"},{"location":"learning/ansible/#documentation-on-cmdline","text":"# System outputs the man page for debug module ansible-doc debug","title":"Documentation on cmdline"},{"location":"learning/ansible/#setup-server","text":"","title":"Setup Server"},{"location":"learning/ansible/#default-configuration","text":"ansible.cfg and hosts files are present inside /etc/ansible Testing ansible on Ubuntu WSL ansible localhost -m ping","title":"Default Configuration"},{"location":"learning/ansible/#enabling-ssh-on-the-vm","text":"If you need SSH enabled on the system, follow the below steps: Ensure the /etc/apt/sources.list file has been updated as per above Run the command: apt-get update Run the command: apt-get install openssh-server Run the command: service sshd start ssh-keygen -t rsa -C \"ansible\" #OR # Generate an SSH key pair for future connections to the VM instances (run the command exactly as it is): ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible-user -C ansible-user -P \"\" #Add the SSH private key to the ssh-agent: ssh-add ~/.ssh/ansible-user #Verify that the key was added to the ssh-agent: $ ssh-add -l","title":"Enabling SSH on the VM"},{"location":"learning/ansible/#access-vm-over-ssh","text":"ssh vagrant@127.0.0.1 -p 2222 -i ~/.ssh/insecure_private_key","title":"Access VM over SSH"},{"location":"learning/ansible/#copy-files-recursively-from-local-desktop-to-remote-server","text":"scp -r ./scripts vagrant@127.0.0.1:/home/vagrant -p 2222 -i ~/.ssh/insecure_private_key","title":"Copy files recursively from local desktop to remote server"},{"location":"learning/ansible/#target-docker-containers-for-ansible-controller","text":"The Docker file used to create the ubuntu-ssh-enabled Docker image is located here.","title":"Target Docker containers for Ansible controller"},{"location":"learning/ansible/#issues-installing-ansible-and-its-dependencies","text":"Once the Debian VM is up and running make the following changes to the /etc/apt/sources.list file to get the Ansible installation working right. deb http://security.debian.org/ jessie/updates main contrib deb-src http://security.debian.org/ jessie/updates main contrib deb http://ftp.debian.org/debian/ jessie-updates main contrib deb-src http://ftp.debian.org/debian/ jessie-updates main contrib deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main deb http://ftp.de.debian.org/debian sid main","title":"Issues installing Ansible and its dependencies"},{"location":"learning/ansible/#ansible-directory-structure-as-per-best-practises","text":"This is the directory layout of this repository with explanation. production.ini # inventory file for production stage development.ini # inventory file for development stage test.ini # inventory file for test stage vpass # ansible-vault password file # This file should not be committed into the repository # therefore file is ignored by git group_vars/ all/ # variables under this directory belongs all the groups apt.yml # ansible-apt role variable file for all groups webservers/ # here we assign variables to webservers groups apt.yml # Each file will correspond to a role i.e. apt.yml nginx.yml # \"\" postgresql/ # here we assign variables to postgresql groups postgresql.yml # Each file will correspond to a role i.e. postgresql postgresql-password.yml # Encrypted password file plays/ ansible.cfg # Ansible.cfg file that holds all ansible config webservers.yml # playbook for webserver tier postgresql.yml # playbook for postgresql tier roles/ roles_requirements.yml# All the information about the roles external/ # All the roles that are in git or ansible galaxy # Roles that are in roles_requirements.yml file will be downloaded into this directory internal/ # All the roles that are not public scripts/ setup/ # All the setup files for updating roles and ansible dependencies","title":"Ansible Directory Structure as per Best Practises"},{"location":"learning/ansible/#webapp--installation-instructions-for-centos-7","text":"","title":"WebApp  Installation Instructions for Centos 7"},{"location":"learning/ansible/#install-python-pip-and-dependencies-on-centos-7","text":"sudo yum install -y epel-release python python-pip sudo pip install flask flask-mysql If you come across a certification validation error while running the above command, please use the below command. sudo pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org flask flask-mysql","title":"Install Python Pip and dependencies on Centos 7"},{"location":"learning/ansible/#install-mysql-server-on-centos-7","text":"wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm sudo yum update sudo yum -y install mysql-server sudo service mysql start The complete playbook to get the same workin on CentOS is here: https://github.com/kodekloudhub/simple_web_application","title":"Install MySQL Server on Centos 7"},{"location":"learning/ansible/#executing-ansible-playbook","text":"","title":"Executing Ansible Playbook"},{"location":"learning/ansible/#launching-ansible-situational-commands","text":"# To check the inventory file ansible-inventory --list -y # Test Connection ansible all -m ping -u root # Ask for Sudo password ansible all -m ping --ask-pass # Using a specific SSH private key and a user ansible -m ping hosts --private-key = ~/.ssh/keys/id_rsa -u centos # Check the disk usage of all servers ansible all -a \"df -h\" -u root # Check the time of `uptime` each host in a group **servers** ansible servers -a \"uptime\" -u root # Specify multiple hosts by separating their names with colons ansible server1:server2 -m ping -u root # Get system dat in json format of target ansible target1 -i myhosts -m setup --private-key = ~/.ssh/ansible-user -u root # Filter json output ansible target1 -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root ansible all -i myhosts -m setup -a \"filter=*ipv4*\" --private-key = ~/.ssh/ansible-user -u root","title":"Launching Ansible situational commands"},{"location":"learning/ansible/#launching-ansible-playbook-situational-commands","text":"ansible-playbook -i myhosts site.yml # Ask for Sudo password ansible-playbook myplaybook.yml --ask-become-pass # Or use the -K option ansible-playbook -i inventory myplaybook.yml -u sammy -K # Execute a play without making any changes to the remote servers ansible-playbook myplaybook.yml --list-tasks # List all hosts that would be affected by a play ansible-playbook myplaybook.yml --list-hosts ansible-playbook -i myhosts playbooks/atmo_playbook.yml --user atmouser # Passing variables which executing playbooks ansible-playbook playbooks/atmo_playbook.yml -e \"ATMOUSERNAME=atmouser\" ansible host01 -i myhosts -m copy -a \"src=test.txt dest=/tmp/\" ansible host01 -i myhosts -m file -a \"dest=/tmp/test mode=644 state=directory\" ansible host01 -i myhosts -m apt -a \"name=sudo state=latest\" ansible host01 -i myhosts -m shell -a \"echo $TERM \" ansible host01 -i myhosts -m command -a \"mkdir folder1\" # Run playbook on one host ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1\" # Run playbook on multiple hosts ansible-playbook playbooks/PLAYBOOK_NAME.yml --limit \"host1,host2\" # Flush Ansible memory f pevious runs ansible-playbook playbooks/PLAYBOOK_NAME.yml --flush-cache # Dry run mode ansible-playbook playbooks/PLAYBOOK_NAME.yml --check # Starts playbook execution from an intermediate task, task name should match ansible-playbook myplaybook.yml --start-at-task = \"Set Up Nginx\" # Increasing debug verbosity ansible-playbook myplaybook.yml -v ansible-playbook myplaybook.yml -vvvv","title":"Launching Ansible Playbook situational commands"},{"location":"learning/ansible/#launching-ansible-vault-situational-commands","text":"# Create new encrypted file, enter password ansible-vault encrypt credentials.yml # View the contents of encrypted file ansible-vault view credentials.yml # Edit the encrypted file ansible-vault edit credentials.yml # Permanently decrypt the file ansible-vault decrypt credentials.yml # Creating multiple vaults per env like dev, prod # create a new vault ID named dev that uses prompt as password source. # Prompt will ask you to enter a password, or a valid path to a password file. ansible-vault create --vault-id dev@prompt credentials_dev.yml ansible-vault create --vault-id prod@prompt credentials_prod.yml # Editing , Decrypting multiple vaults ansible-vault edit credentials_dev.yml --vault-id dev@prompt # Using Password file when using 3rd party automation ansible-vault create --vault-id dev@path/to/passfile credentials_dev.yml # Running playbooks with encrypted password ansible-playbook myplaybook.yml --ask-vault-pass # Passing password file ansible-playbook myplaybook.yml --vault-password-file my_vault_password.py # Passing multi env password ansible-playbook myplaybook.yml --vault-id dev@prompt ansible-playbook myplaybook.yml --vault-id dev@vault_password.py --vault-id test@prompt --vault-id ci@prompt","title":"Launching Ansible Vault situational commands"},{"location":"learning/ansible/#ansible-playbook-examples","text":"Install latest software version - hosts : host01 --- become : true tasks : - name : ensure latest sysstat is installed apt : name : sysstat state : latest Install software on all hosts --- - name : install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name : apache2 update_cache : yes state : latest Copy file only when it does not exists --- - hosts : host01 tasks : - stat : path : /home/ubuntu/folder1/something.j2 register : st - name : Template to copy file template : src : ./something.j2 dest : /home/ubuntu/folder1/something.j2 mode : '0644' when : st.stat.exists == False Add users using Loops # Looping - name : add several users user : name : \"{{ item }}\" state : present groups : \"developer\" with_items : - raj - david - john - lauren Using Looping with debug # show all the hosts in the inventory - debug : msg : \"{{ item }}\" with_items : - \"{{ groups['all'] }}\" # show all the hosts in the current play - debug : msg : \"{{ item }}\" with_items : - \"{{ play_hosts }}\" Conditionals # This Playbook will add Java Packages to different systems (handling Ubuntu/Debian OS) - name : debian | ubuntu | add java ppa repo apt_repository : repo=ppa:webupd8team/java state=present become : yes when : ansible_distribution == 'Ubuntu' - name : debian | ensure the webupd8 launchpad apt repository is present apt_repository : repo=\"{{ item }} http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" update_cache=yes state=present with_items : - deb - deb-src become : yes when : ansible_distribution == 'Debian' Full Play --- - name : install software hosts : host01 sudo : yes tasks : - name : Update and upgrade apt packages apt : upgrade : dist update_cache : yes cache_valid_time : 86400 - name : install software apt : name : \"{{item}}\" update_cache : yes state : installed with_items : - nginx - postgresql - postgresql-contrib - libpq-dev - python-psycopg2 - name : Ensure the Nginx service is running service : name : nginx state : started enabled : yes - name : Ensure the PostgreSQL service is running service : name : postgresql state : started enabled : yes #file: vars.yml --- var : 20 #file: playbook.yml --- - hosts : all vars_files : - vars.yml tasks : - debug : msg=\"Variable 'var' is set to {{ var }}\" #host variables - Variables are defined inline for individual host [ group1 ] host1 http_port=80 host2 http_port=303 #group variables - Variables are applied to entire group of hosts [ group1 : vars ] ntp_server= example.com proxy=proxy.example.com Full Play - Deploying an Nginx static site on Ubuntu # playbook.yml --- - hosts : all become : yes vars : server_name : \"{{ ansible_default_ipv4.address }}\" document_root : /var/www app_root : html_demo_site-main tasks : - name : Update apt cache and install Nginx apt : name : nginx state : latest update_cache : yes - name : Copy website files to the server's document root copy : src : \"{{ app_root }}\" dest : \"{{ document_root }}\" mode : preserve - name : Apply Nginx template template : src : files/nginx.conf.j2 dest : /etc/nginx/sites-available/default notify : Restart Nginx - name : Enable new site file : src : /etc/nginx/sites-available/default dest : /etc/nginx/sites-enabled/default state : link notify : Restart Nginx - name : Allow all access to tcp port 80 ufw : rule : allow port : '80' proto : tcp handlers : - name : Restart Nginx service : name : nginx state : restarted # Copy the static files and unzip to folder root curl -L https://github.com/do-community/html_demo_site/archive/refs/heads/main.zip -o html_demo.zip # files/nginx.conf.j2 server { listen 80; root {{ document_root }}/{{ app_root }}; index index.html index.htm; server_name {{ server_name }}; location / { default_type \"text/html\"; try_files $uri.html $uri $uri/ =404; } } # Executing the playbook with sammy user and prompting for password ansible-playbook -i inventory playbook.yml -u sammy -K","title":"Ansible Playbook Examples"},{"location":"learning/ansible/#using-ansible-system-variables","text":"Whenever you run Playbook, Ansible by default collects information (facts) about each host like host IP address, CPU type, disk space, operating system information etc. ansible host01 -i myhosts -m setup Consider you need the IP address of all the servers in you web group using 'group' variable { % for host in groups.web % } server {{ host.inventory_hostname }} {{ host.ansible_default_ipv4.address }} :8080 { % endfor % } Get a list of all the variables associated with the current host with the help of hostvars and inventory_hostname variables. --- - name : built-in variables hosts : all tasks : - debug : var=hostvars[inventory_hostname] Using register variables # register variable stores the output, after executing command module, in contents variable # stdout is used to access string content of register variable --- - name : check registered variable for emptiness hosts : all tasks : - name : list contents of the directory in the host command : ls /home/ubuntu register : contents - name : check dir is empty debug : msg=\"Directory is empty\" when : contents.stdout == \"\" - name : check dir has contents debug : msg=\"Directory is not empty\" when : contents.stdout != \"\" Variable Precedence => Command Line > Playbook > Facts > Roles CLI: While running the playbook in Command Line redefine the variable # Passing runtime values in plays ansible-playbook -i myhosts test.yml --extra-vars \"ansible_bios_version=Ansible\"","title":"Using ansible system variables"},{"location":"learning/ansible/#async","text":"async - How long to run the task poll - How frequently to check the task status. Default is 10 seconds async_status - Check status of an async task - name : Deploy a mysql DB hosts : db_server roles : - python - mysql_db - name : Deploy a Web Server hosts : web_server roles : - python - flask_web # Below task will run the async in parallel as poll is 0 and register the output - name : Monitor Web Application for 6 Minutes hosts : web_server command : /opt/monitor_webapp.py async : 360 poll : 0 register : webapp_result - name : Monitor Database for 6 Minutes hosts : db_server command : /opt/monitor_database.py async : 360 poll : 0 register : database_result # To avoid job from completing, async_status can be used to poll all async jobs have completed - name : Check status of async task async_status : jid={{ webapp_result.ansible_job_id }} register : job_result until : job_result.finished retries : 30","title":"Async"},{"location":"learning/ansible/#deployment-strategy-and-forks","text":"Serial - Default: All tasks are run after the previous once completes Free: Once the task completes in a host, it continues next execution without waiting for other hosts Batch: Based on serial, but takes action on multiple host (Rolling Updates) Forks: Deployment on multiple servers # Runs playbook on 2 servers at a time - name : Deploy a web application hosts : app_servers serial : 2 vars : db_name : employee_db db_user : db_user db_password : Passw0rd tasks : - name : Install dependencies - name : Install MySQL database - name : Start Mysql Service - name : Create Application Database - name : Create Application DB User - name : Install Python Flask dependencies - name : Copy web-server code - name : Start web-application # Deploy based on random rolling strategy name : Deploy a web application hosts : app_servers serial : - 2 - 3 - 5 # Deploy based on percentage name : Deploy a web application hosts : app_servers serial : \"20%\" # Deploy based on completion name : Deploy a web application hosts : app_servers strategy : free","title":"Deployment Strategy and Forks"},{"location":"learning/ansible/#error-handling","text":"Playbook Error Handling We would like Ansible to stop execution of the entire playbook if a single server was to fail. # To fail playbook on any failure and stop processing on all servers name : Deploy a web application hosts : app_servers any_errors_fatal : true # This will stop all processing # To avoid failure of playbook due to an insignificant task name : Deploy a web application hosts : app_servers tasks : - mail : to : devops@abc.com subject : Server Deployed! body : Webserver is live! ignore_errors : yes # Add this to ignore task failure - command : cat /var/log/server.log register : command_output failed_when : \"'ERROR' in command_output.stdout\" # Conditional failure of task","title":"Error Handling"},{"location":"learning/ansible/#jinja2-templating","text":"Templating: A process a generating dynamic content or expressions String Manipulation - Filters # Substitution The name is {{ my_name }} # Uppercase The name is {{ my_name | upper }} # Lowercase The name is {{ my_name | lower }} # Titlecase The name is {{ my_name | title }} # Replace The name is {{ my_name | replace(\"Bond\", \"Bourne\") }} # Default value The name is {{ first_name | default(\"James\") }} {{ my_name }} Filters - List and Set # Min {{ [ 1 , 2 , 3 ] | min }} => 1 # Max {{ [1,2,3] | min }} => 3 # Unique {{ [1,2,3,2] | unique }} => 1,2,3 # Union {{ [1,2,3,4] | union([4,5]) }} => 1,2,3,4,5 # Intersect {{ [1,2,3,4] | intersect([4,5]) }} => 4 {{ 100 | random }} => generates random number between 1 to 100 # Join {{ [\"The\",\"name\",\"is\",\"Bond\"] | join(\" \")}} => The name is Bond Filters - File {{ \"/etc/hosts\" | basename }} => hosts","title":"Jinja2 Templating"},{"location":"learning/ansible/#lookups","text":"Lookups : To get data from another source on the system # Credentials File csv Hostname,Password web_server,Passw0rd db_server,Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('csvfile', 'web_server file=/tmp/credentials.csv delimiter=,') }}\" => Passw0rd # Credentials File ini [ web_server ] password = Passw0rd [ db_server ] password = Passw0rd # Format - Type of file, Value to Lookup, File to Lookup, Delimiter vars : ansible_ssh_pass : \"{{ lookup('ini', 'password section=web_server file=/tmp/credentials.ini') }}\" => Passw0rd","title":"Lookups"},{"location":"learning/ansible/#tags","text":"Tags are names pinned on individual tasks, roles or an entire play, that allows you to run or skip parts of your Playbook. Tags can help you while testing certain parts of your Playbook. # tag.yml --- - name : Play1-install apache hosts : all sudo : yes tasks : - name : install apache2 apt : name=apache2 update_cache=yes state=latest - name : displaying \"hello world\" debug : msg=\"hello world\" tags : - tag1 - name : Play2-install nginx hosts : all sudo : yes tags : - tag2 tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : debug module displays message in control machine debug : msg=\"have a good day\" tags : - mymessage - name : shell module displays message in host machine. shell : echo \"yet another task\" tags : - mymessage Executing above play using tags # displays the list of tasks in the Playbook ansible-playbook -i myhosts tag.yml --list-tasks # displays only tags in your Playbook ansible-playbook -i myhosts tag.yml --list-tags # executes only certain tasks which are tagged as tag1 and mymessage ansible-playbook -i myhosts tag.yml --tags \"tag1,mymessage\"","title":"Tags"},{"location":"learning/ansible/#includes-outdated-after-20","text":"Ansible gives you the flexibility of organizing your tasks through include keyword, that introduces more abstraction and make your Playbook more easily maintainable, reusable and powerful. --- - name : testing includes hosts : all sudo : yes tasks : - include : apache.yml - include : content.yml - include : create_folder.yml - include : content.yml - include : nginx.yml # apache.yml will not have hosts & tasks but nginx.yml as a separate play will have tasks and can run independently #nginx.yml --- - name : installing nginx hosts : all sudo : yes tasks : - name : install nginx apt : name=nginx update_cache=yes state=latest - name : displaying message debug : msg=\"yayy!! nginx installed\"","title":"Includes (Outdated after 2.0)"},{"location":"learning/ansible/#roles","text":"A Role is completely self contained or encapsulated and completely reusable # cat ansible.cfg [ defaults ] host_key_checking=False inventory = /etc/ansible/myhosts log_path = /home/scrapbook/tutorial/output.txt roles_path = /home/scrapbook/tutorial/roles/ # master-playbook.yml --- - name : my first role in ansible hosts : all sudo : yes roles : - sample_role - sample_role2 # sample_role/tasks/main.yml --- - include : nginx.yml - include : copy-template.yml - include : copy-static.yml # sample_role/tasks/nginx.yml --- - name : Installs nginx apt : pkg=nginx state=installed update_cache=true notify : - start nginx # sample_role/handlers/main.yml --- - name : start nginx service : name=nginx state=started # sample_role/tasks/copy-template.yml --- - name : sample template - x template : src : template-file.j2 dest : /home/ubuntu/copy-template-file.j2 with_items : var_x # sample_role/vars/main.yml var_x : - 'variable x' var_y : - 'variable y' # sample_role/tasks/copy-static.yml # Ensure some-file.txt is present under files folder of the role --- - name : Copy a file copy : src=some-file.txt dest=/home/ubuntu/file1.txt Executing the play # Let us run the master_playbook and check the output: ansible-playbook -i myhosts master_playbook.yml","title":"Roles"},{"location":"learning/ansible/#ansible-galaxy","text":"ansible-galaxy is command line tool for scaffolding the creation of directory structure needed for organizing your code ansible-galaxy init sample_role","title":"Ansible Galaxy"},{"location":"learning/ansible/#ansible-galaxy-useful-commands","text":"Install a Role from Ansible Galaxy To use others role, visit https://galaxy.ansible.com/ and search for the role that will achieve your goal. Goal: Install Apache in the host machines. # Ensure that roles_path are defined in ansible.cfg for a role to successfully install. # Here, apache is role name and geerlingguy is name of the user in GitHub who created the role. ansible-galaxy install geerlingguy.apache # Forcefully Recreate Role ansible-galaxy init geerlingguy.apache --force # Listing Installed Roles ansible-galaxy list # Remove an Installed Role ansible-galaxy remove geerlingguy.apache","title":"ansible-galaxy useful commands"},{"location":"learning/ansible/#environment-variables","text":"Ansible recommends maintaining inventory file for each environment, instead of keeping all your hosts in a single inventory. Each environment directory has one inventory file (hosts) and group_vars directory.","title":"Environment Variables"},{"location":"learning/docker/","text":"Running Docker and passing shell commands \u00b6 docker run -e TERM -e COLORTERM -it \u2013rm alpine sh -uec ' apk update apk add git zsh nano vim git clone \u2013depth=1 romkatv/powerlevel10k.git ~/powerlevel10k echo \"source ~/powerlevel10k/powerlevel10k.zsh-theme\" >>~/.zshrc cd ~/powerlevel10k exec zsh'","title":"Running Docker and passing shell commands"},{"location":"learning/docker/#running-docker-and-passing-shell-commands","text":"docker run -e TERM -e COLORTERM -it \u2013rm alpine sh -uec ' apk update apk add git zsh nano vim git clone \u2013depth=1 romkatv/powerlevel10k.git ~/powerlevel10k echo \"source ~/powerlevel10k/powerlevel10k.zsh-theme\" >>~/.zshrc cd ~/powerlevel10k exec zsh'","title":"Running Docker and passing shell commands"},{"location":"learning/git/","text":"Setting up multiple Github users have different ssh keys \u00b6 https://gist.github.com/oanhnn/80a89405ab9023894df7 It has a solution to test ssh configuration Create a new repository on the command line \u00b6 git init git add README.md git commit -m \"first commit\" git branch -M master git remote add origin leslieclif/dotfiles.git git push -u origin master Git Commands \u00b6 cd into git folder \u2192 ls -la \u2192 cd .git \u2192 ls -la Git help \u00b6 git help To quit help \u2192 q \u00b6 Best practise: Always do a pull before a push to merge changes from remote \u00b6 git pull origin master To git add and git commit for tracked files in a single comand use -a \u00b6 git commit -am \"Commit message\" Amend Commit message \u00b6 git commit \u2013amend \"New commit message\" Check for tracked files in git \u00b6 git ls-files Back out changes that have been commited, but not pushed to remote. Once unstaged, you can remove the changes using checkout \u00b6 git reset HEAD git checkout \u2013 Rename file-name. It also automatically stages the changes, so need to do git add \u00b6 git mv level3\u2013file.txt level3.txt If file is renamed outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A \u00b6 git add -A Moving files and staging the changes \u00b6 git mv level2.txt new-folder If file is moved outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A \u00b6 mv level2.txt .. git add -A file renamed in OS. But say, git has identifed unwanted files during git status and you dont want to add those files, then don't use -A, use -u \u00b6 Individually add the new renamed file first then update git \u00b6 git add level1.txt git add -u Delete files tracked by git \u00b6 git rm doomed.txt If file is delete outside git, it will delete and is not staged. To add and stage the deleted file use -A \u00b6 git add -A Git History \u00b6 git log To quit help \u2192 q \u00b6 Git history in one line \u00b6 git log \u2013oneline \u2013graph \u2013decorate Git history using duration \u00b6 git log \u2013since=\"3 days ago\" Show all user actions \u00b6 git reflog Show commit history \u2192 do git log get commit id \u00b6 git show #TODO: Get a git diff tool Show git config \u00b6 git config \u2013global \u2013list Compare with staging and current changes \u00b6 git diff Compare between current changes and remote last commit \u00b6 git diff HEAD Compare between staging and remote last commit \u00b6 git diff \u2013staged HEAD Compare file changes with staging and current changes \u00b6 git diff \u2013 Compare between commits (do git log to get commits) \u00b6 git diff Compare local and remote branches \u00b6 git diff master origin/master Compare local branches \u00b6 git diff master test-branch Branching \u00b6 List local and remote branches \u00b6 git branch -a Create new branch \u00b6 git branch Rename local branch \u00b6 git branch -m Delete a branch. Note: You have to be on another bracnh before you can delete the target branch \u00b6 git branch -d Create new branch and switch to it in single command \u00b6 git checkout -b Fash forward Merges \u2192 First switch to the target branches, do a git diff to review the changes. \u00b6 git merge Disable fast forward merge \u2192 Give tracing of merge by giving a custom merge message and also the commit history of the branch \u00b6 git merge \u2013no-ff Automatic merge \u00b6 git merge -m \" \" Merge Conflict and Resolution \u00b6 Inside the merging workspace incase of conflict, open the conflicting file in editor or the merge-diff tool. Resolve conflict and close the file. \u00b6 Rebase feature branch from master \u00b6 git checkout feature-branch git rebase master Abort rebase \u00b6 git rebase \u2013abort Rebase conflict resolution \u2192 Use merging tool to fix conflict, save and quit. Add file to git staging, then continue rebase \u00b6 git rebase \u2013continue Pull with Rebase (Rebase local master with remote master) \u00b6 git fetch origin master (non destructive merge which only updates references) git pull \u2013rebase origin master Stash \u00b6 git stash Stash + saving untracked files of git as well \u00b6 git stash -u Get the stash back to local \u00b6 git stash apply List the stash \u00b6 git stash list Drop the stash \u00b6 git stash drop Combination of apply and drop in one command. Brings the last saved state \u00b6 git stash pop Multiple Stashes \u00b6 git stash save \" \" Show any arbitary stash changes whithout popping. Do stash list first to get the stash ID \u00b6 git stash show stash@{1} Apply any arbitary stash changes. Do stash list first to get the stash ID \u00b6 git stash apply stash@{1} Drop any arbitary stash changes that was applied or not needed. \u00b6 git stash drop stash@{1} Stashing changes into a new branch. First see if you have any untracked files that also needs to be saved \u00b6 git stash -u git stash branch newbranchName Tagging \u00b6 Create Lightweight tag \u00b6 git tag mytag List existing tags \u00b6 git tag \u2013list Delete tag \u00b6 git tag \u2013delete mytag Create Annotated tags (It has additional information like release notes) \u00b6 git tag -a v1.0.0 -m \"Release 1.0.0\" Comparing tags \u00b6 git diff v1.0.0 v1.0.1 Tagging a specific commit ID \u00b6 git tag -a v0.0.9 -m \"Release 0.0.9\" Updating an existing tag with new commit id \u00b6 git tag -a v0.0.9 -f -m \"Correct Release 0.0.9\" Pushing tags to remote \u00b6 git push origin v1.0.0 Pushing all local tags to remote \u00b6 git push origin master \u2013tags Deleting tags in remote (puting :before tag name will delete it from remote). Does not delete tag from local \u00b6 git push origin :v0.0.9 Reset HEAD position \u00b6 git reset Using Stash and Branch combination \u00b6 First stash the changes (WIP) in one brabch, then checkout a new test branch and then pop the changes into this test branch \u00b6 git stash git checkout -b test git stash pop Cherry Pick (Hot Fix scenario) \u00b6 git cherry-pick","title":"Git"},{"location":"learning/git/#setting-up-multiple-github-users-have-different-ssh-keys","text":"https://gist.github.com/oanhnn/80a89405ab9023894df7 It has a solution to test ssh configuration","title":"Setting up multiple Github users have different ssh keys"},{"location":"learning/git/#create-a-new-repository-on-the-command-line","text":"git init git add README.md git commit -m \"first commit\" git branch -M master git remote add origin leslieclif/dotfiles.git git push -u origin master","title":"Create a new repository on the command line"},{"location":"learning/git/#git-commands","text":"cd into git folder \u2192 ls -la \u2192 cd .git \u2192 ls -la","title":"Git Commands"},{"location":"learning/git/#git-help","text":"git help","title":"Git help"},{"location":"learning/git/#to-quit-help----q","text":"","title":"To quit help --&gt; q"},{"location":"learning/git/#best-practise-always-do-a-pull-before-a-push-to-merge-changes-from-remote","text":"git pull origin master","title":"Best practise: Always do a pull before a push to merge changes from remote"},{"location":"learning/git/#to-git-add-and-git-commit-for-tracked-files-in-a-single-comand-use--a","text":"git commit -am \"Commit message\"","title":"To git add and git commit for tracked files in a single comand use -a"},{"location":"learning/git/#amend-commit-message","text":"git commit \u2013amend \"New commit message\"","title":"Amend Commit message"},{"location":"learning/git/#check-for-tracked-files-in-git","text":"git ls-files","title":"Check for tracked files in git"},{"location":"learning/git/#back-out-changes-that-have-been-commited-but-not-pushed-to-remote-once-unstaged-you-can-remove-the-changes-using-checkout","text":"git reset HEAD git checkout \u2013","title":"Back out changes that have been commited, but not pushed to remote. Once unstaged, you can remove the changes using checkout"},{"location":"learning/git/#rename-file-name-it-also-automatically-stages-the-changes-so-need-to-do-git-add","text":"git mv level3\u2013file.txt level3.txt","title":"Rename file-name. It also automatically stages the changes, so need to do git add"},{"location":"learning/git/#if-file-is-renamed-outside-git-it-will-delete-and-add-the-files-in-git-history-and-is-not-staged-to-add-new-file-and-stage-the-renamed-files-use--a","text":"git add -A","title":"If file is renamed outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A"},{"location":"learning/git/#moving-files-and-staging-the-changes","text":"git mv level2.txt new-folder","title":"Moving files and staging the changes"},{"location":"learning/git/#if-file-is-moved-outside-git-it-will-delete-and-add-the-files-in-git-history-and-is-not-staged-to-add-new-file-and-stage-the-renamed-files-use--a","text":"mv level2.txt .. git add -A","title":"If file is moved outside git, it will delete and add the files in git history and is not staged. To add new file and stage the renamed files use -A"},{"location":"learning/git/#file-renamed-in-os-but-say-git-has-identifed-unwanted-files-during-git-status-and-you-dont-want-to-add-those-files-then-dont-use--a-use--u","text":"","title":"file renamed in OS. But say, git has identifed unwanted files during git status and you dont want to add those files, then don't use -A, use -u"},{"location":"learning/git/#individually-add-the-new-renamed-file-first-then-update-git","text":"git add level1.txt git add -u","title":"Individually add the new renamed file first then update git"},{"location":"learning/git/#delete-files-tracked-by-git","text":"git rm doomed.txt","title":"Delete files tracked by git"},{"location":"learning/git/#if-file-is-delete-outside-git-it-will-delete-and-is-not-staged-to-add-and-stage-the-deleted-file--use--a","text":"git add -A","title":"If file is delete outside git, it will delete and is not staged. To add and stage the deleted file  use -A"},{"location":"learning/git/#git-history","text":"git log","title":"Git History"},{"location":"learning/git/#to-quit-help----q_1","text":"","title":"To quit help --&gt; q"},{"location":"learning/git/#git-history-in-one-line","text":"git log \u2013oneline \u2013graph \u2013decorate","title":"Git history in one line"},{"location":"learning/git/#git-history-using-duration","text":"git log \u2013since=\"3 days ago\"","title":"Git history using duration"},{"location":"learning/git/#show-all-user-actions","text":"git reflog","title":"Show all user actions"},{"location":"learning/git/#show-commit-history----do-git-log-get-commit-id","text":"git show #TODO: Get a git diff tool","title":"Show commit history --&gt; do git log get commit id"},{"location":"learning/git/#show-git-config","text":"git config \u2013global \u2013list","title":"Show git config"},{"location":"learning/git/#compare-with-staging-and-current-changes","text":"git diff","title":"Compare with staging and current changes"},{"location":"learning/git/#compare-between-current-changes-and-remote-last-commit","text":"git diff HEAD","title":"Compare between current changes and remote last commit"},{"location":"learning/git/#compare-between-staging-and-remote-last-commit","text":"git diff \u2013staged HEAD","title":"Compare between staging and remote last commit"},{"location":"learning/git/#compare-file-changes-with-staging-and-current-changes","text":"git diff \u2013","title":"Compare file changes with staging and current changes"},{"location":"learning/git/#compare-between-commits-do-git-log-to-get-commits","text":"git diff","title":"Compare between commits (do git log to get commits)"},{"location":"learning/git/#compare-local-and-remote-branches","text":"git diff master origin/master","title":"Compare local and remote branches"},{"location":"learning/git/#compare-local-branches","text":"git diff master test-branch","title":"Compare local branches"},{"location":"learning/git/#branching","text":"","title":"Branching"},{"location":"learning/git/#list-local-and-remote-branches","text":"git branch -a","title":"List local and remote branches"},{"location":"learning/git/#create-new-branch","text":"git branch","title":"Create new branch"},{"location":"learning/git/#rename-local-branch","text":"git branch -m","title":"Rename local branch"},{"location":"learning/git/#delete-a-branch-note-you-have-to-be-on-another-bracnh-before-you-can-delete-the-target-branch","text":"git branch -d","title":"Delete a branch. Note: You have to be on another bracnh before you can delete the target branch"},{"location":"learning/git/#create-new-branch-and-switch-to-it-in-single-command","text":"git checkout -b","title":"Create new branch and switch to it in single command"},{"location":"learning/git/#fash-forward-merges-----first-switch-to-the-target-branches-do-a-git-diff-to-review-the-changes","text":"git merge","title":"Fash forward Merges  --&gt; First switch to the target branches, do a git diff to review the changes."},{"location":"learning/git/#disable-fast-forward-merge----give-tracing-of-merge-by-giving-a-custom-merge-message-and-also-the-commit-history-of-the-branch","text":"git merge \u2013no-ff","title":"Disable fast forward merge --&gt; Give tracing of merge by giving a custom merge message and also the commit history of the branch"},{"location":"learning/git/#automatic-merge","text":"git merge -m \" \"","title":"Automatic merge"},{"location":"learning/git/#merge-conflict-and-resolution","text":"","title":"Merge Conflict and Resolution"},{"location":"learning/git/#inside-the-merging-workspace-incase-of-conflict-open-the-conflicting-file-in-editor-or-the-merge-diff-tool-resolve-conflict-and-close-the-file","text":"","title":"Inside the merging workspace incase of conflict, open the conflicting file in editor or the merge-diff tool. Resolve conflict and close the file."},{"location":"learning/git/#rebase-feature-branch-from-master","text":"git checkout feature-branch git rebase master","title":"Rebase feature branch from master"},{"location":"learning/git/#abort-rebase","text":"git rebase \u2013abort","title":"Abort rebase"},{"location":"learning/git/#rebase-conflict-resolution----use-merging-tool-to-fix-conflict-save-and-quit-add-file-to-git-staging-then-continue-rebase","text":"git rebase \u2013continue","title":"Rebase conflict resolution --&gt; Use merging tool to fix conflict, save and quit. Add file to git staging, then continue rebase"},{"location":"learning/git/#pull-with-rebase-rebase-local-master-with-remote-master","text":"git fetch origin master (non destructive merge which only updates references) git pull \u2013rebase origin master","title":"Pull with Rebase (Rebase local master with remote master)"},{"location":"learning/git/#stash","text":"git stash","title":"Stash"},{"location":"learning/git/#stash--saving-untracked-files-of-git-as-well","text":"git stash -u","title":"Stash + saving untracked files of git as well"},{"location":"learning/git/#get-the-stash-back-to-local","text":"git stash apply","title":"Get the stash back to local"},{"location":"learning/git/#list-the-stash","text":"git stash list","title":"List the stash"},{"location":"learning/git/#drop-the-stash","text":"git stash drop","title":"Drop the stash"},{"location":"learning/git/#combination-of-apply-and-drop-in-one-command-brings-the-last-saved-state","text":"git stash pop","title":"Combination of apply and drop in one command. Brings the last saved state"},{"location":"learning/git/#multiple-stashes","text":"git stash save \" \"","title":"Multiple Stashes"},{"location":"learning/git/#show-any-arbitary-stash-changes-whithout-popping-do-stash-list-first-to-get-the-stash-id","text":"git stash show stash@{1}","title":"Show any arbitary stash changes whithout popping. Do stash list first to get the stash ID"},{"location":"learning/git/#apply-any-arbitary-stash-changes-do-stash-list-first-to-get-the-stash-id","text":"git stash apply stash@{1}","title":"Apply any arbitary stash changes. Do stash list first to get the stash ID"},{"location":"learning/git/#drop-any-arbitary-stash-changes-that-was-applied-or-not-needed","text":"git stash drop stash@{1}","title":"Drop any arbitary stash changes that was applied or not needed."},{"location":"learning/git/#stashing-changes-into-a-new-branch-first-see-if-you-have-any-untracked-files-that-also-needs-to-be-saved","text":"git stash -u git stash branch newbranchName","title":"Stashing changes into a new branch. First see if you have any untracked files that also needs to be saved"},{"location":"learning/git/#tagging","text":"","title":"Tagging"},{"location":"learning/git/#create-lightweight-tag","text":"git tag mytag","title":"Create Lightweight tag"},{"location":"learning/git/#list-existing-tags","text":"git tag \u2013list","title":"List existing tags"},{"location":"learning/git/#delete-tag","text":"git tag \u2013delete mytag","title":"Delete tag"},{"location":"learning/git/#create-annotated-tags-it-has-additional-information-like-release-notes","text":"git tag -a v1.0.0 -m \"Release 1.0.0\"","title":"Create Annotated tags (It has additional information like release notes)"},{"location":"learning/git/#comparing-tags","text":"git diff v1.0.0 v1.0.1","title":"Comparing tags"},{"location":"learning/git/#tagging-a-specific-commit-id","text":"git tag -a v0.0.9 -m \"Release 0.0.9\"","title":"Tagging a specific commit ID"},{"location":"learning/git/#updating-an-existing-tag-with-new-commit-id","text":"git tag -a v0.0.9 -f -m \"Correct Release 0.0.9\"","title":"Updating an existing tag with new commit id"},{"location":"learning/git/#pushing-tags-to-remote","text":"git push origin v1.0.0","title":"Pushing tags to remote"},{"location":"learning/git/#pushing-all-local-tags-to-remote","text":"git push origin master \u2013tags","title":"Pushing all local tags to remote"},{"location":"learning/git/#deleting-tags-in-remote-puting-before-tag-name-will-delete-it-from-remote-does-not-delete-tag-from-local","text":"git push origin :v0.0.9","title":"Deleting tags in remote (puting :before tag name will delete it from remote). Does not delete tag from local"},{"location":"learning/git/#reset-head-position","text":"git reset","title":"Reset HEAD position"},{"location":"learning/git/#using-stash-and-branch-combination","text":"","title":"Using Stash and Branch combination"},{"location":"learning/git/#first-stash-the-changes-wip-in-one-brabch-then-checkout-a-new-test-branch-and-then-pop-the-changes-into-this-test-branch","text":"git stash git checkout -b test git stash pop","title":"First stash the changes (WIP) in one brabch, then checkout a new test branch and then pop the changes into this test branch"},{"location":"learning/git/#cherry-pick-hot-fix-scenario","text":"git cherry-pick","title":"Cherry Pick (Hot Fix scenario)"},{"location":"learning/linux/","text":"Introduction \u00b6 Understanding Linux Filesystem Templates folder CronTab Guru Basic commands \u00b6 Important Commands Terminal Ctrl+Alt+T # Open the terminal Ctrl+D # Close the terminal exit # Close the terminal Ctrl + L # Clear the screen, but will keep the current command Ctrl + Shift + # Increases font size of the terminal Utility cal # Calendar current month cal -3 # Current -1, Current , Current +1 month cal 5 1967 # Format is (Month and Year). Gives May 1967 date # Current date in BST (default) date -u # Current date in UTC date --date \u201c30 days\u201d # Gives current date + 30 days (future date) date --date \u201c30 days ago\u201d # Gives current date \u2013 30 days (past date) which echo # Shows where the command is stored in PATH hostname -I # Gives IP address echo $? # Gives the output 0/1 value stored after a command is run wc \u2013l file1 # Gives line count in file1 wc file1 # Give word count in file1 History history # List all the commands executed !! # Run the previous command !50 # Run the command that is on line 50 of history output history \u2013c ; history \u2013w ; # Clears history and writes back to the file Ctrl + r # reverse searches for your input. Press Esc to edit the matched command man Using the Manual There are 8 sections in the manual. Important are 1, 5 and 8 sections man \u2013k <search term> # Search the manual for pages matching <search term>. man -k tmux # example of searching for tmux in the manual pages man -k \"list directory contents\" # Double quote seraches complete words man 1 tmux # Opens section 1 of tmux manual page, 1 is default and can be ignored man ls # Shows section 1 of ls command help cd # Shows the help pages if man pages are not present Redirection of Streams echo \"Hello\" 1 > output.txt # Standard output is redirected to output.txt echo \"Hello\" > output.txt # Standard output is default echo \"World\" 1 >> output.txt # Standard output is appended to output.txt echo \"Error\" 2 > error.txt # Standard error is redirected to error.txt cat -k bla 2 >> error.txt # Program error is redirected and appended to error.txt echo \"Hello World\" 1 >> output.txt 2 >> error.txt # Use both std output and error cat 0 < input.txt # Standard input is read from a file and sent to cat command cat < input.txt # Standard input is default cat 0 < input.txt 1 >> output.txt 2 >> error.txt # Use all 3 data streams Redirection to Terminals tty # Current terminal connected to Linux, gives path cat < input.txt > /dev/pts/1 # In another terminal, Standard input is read from a file and sent to tty 1 terminal Ctrl + Alt + F1 / chvt 1 # Goes to physical terminal with no graphics. Similarly you can change to 2 to 6 tty terminals. Ctrl + Alt + F7 / chvt 7 # Comes back to Graphical terminal Piping date | cut --delimiter \" \" --fields 1 # Output of date is input to cut command - Tee command - Used to store intermediate output in a file and then stream passed horizontally through the pipeline - tee command takes a snapshot of the standard output and then passes it along date > date.txt | cut --delimiter \" \" --fields 1 # Output will not work and date will only be stored in file and not passed to cut command date | tee date.txt | cut --delimiter \" \" --fields 1 # Output of date is first stored in file, then passed to cut command for display to Standard Output date | tee date.txt | cut --delimiter \" \" --fields 1 | tee today.txt cat file1.txt file2.txt | tee unsorted.txt | sort -r > reversed.txt # Output chaining and storing intermediate data in files - XARGS command (Powerful pipeline command) - Allows piped data into command line arguments - date | echo # Output of date is passed to echo, but echo doesn't accept standard input, only commandline arguments date | xargs echo # xargs will convert standard output into command line arguments date | cut --delimiter \" \" --fields 1 | xargs echo # Prints the day of the week Alias Used to store reusable scripts in .bash_aliases file can be used in scripts alias # Shows all the alias setup for the user # Store an alias in the `.bash_aliases` file alias calmagic = 'xargs cal -A 1 -B 1 > /home/leslie/calOutput.txt' # In the terminal use if in a pipe command, STDOUT will be stored in a file echo \"12 2021\" | calmagic File System Navigation # File Listing pwd # Prints absolute path of current working directory(CWD) ls \u2013l # Long list of CWD ls \u2013a # Shows all files including hidden ls -F # Shows directories as ending with / along with other files stat <filename> # Detailed file information file <filename> # File type ls -ld # Detailed folder information # Change Directories cd - # Helps to switch directories. Like a Toggle (Alt + Tab) in windows cd / cd ~ # User Home directory from anywhere cd .. # Back to parent directory of CWD Wildcards Wildcards and How to use - The star wildcard has the broadest meaning of any of the wildcards, as it can represent zero characters, all single characters or any string. - The question mark (?) is used as a wildcard character in shell commands to represent exactly one character, which can be any single character. - The square wildcard can represent any of the characters enclosed in the brackets. ls *.txt # Matches all txt files ls ???.txt # Matches all 3 letter txt files ls file [ 123 ] .txt # Matches all files ending with 1 to 3 ls file [ A-Z ] .txt # Matches all files ending with A to Z ls file [ 0 -9 ][ A-Z ] .txt # Matches all files ending with 0A to 9Z File and Folders # Create Operations touch file1 # Creates a new file1 echo \"Hello\" > hello.txt # Creates and writes using redirection # -p is parent directory which is data and inside that 2 directories called sales & mkt is created mkdir \u2013p /data/ { sales,mkt } # Brace exapansion will allow to create folders. Sequence can be expressed as .. mkdir -p /tmp/ { jan,feb,mar } _ { 2020 ..2023 } # Brace expansion for files, it will create 10 files inside each folder touch { jan,feb,mar } _ { 2020 ..2023 } /file { 1 ..10 } # Delete Operations rm file1 # Deletes file1 rm *.txt # Deletes all txt files # Deletes all files and folders inside the main folder and the main folder as well # CAUTION: Use the recursive option with care rm -r /tmp/ { jan,feb,mar } _ { 2020 ..2023 } / # Deletes only empty directories rmdir /tmp/ # Skips folders which have files # Copy Operations cp /data/sales/file1 /data/mkt/file1 cp /data/sales/* . # Copy all files to CWD cp -r /data/sales /data/backup # Copy folder to backup folder # Move and Rename Operations mv file1 file2 # Rename file in the same folder mv /data/mkt/ /data/hr # Rename folder, Note the slash after first folder mv /data/sales/* /tmp/backup/ # Move files to new location mv /data/mkt/ /tmp/newFolder # Move and rename the folder Nano - Editing M Key can be Alt or Cmd depending on keyboard layout Enable spell checking on nano by editing /etc/nanorc and uncomment set speller in the file. Ctrl + O # Write data out to file Ctrl + R # Copy contents of one file into another Ctrl + K # Cuts entire line, also used as a delete Alt + 6 # Copy entire line Ctrl + U # Paste the line Ctrl + T # Spell check the file Alt + U # Undo changes Alt + E # Redo changes # File operations in vi > filename # Empties an existing file :x # Saves file changes instead of :wq Search Files find ~/projects # Find matches of files and folders from projects and below find . # Find from CWD and below find . -maxdepth 1 # Find from CWD and one level below find . -type f # Find files only find . -type d # Find folder only find . -maxdepth 1 -type d # Find folder only and one level below find . -name \"*.txt\" # Find files ending with matching patterns find . -maxdepth 3 -iname \"*.TXT\" # Find files with case insensitive matching patterns find . -type f -size +100k # Find files greater than 100 Kb # Find files greater than 100 Kb AND less than 5 Mb and count them find . -type f -size +100k -size -5M | wc -l # Find files less than 100 Kb OR greater than 5 Mb and count them find . -type f -size -100k -o -size +5M | wc -l - Find and Execute commands # Find and copy files to backup folder. `\\;` denotes end of exec command find . -type f -size +100k -size +5M -exec cp {} ~/Desktop/backup \\; ### # Find file called needle.txt inside haystack folder ### # Create 100 folders and inside each folder 100 files mkdir -p haystack/folder { 1 ..100 } touch haystack/folder { 1 ..100 } /file { 1 ..100 } # Create file in one random folder touch haystack/folder $( shuf -i 1 -100 -n 1 ) /needle.txt ### # Finding the file using name find haystack/ -type f -name \"needle.txt\" # Move the file to haystack folder find haystack/ -type f -name \"needle.txt\" -exec mv {} ~/tmp/haystack \\; ### View/Read File Contents cat cat file1 file2 > file3 # Concatenate 2 files and write into file3 cat \u2013vet file3 # displays special characters in the file e.g. EOL as $. Useful if sh files are created in windows tac - Flips the file contents vertically tac file3 # Reads the file in reverse rev - Reverses the contents of each line rev file3 # Reads the line in reverse less - Allows to page through big files less file3 # Shows one page at a time. Use Arrow keys to scroll # Output of find piped to less command for scrolling find . -type f -name \"*.txt\" | less head - Shows limited lines from top of output cat file3 | head -n 3 # Shows first 3 lines tail - Shows limited lines from bottom of output cat file3 | tail -n 3 # Shows last 3 lines tail \u2013f /var/log/messages # follows the file and continuously shows the 10 lines Sort sort words.txt > sorted.txt # Sorts in Asc order and redirects to sorted.txt sort -r word.txt > reverse.txt # Sorts in Des order sort -n numbers.txt # Sorts in Asc numeric order based on digit placement sort -nr numbers.txt # Sorts in Des numeric order based on digit placement sort -u numbers0-9.txt # Sorts and shows only unique values - Sorting data in tabular format # Sort on the basis of file size (5th column and its numeric) ls -l /etc | head -n 20 | sort -k 5n # Reverse (r) the output showing largest files first ls -l /etc | head -n 20 | sort -k 5nr # Sort on the basis of largest file size in human readable format ls -lh /etc | head -n 20 | sort -k 5hr # Sort on the basis of month ls -lh /etc | head -n 20 | sort -k 6M Search data - grep grep is case-sensitive search command # grep <search term> file-name grep e words.txt # Shows matching lines as STDOUT grep -c e words.txt # Counts the matching lines # Search in case insensitive manner grep -i gadsby gadsby_manuscript.txt # Search strings using quotes grep -ci \"our boys\" gadsby_manuscript.txt # Invert the search grep -v \"our boys\" gadsby_manuscript.txt # Searches for server and not servers. \\b is the word boundary grep \u2018 \\b server \\b \u2019/etc/ntp.conf # Searches for server beginning in the line. \\b is the word boundary grep \u2018^server \\b \u2019/etc/ntp.conf Filter data using grep ls -lF / | grep opt # Shows details for opt folder only ls -F /etc | grep -v / # Shows only files in etc folder - Remove Commented and Blank Lines # Empty lines can be shown as ^$. \u2013v reverses our search and \u2013e allows more than one expression. O/p is sent to std o/p grep \u2013ve \u2018^#\u2019 \u2013ve\u2019^$\u2019 /etc/ntp.conf # -v ^# says I don\u2019t want to see lines starting with #. ^$ says I don\u2019t want to see lines that begin with EOL marker Archival and Compression Two step process: Create the tar ball, then compress the tar Compression tool comparison # Create the tar ball tar -cvf backup.tar file [ 1 -3 ] .txt # Create, Verbose, Files to archive tar -tf backup.tar # Test for tar file without unzipping # Compress the tar ball ## 3 compression tools - gzip -> bzip2 -> xz (Compression and time increases from left to right) gzip backup.tar # Compresses the tar ball and adds .gz extension to tar ball gunzip backup.tar.gz # Decompress the gzip ### bzip2 backup.tar # Smaller file size than gzip and adds .bz2 extension to tar ball bunzip2 backup.tar.bz2 # Decompress the bzip. Best used for larger file sizes # Open the tar ball contents tar -xvf backup.tar # Extract, Verbose, Files to unarchive ### Create tar and compress in single command # Adding the z option for gzip and renaming the tar as .gz tar -cvzf backup.tar.gz file [ 1 -3 ] .txt tar -xvzf backup.tar.gz # Adding the j option for bzip2 and renaming the tar as .bz2 tar -cvjf backup.tar.bz2 file [ 1 -3 ] .txt tar -xvjf backup.tar.bz2 ### BASH #!/bin/bash # First line in the script \"SHEBANG\" tells type of script ### # To create an executable script, create a `bin` folder in your home. # Move all utility shell scripts to bin. Also remove .sh file extenstions # Make the file as executable `chmod +x data_backup` # Add the `~/bin` to the PATH variable # Edit `.bashrc` with PATH=\"$PATH:$HOME/bin\" # Now all scripts in bin folder are executable from command line ### # Set and unset variables export $VARIABLE # Sets the variable unset VARIABLE # Removes the variable, NOTE \u2013 No $ in variable Cron Scheduling crontab -e < select editor> # Opens the template crontab # Multiple options for each column of crontab using comma. # SPACE is used to delimit the columns of crontab # */<value> can divide the time intervals ### # min hours \"day of month\" month \"day of week (0-6)\" ### * * * * * bash ~/data_backup.sh Package Management apt-cache search docx # Searches apt for programs that can work with MS Word apt-cache show <package> | less # Gives software information # Apt cache information resides in /var/lib/apt/lists sudo apt-get update # Updates the apt lists sudo apt-get upgrade # Upgraded to the latest software versions from the list sudo apt-get install <pkg-name> # Install package sudo apt-get purge <pkg-name> # Remove & uninstall package. Recommended approach sudo apt-get autoremove # Removes any installed package dependecies # Package compressed archives are stored in `/var/cache/apt/archives` sudo apt-get clean # Removes all package compressed acrhives sudo apt-get autoclean # Removes only package compressed acrhives that cannot be downloaded - Source Code for apps OS uname # Shows kernal uname -o # Shows OS uname -m # Shows computer architecture x86_64 (64 bit), x86 (32 bit) lsb_release -a # Distro version Misc fdisk -l # Gives device wise memory details free / free -m # Gives amount of free memory lsblk # Lists all partitions swapon \u2013s # List all swap files ps # Process id of the current bash shell shutdown \u2013h now / poweroff / init 0 # Power downs the system restart / init 6 / reboot # Restarts the system shutdown \u2013r + 1 \u201cWe are restarting\u201d # Restarts the system give all logged in users 1 min to shut down all process su - # Login to root id / id bob # Shows the current user and group id sudo -i # Interactive shell for password of the current user, to get elevated access ssh localhost # ssh connection to same server. Type exit or Ctrl + D to logout of ssh. who / w # Gives the list of terminals that are connected and who has logged on to the server Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Tips \u00b6 Alt + F2 Gives the run command terminal and then type gnome-system-monitor is like task Manger in windows. Gives graphical overview of the system and can kill processes. Putting & after any commands runs it in the background. Run jobs to see all the background running jobs. Type fg to bring the background running jobs to the foreground. Ctrl + C to cancel the job then. !<anycharacter> will sreach for the last command in history starting with that character !?etc executes the last command that contains etc Filesystem \u00b6 Creating Partitions \u00b6 fdisk or gdisk utility to partition. If Mountpoint is /, that is primary partition. Creating Filesystems \u00b6 mkfs.ext4 \u2013b 4096 /dev/sdb1 # Creates 4MB block size file system mkfs.xfs \u2013b size = 64k /dev/sdb2 # Creates 64k block size file system. Xfs is specialized filesystem Mounting Data \u00b6 mkdir \u2013p /data/ { sales,mkt } mount /dev/sdb1 /data/sales # Mounts device to data/sales directory mount /dev/sdb2 /data/mkt Unmounting Data \u00b6 umount /dev/sdb1 or umount /dev/sdb { 1 ,2 } # Unmounts both the devices Virtual Memory or Swap Filesystem \u00b6 They are temporary space requirements Virtual memory in Linux can be a Disk Partition or Swap file. Use gdisk to create swap filesystem. Option L and then hex code 8200. To make the swap filesystem permanent, make an entry in /etc/fstab file, so changes are persistent even after system reboot. partprobe /dev/sdb # Sync saved partition in memory. Or it requires system reboot mkswap /dev/sdb3 # Select the right swap device to create the filesystem swapon /dev/sdb3 # Mount the filesystem Troubleshooting Linux filesystem \u00b6 df \u2013hT # list all filesystem with space details du \u2013hs /etc # gives diskusage of etc directory with memory dumpe2fs /dev/sdb1 | less # human readable details for the device dd if = /dev/sda of = /data/sales/file count = 1 bs = 512 # takes data backup of sda to sales/file of the first 512 bytes dd if = /data/sales/file of = /dev/sda # copies the data back in case of recovery tar \u2013cvf /data/sales/etc.tar /etc # backs up etc directory by creating a tar file umount /dev/sdb1 # unmounts sales directory tune2fs \u2013L \u201cDATA\u201d /dev/sdb1 # adding label to the file system debugfs /dev/sdb1 # enters debug of sdb1 directory. Type quit to exit File Permissions \u00b6 # Format for file permission : User-Group-Others # Symbolic Notation (Default permission) RWX \u2013 RW - R # Octal Notation 7 - 6 - 4 # So RWX is 111 i.e. 7, RW is 110 i.e. 6 and R is 100 i.e. 4 umask 2 # sets default permission to all the files in the directory chmod 777 file1 # Changes permission for a file1 chmod u = rwx,g = rw,o = rw file 2 # Verbose way to set permissions chmod +rx file3 # Sets read & write for User, group and others ls \u2013ld /data # Shows permission for a single directory chgrp users /data # Adds users group to the directory - Even if user does not have write access to a file, he has delete / add file access to a directory. - chmod o+t /data .Users can delete only their files and not other\u2019s. Root will not be able to delete files in this directory. - This permission is sent on the /tmp directory by default at installation. So only user\u2019s own file can be deleted, not of others. Links (Hard and Soft Links) \u00b6 Soft links are also called as Symbolic Links or symlinks . Here one file will be a pointer to the other file. If file has more than one name, it\u2019s called hard link. To find the number of sub directories , use stat dirname . Links number -2 is the total number of sub directories. Each directory has a minimum of 2 links, hence subtract 2. ln file2 file5 # Creates hard link between file2 and file5. # Shows the inode number which is same i.e. the same metadata is present for both. Cat on both the files shows the same data content ls \u2013li file2 file5 ln \u2013s file3 file4 # Creates a symlink between file 3 and file5. Cat on both the files shows the same data content ls \u2013li file3 file4 # Shows the symlink, but they are different files. Inode number is different. readlink file5 # shows where the link is Applying Quotas \u00b6 Quotas can be applied to Space/inodes, Group, User or File System. repquota \u2013auv # Give quota report per user space usage along with limits quotaon /dev/sdb1 # Checks quota limit # enable quotas and edit the hard and soft limits. Soft limit can be exceeded for 7 days, after which it is enforced. edquota \u2013u <username> # enables quota via command line. Soft limit is 21000 is 21MB, hard limit is 26MB setquota \u2013u <username> 21000 26000 0 0 /dev/sdb1 Directory Listing and Alias \u00b6 ls \u2013F /dir1 # shows directory with a / and symlink as @ at the end of the name ls \u2013-color = auto /dir1 # shows the same file types in color alias ls = \u2019ls \u2013-color = auto\u2019 # creates an alias for ls with color ls \u2013lh file1 # list in human readable format ls \u2013lt /etc # shows long listing with time modified in descending order ls \u2013ltr /etc | less # shows reverse listing, q to quit Synchronize Directories \u00b6 mkdir /backup rsync \u2013av /home/ /backup/ # archive home dir to backup dir. / after home and backup is important rsync \u2013av --delete /home/ /backup/ # sync deletions of data as well, otherwise rsync ignores it by default rsync \u2013ave ssh # sync data between servers using e option Process Management \u00b6 Monitor Process \u00b6 which ps # shows the installation directory for ps uptime # shows the uptime of the system along with the load average in the range of 1 min, 5 mins and 15 mins # Rule of Thumb for uptime --> Load average for single core value should be less than 1, for dual core less than 2 etc. which uptime # shows the installation directory for uptime cat /proc/uptime # shows uptime and idle time cat /proc/loadavg # shows load avg for 1,5 and 15 mins, active process running/total process, last process id that was issued Jobs \u00b6 sleep 180 # sleeps for 180 secs in foreground. Ctrl + Z to pause the job. Run bg to put the sleep command in background. jobs # shows running jobs fg 1 # puts the sleep command in foreground Managing Processes \u00b6 ps to display processes and kill to send signals. pgrep, pkill and killall are great shortcuts. The default kill signal is -15 which can also be written as \u2013term or \u2013sigterm . To really kill it is -9, -kill or \u2013sigkill . ps \u2013l # long listing with the process ps \u2013ef # shows all the processes for all users ps \u2013eaf | grep processname pgrep nginx # shows process ids for nginx sleep 900 & pkill sleep # searches for sleep process and kills it killall sleep # searches for all running sleep process and kills it kill \u2013l # shows the multiple kill signals available kill -9 <process id> # forcefully terminates the process, also use kill \u2013kill <process id> top \u2192 kill, renice, sort and display processes Running top, you can toggle between the information displayed at the top lines. l \u2013 on/off load, t \u2013 on/off tasks, m \u2013 on/off memory Sorting of top is on %CPU, f \u2013 shows current fields being shown on output of top. Select the new field to sort and type s Type r for renice and put in the process id. Esc and Enter to quit the shell Type k for kill and put in the process id. Esc and Enter to quit the shell q to quit out of top top # shows all running processes, q to quit top \u2013n 1 # shows the running processes for 1 capture and quits top \u2013n 2 \u2013d 3 # shows 2 captures with a delay of 3 seconds and quits Editors \u00b6 Vi \u00b6 : # Last line mode q, q! # quit the file x, wq, wq! # save and exit the file i, I # insert from cursor position, I for inserting from start of the line a, A # append after the cursor, A for append from last character in the line o, O # insert line below the cursor, O for above the current cursor position dd # delete the line u # undo the changes Line Navigation \u00b6 <Linenumber>G # e.g. 7G, takes cursor to 7th line in the file G # only G takes cursor to end of file w , b # w takes cursor to next word, b takes cursor to one word before ^ , $ # ^takes cursor to start of line, $ to end of the line vi +127 /etc/file1 # opens the file and takes cursor to 127th line vi +/Document /etc/file1.conf # opens the file and takes cursor to first occurrence of \u201cDocument\u201d set number / set nonumber # from last line mode, it will show and stop line number display syntax on # highlighting on, e.g. xml highlighting etc. Read and Write \u00b6 r /etc/hosts # Open an existing file, use : and then you can get content from hosts file into current file w newfile # :, it will copy entire file contents into newfile in the same directory 3 ,7w newfile # it will copy line 3 to 7 into newfile Search and Replace \u00b6 %s/Hi/Hello # Open an existing file, use : and you can search Hi and Replace with Hello. %s signifies entire document search /Hello # searches for Hello in the document. Type n to get next occurrence, N will take cursor in reverse 1 ,20s/Hi/Hello # searches for 1st 20 lines for Hi and replaces with Hello 14 ,20s/^/ / # from 14th to 20th line, it will add 3 spaces from the start of the line, just like Tab BASH Scripting \u00b6 Understanding Variables \u00b6 Local variables \u2192 accessible only to the current shell, FRUIT=\u2019apple\u2019, echo $FRUIT Global variables \u2192 you need to set and then export it to make it global. export FRUIT=\u2019apple\u2019 Simple Script \u00b6 vi hello.sh #!/bin/bash # Path to the interpreter echo \u201cHello World\u201d exit 0 # return code, :wq chmod +x hello.sh hello.sh # execute the script as it\u2019s in the home directory /user/bin Getting user input \u00b6 vi hello.sh #!/bin/bash echo \u2013e \u201cEnter your name: \\c \u201d # -e is the escape sequence, -c is for the prompt read INPUT_NAME # read the input data into a variable echo \u201cHello $INPUT_NAME \u201d exit 0 User Input types \u00b6 $1 $2 # $1 is the 1st input parameter, 2nd Parameter and so on. $0 # is the script name itself $# # count of input parameters $* # is collection of all the arguments Multiple inputs using positional parameters \u00b6 vi hello.sh #!/bin/bash echo \u201cHello $1 $2 \u201d # $1 is the 1st input parameter, $0 is the script name itself, $2 is the 2nd input parameter and so on exit 0 Code Snippets \u00b6 Gedit \u2192 Gnome Editor \u2192 Add the Snippet Plugin (Applications \u2192 Accessories \u2192 gedit. Preferences in gedit tab \u2192 Plugins enable Snippet Plugin and restart gedit) Conditional Statement - IF \u00b6 if [[condition]] \u2192 testing for string condition if ((condition)) \u2192 testing for numeric condition e.g. if (( $# < 1 )) \u2192 if count of input parameter vi hello.sh #!/bin/bash if (( $# 1 )) then echo \u201cUsage: $0 <name>\u201d exit 1 fi echo \u201cHello $1 $2 \u201d exit 0 Case Statement \u00b6 vi hello.sh #!/bin/bash if [[ ! \u2013d $1 ]] # if the 1st argument is not a directory then echo \u201cUsage: $0 <directory>\u201d exit 1 fi case $2 in \u201cdirectory\u201d ) find $1 \u2013maxdepth 1 \u2013type d ;; # break \u201clink\u201d ) find $1 \u2013maxdepth 1 \u2013type l ;; # break * ) # default statement echo \u201cUsage: $0 <directory> directory | link\u201d ;; esac exit 0 For \u00b6 vi hello.sh #!/bin/bash for u in $* # $* is collection of arguments, u is temporary variable do # do block useradd $u # access to temp variable is via $ echo Password1 | passwd \u2013stdin $u # use the passwd command and get the user input from keyboard passwd \u2013e $u # expire the password, so they can change it at first login done echo \u201cFinished\u201d # at time of execution ./hello.sh fred mary john vi listsize.sh #!/bin/bash for file in $( ls ) # for each file, in the output of ls do [[ ! \u2013f ]] && continue # not a file then continue to next # use the stats to get statistics of the file, to get the last accessed date and then format the date LA = $( stat \u2013c %x $file | cut \u2013d \u201c \u201d \u2013f1 ) echo \u201c $file is $( du \u2013b $file ) bytes and was last accessed on $LA \u201d # use du to get file size done While \u00b6 vi loop.sh #!/bin/bash -x # -x is for debug mode COUNT = 10 while (( COUNT > 0 )) do echo \u2013e \u201c $COUNT \\c \u201d # \\c will suppress the line feed (enter) sleep 1 (( COUNT -- )) # round brackets to avoid using $ symbol done - Use the until when you want to stop the loop when the condition becomes true. User Management \u00b6 Managing Users: User Lifecycle ==> useradd, usermod, userdel Local databases ==> /etc/passwd, /etc/shadow (encrypted) passwd (to set the password) pwconv (move pass to encrypted) pwunconv (move back to unencrypted) # /etc/passwd file structure # It has 7 filed separated by : Login Name, Optional encrypted password or \u201cx\u201d, Numerical UID, Numerical GID, Username or comment, User home directory, Optional command interpreter # /etc/shadow file structure where the actual passwords are stored # It has 8 filed separated by : Login Name, encrypted password ( if it begins with ! the account is locked ) , Date of last password change, Minimum password age, Maximum password age, Password warning period, password inactivity, account expiry date # /etc/login.defs The password ageing defaults can be configured with this file useradd \u2013D # shows the default settings for a user that is added cat /etc/default/useradd # shows where the defaults are set useradd bob # only adds the user, no home directory is created. Once the user logs in, it will get created tail -3 /etc/passwd # shows that bob is added useradd \u2013m bob # also creates the home directory tail -3 /etc/shadow # shows the password for the user passwd bob # add the password for bob passwd \u2013l bob # locks the account passwd \u2013u bob # unlocks the account usermod bob \u2013c \u201cBob Smith\u201d # adding additional details for the user userdel \u2013r bob # removes the user and home directory Group Management \u00b6 Group Lifecycle \u2192 groupadd, groupmod, groupdel Local databases \u2192 /etc/group, /etc/gshadow (encrypted) gpasswd (to set password) newgrp (switch to new groups) # /etc/group structure # It has 4 fields: Group Name, Password, Numerical GID, User list that is comma separated # /etc/gshadow structure # It has 4 field: Group Name, Encrypted password, Admin list that is comma separated, # this can be managed used \u2013A cmd # Members, this can be managed using the \u2013M cmd Private groups are enabled by default. The user added is also added to the same group. If this is disabled, users will belong to the groups users. Use useradd \u2013N to overwrite private groups. This can be enabled or disabled by setting USERGROUPS_ENAB in /etc/login.defs useradd \u2013m \u2013g users jim # -g is Primary Group, -G is secondary groups. Secondary groups are more traditional groups id jim usermod \u2013G sudo,adm jim # added jim to secondary groups sudo, adm useradd \u2013N \u2013m sally # adds sally to the default group gpasswd \u2013M jim,sally sudo # adds 2 users to sudo group groupadd sales gpasswd sales # sets the new password for sales newgrp sales # add the user to the sales group temporarily. If the user logs out, he is removed from the group Automate System Tasks \u00b6 Regular Tasks \u2192 cron (more than once a day but misses job if turned off), anacron (run jobs missed on startup but jobs can run just once a day) Once Off \u2192 at (runs at specified time and date), batch (runs when load average drops below 0.8) System Cron Jobs \u00b6 # /etc/crontab, /etc/cron.d # cron files # /etc/cron.<time> # where time is hourly, daily, weekly and monthly, contains scripts that need to be executed # Adding a system cron job cd /etc/cron.d vi daily-backup # add a new file 30 20 * * 1 -5 root /root/back.sh # run back.sh from Mon to Fri at 20:30 # Adding a user cron job crontab \u2013e # edit the user crontab file */10 10 1 1 1 tail /etc/passwd # Runs once on 1st day if it\u2019s a Mon of Jan, at 10 am for every 10 mins crontab \u2013l # list all cron jobs crontab \u2013r # remove the cron job # anacron: /etc/anacrontab structure # It has 4 fields Period in days or macro ( Daily, Monthly ) , Delay ( minutes after system startup for job to run ) , Job Identifier ( used to name timestamp file indicating when job was last run ) Command ( that needs to be executed ) @weekly 120 weekly-backup ls /etc // weekly, 120 mins after startup it will run weekly-backup Batch \u00b6 at and batch commands at noon tomorrow # Enter the command line, Ctrl + D to save at> ls /etc # enter the command that needs to be executed atq # shows the jobs queue atrm # remove the job batch # Enter the command line, Ctrl + D to save at> ls /etc > /root/file1 # redirect the o/p to file1. It will run if the system load avg is less than 0.8. Security for Cron \u00b6 Everyone is allowed to run their own cron and at jobs, unless you add entries to /etc/cron.allow or /etc/at.allow. No one is denied unless you add entries to /etc/cron.deny or /etc/at.deny Networking Fundamentals \u00b6 Network Time Protocol (NTP) \u00b6 Configuring Network Time Protocol (NTP) \u00b6 # vi /etc/ntp.conf # prefixing i with date creates a backup of the original file. Removes commented and blank lines sed \u2013i. $( date +%F ) \u2018/^#d ; /^$/d\u2019 /etc/ntp.conf Implementing the configuration file changes \u00b6 vi /etc/ntp.conf # Add lines other can default just below the driftfile command statsdir /var/log/ntpstats # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server instead of on the internet debian servers Save the file and restart the service \u00b6 service ntp restart # sudo if no access # check if the ntpstats directory is accessible to the ntp service # The user should be ntp and it should have write access ls \u2013ld /var/log/ntpstats/ - Date \u2192 Current system date and time. This is the time in memory. - HwClock \u2192 Hardware date and time set by the BIOS. hwclock \u2013r # shows the hardware clock hwclock \u2013-systohc # sets the hardware clock from system clock Hwclock \u2013-hctosys # sets the system clock from hardware clock NTP Tools \u00b6 ntpdate (once off adjustment) ntpq (query the ntp server) ntpq \u2013p (shows peers) ntpstat (Shows status but not on debian. Try ntpdc \u2013c sysinfo) ntpq \u2013c \u201cassociations\u201d \u2192 shows associations # Configuring NTP on centos --> Install ntp ntpdate 192 .168.0.3 # one off update with a local machine in the network # vi /etc/ntp.conf # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server systemctl start ntpd # save and restart systemctl enable ntpd # enable to start at system startup Managing System Log Daemons \u00b6 Rocket-Fast System for Log Processing (rsyslogd) \u00b6 rsyslogd \u2013v # vi /etc/rsyslog.conf # Adding a simple log rule # For any log event greater than or equal to info make a log entry in local5 log. Local5 could be a simple application local5.info /var/log/local5 systemctl restart rsyslog.service # restart the service # to test this in working using command line logger \u2013p local5.info \u201cScript started\u201d # p is priority, if you see /var/log/local5 file, the log would be present /var/log/ folder structure \u00b6 messages (Nearly everything is logged here) secure (su and sudo events amongst others) dmesg (kernel ring buffer messages) Logrotate \u00b6 ls /etc/cron.daily # has the logrotate script which will rotate log files cd /etc/logrotate.d/ # folder where all apps rotation policy is set cp syslog local5 # copy existing app conf for local5 app # vi local5 # make edits to point to /var/log/local5 file /var/log/local5 { weekly # period for rotation size +10 # size of the file for rotation compress # use compression for the rotated log file rotate 4 # keep 4 weeks of logs before overwriting } # manually running the rotate logrotate /etc/logrotate.conf # on execution, all files mentioned will be interrogated and log backup will be created Journalctl \u00b6 Responsible for viewing and log management. Need to be a member of adm group to read this. By default journal is memory resident i.e. it will be lost on restart journalctl # view the journal journalctl \u2013n 10 # shows the last 10 entries journalctl \u2013n 10 \u2013p err # shows the last 10 entries with priority errors mkdir /var/log/journal # to make the journal data persistent systemctl restart system-journald systemctl status system-journald # shows that the journal data is persistent usermod \u2013a \u2013G adm username # adding user to adm group, -a is append chgrp \u2013R adm /var/log/journal # recursively give adm group access jornalctl \u2013-disk-usage # shows disk usage journalctl \u2013-verify # verify the journal integrity SSH \u00b6 Remote access using SSH \u00b6 Server Configuration /etc/ssh/sshd_config The public key of the server is used to authenticate to the client. The public key of the server is stored in /etc/ssh/ssh_host_rsa_key.pub It is down to the client to check the public key using: StrictHostkeyChecking Server public keys are stored centrally in /etc/ssh/ssh_known_host or locally under ~/.ssh/known_hosts SSH Server Configuration \u00b6 netstat \u2013antl # shows the open tcp ports of the server grep ssh /etc/services # shows the services using ssh lsof \u2013i # Also shows open ports # vi /etc/ssh/sshd_config # Uncomment AddressFamily line and change as below AddressFamily inet # Now ssh will only listen on IPv6 systemctl restart sshd # vi /etc/ssh/sshd_config # Uncomment below lines and modify LoginGraceTime 1m # To avoid denial of service attacks and freeing up your service quickly PermitRootLogin no # 2 level authentication, first as normal user and then root SyslogFacility AUTHPRIV ClientAliveInterval 300 ClientAliveCountMax 0 MaxSessions 10 systemctl restart sshd Client Configuration and Authentication \u00b6 Client Configuration /etc/ssh/ssh_config Generate Private and Public keypair using ssh_keygen Use ssh-copy-id to copy to host we want to authenticate with. To provide Single Sign On using ssh-agent Client/User public keys are stored in ~/.ssh/authorized_keys using ssh-copy-id. To connect to server using ssh \u00b6 cd # home directory ls \u2013a # to show all hidden files ssh pi@192.168.0.97 # ssh using user and ip address. Add the password of the user to authenticate cd .ssh cat known_hosts # shows the client ip and public keys exit or logout or Ctrl + D # to end the ssh session To generate keypairs \u00b6 cd .ssh # On the client home directory ssh-keygen \u2013t rsa # generate key pair. Private key is encrypted using a passphrase # This will copy the generated public key to the target server. To which user\u2019s directory at the server we will # connect as. Give the password of the server\u2019s account. ssh-copy-id \u2013i id_rsa.pub pi@192.168.0.97 ssh pi@192.168.0.97 # now connect to the server using passphrase of the private key # From another terminal say tty we can now add the private key once and don\u2019t need to authenticate to the target server ssh-agent bash # fire up another bash terminal ssh-add .ssh/id_rsa # add the private key from the home directory. Enter the passphrase ssh \u2013l or ssh \u2013L # list all identities added ssh pi@192.168.0.97 SSH Tunnels \u00b6 ssh \u2013f \u2013N \u2013L 80 :localhost:80 user@s1.com # -f = execute in background, -N = We are not running any commands on remote host # -L = listening on port 80, we are listening on localhost and forwarding to port 80 on the remote host # On the remote host it has to listen on ssh called s1.com. We connect as user called user. # Example # Webservice on 192.168.0.3 # on a different machine, login as standard user cd .ssh ssh \u2013f \u2013N \u2013L 9000 :localhost:80 andrew@192.168.0.3 # we are listening on port 9000 on the localhost and forwarding traffic to port 80 om 192.168.0.3 netstat \u2013antlp # we can see that localhost:9000 is listening on ssh # On the client machine open the browser and type in http://127.0.0.1:9000 we will see the webservice data kill <process id of ssh> # shutdown the ssh tunneling process after finishing the work Configuring Network Protocols in Linux \u00b6 /etc/services \u00b6 Network services are identified by a port address Common services and associated port address is listed in /etc/services netstat \u2013alt will list services listening via TCP this resolves address to name in /etc/services # To verify the above we can use strace to map the netstat data with the services that are running strace netstat \u2013alt 2 > & 1 | grep /etc/services grep http /etc/services # service and port mapping dig command \u00b6 which dig # get the path of dig rpm \u2013qf /usr/bin/dig # dig is not installed by default. Hence needs to be installed. dig \u2013t AAAA ipv6.bbc.co.uk # shows the IPv6 address Interface Configuration Files \u00b6 ifconfig # shows ip address details ifconfig eth0 192 .168.0.99 netmask 255 .255.255.0 up # sets ip address for Ethernet card ip address show # same as ifconfig # These settings are lost upon restart unless they are written to configuration files. /etc/sysconfig/network-scripts/ # centos /etc/network/interfaces # debian To make the IP address static \u00b6 cd /etc/sysconfig/network-scripts/ vi ifcfg-ens32 # open the config file # Replace and add the lines BOOTPROTO = \u201dstatic\u201d # change from dhcp IPADDR = 192 .168.0.240 # select a static ip address NETMASK = 255 .255.255.0 # add a class c subnet GATEWAY = 192 .168.0.1 # add the gateway address DNS1 = 8 .8.8.8 # add google as the DNS server # Restart the services systemctl restart network.service Networking Tools \u00b6 nmap \u00b6 nmap localhost # shows all open ports used by localhost nmap 192 .168.0.3 # shows open ports at remote host nmap \u2013v 192 .168.0.3 # verbose mode nmap \u2013iL ip.txt # input file containing all ip address to be scanned netstat \u00b6 netstat \u2013a # shows all connections netstat \u2013at # shows all tcp connections netstat \u2013alt # shows all listening tcp connections netstat \u2013altpe # shows all user and process ids and listening tcp connections netstat \u2013s # statistics netstat \u2013i # shows interfaces netstat \u2013g # multicast groups netstat \u2013nr # network route tables Show Sockets (ss) \u00b6 ss \u2013t \u2013a # shows all tcp connections ss \u2013o state established \u2018 ( dport = :ssh or sport = :ssh ) \u2019 # shows all ssh connections ss \u2013x src /tmp/.X11-unix/* # shows X11 connections using socket files lsof \u00b6 lsof \u2013i -4 # list all ipv4 connections lsof \u2013i :23, 24 # list all port 22 and 23 connections lsof \u2013p 1385 # list process id 1385 connections Testing Network Connectivity \u00b6 ping www.centos.org # test network connectivity ping \u2013c3 www.centos.org # sends only 3 pings traceroute www.centos.org # describes the route to destination from source tracepath www.centos.org # shows maximum transmission unit size Host Name Resolution Tools \u00b6 hostname # Confirms the hostname cat /etc/hostname # shows the current host name hostname myComputer # changes the hostname to myComputer. You need to login as root to change this dig www.centos.org # resolves to ip address dig \u2013t MX centos.org # mail servers associated with centos Managing Interfaces \u00b6 ip a s # Shows ip addresses ip n s # Shows Neighbor shows looking at ARP cache ip r s # Shows root table ip ma s # Shows Multicast groups ip l # Shows network cards ifdown eth0 # Brings down interface ifup eth0 # Brings up interface Securing Access to your Server \u00b6 # Temporary disables logins for users other than root. Just the existence of this file will prevent user logins and is controlled via PAM(Pluggable Authentication Modules) /etc/nologin # Create a blank file, but if the user tries to login via ssh username@localhost, the connection will be immediately closed. Only root can access the server then. touch /etc/nologin rm /etc/nologin # Now users can login. This can be used as a temporary measure cd /etc/pam.d/ # Config files for authentication modules grep nologin * # Shows instances where nologin file exists last # shows last user activity present in /var/log/wtmp file. lastlog # List the last login time for each user lastlog | grep \u2013v Never # Reverse the grep search to check for all user logins ulimit \u00b6 Puts restrictions on system resources ulimit \u2013a # Shows system limitations that can be applied ulimit \u2013u 8000 # -u is for avoiding fork bombs and is defaulted to 4096. A std user can set a new value to 8000. # It will remain for his profile till system restart. cd /etc/security # To set the limits which can be persisted even after restart cat limits.conf # Shows soft (can be changed by processes) and hard (can\u2019t be changed by processes) limits cd limits.d/ # ls to see the files inside this directory. Edit the file and add an entry for the user account with soft or hard limits and save the file. Avoiding fork bombs \u00b6 They are a potential Denial of Service Attack ulimit \u2013u ##### Do not run on Production Machines. Test only in laptop ##### ps \u2013u username | wc -l # Shows the number of processes running under the user and gives the count # On the command line create a function called foo foo (){ echo hello } # Execute the function by just calling it and pressing enter foo # Similarly to execute a fork bomb, instead of foo, call it : : (){ : | : & # Here the function calls itself and pipes the output to itself } ; : # End the function with a semicolon and then call the function : # BEWARE: Do this only at your own risk. Ensure ulimit is set to protect the resources on the server # xinetd \u00b6 Called the super daemon as it can manage a lot of smaller services, secures access to your server /etc/xinetd.d /etc/xinetd.conf tftp server (Trivial file transfer protocol) # Sample configuration service tftp { socket_type = dgram # data gram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /tftpboot # root directory of the server disable = no } Implementing TFTP using xinetd.d service \u00b6 ls /etc/xinetd.d/ # blank directory yum install tftp-server tftp # install client and server, also xinetd as it\u2019s a dependency vi /etc/xinetd.d/tftp # after installation, delete disable line from the configuration to enable tftp # if the server directory doent exist, create it mkdir \u2013p /var/lib/tftpboot systemctl enable xinetd systemctl start xinetd netstat \u2013aulpe | grep tftp # shows the port # As a root user, create a temp file inside var/lib/tftpboot directory with hello text vi var/lib/tftpboot/file1 # Logout and login as standard user. Use TFTP to transfer the file to standard user tftp 127 .0.0.1 # Press enter get file1 # Get the file1 created by root using tftp quit # At the same file location do a cat to see the contents of the file cat file1 TCP Wrappers \u00b6 Alternative to firewalling on the server # To check if service supports TCP wrappers. Once we can determine this, we can use hosts.allow or hosts.deny ldd </path to service name> | grep libwrap # To set this up for a service, 2 entries are made # This in /etc/hosts.allow file to allow access to 0.3 IP which is raspberry pi. tftpd is the name of the binary in .tftpd : 192 .168.0.3 # This in /etc/hosts.deny file to deny access to all other IP except for 0.3 IP in .tftpd : ALL # If the client appears in both files then allow takes precedence and access is granted ldd /usr/sbin/xinetd | grep libwrap tftp 192 .168.0.240 # login from raspberry pi and access the remote server, disable your firewall before trying this. Delegating Admin rights using sudoers \u00b6 id # check if the user is already an admin and part of wheel group in centos cd /etc grep wheel sudoers # check the current setup visudo # to edit sudoers file %wheel ALL =( root ) ALL # Uncomment the line for %wheel, change (ALL) to (root), so only root can change the sudoers. Save and exit Data Encryption: \u00b6 Using GPG to encrypt data between users \u00b6 password = $( mkpassword \u2013m sha-512 Password1 ) # encrypt the password using sha and mkpassword and put in a variable echo $password for u in marta ivan ; do # take an input from marta and ivan for users sudo useradd \u2013m \u2013p $password $u # add the user and set the encrypted password done # Run the command and create 2 users ivan and marta # Install GPG if not present dkpg \u2013S $( which gpg ) # Login as ivan and generate private and public keys for gpg encryption su - ivan gpg --gen-key # Take the default settings gpg --list-key # List the keys gpg --export \u2013a <email from gpg gen-key step> > /tmp/ivankey # export the public key and place in tmp folder for marta to access exit su - marta vi secret.txt # create a plain txt file for encryption chmod 400 secret.txt # make the file writeable by marta only gpg --import /tmp/ivankey # import ivan public key gpg \u2013e \u2013r <ivan mailid> secret.txt # add the recipient and encrypt the file mv secret.txt.gpg /tmp/ # move the encrypted file to tmp and exit su \u2013 ivan gpg \u2013d /tmp/secret.txt.gpg # decrypt the file and enter the passphrase for the private key Implementing LUKS for Full Disk Encryption \u00b6 example Laptops, USB sudo apt-get install cryptsetup # Install LUKS tools lsblk # list the blocks sudo fdisk /dev/sdb # partition the sdb drive and create a new partition called sdb1 sudo cryptsetup \u2013y luksFormat /dev/sdb1 # Allows to store encrypted data in this partition. Give a passphrase sudo cryptsetup luksDump /dev/sdb1 # Shows details of the encrypted partition sudo cryptsetup luksOpen /dev/sdb1 private_data # Assign a mapper called private_data ls /dev/mapper # Shows the new mapper setup and that is a device sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted EFS for data encryption \u00b6 sudo apt-get install ecryptfs-utils # Install EFS tools su \u2013 ivan # Login as std user ecryptfs-setup-private # Create a private key with passphrase. logout and login back as ivan ls # Private directory is created by EFS echo hello > Private/data # Write to private directory cat Private/data # ivan can see the data ecryptfs-umount-private # unmount the private directory ls # Now ivan cannot see the data cd /.Private/ # Go to the ivan\u2019s hidden Private folder with a dot ls # You can see the encrypted data file ecryptfs-mount-private # Mount the directory again Compiling software from source \u00b6 Install gcc compiler sudo apt-get install gcc make # go to gnu.org/software/Downloads -> coreutils -> file ending with .xz # tar -xvJf coreutils-8.28.tar.xz # J is for xz, j is for bzip2 # cd into src folder and select ls.c file # Add a line in main function printf(\"Hello World\\n\"); # After changes, change directory one level up and run the configure script bash configure # This will check for any configuration changes to the src files and update the make file # Excute the make command make # After binary code is compiled, it needs to be updated in OS sudo make install # Close the terminal and restart. New software is working ;)","title":"Linux"},{"location":"learning/linux/#introduction","text":"Understanding Linux Filesystem Templates folder CronTab Guru","title":"Introduction"},{"location":"learning/linux/#basic-commands","text":"Important Commands Terminal Ctrl+Alt+T # Open the terminal Ctrl+D # Close the terminal exit # Close the terminal Ctrl + L # Clear the screen, but will keep the current command Ctrl + Shift + # Increases font size of the terminal Utility cal # Calendar current month cal -3 # Current -1, Current , Current +1 month cal 5 1967 # Format is (Month and Year). Gives May 1967 date # Current date in BST (default) date -u # Current date in UTC date --date \u201c30 days\u201d # Gives current date + 30 days (future date) date --date \u201c30 days ago\u201d # Gives current date \u2013 30 days (past date) which echo # Shows where the command is stored in PATH hostname -I # Gives IP address echo $? # Gives the output 0/1 value stored after a command is run wc \u2013l file1 # Gives line count in file1 wc file1 # Give word count in file1 History history # List all the commands executed !! # Run the previous command !50 # Run the command that is on line 50 of history output history \u2013c ; history \u2013w ; # Clears history and writes back to the file Ctrl + r # reverse searches for your input. Press Esc to edit the matched command man Using the Manual There are 8 sections in the manual. Important are 1, 5 and 8 sections man \u2013k <search term> # Search the manual for pages matching <search term>. man -k tmux # example of searching for tmux in the manual pages man -k \"list directory contents\" # Double quote seraches complete words man 1 tmux # Opens section 1 of tmux manual page, 1 is default and can be ignored man ls # Shows section 1 of ls command help cd # Shows the help pages if man pages are not present Redirection of Streams echo \"Hello\" 1 > output.txt # Standard output is redirected to output.txt echo \"Hello\" > output.txt # Standard output is default echo \"World\" 1 >> output.txt # Standard output is appended to output.txt echo \"Error\" 2 > error.txt # Standard error is redirected to error.txt cat -k bla 2 >> error.txt # Program error is redirected and appended to error.txt echo \"Hello World\" 1 >> output.txt 2 >> error.txt # Use both std output and error cat 0 < input.txt # Standard input is read from a file and sent to cat command cat < input.txt # Standard input is default cat 0 < input.txt 1 >> output.txt 2 >> error.txt # Use all 3 data streams Redirection to Terminals tty # Current terminal connected to Linux, gives path cat < input.txt > /dev/pts/1 # In another terminal, Standard input is read from a file and sent to tty 1 terminal Ctrl + Alt + F1 / chvt 1 # Goes to physical terminal with no graphics. Similarly you can change to 2 to 6 tty terminals. Ctrl + Alt + F7 / chvt 7 # Comes back to Graphical terminal Piping date | cut --delimiter \" \" --fields 1 # Output of date is input to cut command - Tee command - Used to store intermediate output in a file and then stream passed horizontally through the pipeline - tee command takes a snapshot of the standard output and then passes it along date > date.txt | cut --delimiter \" \" --fields 1 # Output will not work and date will only be stored in file and not passed to cut command date | tee date.txt | cut --delimiter \" \" --fields 1 # Output of date is first stored in file, then passed to cut command for display to Standard Output date | tee date.txt | cut --delimiter \" \" --fields 1 | tee today.txt cat file1.txt file2.txt | tee unsorted.txt | sort -r > reversed.txt # Output chaining and storing intermediate data in files - XARGS command (Powerful pipeline command) - Allows piped data into command line arguments - date | echo # Output of date is passed to echo, but echo doesn't accept standard input, only commandline arguments date | xargs echo # xargs will convert standard output into command line arguments date | cut --delimiter \" \" --fields 1 | xargs echo # Prints the day of the week Alias Used to store reusable scripts in .bash_aliases file can be used in scripts alias # Shows all the alias setup for the user # Store an alias in the `.bash_aliases` file alias calmagic = 'xargs cal -A 1 -B 1 > /home/leslie/calOutput.txt' # In the terminal use if in a pipe command, STDOUT will be stored in a file echo \"12 2021\" | calmagic File System Navigation # File Listing pwd # Prints absolute path of current working directory(CWD) ls \u2013l # Long list of CWD ls \u2013a # Shows all files including hidden ls -F # Shows directories as ending with / along with other files stat <filename> # Detailed file information file <filename> # File type ls -ld # Detailed folder information # Change Directories cd - # Helps to switch directories. Like a Toggle (Alt + Tab) in windows cd / cd ~ # User Home directory from anywhere cd .. # Back to parent directory of CWD Wildcards Wildcards and How to use - The star wildcard has the broadest meaning of any of the wildcards, as it can represent zero characters, all single characters or any string. - The question mark (?) is used as a wildcard character in shell commands to represent exactly one character, which can be any single character. - The square wildcard can represent any of the characters enclosed in the brackets. ls *.txt # Matches all txt files ls ???.txt # Matches all 3 letter txt files ls file [ 123 ] .txt # Matches all files ending with 1 to 3 ls file [ A-Z ] .txt # Matches all files ending with A to Z ls file [ 0 -9 ][ A-Z ] .txt # Matches all files ending with 0A to 9Z File and Folders # Create Operations touch file1 # Creates a new file1 echo \"Hello\" > hello.txt # Creates and writes using redirection # -p is parent directory which is data and inside that 2 directories called sales & mkt is created mkdir \u2013p /data/ { sales,mkt } # Brace exapansion will allow to create folders. Sequence can be expressed as .. mkdir -p /tmp/ { jan,feb,mar } _ { 2020 ..2023 } # Brace expansion for files, it will create 10 files inside each folder touch { jan,feb,mar } _ { 2020 ..2023 } /file { 1 ..10 } # Delete Operations rm file1 # Deletes file1 rm *.txt # Deletes all txt files # Deletes all files and folders inside the main folder and the main folder as well # CAUTION: Use the recursive option with care rm -r /tmp/ { jan,feb,mar } _ { 2020 ..2023 } / # Deletes only empty directories rmdir /tmp/ # Skips folders which have files # Copy Operations cp /data/sales/file1 /data/mkt/file1 cp /data/sales/* . # Copy all files to CWD cp -r /data/sales /data/backup # Copy folder to backup folder # Move and Rename Operations mv file1 file2 # Rename file in the same folder mv /data/mkt/ /data/hr # Rename folder, Note the slash after first folder mv /data/sales/* /tmp/backup/ # Move files to new location mv /data/mkt/ /tmp/newFolder # Move and rename the folder Nano - Editing M Key can be Alt or Cmd depending on keyboard layout Enable spell checking on nano by editing /etc/nanorc and uncomment set speller in the file. Ctrl + O # Write data out to file Ctrl + R # Copy contents of one file into another Ctrl + K # Cuts entire line, also used as a delete Alt + 6 # Copy entire line Ctrl + U # Paste the line Ctrl + T # Spell check the file Alt + U # Undo changes Alt + E # Redo changes # File operations in vi > filename # Empties an existing file :x # Saves file changes instead of :wq Search Files find ~/projects # Find matches of files and folders from projects and below find . # Find from CWD and below find . -maxdepth 1 # Find from CWD and one level below find . -type f # Find files only find . -type d # Find folder only find . -maxdepth 1 -type d # Find folder only and one level below find . -name \"*.txt\" # Find files ending with matching patterns find . -maxdepth 3 -iname \"*.TXT\" # Find files with case insensitive matching patterns find . -type f -size +100k # Find files greater than 100 Kb # Find files greater than 100 Kb AND less than 5 Mb and count them find . -type f -size +100k -size -5M | wc -l # Find files less than 100 Kb OR greater than 5 Mb and count them find . -type f -size -100k -o -size +5M | wc -l - Find and Execute commands # Find and copy files to backup folder. `\\;` denotes end of exec command find . -type f -size +100k -size +5M -exec cp {} ~/Desktop/backup \\; ### # Find file called needle.txt inside haystack folder ### # Create 100 folders and inside each folder 100 files mkdir -p haystack/folder { 1 ..100 } touch haystack/folder { 1 ..100 } /file { 1 ..100 } # Create file in one random folder touch haystack/folder $( shuf -i 1 -100 -n 1 ) /needle.txt ### # Finding the file using name find haystack/ -type f -name \"needle.txt\" # Move the file to haystack folder find haystack/ -type f -name \"needle.txt\" -exec mv {} ~/tmp/haystack \\; ### View/Read File Contents cat cat file1 file2 > file3 # Concatenate 2 files and write into file3 cat \u2013vet file3 # displays special characters in the file e.g. EOL as $. Useful if sh files are created in windows tac - Flips the file contents vertically tac file3 # Reads the file in reverse rev - Reverses the contents of each line rev file3 # Reads the line in reverse less - Allows to page through big files less file3 # Shows one page at a time. Use Arrow keys to scroll # Output of find piped to less command for scrolling find . -type f -name \"*.txt\" | less head - Shows limited lines from top of output cat file3 | head -n 3 # Shows first 3 lines tail - Shows limited lines from bottom of output cat file3 | tail -n 3 # Shows last 3 lines tail \u2013f /var/log/messages # follows the file and continuously shows the 10 lines Sort sort words.txt > sorted.txt # Sorts in Asc order and redirects to sorted.txt sort -r word.txt > reverse.txt # Sorts in Des order sort -n numbers.txt # Sorts in Asc numeric order based on digit placement sort -nr numbers.txt # Sorts in Des numeric order based on digit placement sort -u numbers0-9.txt # Sorts and shows only unique values - Sorting data in tabular format # Sort on the basis of file size (5th column and its numeric) ls -l /etc | head -n 20 | sort -k 5n # Reverse (r) the output showing largest files first ls -l /etc | head -n 20 | sort -k 5nr # Sort on the basis of largest file size in human readable format ls -lh /etc | head -n 20 | sort -k 5hr # Sort on the basis of month ls -lh /etc | head -n 20 | sort -k 6M Search data - grep grep is case-sensitive search command # grep <search term> file-name grep e words.txt # Shows matching lines as STDOUT grep -c e words.txt # Counts the matching lines # Search in case insensitive manner grep -i gadsby gadsby_manuscript.txt # Search strings using quotes grep -ci \"our boys\" gadsby_manuscript.txt # Invert the search grep -v \"our boys\" gadsby_manuscript.txt # Searches for server and not servers. \\b is the word boundary grep \u2018 \\b server \\b \u2019/etc/ntp.conf # Searches for server beginning in the line. \\b is the word boundary grep \u2018^server \\b \u2019/etc/ntp.conf Filter data using grep ls -lF / | grep opt # Shows details for opt folder only ls -F /etc | grep -v / # Shows only files in etc folder - Remove Commented and Blank Lines # Empty lines can be shown as ^$. \u2013v reverses our search and \u2013e allows more than one expression. O/p is sent to std o/p grep \u2013ve \u2018^#\u2019 \u2013ve\u2019^$\u2019 /etc/ntp.conf # -v ^# says I don\u2019t want to see lines starting with #. ^$ says I don\u2019t want to see lines that begin with EOL marker Archival and Compression Two step process: Create the tar ball, then compress the tar Compression tool comparison # Create the tar ball tar -cvf backup.tar file [ 1 -3 ] .txt # Create, Verbose, Files to archive tar -tf backup.tar # Test for tar file without unzipping # Compress the tar ball ## 3 compression tools - gzip -> bzip2 -> xz (Compression and time increases from left to right) gzip backup.tar # Compresses the tar ball and adds .gz extension to tar ball gunzip backup.tar.gz # Decompress the gzip ### bzip2 backup.tar # Smaller file size than gzip and adds .bz2 extension to tar ball bunzip2 backup.tar.bz2 # Decompress the bzip. Best used for larger file sizes # Open the tar ball contents tar -xvf backup.tar # Extract, Verbose, Files to unarchive ### Create tar and compress in single command # Adding the z option for gzip and renaming the tar as .gz tar -cvzf backup.tar.gz file [ 1 -3 ] .txt tar -xvzf backup.tar.gz # Adding the j option for bzip2 and renaming the tar as .bz2 tar -cvjf backup.tar.bz2 file [ 1 -3 ] .txt tar -xvjf backup.tar.bz2 ### BASH #!/bin/bash # First line in the script \"SHEBANG\" tells type of script ### # To create an executable script, create a `bin` folder in your home. # Move all utility shell scripts to bin. Also remove .sh file extenstions # Make the file as executable `chmod +x data_backup` # Add the `~/bin` to the PATH variable # Edit `.bashrc` with PATH=\"$PATH:$HOME/bin\" # Now all scripts in bin folder are executable from command line ### # Set and unset variables export $VARIABLE # Sets the variable unset VARIABLE # Removes the variable, NOTE \u2013 No $ in variable Cron Scheduling crontab -e < select editor> # Opens the template crontab # Multiple options for each column of crontab using comma. # SPACE is used to delimit the columns of crontab # */<value> can divide the time intervals ### # min hours \"day of month\" month \"day of week (0-6)\" ### * * * * * bash ~/data_backup.sh Package Management apt-cache search docx # Searches apt for programs that can work with MS Word apt-cache show <package> | less # Gives software information # Apt cache information resides in /var/lib/apt/lists sudo apt-get update # Updates the apt lists sudo apt-get upgrade # Upgraded to the latest software versions from the list sudo apt-get install <pkg-name> # Install package sudo apt-get purge <pkg-name> # Remove & uninstall package. Recommended approach sudo apt-get autoremove # Removes any installed package dependecies # Package compressed archives are stored in `/var/cache/apt/archives` sudo apt-get clean # Removes all package compressed acrhives sudo apt-get autoclean # Removes only package compressed acrhives that cannot be downloaded - Source Code for apps OS uname # Shows kernal uname -o # Shows OS uname -m # Shows computer architecture x86_64 (64 bit), x86 (32 bit) lsb_release -a # Distro version Misc fdisk -l # Gives device wise memory details free / free -m # Gives amount of free memory lsblk # Lists all partitions swapon \u2013s # List all swap files ps # Process id of the current bash shell shutdown \u2013h now / poweroff / init 0 # Power downs the system restart / init 6 / reboot # Restarts the system shutdown \u2013r + 1 \u201cWe are restarting\u201d # Restarts the system give all logged in users 1 min to shut down all process su - # Login to root id / id bob # Shows the current user and group id sudo -i # Interactive shell for password of the current user, to get elevated access ssh localhost # ssh connection to same server. Type exit or Ctrl + D to logout of ssh. who / w # Gives the list of terminals that are connected and who has logged on to the server Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal Terminal","title":"Basic commands"},{"location":"learning/linux/#tips","text":"Alt + F2 Gives the run command terminal and then type gnome-system-monitor is like task Manger in windows. Gives graphical overview of the system and can kill processes. Putting & after any commands runs it in the background. Run jobs to see all the background running jobs. Type fg to bring the background running jobs to the foreground. Ctrl + C to cancel the job then. !<anycharacter> will sreach for the last command in history starting with that character !?etc executes the last command that contains etc","title":"Tips"},{"location":"learning/linux/#filesystem","text":"","title":"Filesystem"},{"location":"learning/linux/#creating-partitions","text":"fdisk or gdisk utility to partition. If Mountpoint is /, that is primary partition.","title":"Creating Partitions"},{"location":"learning/linux/#creating-filesystems","text":"mkfs.ext4 \u2013b 4096 /dev/sdb1 # Creates 4MB block size file system mkfs.xfs \u2013b size = 64k /dev/sdb2 # Creates 64k block size file system. Xfs is specialized filesystem","title":"Creating Filesystems"},{"location":"learning/linux/#mounting-data","text":"mkdir \u2013p /data/ { sales,mkt } mount /dev/sdb1 /data/sales # Mounts device to data/sales directory mount /dev/sdb2 /data/mkt","title":"Mounting Data"},{"location":"learning/linux/#unmounting-data","text":"umount /dev/sdb1 or umount /dev/sdb { 1 ,2 } # Unmounts both the devices","title":"Unmounting Data"},{"location":"learning/linux/#virtual-memory-or-swap-filesystem","text":"They are temporary space requirements Virtual memory in Linux can be a Disk Partition or Swap file. Use gdisk to create swap filesystem. Option L and then hex code 8200. To make the swap filesystem permanent, make an entry in /etc/fstab file, so changes are persistent even after system reboot. partprobe /dev/sdb # Sync saved partition in memory. Or it requires system reboot mkswap /dev/sdb3 # Select the right swap device to create the filesystem swapon /dev/sdb3 # Mount the filesystem","title":"Virtual Memory or Swap Filesystem"},{"location":"learning/linux/#troubleshooting-linux-filesystem","text":"df \u2013hT # list all filesystem with space details du \u2013hs /etc # gives diskusage of etc directory with memory dumpe2fs /dev/sdb1 | less # human readable details for the device dd if = /dev/sda of = /data/sales/file count = 1 bs = 512 # takes data backup of sda to sales/file of the first 512 bytes dd if = /data/sales/file of = /dev/sda # copies the data back in case of recovery tar \u2013cvf /data/sales/etc.tar /etc # backs up etc directory by creating a tar file umount /dev/sdb1 # unmounts sales directory tune2fs \u2013L \u201cDATA\u201d /dev/sdb1 # adding label to the file system debugfs /dev/sdb1 # enters debug of sdb1 directory. Type quit to exit","title":"Troubleshooting Linux filesystem"},{"location":"learning/linux/#file-permissions","text":"# Format for file permission : User-Group-Others # Symbolic Notation (Default permission) RWX \u2013 RW - R # Octal Notation 7 - 6 - 4 # So RWX is 111 i.e. 7, RW is 110 i.e. 6 and R is 100 i.e. 4 umask 2 # sets default permission to all the files in the directory chmod 777 file1 # Changes permission for a file1 chmod u = rwx,g = rw,o = rw file 2 # Verbose way to set permissions chmod +rx file3 # Sets read & write for User, group and others ls \u2013ld /data # Shows permission for a single directory chgrp users /data # Adds users group to the directory - Even if user does not have write access to a file, he has delete / add file access to a directory. - chmod o+t /data .Users can delete only their files and not other\u2019s. Root will not be able to delete files in this directory. - This permission is sent on the /tmp directory by default at installation. So only user\u2019s own file can be deleted, not of others.","title":"File Permissions"},{"location":"learning/linux/#links-hard-and-soft-links","text":"Soft links are also called as Symbolic Links or symlinks . Here one file will be a pointer to the other file. If file has more than one name, it\u2019s called hard link. To find the number of sub directories , use stat dirname . Links number -2 is the total number of sub directories. Each directory has a minimum of 2 links, hence subtract 2. ln file2 file5 # Creates hard link between file2 and file5. # Shows the inode number which is same i.e. the same metadata is present for both. Cat on both the files shows the same data content ls \u2013li file2 file5 ln \u2013s file3 file4 # Creates a symlink between file 3 and file5. Cat on both the files shows the same data content ls \u2013li file3 file4 # Shows the symlink, but they are different files. Inode number is different. readlink file5 # shows where the link is","title":"Links (Hard and Soft Links)"},{"location":"learning/linux/#applying-quotas","text":"Quotas can be applied to Space/inodes, Group, User or File System. repquota \u2013auv # Give quota report per user space usage along with limits quotaon /dev/sdb1 # Checks quota limit # enable quotas and edit the hard and soft limits. Soft limit can be exceeded for 7 days, after which it is enforced. edquota \u2013u <username> # enables quota via command line. Soft limit is 21000 is 21MB, hard limit is 26MB setquota \u2013u <username> 21000 26000 0 0 /dev/sdb1","title":"Applying Quotas"},{"location":"learning/linux/#directory-listing-and-alias","text":"ls \u2013F /dir1 # shows directory with a / and symlink as @ at the end of the name ls \u2013-color = auto /dir1 # shows the same file types in color alias ls = \u2019ls \u2013-color = auto\u2019 # creates an alias for ls with color ls \u2013lh file1 # list in human readable format ls \u2013lt /etc # shows long listing with time modified in descending order ls \u2013ltr /etc | less # shows reverse listing, q to quit","title":"Directory Listing and Alias"},{"location":"learning/linux/#synchronize-directories","text":"mkdir /backup rsync \u2013av /home/ /backup/ # archive home dir to backup dir. / after home and backup is important rsync \u2013av --delete /home/ /backup/ # sync deletions of data as well, otherwise rsync ignores it by default rsync \u2013ave ssh # sync data between servers using e option","title":"Synchronize Directories"},{"location":"learning/linux/#process-management","text":"","title":"Process Management"},{"location":"learning/linux/#monitor-process","text":"which ps # shows the installation directory for ps uptime # shows the uptime of the system along with the load average in the range of 1 min, 5 mins and 15 mins # Rule of Thumb for uptime --> Load average for single core value should be less than 1, for dual core less than 2 etc. which uptime # shows the installation directory for uptime cat /proc/uptime # shows uptime and idle time cat /proc/loadavg # shows load avg for 1,5 and 15 mins, active process running/total process, last process id that was issued","title":"Monitor Process"},{"location":"learning/linux/#jobs","text":"sleep 180 # sleeps for 180 secs in foreground. Ctrl + Z to pause the job. Run bg to put the sleep command in background. jobs # shows running jobs fg 1 # puts the sleep command in foreground","title":"Jobs"},{"location":"learning/linux/#managing-processes","text":"ps to display processes and kill to send signals. pgrep, pkill and killall are great shortcuts. The default kill signal is -15 which can also be written as \u2013term or \u2013sigterm . To really kill it is -9, -kill or \u2013sigkill . ps \u2013l # long listing with the process ps \u2013ef # shows all the processes for all users ps \u2013eaf | grep processname pgrep nginx # shows process ids for nginx sleep 900 & pkill sleep # searches for sleep process and kills it killall sleep # searches for all running sleep process and kills it kill \u2013l # shows the multiple kill signals available kill -9 <process id> # forcefully terminates the process, also use kill \u2013kill <process id> top \u2192 kill, renice, sort and display processes Running top, you can toggle between the information displayed at the top lines. l \u2013 on/off load, t \u2013 on/off tasks, m \u2013 on/off memory Sorting of top is on %CPU, f \u2013 shows current fields being shown on output of top. Select the new field to sort and type s Type r for renice and put in the process id. Esc and Enter to quit the shell Type k for kill and put in the process id. Esc and Enter to quit the shell q to quit out of top top # shows all running processes, q to quit top \u2013n 1 # shows the running processes for 1 capture and quits top \u2013n 2 \u2013d 3 # shows 2 captures with a delay of 3 seconds and quits","title":"Managing Processes"},{"location":"learning/linux/#editors","text":"","title":"Editors"},{"location":"learning/linux/#vi","text":": # Last line mode q, q! # quit the file x, wq, wq! # save and exit the file i, I # insert from cursor position, I for inserting from start of the line a, A # append after the cursor, A for append from last character in the line o, O # insert line below the cursor, O for above the current cursor position dd # delete the line u # undo the changes","title":"Vi"},{"location":"learning/linux/#line-navigation","text":"<Linenumber>G # e.g. 7G, takes cursor to 7th line in the file G # only G takes cursor to end of file w , b # w takes cursor to next word, b takes cursor to one word before ^ , $ # ^takes cursor to start of line, $ to end of the line vi +127 /etc/file1 # opens the file and takes cursor to 127th line vi +/Document /etc/file1.conf # opens the file and takes cursor to first occurrence of \u201cDocument\u201d set number / set nonumber # from last line mode, it will show and stop line number display syntax on # highlighting on, e.g. xml highlighting etc.","title":"Line Navigation"},{"location":"learning/linux/#read-and-write","text":"r /etc/hosts # Open an existing file, use : and then you can get content from hosts file into current file w newfile # :, it will copy entire file contents into newfile in the same directory 3 ,7w newfile # it will copy line 3 to 7 into newfile","title":"Read and Write"},{"location":"learning/linux/#search-and-replace","text":"%s/Hi/Hello # Open an existing file, use : and you can search Hi and Replace with Hello. %s signifies entire document search /Hello # searches for Hello in the document. Type n to get next occurrence, N will take cursor in reverse 1 ,20s/Hi/Hello # searches for 1st 20 lines for Hi and replaces with Hello 14 ,20s/^/ / # from 14th to 20th line, it will add 3 spaces from the start of the line, just like Tab","title":"Search and Replace"},{"location":"learning/linux/#bash-scripting","text":"","title":"BASH Scripting"},{"location":"learning/linux/#understanding-variables","text":"Local variables \u2192 accessible only to the current shell, FRUIT=\u2019apple\u2019, echo $FRUIT Global variables \u2192 you need to set and then export it to make it global. export FRUIT=\u2019apple\u2019","title":"Understanding Variables"},{"location":"learning/linux/#simple-script","text":"vi hello.sh #!/bin/bash # Path to the interpreter echo \u201cHello World\u201d exit 0 # return code, :wq chmod +x hello.sh hello.sh # execute the script as it\u2019s in the home directory /user/bin","title":"Simple Script"},{"location":"learning/linux/#getting-user-input","text":"vi hello.sh #!/bin/bash echo \u2013e \u201cEnter your name: \\c \u201d # -e is the escape sequence, -c is for the prompt read INPUT_NAME # read the input data into a variable echo \u201cHello $INPUT_NAME \u201d exit 0","title":"Getting user input"},{"location":"learning/linux/#user-input-types","text":"$1 $2 # $1 is the 1st input parameter, 2nd Parameter and so on. $0 # is the script name itself $# # count of input parameters $* # is collection of all the arguments","title":"User Input types"},{"location":"learning/linux/#multiple-inputs-using-positional-parameters","text":"vi hello.sh #!/bin/bash echo \u201cHello $1 $2 \u201d # $1 is the 1st input parameter, $0 is the script name itself, $2 is the 2nd input parameter and so on exit 0","title":"Multiple inputs using positional parameters"},{"location":"learning/linux/#code-snippets","text":"Gedit \u2192 Gnome Editor \u2192 Add the Snippet Plugin (Applications \u2192 Accessories \u2192 gedit. Preferences in gedit tab \u2192 Plugins enable Snippet Plugin and restart gedit)","title":"Code Snippets"},{"location":"learning/linux/#conditional-statement---if","text":"if [[condition]] \u2192 testing for string condition if ((condition)) \u2192 testing for numeric condition e.g. if (( $# < 1 )) \u2192 if count of input parameter vi hello.sh #!/bin/bash if (( $# 1 )) then echo \u201cUsage: $0 <name>\u201d exit 1 fi echo \u201cHello $1 $2 \u201d exit 0","title":"Conditional Statement - IF"},{"location":"learning/linux/#case-statement","text":"vi hello.sh #!/bin/bash if [[ ! \u2013d $1 ]] # if the 1st argument is not a directory then echo \u201cUsage: $0 <directory>\u201d exit 1 fi case $2 in \u201cdirectory\u201d ) find $1 \u2013maxdepth 1 \u2013type d ;; # break \u201clink\u201d ) find $1 \u2013maxdepth 1 \u2013type l ;; # break * ) # default statement echo \u201cUsage: $0 <directory> directory | link\u201d ;; esac exit 0","title":"Case Statement"},{"location":"learning/linux/#for","text":"vi hello.sh #!/bin/bash for u in $* # $* is collection of arguments, u is temporary variable do # do block useradd $u # access to temp variable is via $ echo Password1 | passwd \u2013stdin $u # use the passwd command and get the user input from keyboard passwd \u2013e $u # expire the password, so they can change it at first login done echo \u201cFinished\u201d # at time of execution ./hello.sh fred mary john vi listsize.sh #!/bin/bash for file in $( ls ) # for each file, in the output of ls do [[ ! \u2013f ]] && continue # not a file then continue to next # use the stats to get statistics of the file, to get the last accessed date and then format the date LA = $( stat \u2013c %x $file | cut \u2013d \u201c \u201d \u2013f1 ) echo \u201c $file is $( du \u2013b $file ) bytes and was last accessed on $LA \u201d # use du to get file size done","title":"For"},{"location":"learning/linux/#while","text":"vi loop.sh #!/bin/bash -x # -x is for debug mode COUNT = 10 while (( COUNT > 0 )) do echo \u2013e \u201c $COUNT \\c \u201d # \\c will suppress the line feed (enter) sleep 1 (( COUNT -- )) # round brackets to avoid using $ symbol done - Use the until when you want to stop the loop when the condition becomes true.","title":"While"},{"location":"learning/linux/#user-management","text":"Managing Users: User Lifecycle ==> useradd, usermod, userdel Local databases ==> /etc/passwd, /etc/shadow (encrypted) passwd (to set the password) pwconv (move pass to encrypted) pwunconv (move back to unencrypted) # /etc/passwd file structure # It has 7 filed separated by : Login Name, Optional encrypted password or \u201cx\u201d, Numerical UID, Numerical GID, Username or comment, User home directory, Optional command interpreter # /etc/shadow file structure where the actual passwords are stored # It has 8 filed separated by : Login Name, encrypted password ( if it begins with ! the account is locked ) , Date of last password change, Minimum password age, Maximum password age, Password warning period, password inactivity, account expiry date # /etc/login.defs The password ageing defaults can be configured with this file useradd \u2013D # shows the default settings for a user that is added cat /etc/default/useradd # shows where the defaults are set useradd bob # only adds the user, no home directory is created. Once the user logs in, it will get created tail -3 /etc/passwd # shows that bob is added useradd \u2013m bob # also creates the home directory tail -3 /etc/shadow # shows the password for the user passwd bob # add the password for bob passwd \u2013l bob # locks the account passwd \u2013u bob # unlocks the account usermod bob \u2013c \u201cBob Smith\u201d # adding additional details for the user userdel \u2013r bob # removes the user and home directory","title":"User Management"},{"location":"learning/linux/#group-management","text":"Group Lifecycle \u2192 groupadd, groupmod, groupdel Local databases \u2192 /etc/group, /etc/gshadow (encrypted) gpasswd (to set password) newgrp (switch to new groups) # /etc/group structure # It has 4 fields: Group Name, Password, Numerical GID, User list that is comma separated # /etc/gshadow structure # It has 4 field: Group Name, Encrypted password, Admin list that is comma separated, # this can be managed used \u2013A cmd # Members, this can be managed using the \u2013M cmd Private groups are enabled by default. The user added is also added to the same group. If this is disabled, users will belong to the groups users. Use useradd \u2013N to overwrite private groups. This can be enabled or disabled by setting USERGROUPS_ENAB in /etc/login.defs useradd \u2013m \u2013g users jim # -g is Primary Group, -G is secondary groups. Secondary groups are more traditional groups id jim usermod \u2013G sudo,adm jim # added jim to secondary groups sudo, adm useradd \u2013N \u2013m sally # adds sally to the default group gpasswd \u2013M jim,sally sudo # adds 2 users to sudo group groupadd sales gpasswd sales # sets the new password for sales newgrp sales # add the user to the sales group temporarily. If the user logs out, he is removed from the group","title":"Group Management"},{"location":"learning/linux/#automate-system-tasks","text":"Regular Tasks \u2192 cron (more than once a day but misses job if turned off), anacron (run jobs missed on startup but jobs can run just once a day) Once Off \u2192 at (runs at specified time and date), batch (runs when load average drops below 0.8)","title":"Automate System Tasks"},{"location":"learning/linux/#system-cron-jobs","text":"# /etc/crontab, /etc/cron.d # cron files # /etc/cron.<time> # where time is hourly, daily, weekly and monthly, contains scripts that need to be executed # Adding a system cron job cd /etc/cron.d vi daily-backup # add a new file 30 20 * * 1 -5 root /root/back.sh # run back.sh from Mon to Fri at 20:30 # Adding a user cron job crontab \u2013e # edit the user crontab file */10 10 1 1 1 tail /etc/passwd # Runs once on 1st day if it\u2019s a Mon of Jan, at 10 am for every 10 mins crontab \u2013l # list all cron jobs crontab \u2013r # remove the cron job # anacron: /etc/anacrontab structure # It has 4 fields Period in days or macro ( Daily, Monthly ) , Delay ( minutes after system startup for job to run ) , Job Identifier ( used to name timestamp file indicating when job was last run ) Command ( that needs to be executed ) @weekly 120 weekly-backup ls /etc // weekly, 120 mins after startup it will run weekly-backup","title":"System Cron Jobs"},{"location":"learning/linux/#batch","text":"at and batch commands at noon tomorrow # Enter the command line, Ctrl + D to save at> ls /etc # enter the command that needs to be executed atq # shows the jobs queue atrm # remove the job batch # Enter the command line, Ctrl + D to save at> ls /etc > /root/file1 # redirect the o/p to file1. It will run if the system load avg is less than 0.8.","title":"Batch"},{"location":"learning/linux/#security-for-cron","text":"Everyone is allowed to run their own cron and at jobs, unless you add entries to /etc/cron.allow or /etc/at.allow. No one is denied unless you add entries to /etc/cron.deny or /etc/at.deny","title":"Security for Cron"},{"location":"learning/linux/#networking-fundamentals","text":"","title":"Networking Fundamentals"},{"location":"learning/linux/#network-time-protocol-ntp","text":"","title":"Network Time Protocol (NTP)"},{"location":"learning/linux/#configuring-network-time-protocol-ntp","text":"# vi /etc/ntp.conf # prefixing i with date creates a backup of the original file. Removes commented and blank lines sed \u2013i. $( date +%F ) \u2018/^#d ; /^$/d\u2019 /etc/ntp.conf","title":"Configuring Network Time Protocol (NTP)"},{"location":"learning/linux/#implementing-the-configuration-file-changes","text":"vi /etc/ntp.conf # Add lines other can default just below the driftfile command statsdir /var/log/ntpstats # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server instead of on the internet debian servers","title":"Implementing the configuration file changes"},{"location":"learning/linux/#save-the-file-and-restart-the-service","text":"service ntp restart # sudo if no access # check if the ntpstats directory is accessible to the ntp service # The user should be ntp and it should have write access ls \u2013ld /var/log/ntpstats/ - Date \u2192 Current system date and time. This is the time in memory. - HwClock \u2192 Hardware date and time set by the BIOS. hwclock \u2013r # shows the hardware clock hwclock \u2013-systohc # sets the hardware clock from system clock Hwclock \u2013-hctosys # sets the system clock from hardware clock","title":"Save the file and restart the service"},{"location":"learning/linux/#ntp-tools","text":"ntpdate (once off adjustment) ntpq (query the ntp server) ntpq \u2013p (shows peers) ntpstat (Shows status but not on debian. Try ntpdc \u2013c sysinfo) ntpq \u2013c \u201cassociations\u201d \u2192 shows associations # Configuring NTP on centos --> Install ntp ntpdate 192 .168.0.3 # one off update with a local machine in the network # vi /etc/ntp.conf # Delete one of the server 0 line and add the local server here server 192 .168.0.3 iburst prefer # this will synchronize with the local server systemctl start ntpd # save and restart systemctl enable ntpd # enable to start at system startup","title":"NTP Tools"},{"location":"learning/linux/#managing-system-log-daemons","text":"","title":"Managing System Log Daemons"},{"location":"learning/linux/#rocket-fast-system-for-log-processing-rsyslogd","text":"rsyslogd \u2013v # vi /etc/rsyslog.conf # Adding a simple log rule # For any log event greater than or equal to info make a log entry in local5 log. Local5 could be a simple application local5.info /var/log/local5 systemctl restart rsyslog.service # restart the service # to test this in working using command line logger \u2013p local5.info \u201cScript started\u201d # p is priority, if you see /var/log/local5 file, the log would be present","title":"Rocket-Fast System for Log Processing (rsyslogd)"},{"location":"learning/linux/#varlog-folder-structure","text":"messages (Nearly everything is logged here) secure (su and sudo events amongst others) dmesg (kernel ring buffer messages)","title":"/var/log/ folder structure"},{"location":"learning/linux/#logrotate","text":"ls /etc/cron.daily # has the logrotate script which will rotate log files cd /etc/logrotate.d/ # folder where all apps rotation policy is set cp syslog local5 # copy existing app conf for local5 app # vi local5 # make edits to point to /var/log/local5 file /var/log/local5 { weekly # period for rotation size +10 # size of the file for rotation compress # use compression for the rotated log file rotate 4 # keep 4 weeks of logs before overwriting } # manually running the rotate logrotate /etc/logrotate.conf # on execution, all files mentioned will be interrogated and log backup will be created","title":"Logrotate"},{"location":"learning/linux/#journalctl","text":"Responsible for viewing and log management. Need to be a member of adm group to read this. By default journal is memory resident i.e. it will be lost on restart journalctl # view the journal journalctl \u2013n 10 # shows the last 10 entries journalctl \u2013n 10 \u2013p err # shows the last 10 entries with priority errors mkdir /var/log/journal # to make the journal data persistent systemctl restart system-journald systemctl status system-journald # shows that the journal data is persistent usermod \u2013a \u2013G adm username # adding user to adm group, -a is append chgrp \u2013R adm /var/log/journal # recursively give adm group access jornalctl \u2013-disk-usage # shows disk usage journalctl \u2013-verify # verify the journal integrity","title":"Journalctl"},{"location":"learning/linux/#ssh","text":"","title":"SSH"},{"location":"learning/linux/#remote-access-using-ssh","text":"Server Configuration /etc/ssh/sshd_config The public key of the server is used to authenticate to the client. The public key of the server is stored in /etc/ssh/ssh_host_rsa_key.pub It is down to the client to check the public key using: StrictHostkeyChecking Server public keys are stored centrally in /etc/ssh/ssh_known_host or locally under ~/.ssh/known_hosts","title":"Remote access using SSH"},{"location":"learning/linux/#ssh-server-configuration","text":"netstat \u2013antl # shows the open tcp ports of the server grep ssh /etc/services # shows the services using ssh lsof \u2013i # Also shows open ports # vi /etc/ssh/sshd_config # Uncomment AddressFamily line and change as below AddressFamily inet # Now ssh will only listen on IPv6 systemctl restart sshd # vi /etc/ssh/sshd_config # Uncomment below lines and modify LoginGraceTime 1m # To avoid denial of service attacks and freeing up your service quickly PermitRootLogin no # 2 level authentication, first as normal user and then root SyslogFacility AUTHPRIV ClientAliveInterval 300 ClientAliveCountMax 0 MaxSessions 10 systemctl restart sshd","title":"SSH Server Configuration"},{"location":"learning/linux/#client-configuration-and-authentication","text":"Client Configuration /etc/ssh/ssh_config Generate Private and Public keypair using ssh_keygen Use ssh-copy-id to copy to host we want to authenticate with. To provide Single Sign On using ssh-agent Client/User public keys are stored in ~/.ssh/authorized_keys using ssh-copy-id.","title":"Client Configuration and Authentication"},{"location":"learning/linux/#to-connect-to-server-using-ssh","text":"cd # home directory ls \u2013a # to show all hidden files ssh pi@192.168.0.97 # ssh using user and ip address. Add the password of the user to authenticate cd .ssh cat known_hosts # shows the client ip and public keys exit or logout or Ctrl + D # to end the ssh session","title":"To connect to server using ssh"},{"location":"learning/linux/#to-generate-keypairs","text":"cd .ssh # On the client home directory ssh-keygen \u2013t rsa # generate key pair. Private key is encrypted using a passphrase # This will copy the generated public key to the target server. To which user\u2019s directory at the server we will # connect as. Give the password of the server\u2019s account. ssh-copy-id \u2013i id_rsa.pub pi@192.168.0.97 ssh pi@192.168.0.97 # now connect to the server using passphrase of the private key # From another terminal say tty we can now add the private key once and don\u2019t need to authenticate to the target server ssh-agent bash # fire up another bash terminal ssh-add .ssh/id_rsa # add the private key from the home directory. Enter the passphrase ssh \u2013l or ssh \u2013L # list all identities added ssh pi@192.168.0.97","title":"To generate keypairs"},{"location":"learning/linux/#ssh-tunnels","text":"ssh \u2013f \u2013N \u2013L 80 :localhost:80 user@s1.com # -f = execute in background, -N = We are not running any commands on remote host # -L = listening on port 80, we are listening on localhost and forwarding to port 80 on the remote host # On the remote host it has to listen on ssh called s1.com. We connect as user called user. # Example # Webservice on 192.168.0.3 # on a different machine, login as standard user cd .ssh ssh \u2013f \u2013N \u2013L 9000 :localhost:80 andrew@192.168.0.3 # we are listening on port 9000 on the localhost and forwarding traffic to port 80 om 192.168.0.3 netstat \u2013antlp # we can see that localhost:9000 is listening on ssh # On the client machine open the browser and type in http://127.0.0.1:9000 we will see the webservice data kill <process id of ssh> # shutdown the ssh tunneling process after finishing the work","title":"SSH Tunnels"},{"location":"learning/linux/#configuring-network-protocols-in-linux","text":"","title":"Configuring Network Protocols in Linux"},{"location":"learning/linux/#etcservices","text":"Network services are identified by a port address Common services and associated port address is listed in /etc/services netstat \u2013alt will list services listening via TCP this resolves address to name in /etc/services # To verify the above we can use strace to map the netstat data with the services that are running strace netstat \u2013alt 2 > & 1 | grep /etc/services grep http /etc/services # service and port mapping","title":"/etc/services"},{"location":"learning/linux/#dig-command","text":"which dig # get the path of dig rpm \u2013qf /usr/bin/dig # dig is not installed by default. Hence needs to be installed. dig \u2013t AAAA ipv6.bbc.co.uk # shows the IPv6 address","title":"dig command"},{"location":"learning/linux/#interface-configuration-files","text":"ifconfig # shows ip address details ifconfig eth0 192 .168.0.99 netmask 255 .255.255.0 up # sets ip address for Ethernet card ip address show # same as ifconfig # These settings are lost upon restart unless they are written to configuration files. /etc/sysconfig/network-scripts/ # centos /etc/network/interfaces # debian","title":"Interface Configuration Files"},{"location":"learning/linux/#to-make-the-ip-address-static","text":"cd /etc/sysconfig/network-scripts/ vi ifcfg-ens32 # open the config file # Replace and add the lines BOOTPROTO = \u201dstatic\u201d # change from dhcp IPADDR = 192 .168.0.240 # select a static ip address NETMASK = 255 .255.255.0 # add a class c subnet GATEWAY = 192 .168.0.1 # add the gateway address DNS1 = 8 .8.8.8 # add google as the DNS server # Restart the services systemctl restart network.service","title":"To make the IP address static"},{"location":"learning/linux/#networking-tools","text":"","title":"Networking Tools"},{"location":"learning/linux/#nmap","text":"nmap localhost # shows all open ports used by localhost nmap 192 .168.0.3 # shows open ports at remote host nmap \u2013v 192 .168.0.3 # verbose mode nmap \u2013iL ip.txt # input file containing all ip address to be scanned","title":"nmap"},{"location":"learning/linux/#netstat","text":"netstat \u2013a # shows all connections netstat \u2013at # shows all tcp connections netstat \u2013alt # shows all listening tcp connections netstat \u2013altpe # shows all user and process ids and listening tcp connections netstat \u2013s # statistics netstat \u2013i # shows interfaces netstat \u2013g # multicast groups netstat \u2013nr # network route tables","title":"netstat"},{"location":"learning/linux/#show-sockets-ss","text":"ss \u2013t \u2013a # shows all tcp connections ss \u2013o state established \u2018 ( dport = :ssh or sport = :ssh ) \u2019 # shows all ssh connections ss \u2013x src /tmp/.X11-unix/* # shows X11 connections using socket files","title":"Show Sockets (ss)"},{"location":"learning/linux/#lsof","text":"lsof \u2013i -4 # list all ipv4 connections lsof \u2013i :23, 24 # list all port 22 and 23 connections lsof \u2013p 1385 # list process id 1385 connections","title":"lsof"},{"location":"learning/linux/#testing-network-connectivity","text":"ping www.centos.org # test network connectivity ping \u2013c3 www.centos.org # sends only 3 pings traceroute www.centos.org # describes the route to destination from source tracepath www.centos.org # shows maximum transmission unit size","title":"Testing Network Connectivity"},{"location":"learning/linux/#host-name-resolution-tools","text":"hostname # Confirms the hostname cat /etc/hostname # shows the current host name hostname myComputer # changes the hostname to myComputer. You need to login as root to change this dig www.centos.org # resolves to ip address dig \u2013t MX centos.org # mail servers associated with centos","title":"Host Name Resolution Tools"},{"location":"learning/linux/#managing-interfaces","text":"ip a s # Shows ip addresses ip n s # Shows Neighbor shows looking at ARP cache ip r s # Shows root table ip ma s # Shows Multicast groups ip l # Shows network cards ifdown eth0 # Brings down interface ifup eth0 # Brings up interface","title":"Managing Interfaces"},{"location":"learning/linux/#securing-access-to-your-server","text":"# Temporary disables logins for users other than root. Just the existence of this file will prevent user logins and is controlled via PAM(Pluggable Authentication Modules) /etc/nologin # Create a blank file, but if the user tries to login via ssh username@localhost, the connection will be immediately closed. Only root can access the server then. touch /etc/nologin rm /etc/nologin # Now users can login. This can be used as a temporary measure cd /etc/pam.d/ # Config files for authentication modules grep nologin * # Shows instances where nologin file exists last # shows last user activity present in /var/log/wtmp file. lastlog # List the last login time for each user lastlog | grep \u2013v Never # Reverse the grep search to check for all user logins","title":"Securing Access to your Server"},{"location":"learning/linux/#ulimit","text":"Puts restrictions on system resources ulimit \u2013a # Shows system limitations that can be applied ulimit \u2013u 8000 # -u is for avoiding fork bombs and is defaulted to 4096. A std user can set a new value to 8000. # It will remain for his profile till system restart. cd /etc/security # To set the limits which can be persisted even after restart cat limits.conf # Shows soft (can be changed by processes) and hard (can\u2019t be changed by processes) limits cd limits.d/ # ls to see the files inside this directory. Edit the file and add an entry for the user account with soft or hard limits and save the file.","title":"ulimit"},{"location":"learning/linux/#avoiding-fork-bombs","text":"They are a potential Denial of Service Attack ulimit \u2013u ##### Do not run on Production Machines. Test only in laptop ##### ps \u2013u username | wc -l # Shows the number of processes running under the user and gives the count # On the command line create a function called foo foo (){ echo hello } # Execute the function by just calling it and pressing enter foo # Similarly to execute a fork bomb, instead of foo, call it : : (){ : | : & # Here the function calls itself and pipes the output to itself } ; : # End the function with a semicolon and then call the function : # BEWARE: Do this only at your own risk. Ensure ulimit is set to protect the resources on the server #","title":"Avoiding fork bombs"},{"location":"learning/linux/#xinetd","text":"Called the super daemon as it can manage a lot of smaller services, secures access to your server /etc/xinetd.d /etc/xinetd.conf tftp server (Trivial file transfer protocol) # Sample configuration service tftp { socket_type = dgram # data gram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /tftpboot # root directory of the server disable = no }","title":"xinetd"},{"location":"learning/linux/#implementing-tftp-using-xinetdd-service","text":"ls /etc/xinetd.d/ # blank directory yum install tftp-server tftp # install client and server, also xinetd as it\u2019s a dependency vi /etc/xinetd.d/tftp # after installation, delete disable line from the configuration to enable tftp # if the server directory doent exist, create it mkdir \u2013p /var/lib/tftpboot systemctl enable xinetd systemctl start xinetd netstat \u2013aulpe | grep tftp # shows the port # As a root user, create a temp file inside var/lib/tftpboot directory with hello text vi var/lib/tftpboot/file1 # Logout and login as standard user. Use TFTP to transfer the file to standard user tftp 127 .0.0.1 # Press enter get file1 # Get the file1 created by root using tftp quit # At the same file location do a cat to see the contents of the file cat file1","title":"Implementing TFTP using xinetd.d service"},{"location":"learning/linux/#tcp-wrappers","text":"Alternative to firewalling on the server # To check if service supports TCP wrappers. Once we can determine this, we can use hosts.allow or hosts.deny ldd </path to service name> | grep libwrap # To set this up for a service, 2 entries are made # This in /etc/hosts.allow file to allow access to 0.3 IP which is raspberry pi. tftpd is the name of the binary in .tftpd : 192 .168.0.3 # This in /etc/hosts.deny file to deny access to all other IP except for 0.3 IP in .tftpd : ALL # If the client appears in both files then allow takes precedence and access is granted ldd /usr/sbin/xinetd | grep libwrap tftp 192 .168.0.240 # login from raspberry pi and access the remote server, disable your firewall before trying this.","title":"TCP Wrappers"},{"location":"learning/linux/#delegating-admin-rights-using-sudoers","text":"id # check if the user is already an admin and part of wheel group in centos cd /etc grep wheel sudoers # check the current setup visudo # to edit sudoers file %wheel ALL =( root ) ALL # Uncomment the line for %wheel, change (ALL) to (root), so only root can change the sudoers. Save and exit","title":"Delegating Admin rights using sudoers"},{"location":"learning/linux/#data-encryption","text":"","title":"Data Encryption:"},{"location":"learning/linux/#using-gpg-to-encrypt-data-between-users","text":"password = $( mkpassword \u2013m sha-512 Password1 ) # encrypt the password using sha and mkpassword and put in a variable echo $password for u in marta ivan ; do # take an input from marta and ivan for users sudo useradd \u2013m \u2013p $password $u # add the user and set the encrypted password done # Run the command and create 2 users ivan and marta # Install GPG if not present dkpg \u2013S $( which gpg ) # Login as ivan and generate private and public keys for gpg encryption su - ivan gpg --gen-key # Take the default settings gpg --list-key # List the keys gpg --export \u2013a <email from gpg gen-key step> > /tmp/ivankey # export the public key and place in tmp folder for marta to access exit su - marta vi secret.txt # create a plain txt file for encryption chmod 400 secret.txt # make the file writeable by marta only gpg --import /tmp/ivankey # import ivan public key gpg \u2013e \u2013r <ivan mailid> secret.txt # add the recipient and encrypt the file mv secret.txt.gpg /tmp/ # move the encrypted file to tmp and exit su \u2013 ivan gpg \u2013d /tmp/secret.txt.gpg # decrypt the file and enter the passphrase for the private key","title":"Using GPG to encrypt data between users"},{"location":"learning/linux/#implementing-luks-for-full-disk-encryption","text":"example Laptops, USB sudo apt-get install cryptsetup # Install LUKS tools lsblk # list the blocks sudo fdisk /dev/sdb # partition the sdb drive and create a new partition called sdb1 sudo cryptsetup \u2013y luksFormat /dev/sdb1 # Allows to store encrypted data in this partition. Give a passphrase sudo cryptsetup luksDump /dev/sdb1 # Shows details of the encrypted partition sudo cryptsetup luksOpen /dev/sdb1 private_data # Assign a mapper called private_data ls /dev/mapper # Shows the new mapper setup and that is a device sudo mkfs.ext4 /dev/mapper/private_data # format the device as ext4 sudo mount /dev/mapper/private_data /mnt # The device is mounted and any data that is stored will get encrypted","title":"Implementing LUKS for Full Disk Encryption"},{"location":"learning/linux/#efs-for-data-encryption","text":"sudo apt-get install ecryptfs-utils # Install EFS tools su \u2013 ivan # Login as std user ecryptfs-setup-private # Create a private key with passphrase. logout and login back as ivan ls # Private directory is created by EFS echo hello > Private/data # Write to private directory cat Private/data # ivan can see the data ecryptfs-umount-private # unmount the private directory ls # Now ivan cannot see the data cd /.Private/ # Go to the ivan\u2019s hidden Private folder with a dot ls # You can see the encrypted data file ecryptfs-mount-private # Mount the directory again","title":"EFS for data encryption"},{"location":"learning/linux/#compiling-software-from-source","text":"Install gcc compiler sudo apt-get install gcc make # go to gnu.org/software/Downloads -> coreutils -> file ending with .xz # tar -xvJf coreutils-8.28.tar.xz # J is for xz, j is for bzip2 # cd into src folder and select ls.c file # Add a line in main function printf(\"Hello World\\n\"); # After changes, change directory one level up and run the configure script bash configure # This will check for any configuration changes to the src files and update the make file # Excute the make command make # After binary code is compiled, it needs to be updated in OS sudo make install # Close the terminal and restart. New software is working ;)","title":"Compiling software from source"},{"location":"learning/python/","text":"# Range and For for index in range ( 6 ): print ( index ) # Range function is used generate a sequence of integers index = range ( 10 , - 1 , - 1 ) # start, stop and step, stops at 0 not including -1 # set class provides a mapping of unique immutable elements # One use of set is to remove duplicate elements dup_list = ( 'c' , 'd' , 'c' , 'e' ) beta = set ( dup_list ) uniq_list = list ( beta ) # dict class is an associative array of keys and values. keys must be unique immutable objects dict_syn = { 'k1' : 'v1' , 'k2' : 'v2' } dict_syn = dict ( k1 = 'v1' , k2 = 'v2' ) dict_syn [ 'k3' ] = 'v3' # adding new key value del ( dict_syn [ 'k3' ]) # delete key value print ( dict_syn . keys ()) # prints all keys print ( dict_syn . values ()) # prints all values # User Input name = input ( 'Name :' ) # Functions * A function is a piece of code , capable of performing a similar task repeatedly . * It is defined using ** def ** keyword in python . def < function_name > ( < parameter1 > , < parameter2 > , ... ): 'Function documentation' function_body return < value > * Parameters , return expression and documentation string are optional . def square ( n ): \"Returns Square of a given number\" return n ** 2 print ( square . __doc__ ) // prints the function documentation string * 4 types of arguments * Required Arguments : non - keyword arguments def showname ( name , age ) showname ( \"Jack\" , 40 ) // name = \"Jack\" , age = 40 showname ( 40 , \"Jack\" ) // name = 40 , age = \"Jack\" * Keyword Arguments : identified by paramater names def showname ( name , age ) showname ( age = 40 , name = \"Jack\" ) * Default Arguments : Assumes a default argument , if an arg is not passsed . def showname ( name , age = 50 ) showname ( \"Jack\" ) // name = \"Jack\" , age = 50 showname ( age = 40 , \"Jack\" ) // name = \"Jack\" , age = 40 showname ( name = \"Jack\" , age = 40 ) // name = \"Jack\" , age = 40 showname ( name = \"Jack\" , 40 ) // Python does not allow passing non - keyword after keyword arg . This will fail . * Variable Length Arguments : Function preocessed with more arguments than specified while defining the function def showname ( name , * vartuple , ** vardict ) # *vartuple = Variable non keyword argument which will be a tuple. Denoted by * # **vardict = Variable keyword argument which will be a dictionary. Denoted by ** showname ( \"Jack\" ) // name = \"Jack\" showname ( \"Jack\" , 35 , 'M' , 'Kansas' ) // name = \"Jack\" , * vartuple = ( 35 , 'M' , 'Kansas' ) showname ( \"Jack\" , 35 , city = 'Kansas' , sex = 'M' ) // name = \"Jack\" , * vartuple = ( 35 ), ** vardict = { city = 'Kansas' , sex = 'M' } # An Iterator is an object, which allows a programmer to traverse through all the elements of a collection, regardless of its specific implementation. x = [ 6 , 3 , 1 ] s = iter ( x ) print ( next ( s )) # -> 6 # List Comprehensions -> Alternative to for loops. * More concise , readable , efficient and mimic functional programming style . * Used to : Apply a method to all or specific elements of a list , and Filter elements of a list satisfying specific criteria . x = [ 6 , 3 , 1 ] y = [ i ** 2 for i in x ] # List Comprehension expression print ( y ) # -> [36, 9, 1] * Filter positive numbers ( using for and if ) vec = [ - 4 , - 2 , 0 , 2 , 4 ] pos_elm = [ x for x in vec if x >= 0 ] # Can be read as for every elem x in vec, filter x if x is greater than or equal to 0 print ( pos_elm ) # -> [0, 2, 4] * Applying a method to a list def add10 ( x ): return x + 10 n = [ 34 , 56 , 75 , 3 ] mod_n = [ add10 ( num ) for num in n ] print ( mod_n ) # A Generator is a function that produces a sequence of results instead of a single value def arithmatic_series ( a , r ): while a < 50 : yield a # yield is used in place of return which suspends processing a += r s = arithmatic_series ( 3 , 10 ) # Execution of further 'arithmetic series' can be resumed only by calling nextfunction again on generator 's' print ( s ) // Generator #output=3 print ( next ( s )) // Generator starts execution # output=13 print ( next ( s )) // resumed # output=23 # A Generator expresions are generator versions of list comprehensions. They return a generator instead of a list. x = [ 6 , 3 , 1 ] g = ( i ** 2 for i in x ) # generator expression print ( next ( g )) # -> 36 # Dictionary Comprehensions -> takes the form {key: value for (key, value) in iterable} myDict = { x : x ** 2 for x in [ 1 , 2 , 3 , 4 , 5 ]} print ( myDict ) # Output {1: 1, 2: 4, 3: 9, 4: 16, 5: 25} # Calculate the frequency of each identified unique word in the list words = [ 'Hello' , 'Hi' , 'Hello' ] freq = { w : words . count ( w ) for w in words } print ( freq ) # Output {'Hello': 2, 'Hi': 1} Create the dictionary frequent_words , which filter words having frequency greater than one words = [ 'Hello' , 'Hi' , 'Hello' ] freq = { w : words . count ( w ) for w in words if words . count ( w ) > 1 } print ( freq ) # Output {'Hello': 2} # Defining Classes * Syntax class < ClassName > ( < parent1 > , ... ): class_body # Creating Objects * An object is created by calling the class name followed by a pair of parenthesis . class Person : pass p1 = Person () # Creating the object 'p1' print ( p1 ) # -> '<__main__.Person object at 0x0A...>' # tells you what class it belongs to and hints on memory address it is referenced to. # initializer method -> __init__ * defined inside the class and called by default , during an object creation . * It also takes self as the first argument , which refers to the current object . class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname p1 = Person ( 'George' , 'Smith' ) print ( p1 . fname , '-' , p1 . lname ) # -> 'George - Smith' # Documenting a Class * Each class or a method definition can have an optional first line , known as docstring . class Person : 'Represents a person.' # Inheritance * Inheritance describes is a kind of relationship between two or more classes , abstracting common details into super class and storing specific ones in the subclass . * To create a child class , specify the parent class name inside the pair of parenthesis , followed by it 's name. class Child ( Parent ): pass * Every child class inherits all the behaviours exhibited by their parent class . * In Python , every class uses inheritance and is inherited from ** object ** by default . class MySubClass ( object ): # object is known as parent or super class. pass # Inheritance in Action class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname class Employee ( Person ): all_employees = [] def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) # Employee class utilizes __init __ method of the parent class Person to create its object. self . empid = empid Employee . all_employees . append ( self ) e1 = Employee ( 'Jack' , 'simmons' , 456342 ) print ( e1 . fname , '-' , e1 . empid ) # Output -> Jack - 456342 # Polymorphism * Polymorphism allows a subclass to override or change a specific behavior , exhibited by the parent class class Employee ( Person ): all_employees = EmployeesList () def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) self . empid = empid Employee . all_employees . append ( self ) def getSalary ( self ): return 'You get Monthly salary.' def getBonus ( self ): return 'You are eligible for Bonus.' * Definition of ContractEmployee class derived from Employee. It overrides functionality of getSalary and getBonus methods found in it 's parent class Employee. class ContractEmployee ( Employee ): def getSalary ( self ): return 'You will not get Salary from Organization.' def getBonus ( self ): return 'You are not eligible for Bonus.' e1 = Employee ( 'Jack' , 'simmons' , 456342 ) e2 = ContractEmployee ( 'John' , 'williams' , 123656 ) print ( e1 . getBonus ()) # Output - You are eligible for Bonus. print ( e2 . getBonus ()) # Output - You are not eligible for Bonus. # Abstraction * Abstraction means working with something you know how to use without knowing how it works internally . * It is hiding the defaults and sharing only necessary information . # Encapsulation * Encapsulation allows binding data and associated methods together in a unit i . e class . * Bringing related data and methods inside a class to avoid misuse outside . * These principles together allows a programmer to define an interface for applications , i . e . to define all tasks the program is capable to execute and their respective input and output data . * A good example is a television set . We don \u2019 t need to know the inner workings of a TV , in order to use it . All we need to know is how to use the remote control ( i . e the interface for the user to interact with the TV ) . # Abstracting Data * Direct access to data can be restricted by making required attributes or methods private , ** just by prefixing it 's name with one or two underscores.** * An attribute or a method starting with : + ** no underscores ** is a ** public ** one . + ** a single underscore ** is ** private ** , however , still accessible from outside. + ** double underscores ** is ** strongly private ** and not accessible from outside. # Abstraction and Encapsulation Example * ** empid ** attribute of Employee class is made private and is accessible outside the class only using the method ** getEmpid **. class Employee ( Person ): all_employees = EmployeesList () def __init__ ( self , fname , lname , empid ): Person . __init__ ( self , fname , lname ) self . __empid = empid Employee . all_employees . append ( self ) def getEmpid ( self ): return self . __empid e1 = Employee ( 'Jack' , 'simmons' , 456342 ) print ( e1 . fname , e1 . lname ) # Output -> Jack simmons print ( e1 . getEmpid ()) # Output -> 456342 print ( e1 . __empid ) # Output -> AttributeError: Employee instance has no attribute '__empid' # Exceptions * Python allows a programmer to handle such exceptions using ** try ... except ** clauses , thus avoiding the program to crash . * Some of the python expressions , though written correctly in syntax , result in error during execution . ** Such scenarios have to be handled .** * In Python , every error message has two parts . The first part tells what type of exception it is and second part explains the details of error . # Handling Exception * A try block is followed by one or more except clauses . * The code to be handled is written inside try clause and the code to be executed when an exception occurs is written inside except clause . try : a = pow ( 2 , 4 ) print ( \"Value of 'a' :\" , a ) b = pow ( 2 , 'hello' ) # results in exception print ( \"Value of 'b' :\" , b ) except TypeError as e : print ( 'oops!!!' ) print ( 'Out of try ... except.' ) Output -> Value of 'a' : 16 --> oops !!! --> Out of try ... except . # Raising Exceptions * ** raise ** keyword is used when a programmer wants a specific exception to occur . try : a = 2 ; b = 'hello' if not ( isinstance ( a , int ) and isinstance ( b , int )): raise TypeError ( 'Two inputs must be integers.' ) c = a ** b except TypeError as e : print ( e ) # User Defined Exception Functions * Python also allows a programmer to create custom exceptions , derived from base Exception class . class CustomError ( Exception ): def __init__ ( self , value ): self . value = value def __str__ ( self ): return str ( self . value ) try : a = 2 ; b = 'hello' if not ( isinstance ( a , int ) and isinstance ( b , int )): raise CustomError ( 'Two inputs must be integers.' ) # CustomError is raised in above example, instead of TypeError. c = a ** b except CustomError as e : print ( e ) # Using 'finally' clause * ** finally ** clause is an optional one that can be used with try ... except clauses . * All the statements under finally clause are executed irrespective of exception occurrence . def divide ( a , b ): try : result = a / b return result except ZeroDivisionError : print ( \"Dividing by Zero.\" ) finally : print ( \"In finally clause.\" ) # Statements inside finally clause are ALWAYS executed before the return back # Using 'else' clause * ** else ** clause is also an optional clause with try ... except clauses . * Statements under else clause are executed ** only when no exception occurs in try clause **. try : a = 14 / 7 except ZeroDivisionError : print ( 'oops!!!' ) else : print ( 'First ELSE' ) try : a = 14 / 0 except ZeroDivisionError : print ( 'oops!!!' ) else : print ( 'Second ELSE' ) Output : First ELSE --> oops !!! # Module * Any file containing logically organized Python code can be used as a module . * A module generally contains ** any of the defined functions , classes and variables **. A module can also include executable code . * Any Python source file can be used as a module by using an import statement in some other Python source file . # Packages * A package is a collection of modules present in a folder . * The name of the package is the name of the folder itself . * A package generally contains an empty file named ** __init__ . py ** in the same folder , which is required to treat the folder as a package . # Import Modules import math # Recommended method of importing a module import math as m from math import pi , tan from math import pi as pie , tan as tangent # Working with Files * Data from an opened file can be read using any of the methods : ** read , readline and readlines **. * Data can be written to a file using either ** write ** or ** writelines ** method . * A file ** must be opened ** , before it is used for reading or writing . fp = open ( 'temp.txt' , 'r' ) # opening ( operations 'r' & 'w') content = fp . read () # reading fp . close () # closing # read() -> Reads the entire contents of a file as bytes. # readline() -> Reads a single line at a time. # readlines() -> Reads a all the line & each line is stored as an element of a list. # write() -> Writes a single string to output file. # writelines() -> Writes multiple lines to output file & each string is stored as an element of a list. * Reading contents of file and storing as a dictionary fp = open ( 'emp_data.txt' , 'r' ) emps = fp . readlines () # Preprocessing data emps = [ emp . strip ( ' \\n ' ) for emp in emps ] emps = [ emp . split ( ';' ) for emp in emps ] header = emps . pop # remove header record separately emps = [ dict ( zip ( header , emp ) for emp in emps ] # header record is used to combine with data to form a dictionary print ( emps [: 2 ]) # prints first 2 records * Filtering data based on criteria fil_emps = [ emp [ 'Emp_name' ] for emp in emps if emp [ 'Emp_work_location' ] == 'HYD' ] * Filtering data based on pattern import re pattern = re . compile ( r 'oracle' , re . IGNORECASE ) # Regular Expression oracle_emps = [ emp [ 'Emp_name' ] for emp in emps if pattern . search ( emp [ 'Emp_skillset' ])] * Filter and Sort data in ascending order fil_emps = [ emp for emp in emps if emp [ 'Emp_designation' ] == 'ASE' ] fil_emps = sorted ( fil_emps , key = lambda k : k [ 'Emp_name' ]) print ( emp [ 'Emp_name' ] for emp in fil_emps ) * Sorting all employees based on custom sorting criteria order = { 'ASE' : 1 , 'ITA' : 2 , 'AST' : 3 } sorted_emp = sorted ( emp , key = lambda k : order [ k [ 'designation' ]]) * Filter data and write into files fil_emps = [ emp for emp in emps if emp [ 'Emp_Designation' ] == 'ITA' ] ofp = open ( outputtext . txt , 'w' ) keys = fil_emps [ 0 ] . keys () # Remove header from key name for key in keys : ofp . write ( key + \" \\t \" ) ofp . write ( \" \\n \" ) for emp in fil_emps : for key in keys : ofp . write ( emp [ key ] + \" \\t \" ) ofp . write ( \" \\n \" ) ofp . close () # Regular Expressions * Regex are useful to construct patterns that helps in filtering the text possessing the pattern . * ** re module ** is used to deal with regex . * ** search ** method takes pattern and text to scan and returns a Match object . Return None if not found . * Match object holds info on the nature of the match like ** original input string , Regular expression used , location within the original string ** match = re . search ( pattern , text ) start_index = match . start () # start location of match end_index = match . end () regex = match . re . pattern () print ( 'Found \" {} \" pattern in \" {} \" from {} to {} ' . format ( st , text , start_index , end_index )) # Compiling Expressions * In Python , its more efficient t compile the patterns that are frequently used . * ** compile ** function of re module converts an expression string into a ** RegexObject **. patterns = [ 'this' , 'that' ] regexes = [ re . compile ( p ) for p in patterns ] for regex in regexes : if regex . search ( text ): # pattern is not required print ( 'Match found' ) * search method only returns the first matching occurrence . # Finding Multiple Matches * findall method returns all the substrings of the pattern without overlapping pattern = 'ab' for match in re . findall ( pattern , text ): print ( 'match found - {} ' . format ( match )) # Grouping Matches * Adding groups to a pattern enables us to isolate parts of the matching text , expanding those capabilities to create a parser . * Groups are defined by enclosing patterns within parenthesis text = 'This is some text -- with punctuations.' for pattern in [ r '^(\\w+)' , # word at the start of the string r '(\\w+)\\S*$' , # word at the end of the string with punctuation r '(\\bt\\w+)\\W+(\\w+)' , # word staring with 't' and the next word r '(\\w+t)\\b' ]: # word ending with t regex = re . compile ( pattern ) match = regex . search ( text ) print ( match . groups ()) # Output -> ('This',) ('punctuations',) ('text','with') ('text',) # Naming Grouped Matches * Accessing the groups with defined names text = 'This is some text -- with punctuations.' for pattern in [ r '^(?P<first_word>\\w+)' , # word at the start of the string r '(?P<last_word>\\w+)\\S*$' , # word at the end of the string with punctuation r '(?P<t_word>\\bt\\w+)\\W+(?P<other_word>\\w+)' , # word staring with 't' and the next word r '(?P<ends_with_t>\\w+t)\\b' ]: # word ending with t regex = re . compile ( pattern ) match = regex . search ( text ) print ( \"Groups: \" , match . groups ()) # Output -> ('This',) ('punctuations',) ('text','with') ('text',) print ( \"Group Dictionary: \" , match . groupdict ()) # Output -> {'first_word':'This'} {'last_word': 'punctuations'} {'t_word':'text', 'other_word':'with'} {'ends_with_t':'text'} # Data Handling # Handling XML files * ** lxml ** 3 rd party module is a highly feature rich with ElementTree API and supports querying wthe xml content using XPATH . * In the ElementTree API , an element acts like a list . The items of the list are the elements children . * XML search is faster in lxml . < ? xml > < employee > < skill name = \"Python\" /> </ employee > from lxml import etree tree = etree . parse ( 'sample.xml' ) root = tree . getroot () # gets doc root <?xml> skills = tree . findall ( '//skill' ) # gets all skill tags for skill in skills : print ( \"Skills: \" , skill . attrib [ 'name' ]) # Adding new skill in the xml skill = etree . SubElement ( root , 'skill' , attrib = { 'name' : 'PHP' }) # Handling HTML files * ** lxml ** 3 rd party module is used for parsing HTML files as well . import urllib.request from lxml import etree def readURL ( url ): urlfile = urllib . request . urlopen ( url ) if urlfile . getcode () == 200 : contents = urlfile . read () return contents if __name__ == '__main__' : url = 'http://xkcd.com' html = readURL ( url ) # Data Serialization * Process of converting ** data types / objects ** into ** Transmittable / Storable ** format is called Data Serialization . * In python , ** pickle and json ** modules are used for Data Serialization . * Serialized data can then be written to file / Socket / Pipe . From these it can be de - serialized and stored into a new Object . json . dump ( data , file , indent = 2 ) # serialized data is written to file with indentation using dump method data_new = json . load ( file ) # de-serialized data is written to new object using load method # Database Connectivity * ** Python Database API ( DB - API ) ** is a standard interface to interact with various databases . * Different DB API \u2019 s are used for accessing different databases . Hence a programmer has to install DB API corresponding to the database one is working with . * Working with a database includes the following steps : + Importing the corresponding DB - API module . + Acquiring a connection with the database . + Executing SQL statements and stored procedures . + Closing the connection import sqlite3 # establishing a database connection con = sqlite3 . connect ( 'D: \\\\ TEST.db' ) # preparing a cursor object cursor = con . cursor () # preparing sql statements sql1 = 'DROP TABLE IF EXISTS EMPLOYEE' # closing the database connection con . close () # Inserting Data * Single rows are inserted using ** execute ** and multiple rows using ** executeMany ** method of created cursor object . # preparing sql statement rec = ( 456789 , 'Frodo' , 45 , 'M' , 100000.00 ) sql = ''' INSERT INTO EMPLOYEE VALUES ( ?, ?, ?, ?, ?) ''' # executing sql statement using try ... except blocks try : cursor . execute ( sql , rec ) con . commit () except Exception as e : print ( \"Error Message :\" , str ( e )) con . rollback () # Fetching Data * ** fetchone ** : It retrieves one record at a time in the form of a tuple . * ** fetchall ** : It retrieves all fetched records at a point in the form of tuple of tuples . # fetching the records records = cursor . fetchall () # Displaying the records for record in records : print ( record ) # Object Relational Mappers * An object - relational mapper ( ORM ) is a library that automates the transfer of data stored in relational database tables into objects that are adopted in application code . * ORMs offer a high - level abstraction upon a relational database , which permits a developer to write Python code rather than SQL to create , read , update and delete data and schemas in their database . * Such an ability to write Python code instead of SQL speeds up web application development . # Higher Order Functions * A ** Higher Order function ** is a function , which is capable of doing any one of the following things : + It can be functioned as a ** data ** and be assigned to a variable . + It can accept any other ** function as an argument **. + It can return a ** function as its result **. * The ability to build Higher order functions , ** allows a programmer to create Closures , which in turn are used to create Decorators **. # Function as a Data def greet (): return 'Hello Everyone!' print ( greet ()) wish = greet # 'greet' function assigned to variable 'wish' print ( type ( wish )) # Output -> <type 'function'> print ( wish ()) # Output -> Hello Everyone! # Function as an Argument def add ( x , y ): return x + y def sub ( x , y ): return x - y def prod ( x , y ): return x * y def do ( func , x , y ): return func ( x , y ) print ( do ( add , 12 , 4 )) # 'add' as arg # Output -> 16 print ( do ( sub , 12 , 4 )) # 'sub' as arg # Output -> 8 print ( do ( prod , 12 , 4 )) # 'prod' as arg # Output -> 48 # Returning a Function def outer (): def inner (): s = 'Hello world!' return s return inner () print ( outer ()) # Output -> Hello world! * You can observe from the output that the ** return value of 'outer' function is the return value of 'inner' function ** i . e 'Hello world!' . def outer (): def inner (): s = 'Hello world!' return s return inner # Removed '()' to return 'inner' function itself print ( outer ()) #returns 'inner' function # Output -> <function inner at 0xxxxxx> func = outer () print ( type ( func )) # Output -> <type 'function'> print ( func ()) # calling 'inner' function # Output -> Hello world! * Parenthesis after the ** inner ** function are removed so that the ** outer ** function returns ** inner function **. # Closures * A Closure is a ** function returned by a higher order function ** , whose return value depends on the data associated with the higher order function . def multiple_of ( x ): def multiple ( y ): return x * y return multiple c1 = multiple_of ( 5 ) # 'c1' is a closure c2 = multiple_of ( 6 ) # 'c2' is a closure print ( c1 ( 4 )) # Output -> 5 * 4 = 20 print ( c2 ( 4 )) # Output -> 6 * 4 = 24 * The first closure function , c1 binds the value 5 to argument x and when called with an argument 4 , it executes the body of multiple function and returns the product of 5 and 4. * Similarly c2 binds the value 6 to argument x and when called with argument 4 returns 24. # Decorators * Decorators are evolved from the concept of closures . * A decorator function is a higher order function that takes a function as an argument and returns the inner function . * A decorator is capable of adding extra functionality to an existing function , without altering it . * The decorator function is prefixed with **@ symbol ** and written above the function definition . + Shows the creation of closure function wish using the higher order function outer . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner def greet (): print ( 'Hello!' ) wish = outer ( greet ) # Output -> Accessing : greet wish () # Output -> Hello! - wish is the closure function obtained by calling an outer function with the argument greet . When wish function is called , inner function gets executed . + The second one shows the creation of decorator function outer , which is used to decorate function greet . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner def greet (): return 'Hello!' greet = outer ( greet ) # decorating 'greet' # Output -> No Output as return is used instead of print greet () # calling new 'greet' # Output -> Accessing : greet - The function returned by outer is assigned to greet i . e the function name passed as argument to outer . This makes outer a decorator to greet . + Third one displays decorating the greet function with decorator function , outer , using @ symbol . def outer ( func ): def inner (): print ( \"Accessing :\" , func . __name__ ) return func () return inner @outer # This is same as **greet = outer(greet)** def greet (): return 'Hello!' greet () # Output -> Accessing : greet # Descriptors * Python descriptors allow a programmer to create managed attributes . * In other object - oriented languages , you will find ** getter and setter ** methods to manage attributes . * However , Python allows a programmer to manage the attributes simply with the attribute name , without losing their protection . * This is achieved by defining a ** descriptor class ** , that implements any of ** __get__ , __set__ , __delete__ ** methods . class EmpNameDescriptor : def __get__ ( self , obj , owner ): return self . __empname def __set__ ( self , obj , value ): if not isinstance ( value , str ): raise TypeError ( \"'empname' must be a string.\" ) self . __empname = value * The descriptor , EmpNameDescriptor is defined to manage empname attribute . It checks if the value of empname attribute is a string or not . class EmpIdDescriptor : def __get__ ( self , obj , owner ): return self . __empid def __set__ ( self , obj , value ): if hasattr ( obj , 'empid' ): raise ValueError ( \"'empid' is read only attribute\" ) if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value * The descriptor , EmpIdDescriptor is defined to manage empid attribute . class Employee : empid = EmpIdDescriptor () empname = EmpNameDescriptor () def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name * Employee class is defined such that , it creates empid and empname attributes from descriptors EmpIdDescriptor and EmpNameDescriptor . e1 = Employee ( 123456 , 'John' ) print ( e1 . empid , '-' , e1 . empname ) # Output -> '123456 - John' e1 . empid = 76347322 # Output -> ValueError: 'empid' is read only attribute # Properties * Descriptors can also be created using property () type . + Syntax : property ( fget = None , fset = None , fdel = None , doc = None ) - where , fget : attribute get method fset : attribute set method fdel \u2013 attribute delete method doc \u2013 docstring class Employee : def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name def getEmpID ( self ): return self . __empid def setEmpID ( self , value ): if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value empid = property ( getEmpID , setEmpID ) # Property Decorators * Descriptors can also be created with property decorators . * While using property decorators , an attribute 's get method will be same as its name and will be decorated with property. * In a case of defining any set or delete methods , they will be decorated with respective setter and deleter methods . class Employee : def __init__ ( self , emp_id , emp_name ): self . empid = emp_id self . empname = emp_name @property def empid ( self ): return self . __empid @empid . setter def empid ( self , value ): if not isinstance ( value , int ): raise TypeError ( \"'empid' must be an integer.\" ) self . __empid = value e1 = Employee ( 123456 , 'John' ) print ( e1 . empid , '-' , e1 . empname ) # Output -> '123456 - John' # Introduction to Class and Static Methods Based on the ** scope ** , functions / methods are of two types . They are : * Class methods * Static methods # Class Methods * A method defined inside a class is bound to its object , by default . * However , if the method is bound to a Class , then it is known as ** classmethod **. class Circle ( object ): no_of_circles = 0 def __init__ ( self , radius ): self . __radius = radius Circle . no_of_circles += 1 def getCirclesCount ( self ): return Circle . no_of_circles c1 = Circle ( 3.5 ) c2 = Circle ( 5.2 ) c3 = Circle ( 4.8 ) print ( c1 . getCirclesCount ()) # -> 3 print ( Circle . getCirclesCount ( c3 )) # -> 3 print ( Circle . getCirclesCount ()) # -> TypeError: getCirclesCount() missing 1 required positional argument: 'self' class Circle ( object ): no_of_circles = 0 def __init__ ( self , radius ): self . __radius = radius Circle . no_of_circles += 1 @classmethod def getCirclesCount ( self ): return Circle . no_of_circles c1 = Circle ( 3.5 ) c2 = Circle ( 5.2 ) c3 = Circle ( 4.8 ) print ( c1 . getCirclesCount ()) # -> 3 print ( Circle . getCirclesCount ()) # -> 3 # Static Method * A method defined inside a class and not bound to either a class or an object is known as ** Static ** Method . * Decorating a method using ** @staticmethod ** decorator makes it a static method . def square ( x ): return x ** 2 class Circle ( object ): def __init__ ( self , radius ): self . __radius = radius def area ( self ): return 3.14 * square ( self . __radius ) c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 print ( square ( 10 )) # -> 100 * square function is not packaged properly and does not appear as integral part of class Circle . class Circle ( object ): def __init__ ( self , radius ): self . __radius = radius @staticmethod def square ( x ): return x ** 2 def area ( self ): return 3.14 * self . square ( self . __radius ) c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 print ( square ( 10 )) # -> NameError: name 'square' is not defined * square method is no longer accessible from outside the class Circle . * However , it is possible to access the static method using Class or the Object as shown below . print ( Circle . square ( 10 )) # -> 100 print ( c1 . square ( 10 )) # -> 100 # Abstract Base Classes * An ** Abstract Base Class ** or ** ABC ** mandates the derived classes to implement specific methods from the base class . * It is not possible to create an object from a defined ABC class . * Creating objects of derived classes is possible only when derived classes override existing functionality of all abstract methods defined in an ABC class . * In Python , an Abstract Base Class can be created using module abc . from abc import ABC , abstractmethod class Shape ( ABC ): @abstractmethod def area ( self ): pass @abstractmethod def perimeter ( self ): pass * Abstract base class Shape is defined with two abstract methods area and perimeter . class Circle ( Shape ): def __init__ ( self , radius ): self . __radius = radius @staticmethod def square ( x ): return x ** 2 def area ( self ): return 3.14 * self . square ( self . __radius ) def perimeter ( self ): return 2 * 3.14 * self . __radius c1 = Circle ( 3.9 ) print ( c1 . area ()) # -> 47.7594 # Context Manager * A Context Manager allows a programmer to perform required activities , automatically , while entering or exiting a Context . * For example , opening a file , doing few file operations , and closing the file is manged using Context Manager as shown below . with open ( 'sample.txt' , 'w' ) as fp : content = fp . read () * The keyword ** with ** is used in Python to enable a context manager . It automatically takes care of closing the file . import sqlite3 class DbConnect ( object ): def __init__ ( self , dbname ): self . dbname = dbname def __enter__ ( self ): self . dbConnection = sqlite3 . connect ( self . dbname ) return self . dbConnection def __exit__ ( self , exc_type , exc_val , exc_tb ): self . dbConnection . close () with DbConnect ( 'TEST.db' ) as db : cursor = db . cursor () ''' Few db operations ... ''' * Example from contextlib import contextmanager @contextmanager def context (): print ( 'Entering Context' ) yield print ( \"Exiting Context\" ) with context (): print ( 'In Context' ) # Output -> Entering Context -> In Context -> Exiting Context # Coroutines * A Coroutine is ** generator ** which is capable of constantly receiving input data , process input data and may or may not return any output . * Coroutines are majorly used to build better ** Data Processing Pipelines **. * Similar to a generator , execution of a coroutine stops when it reaches ** yield ** statement . * A Coroutine uses ** send ** method to send any input value , which is captured by yield expression . def TokenIssuer (): tokenId = 0 while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) t = TokenIssuer () next ( t ) t . send ( 'George' ) # -> Token number of George: 1 t . send ( 'Rosy' ) # -> Token number of Rosy: 2 * ** TokenIssuer ** is a coroutine function , which uses yield to accept name as input . * Execution of coroutine function begins only when next is called on coroutine t . * This results in the execution of all the statements till a yield statement is encountered . * Further execution of function resumes when an input is passed using send , and processes all statements till next yield statement . def TokenIssuer ( tokenId = 0 ): try : while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) except GeneratorExit : print ( 'Last issued Token is :' , tokenId ) t = TokenIssuer ( 100 ) next ( t ) t . send ( 'George' ) # Token number of George: 101 t . send ( 'Rosy' ) # Token number of Rosy: 102 t . send ( 'Smith' ) # Token number of Smith: 103 t . close () # Last issued Token is: 103 * The coroutine function TokenIssuer takes an argument , which is used to set a starting number for tokens . * When coroutine t is closed , statements under GeneratorExit block are executed . * Many programmers may forget that passing input to coroutine is possible only after the first next function call , which results in error . * Such a scenario can be avoided using a decorator . def coroutine_decorator ( func ): def wrapper ( * args , ** kwdargs ): c = func ( * args , ** kwdargs ) next ( c ) return c return wrapper @coroutine_decorator def TokenIssuer ( tokenId = 0 ): try : while True : name = yield tokenId += 1 print ( 'Token number of' , name , ':' , tokenId ) except GeneratorExit : print ( 'Last issued Token is :' , tokenId ) t = TokenIssuer ( 100 ) t . send ( 'George' ) t . send ( 'Rosy' ) t . send ( 'Smith' ) t . close () * coroutine_decorator takes care of calling next on the created coroutine t . def nameFeeder (): while True : fname = yield print ( 'First Name:' , fname ) lname = yield print ( 'Last Name:' , lname ) n = nameFeeder () next ( n ) n . send ( 'George' ) n . send ( 'Williams' ) n . send ( 'John' ) First Name : George Last Name : Williams First Name : John","title":"Python"},{"location":"learning/terraform/","text":"Example using Docker images \u00b6 // Main . tf resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image . nginx . latest name = \"webserver\" ports { internal = 80 external = 8050 } } Infrastructure as Code \u00b6 IaC is an important DevOps cornerstone that enables you to define, automatically manage, and provision infrastructure through source code. Infrastructure is managed as a software system . IaC is frequently referred to as Programmable Infrastructure . Benefits of Iac \u00b6 In IaC, A tool will monitor the state of the infrastructure . A script will be sent automatically to fix the issue . A script can be written to add new instances, and it can be reused . Faster process , no/less human involvement. IaC Implemetation Approaches \u00b6 ##Declarative + Focuses on the desired end state of infrastructure (Functional) . + Tools perform the necessary actions to reach that state . + Automatically takes care of the order and executes it . + Examples are Terraform and CloudFormation. Imperative \u00b6 Focuses on how to achieve the desired state (Procedural) . Follows the order of tasks and it executes in that order . Examples are Chef and Ansible. Configuration Management \u00b6 It is a system that keeps the track of organization's hardware, software, and other related information. It includes the software updates and versions present in the system. Configuration management is also called as configuration control . E.g.: Chef and Ansible Orchestration: \u00b6 It automates the coordination, management, and configuration of systems and hardware. E.g.: Terraform and Nomad Installing Terraform \u00b6 sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip unzip terraform_0.11.11_linux_amd64.zip sudo mv terraform /usr/local/bin/ terraform \u2013version Terraform Lifecycle \u00b6 Terraform init - Initializes the working directory having Terraform configuration files. Terraform plan - Creates an execution plan and it mentions the changes it is going to make. Terraform apply - When you are not amazed by seeing the results of the terraform plan, you can go ahead and run terraform apply to apply the changes. The main difference between plan and apply is - apply asks for a prompt like Do you want to perform these actions? and then implement the changes, whereas, plan shows the possible end state of your infrastructure without actually implementing them. Terraform destroy - Destroys the created infrastructure. Terraform Configuration \u00b6 A set of files that describe the infrastructure in Terraform is called as Terraform Configuration. The entire configuration file is saved with .tf extension. Make sure to have all the configurations maintained in a single .tf file instead of many. Terraform loads all the .tf files present in the working directory. Creating Virtual Network \u00b6 create a virtual network without using the Azure portal and azure CLI commands. resource \"azurerm_virtual_network\"\"myterraformnetwork\" { name = \"myvnet\" address_space = [\"10.0.0.0/16\"] location=\"East US\" resource_group_name=\"er-tyjy\" } Resource blocks define the infrastructure (resource \"azurerm_virtual_network\" \"myterraformnetwork\"). It contains the two strings - type of the resource(azurerm_virtualnetwork) and name of the resource(myterraformnetwork). Terraform uses plugin based architecture to support the various services providers and infrastructure available. When you run Terraform init command, it downloads and installs provider binary for the corresponding providers. In this case, it is Azure. Now, you can run the terraform plan command. It shows all the changes it is going to make. If the output is as you expected and there are no errors, then you can run Terraform apply. Terraform Validate \u00b6 You can run this command whenever you like to check whether the code is correct. Once you run this command, if there is no output, it means that there are no errors in the code. Variables \u00b6 Terraform provides the following variable types: Strings - The default data type is a string . List - A list is like simple arrays Map - A map value is nothing but a lookup table for string keys to get the string values. In terraform, to reduce the complexity of code and for easiness , all the variables are created in the variables.tf file. variables.tf variable \"rg\" { default = \"terraform-lab2\" } variable \"locations\" { default = [\"ap-southeast-1\" , \"ap-southeast-2\"] } variable \"tags\" { type = \"map\" default = { environment = \"training\" source=\"citadel\" } } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-east-2 = \"image-4321\" } } For example, you have defined resource_group name in the variable.tf file. you can call them in main.tf file like this \"${var.resource_group}\" Sensitive Parameters \u00b6 There may be sensitive parameters which should be shown only when running the terraform plan but not terraform apply. output \"sensitiveoutput\" { sensitive = true value = VALUE } When you run terraform apply, the output is labeled as sensitive. If there is any sensitive information, then you can protect it by using a sensitive parameter. terraform.tfvars File \u00b6 In terraform.tfvars file, we mention the \"key/value\" pair. The main.tf file takes the input from a terraform. tfvars file. If it is not present, it gets the input from variables.tf file. resource_group = \"user-ybje\" location = \"East US\" terraform fmt \u00b6 It rewrites the confguration files to canonical style and format. State File - terraform.tfstate \u00b6 It is a JSON file that maps the real world resources to the configuration files. In terraform, there are three states. Desired state - Represents how the infrastructure to be present. Actual state - Represents the current state of infrastructure. Known state - To bridge desired state and actual state, there is a known state. You can get the details of known state from a tfstate file. When you run terraform plan command , terraform performs a quick refresh and lists the actions needed to achieve the desired state. When terraform apply command is run, it applies the changes and updates the actual state. Modules \u00b6 A module is like a reusable blueprint of infrastructure . A module is nothing but a folder of Terraform files. In terraform the modules are categorized into two types: Root modules - The current directory of Terraform files on which you are running the commands. Child modules - The modules sourced by the root module. To create a child module, add resources to it. Module syntax: module \"child\" { source = \"../child\" } The difference between adding the resources and module is for adding the resource (Eg: resource \"azurerm_virtual_network\" 'test\") you need type and name but for adding a module (Eg: module \"child\") only the name is enough. The name of the module should be unique within configurations because it is used as a reference to the module and outputs. main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } Updates \u00b6 terraform get - This command is used to download and install the modules for that configuration. terraform get -update - It checks the downloaded modules and checks for the recent versions if any. Module Outputs \u00b6 If you need output from any module, you can get it by passing the required parameter. For example, the output of virtual network location can be found by following the below syntax. You can add this syntax to .tf file(main.tf or variables.tf) . output \"virtual_network_id\" { value = \"${azurerm_virtual_network.test.location}\" } Where virtual_network_id is a unique name and \"${azurerm_virtual_network.test.location}\" is the location of the virtual network using string interpolation. Benefits of Modules \u00b6 Code reuse : When there is a need to provision the group of resources on another resource at the same time, a module can be used instead of copying the same code. It helps in resolving the bugs easily. If you want to make changes, changing the code in one place is enough. Abstraction layer : It makes complex configurations easier to conceptualize. For example, if you like to add vault(Another harshicop's tool for managing secrets) cluster to the environment, it requires dozens of components. Instead of worrying about individual components, it gives ready-to-use vault cluster. Black box : To create a vault, you only need some configuration parameters without worrying about the complexity. You don't need any knowledge on how to install vault, configuring vault cluster and working of the vault. You can create a working cluster in a few minutes. Best practices in organization : When a module is created, you can give it to the other teams. It helps in easily developing the infrastructure. **Versioned artifacts: They are immutable artifacts and can be promoted from one environment to others. Introduction to Meta Parameters \u00b6 There are some difficulties with the declarative language. For example, since it is a declarative language, there is no concept of for loops. How do you repeat a piece of code without copy paste? * Terraform provides primitives like meta parameter called as count, life cycle blocks like create_before_destroy, ternary operator and a large number of interpolation functions . This helps in performing certain types of loops, if statements and zero downtime deployments. Count \u00b6 Count - It defines how many parameters you like to create. Modify main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } To increase Vnets to 5, update count = 5 . To decrease Vnets to 2, update count = 2 , last 3 Vnets will get deleted. Elements \u00b6 Suppose you like to create the three virtual networks with names vnet-A, vnet-B and vnet-C, you can do this easily with the help of list . Mention how many vnets you are going to create with the help of list and define it in variables.tf file variable \"name\" { type= \"list\" default = [\"A\",\"B\",\"C\"] } You can call this variable in the main.tf file in the following way count = \"${length(var.name)}\" - It returns number of elements present in the list. you should store it in meta parameter count. \"${element(var.name,count.index)}\" - It acts as a loop, it takes the input from list and repeats until there are no elements in list. variables.tf ** variable \"resource_group\" { default = \"user-nuqo\" } variable \"location\" { default = \"East US\" } variable \"name\" { type = \"list\" default = [\"A\",\"B\",\"C\"] } **main.tf resource \"azurerm_virtual_network\"\"multiple\" { name = \"vnet-${element(var.name,count.index)}\" resource_group_name = \"${var.resource_group}\" location = \"${var.location}\" address_space=[\"10.0.0.0/16\"] count=\"${length(var.name)}\" } Conditions \u00b6 For example, a vnet has to be created only, if the variable number of vnets is 3 or else no. You can do this by using the ternary operator. variables.tf variable \"no_of_vnets\" { default= 3 } main.tf count = \"${var.no_of_vnets ==3 ?1 : 0}\" * First, it will execute the ternary operator. If it is true, it takes the output as 1 or else 0. * Now, in the above case, the output is 1, the count becomes one, and the vnet is created. Inheriting Variables \u00b6 Inheriting the variables between modules keeps your Terraform code DRY. Don't Repeat Yourself (DRY) It aims at reducing repeating patterns and code. There is no need to write the duplicate code for each environment you are deploying. You can write the minimum code and achieve the goal by having maximum re-usability. Module File Structure \u00b6 You will follow the below file structure to inherit the variables between modules. modules | |___mod | |__mod.tf | |___vnet |__vnet.tf |__vars.tf There is one vnet and you can configure the vnet by passing the parameters from module. The variables in the vnet are overriden by the values provided in mod.tf file. Create a folder with name Modules Create two subfolders of names vnet and mod in the nested_modules folder. In the vnet folder, create a virtual network using Terraform configuration. Maintain two files one for main configuration(vnet.tf) and other for declaring variables(vars.tf) . Now, create main.tf file like this resource \"azurerm_virtual_network\"\"vnet\" { name = \"${var.name}\" address_space = [\"10.0.0.0/16\"] resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" } Now, vars.tf file looks like this variable \"name\" { description = \"name of the vnet\" } variable \"resourcegroup\" { description = \"name of the resource group\" } variable \"location\" { description =\"name of the location\" } variable \"address\" { type =\"list\" description = \"specify the address\" } Create a folder with name mod in the modules directory. Create a file with name mod.tf and write the below code in it. We are passing the variables from the root module (mod) to the child module (vnet) using source path . module \"child\" { source = \"../vnet\" name = \"modvnet\" resourcegroup = \"user-vcom\" location = \"eastus\" address = \"10.0.1.0/16\" } Run terraform plan to visualize how the directories will be created. You can see the name of the vnet is taken from the module file instead of getting it from vars.tf Nested Modules \u00b6 For any module, root module should exist. Root module is basically a directory that holds the terraform configuration files for the desired infrastructure. They also provide the entry point to the nested modules you are going to utilize. For any module main.tf, vars.tf, and outputs.tf naming conventions are recommended. If you are using nested modules, create them under the subdirectory with name modules . checking_modules |__modules | |____main.tf | |______virtual_network | |_main.tf | |_vars.tf | |______storage_account |_main.tf |_vars.tf You will create three folders with names modules, virtual_network, and storage account. Make sure that you are following the terraform standards like writing the terraform configuration in the main file and defining the variables alone in the variables(vars.tf) file. stoacctmain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"${var.name}\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"${var.account_replication}\" account_tier = \"${var.account_tier}\" } stoacctvars.tf variable \"rg\" { default = \"user-wmle\" } variable \"loc\" { default = \"eastus\" } variable \"name\" { default =\"storagegh\" } variable \"account_replication\" { default = \"LRS\" } variable \"account_tier\" { default = \"Basic\" } Root Module (main.tf) file under checking_modules module \"vnet\" { source = \"../virtual_network\" name = \"mymodvnet\" } module \"stoacct\" { source = \"../storage_account\" name = \"mymodstorageaccount\" } Not only the parameters mentioned above, but you can also pass any parameter from root module to the child modules. You can pass the parameters from the module to avoid the duplicity between them. Remote Backends \u00b6 It is better to store the code in a central repository . However, is having some disadvantages of doing so: You have to maintain push, pull configurations . If one pushes the wrong configuration and commits it, it becomes a problem. State file consists of sensitive information . So state files are non-editable. Backend in terraform explains how the state file is loaded and operations are executed. Backend can get initialize only after running terraform init command. So, terraform init is required to be run every time when backend is configured when any changes made to the backend when the backend configuration is removed completely Terraform cannot perform auto-initialize because it may require additional info from the user, to perform state migrations, etc.. Below are the steps which you will be maintaining for creating remote backend in Azure. Create a storage account Create a storage container Create a backend Get the storage account access key and resource group name, give it as a parameter to the backend. Below is the complete file structure for sample backend. Backend |____stoacct.tf |____stocontainer.tf |____backend.tf |____vars.tf |____output.tf stoacct.tf resource \"azurerm_storage_account\" \"storageaccount\" { name = \"storageaccountname\" resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" account_tier = \"${var.accounttier}\" account_replication_type = \"GRS\" tags { environment = \"staging\" } } stocontainer.tf resource \"azurerm_storage_container\" \"storagecontainer\" { name = \"vhds\" resource_group_name = \"${var.resourcegroup}\" storage_account_name = \"${azurerm_storage_account.storageaccount.name}\" container_access_type = \"private\" } vars.tf variable \"resourcegroup\" { default = \"user-abcd\" } variable \"location\" { default = \"eastus\" } variable \"accounttier\" { default = \"Basic\" } output.tf output \"storageacctname\" { value = \"${azurerm_storage_account.storageaccount.name}\" } output \"storageacctcontainer\" { value = \"${azurerm_storage_account.storagecontainer.name}\" } output \"access_key\" { value = \"${azurerm_storage_account.storageaccount.primary_access_key\" } Before creating backend, run the command to get the storage account keys list( az storage account keys list \u2013account-name storageacctname ) and copy one of the key somewhere. It is useful for configuring the backend. backend.tf terraform{ backend \"azurerm\" { storage_account_name = \"storageacct21\" container_name = \"mycontainer\" key = \"terraform.tfstate.change\" resource_group_name = \"user-okvt\" access_key = \"aJlf+XjZhxwRp4gsU4hkGrQJzO7xBzz7B9rSzMLB/ozwcM6k/1N.......==\" } } where Parameters: storage_account_name - name of the storage account. container_name - name of the container. key- name of the tfstate blob. resource_group_name - name of the resource group. access_key - Storage account access key(any one). Points to Remember \u00b6 You cannot use interpolation syntax to configure backends. After creating backend run terraform init, it will be in the locked state. By running terraform apply command automatically, the lease status is changed to locked. After it is applied, it will come to an unlocked state. This backend supports consistency checking and state locking using the native capabilities of the Azure storage account. Terragrunt \u00b6 Terragrunt is referred to as a thin wrapper for Terraform which provides extra tools to keep the terraform configurations DRY, manage remote state and to work with multiple terraform modules. Terragrunt Commands \u00b6 terragrunt get terragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt supports the following use cases: Keeps terraform code DRY. Remote state configuration DRY. CLI flags DRY. Executing terraform commands on multiple modules at a time. Advantages of using terragrunt: Provides locking mechanism. Allows you to use remote state always. Build-In Functions \u00b6 lookup \u00b6 This helps in performing a dynamic lookup into a map variable. The syntax for lookup variable is lookup(map,key, [default]) . map: The map parameter is a variable like var.name. key: The key parameter includes the key from where it should find the environment. If the key doesn't exist in the map, interpolation will fail, if it doesn't find a third argument (default). The lookup will not work on nested lists or maps. It only works on flat maps. Local Values \u00b6 Local values help in assigning names to the expressions , which can be used multiple times within a module without rewriting it. Local values are defined in the local blocks. It is recommended to group the logically related local values together as a single block, if there is a dependency between them. locals { service_name = \"Fresco\" owner = \"Team\" } Example locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } If a single value or a result is used in many places and it is likely to be changed in the future, then you can go with locals. It is recommended to not use many local values because it makes the read configurations hard to the future maintainers. Data Source \u00b6 Data source allows the data to be computed or fetched for use in Terraform configuration. Data sources allow the terraform configurations to build on the information defined outside the Terraform or by another Terraform configuration. Providers play a major role in implementing and defining data sources. Data source helps in two major ways: It provides a read-only view for the pre-existing data. It can compute new values on the fly. Every data source in the Terraform is mapped to the provider depending on the longest-prefix-matching . data \"azurerm_resource_group\" \"passed\" { name = \"${var.resource_group_name}\" } Concat and Contains \u00b6 concat(list1,list2) * It combines two or more lists into a single list. contains(list, element) * It returns true if the element is present in the list or else false. E.g. contains(var.list_of_strings, \"an_element\") Workspaces \u00b6 Every terraform configurations has associate backends which defines how the operations are executed and where the persistent data like terraform state are stored. Persistent data stored in backend belongs to a workspace. Previously, the backend has only one workspace which is called as default and there will be only one state file associated with the configuration. Some of the backends support multiple workspaces, i.e. It allows multiple state files to be stored within a single configuration. The configuration is still having only one backend, and the multiple distinct instances can be deployed without configuring to a new backend or by changing authentication credentials. default workspace is special because you can't delete the default workspace. You cannot delete your active workspace (the workspace in which you are currently working). If you haven't created the workspace before, then you are working on the default workspace. Workspace Commands \u00b6 terraform workspace new - It creates the workspace with specified name and switches to it. terraform workspace list - It lists the workspaces available. terraform workspace select - It switches to the specified workspace. terraform workspace delete - It deletes the mentioned workspace. Configuring Multiple Providers \u00b6 Below is the file structure that you will maintain for working with multiple providers. multiple_providers | |___awsmain.tf | |___azuremain.tf | |___vars.tf | |___out.tf Create an S3 bucket in Azure which helps in storing the file in AWS. awsmain.tf resource \"aws_s3_bucket\" \"b\" { bucket = \"my-tf-test-bucket-21\" acl = \"private\" tags = { Name = \"My bucket test\" Environment = \"Dev\" } } Resource has two parameters: name(aws_s3_bucket) and type(\"b\"). Name of the resource may vary from provider to provider. Type of the resource depends on the user. Bucket indicates the name of the bucket. If it is not assigned, terraform assigns a random unique name. acl (access control list) - It helps to manage access to the buckets and objects. tag - It is a label which is assigned to the AWS resource by you or AWS. Environment - On which environment do you like to deploy S3 bucket (Dev, Prod or Stage). azuremain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"storageacct21\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"LRS\" account_tier = \"Standard\" } Parameters name - It indicates the name of the storage account. resource_group_name and location- name of the resource group and location. account_replication_type - Type of the account replication. account_tier - Account tier type out.tf output \"account_tier\" { value = \"azurerm_storage_Account.storagecct.account_tier\" }","title":"Terraform"},{"location":"learning/terraform/#example-using-docker-images","text":"// Main . tf resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image . nginx . latest name = \"webserver\" ports { internal = 80 external = 8050 } }","title":"Example using Docker images"},{"location":"learning/terraform/#infrastructure-as-code","text":"IaC is an important DevOps cornerstone that enables you to define, automatically manage, and provision infrastructure through source code. Infrastructure is managed as a software system . IaC is frequently referred to as Programmable Infrastructure .","title":"Infrastructure as Code"},{"location":"learning/terraform/#benefits-of-iac","text":"In IaC, A tool will monitor the state of the infrastructure . A script will be sent automatically to fix the issue . A script can be written to add new instances, and it can be reused . Faster process , no/less human involvement.","title":"Benefits of Iac"},{"location":"learning/terraform/#iac-implemetation-approaches","text":"##Declarative + Focuses on the desired end state of infrastructure (Functional) . + Tools perform the necessary actions to reach that state . + Automatically takes care of the order and executes it . + Examples are Terraform and CloudFormation.","title":"IaC Implemetation Approaches"},{"location":"learning/terraform/#imperative","text":"Focuses on how to achieve the desired state (Procedural) . Follows the order of tasks and it executes in that order . Examples are Chef and Ansible.","title":"Imperative"},{"location":"learning/terraform/#configuration-management","text":"It is a system that keeps the track of organization's hardware, software, and other related information. It includes the software updates and versions present in the system. Configuration management is also called as configuration control . E.g.: Chef and Ansible","title":"Configuration Management"},{"location":"learning/terraform/#orchestration","text":"It automates the coordination, management, and configuration of systems and hardware. E.g.: Terraform and Nomad","title":"Orchestration:"},{"location":"learning/terraform/#installing-terraform","text":"sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip unzip terraform_0.11.11_linux_amd64.zip sudo mv terraform /usr/local/bin/ terraform \u2013version","title":"Installing Terraform"},{"location":"learning/terraform/#terraform-lifecycle","text":"Terraform init - Initializes the working directory having Terraform configuration files. Terraform plan - Creates an execution plan and it mentions the changes it is going to make. Terraform apply - When you are not amazed by seeing the results of the terraform plan, you can go ahead and run terraform apply to apply the changes. The main difference between plan and apply is - apply asks for a prompt like Do you want to perform these actions? and then implement the changes, whereas, plan shows the possible end state of your infrastructure without actually implementing them. Terraform destroy - Destroys the created infrastructure.","title":"Terraform Lifecycle"},{"location":"learning/terraform/#terraform-configuration","text":"A set of files that describe the infrastructure in Terraform is called as Terraform Configuration. The entire configuration file is saved with .tf extension. Make sure to have all the configurations maintained in a single .tf file instead of many. Terraform loads all the .tf files present in the working directory.","title":"Terraform Configuration"},{"location":"learning/terraform/#creating-virtual-network","text":"create a virtual network without using the Azure portal and azure CLI commands. resource \"azurerm_virtual_network\"\"myterraformnetwork\" { name = \"myvnet\" address_space = [\"10.0.0.0/16\"] location=\"East US\" resource_group_name=\"er-tyjy\" } Resource blocks define the infrastructure (resource \"azurerm_virtual_network\" \"myterraformnetwork\"). It contains the two strings - type of the resource(azurerm_virtualnetwork) and name of the resource(myterraformnetwork). Terraform uses plugin based architecture to support the various services providers and infrastructure available. When you run Terraform init command, it downloads and installs provider binary for the corresponding providers. In this case, it is Azure. Now, you can run the terraform plan command. It shows all the changes it is going to make. If the output is as you expected and there are no errors, then you can run Terraform apply.","title":"Creating Virtual Network"},{"location":"learning/terraform/#terraform-validate","text":"You can run this command whenever you like to check whether the code is correct. Once you run this command, if there is no output, it means that there are no errors in the code.","title":"Terraform Validate"},{"location":"learning/terraform/#variables","text":"Terraform provides the following variable types: Strings - The default data type is a string . List - A list is like simple arrays Map - A map value is nothing but a lookup table for string keys to get the string values. In terraform, to reduce the complexity of code and for easiness , all the variables are created in the variables.tf file. variables.tf variable \"rg\" { default = \"terraform-lab2\" } variable \"locations\" { default = [\"ap-southeast-1\" , \"ap-southeast-2\"] } variable \"tags\" { type = \"map\" default = { environment = \"training\" source=\"citadel\" } } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-east-2 = \"image-4321\" } } For example, you have defined resource_group name in the variable.tf file. you can call them in main.tf file like this \"${var.resource_group}\"","title":"Variables"},{"location":"learning/terraform/#sensitive-parameters","text":"There may be sensitive parameters which should be shown only when running the terraform plan but not terraform apply. output \"sensitiveoutput\" { sensitive = true value = VALUE } When you run terraform apply, the output is labeled as sensitive. If there is any sensitive information, then you can protect it by using a sensitive parameter.","title":"Sensitive Parameters"},{"location":"learning/terraform/#terraformtfvars-file","text":"In terraform.tfvars file, we mention the \"key/value\" pair. The main.tf file takes the input from a terraform. tfvars file. If it is not present, it gets the input from variables.tf file. resource_group = \"user-ybje\" location = \"East US\"","title":"terraform.tfvars File"},{"location":"learning/terraform/#terraform-fmt","text":"It rewrites the confguration files to canonical style and format.","title":"terraform fmt"},{"location":"learning/terraform/#state-file---terraformtfstate","text":"It is a JSON file that maps the real world resources to the configuration files. In terraform, there are three states. Desired state - Represents how the infrastructure to be present. Actual state - Represents the current state of infrastructure. Known state - To bridge desired state and actual state, there is a known state. You can get the details of known state from a tfstate file. When you run terraform plan command , terraform performs a quick refresh and lists the actions needed to achieve the desired state. When terraform apply command is run, it applies the changes and updates the actual state.","title":"State File - terraform.tfstate"},{"location":"learning/terraform/#modules","text":"A module is like a reusable blueprint of infrastructure . A module is nothing but a folder of Terraform files. In terraform the modules are categorized into two types: Root modules - The current directory of Terraform files on which you are running the commands. Child modules - The modules sourced by the root module. To create a child module, add resources to it. Module syntax: module \"child\" { source = \"../child\" } The difference between adding the resources and module is for adding the resource (Eg: resource \"azurerm_virtual_network\" 'test\") you need type and name but for adding a module (Eg: module \"child\") only the name is enough. The name of the module should be unique within configurations because it is used as a reference to the module and outputs. main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 }","title":"Modules"},{"location":"learning/terraform/#updates","text":"terraform get - This command is used to download and install the modules for that configuration. terraform get -update - It checks the downloaded modules and checks for the recent versions if any.","title":"Updates"},{"location":"learning/terraform/#module-outputs","text":"If you need output from any module, you can get it by passing the required parameter. For example, the output of virtual network location can be found by following the below syntax. You can add this syntax to .tf file(main.tf or variables.tf) . output \"virtual_network_id\" { value = \"${azurerm_virtual_network.test.location}\" } Where virtual_network_id is a unique name and \"${azurerm_virtual_network.test.location}\" is the location of the virtual network using string interpolation.","title":"Module Outputs"},{"location":"learning/terraform/#benefits-of-modules","text":"Code reuse : When there is a need to provision the group of resources on another resource at the same time, a module can be used instead of copying the same code. It helps in resolving the bugs easily. If you want to make changes, changing the code in one place is enough. Abstraction layer : It makes complex configurations easier to conceptualize. For example, if you like to add vault(Another harshicop's tool for managing secrets) cluster to the environment, it requires dozens of components. Instead of worrying about individual components, it gives ready-to-use vault cluster. Black box : To create a vault, you only need some configuration parameters without worrying about the complexity. You don't need any knowledge on how to install vault, configuring vault cluster and working of the vault. You can create a working cluster in a few minutes. Best practices in organization : When a module is created, you can give it to the other teams. It helps in easily developing the infrastructure. **Versioned artifacts: They are immutable artifacts and can be promoted from one environment to others.","title":"Benefits of Modules"},{"location":"learning/terraform/#introduction-to-meta-parameters","text":"There are some difficulties with the declarative language. For example, since it is a declarative language, there is no concept of for loops. How do you repeat a piece of code without copy paste? * Terraform provides primitives like meta parameter called as count, life cycle blocks like create_before_destroy, ternary operator and a large number of interpolation functions . This helps in performing certain types of loops, if statements and zero downtime deployments.","title":"Introduction to Meta Parameters"},{"location":"learning/terraform/#count","text":"Count - It defines how many parameters you like to create. Modify main.tf file so that it creates three virtual networks at a time. resource \"azurerm_virtual_network\"\"multiplevnets\" { name = \"multiplevnets-${count.index}\" resource_group_name=\"${var.resource_group\"} location=\"${var.location}\" address_space=[\"10.0.0.0/16\"] count = 3 } To increase Vnets to 5, update count = 5 . To decrease Vnets to 2, update count = 2 , last 3 Vnets will get deleted.","title":"Count"},{"location":"learning/terraform/#elements","text":"Suppose you like to create the three virtual networks with names vnet-A, vnet-B and vnet-C, you can do this easily with the help of list . Mention how many vnets you are going to create with the help of list and define it in variables.tf file variable \"name\" { type= \"list\" default = [\"A\",\"B\",\"C\"] } You can call this variable in the main.tf file in the following way count = \"${length(var.name)}\" - It returns number of elements present in the list. you should store it in meta parameter count. \"${element(var.name,count.index)}\" - It acts as a loop, it takes the input from list and repeats until there are no elements in list. variables.tf ** variable \"resource_group\" { default = \"user-nuqo\" } variable \"location\" { default = \"East US\" } variable \"name\" { type = \"list\" default = [\"A\",\"B\",\"C\"] } **main.tf resource \"azurerm_virtual_network\"\"multiple\" { name = \"vnet-${element(var.name,count.index)}\" resource_group_name = \"${var.resource_group}\" location = \"${var.location}\" address_space=[\"10.0.0.0/16\"] count=\"${length(var.name)}\" }","title":"Elements"},{"location":"learning/terraform/#conditions","text":"For example, a vnet has to be created only, if the variable number of vnets is 3 or else no. You can do this by using the ternary operator. variables.tf variable \"no_of_vnets\" { default= 3 } main.tf count = \"${var.no_of_vnets ==3 ?1 : 0}\" * First, it will execute the ternary operator. If it is true, it takes the output as 1 or else 0. * Now, in the above case, the output is 1, the count becomes one, and the vnet is created.","title":"Conditions"},{"location":"learning/terraform/#inheriting-variables","text":"Inheriting the variables between modules keeps your Terraform code DRY. Don't Repeat Yourself (DRY) It aims at reducing repeating patterns and code. There is no need to write the duplicate code for each environment you are deploying. You can write the minimum code and achieve the goal by having maximum re-usability.","title":"Inheriting Variables"},{"location":"learning/terraform/#module-file-structure","text":"You will follow the below file structure to inherit the variables between modules. modules | |___mod | |__mod.tf | |___vnet |__vnet.tf |__vars.tf There is one vnet and you can configure the vnet by passing the parameters from module. The variables in the vnet are overriden by the values provided in mod.tf file. Create a folder with name Modules Create two subfolders of names vnet and mod in the nested_modules folder. In the vnet folder, create a virtual network using Terraform configuration. Maintain two files one for main configuration(vnet.tf) and other for declaring variables(vars.tf) . Now, create main.tf file like this resource \"azurerm_virtual_network\"\"vnet\" { name = \"${var.name}\" address_space = [\"10.0.0.0/16\"] resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" } Now, vars.tf file looks like this variable \"name\" { description = \"name of the vnet\" } variable \"resourcegroup\" { description = \"name of the resource group\" } variable \"location\" { description =\"name of the location\" } variable \"address\" { type =\"list\" description = \"specify the address\" } Create a folder with name mod in the modules directory. Create a file with name mod.tf and write the below code in it. We are passing the variables from the root module (mod) to the child module (vnet) using source path . module \"child\" { source = \"../vnet\" name = \"modvnet\" resourcegroup = \"user-vcom\" location = \"eastus\" address = \"10.0.1.0/16\" } Run terraform plan to visualize how the directories will be created. You can see the name of the vnet is taken from the module file instead of getting it from vars.tf","title":"Module File Structure"},{"location":"learning/terraform/#nested-modules","text":"For any module, root module should exist. Root module is basically a directory that holds the terraform configuration files for the desired infrastructure. They also provide the entry point to the nested modules you are going to utilize. For any module main.tf, vars.tf, and outputs.tf naming conventions are recommended. If you are using nested modules, create them under the subdirectory with name modules . checking_modules |__modules | |____main.tf | |______virtual_network | |_main.tf | |_vars.tf | |______storage_account |_main.tf |_vars.tf You will create three folders with names modules, virtual_network, and storage account. Make sure that you are following the terraform standards like writing the terraform configuration in the main file and defining the variables alone in the variables(vars.tf) file. stoacctmain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"${var.name}\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"${var.account_replication}\" account_tier = \"${var.account_tier}\" } stoacctvars.tf variable \"rg\" { default = \"user-wmle\" } variable \"loc\" { default = \"eastus\" } variable \"name\" { default =\"storagegh\" } variable \"account_replication\" { default = \"LRS\" } variable \"account_tier\" { default = \"Basic\" } Root Module (main.tf) file under checking_modules module \"vnet\" { source = \"../virtual_network\" name = \"mymodvnet\" } module \"stoacct\" { source = \"../storage_account\" name = \"mymodstorageaccount\" } Not only the parameters mentioned above, but you can also pass any parameter from root module to the child modules. You can pass the parameters from the module to avoid the duplicity between them.","title":"Nested Modules"},{"location":"learning/terraform/#remote-backends","text":"It is better to store the code in a central repository . However, is having some disadvantages of doing so: You have to maintain push, pull configurations . If one pushes the wrong configuration and commits it, it becomes a problem. State file consists of sensitive information . So state files are non-editable. Backend in terraform explains how the state file is loaded and operations are executed. Backend can get initialize only after running terraform init command. So, terraform init is required to be run every time when backend is configured when any changes made to the backend when the backend configuration is removed completely Terraform cannot perform auto-initialize because it may require additional info from the user, to perform state migrations, etc.. Below are the steps which you will be maintaining for creating remote backend in Azure. Create a storage account Create a storage container Create a backend Get the storage account access key and resource group name, give it as a parameter to the backend. Below is the complete file structure for sample backend. Backend |____stoacct.tf |____stocontainer.tf |____backend.tf |____vars.tf |____output.tf stoacct.tf resource \"azurerm_storage_account\" \"storageaccount\" { name = \"storageaccountname\" resource_group_name = \"${var.resourcegroup}\" location = \"${var.location}\" account_tier = \"${var.accounttier}\" account_replication_type = \"GRS\" tags { environment = \"staging\" } } stocontainer.tf resource \"azurerm_storage_container\" \"storagecontainer\" { name = \"vhds\" resource_group_name = \"${var.resourcegroup}\" storage_account_name = \"${azurerm_storage_account.storageaccount.name}\" container_access_type = \"private\" } vars.tf variable \"resourcegroup\" { default = \"user-abcd\" } variable \"location\" { default = \"eastus\" } variable \"accounttier\" { default = \"Basic\" } output.tf output \"storageacctname\" { value = \"${azurerm_storage_account.storageaccount.name}\" } output \"storageacctcontainer\" { value = \"${azurerm_storage_account.storagecontainer.name}\" } output \"access_key\" { value = \"${azurerm_storage_account.storageaccount.primary_access_key\" } Before creating backend, run the command to get the storage account keys list( az storage account keys list \u2013account-name storageacctname ) and copy one of the key somewhere. It is useful for configuring the backend. backend.tf terraform{ backend \"azurerm\" { storage_account_name = \"storageacct21\" container_name = \"mycontainer\" key = \"terraform.tfstate.change\" resource_group_name = \"user-okvt\" access_key = \"aJlf+XjZhxwRp4gsU4hkGrQJzO7xBzz7B9rSzMLB/ozwcM6k/1N.......==\" } } where Parameters: storage_account_name - name of the storage account. container_name - name of the container. key- name of the tfstate blob. resource_group_name - name of the resource group. access_key - Storage account access key(any one).","title":"Remote Backends"},{"location":"learning/terraform/#points-to-remember","text":"You cannot use interpolation syntax to configure backends. After creating backend run terraform init, it will be in the locked state. By running terraform apply command automatically, the lease status is changed to locked. After it is applied, it will come to an unlocked state. This backend supports consistency checking and state locking using the native capabilities of the Azure storage account.","title":"Points to Remember"},{"location":"learning/terraform/#terragrunt","text":"Terragrunt is referred to as a thin wrapper for Terraform which provides extra tools to keep the terraform configurations DRY, manage remote state and to work with multiple terraform modules.","title":"Terragrunt"},{"location":"learning/terraform/#terragrunt-commands","text":"terragrunt get terragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt supports the following use cases: Keeps terraform code DRY. Remote state configuration DRY. CLI flags DRY. Executing terraform commands on multiple modules at a time. Advantages of using terragrunt: Provides locking mechanism. Allows you to use remote state always.","title":"Terragrunt Commands"},{"location":"learning/terraform/#build-in-functions","text":"","title":"Build-In Functions"},{"location":"learning/terraform/#lookup","text":"This helps in performing a dynamic lookup into a map variable. The syntax for lookup variable is lookup(map,key, [default]) . map: The map parameter is a variable like var.name. key: The key parameter includes the key from where it should find the environment. If the key doesn't exist in the map, interpolation will fail, if it doesn't find a third argument (default). The lookup will not work on nested lists or maps. It only works on flat maps.","title":"lookup"},{"location":"learning/terraform/#local-values","text":"Local values help in assigning names to the expressions , which can be used multiple times within a module without rewriting it. Local values are defined in the local blocks. It is recommended to group the logically related local values together as a single block, if there is a dependency between them. locals { service_name = \"Fresco\" owner = \"Team\" } Example locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } If a single value or a result is used in many places and it is likely to be changed in the future, then you can go with locals. It is recommended to not use many local values because it makes the read configurations hard to the future maintainers.","title":"Local Values"},{"location":"learning/terraform/#data-source","text":"Data source allows the data to be computed or fetched for use in Terraform configuration. Data sources allow the terraform configurations to build on the information defined outside the Terraform or by another Terraform configuration. Providers play a major role in implementing and defining data sources. Data source helps in two major ways: It provides a read-only view for the pre-existing data. It can compute new values on the fly. Every data source in the Terraform is mapped to the provider depending on the longest-prefix-matching . data \"azurerm_resource_group\" \"passed\" { name = \"${var.resource_group_name}\" }","title":"Data Source"},{"location":"learning/terraform/#concat-and-contains","text":"concat(list1,list2) * It combines two or more lists into a single list. contains(list, element) * It returns true if the element is present in the list or else false. E.g. contains(var.list_of_strings, \"an_element\")","title":"Concat and Contains"},{"location":"learning/terraform/#workspaces","text":"Every terraform configurations has associate backends which defines how the operations are executed and where the persistent data like terraform state are stored. Persistent data stored in backend belongs to a workspace. Previously, the backend has only one workspace which is called as default and there will be only one state file associated with the configuration. Some of the backends support multiple workspaces, i.e. It allows multiple state files to be stored within a single configuration. The configuration is still having only one backend, and the multiple distinct instances can be deployed without configuring to a new backend or by changing authentication credentials. default workspace is special because you can't delete the default workspace. You cannot delete your active workspace (the workspace in which you are currently working). If you haven't created the workspace before, then you are working on the default workspace.","title":"Workspaces"},{"location":"learning/terraform/#workspace-commands","text":"terraform workspace new - It creates the workspace with specified name and switches to it. terraform workspace list - It lists the workspaces available. terraform workspace select - It switches to the specified workspace. terraform workspace delete - It deletes the mentioned workspace.","title":"Workspace Commands"},{"location":"learning/terraform/#configuring-multiple-providers","text":"Below is the file structure that you will maintain for working with multiple providers. multiple_providers | |___awsmain.tf | |___azuremain.tf | |___vars.tf | |___out.tf Create an S3 bucket in Azure which helps in storing the file in AWS. awsmain.tf resource \"aws_s3_bucket\" \"b\" { bucket = \"my-tf-test-bucket-21\" acl = \"private\" tags = { Name = \"My bucket test\" Environment = \"Dev\" } } Resource has two parameters: name(aws_s3_bucket) and type(\"b\"). Name of the resource may vary from provider to provider. Type of the resource depends on the user. Bucket indicates the name of the bucket. If it is not assigned, terraform assigns a random unique name. acl (access control list) - It helps to manage access to the buckets and objects. tag - It is a label which is assigned to the AWS resource by you or AWS. Environment - On which environment do you like to deploy S3 bucket (Dev, Prod or Stage). azuremain.tf resource \"azurerm_storage_account\"\"storageacct\" { name = \"storageacct21\" resource_group_name = \"${var.rg}\" location = \"${var.loc}\" account_replication_type = \"LRS\" account_tier = \"Standard\" } Parameters name - It indicates the name of the storage account. resource_group_name and location- name of the resource group and location. account_replication_type - Type of the account replication. account_tier - Account tier type out.tf output \"account_tier\" { value = \"azurerm_storage_Account.storagecct.account_tier\" }","title":"Configuring Multiple Providers"},{"location":"learning/tmux/","text":"tmux ls # List all sessions tmux attach -t 0 # -t #n indicates the session number from ls output tmux new -s work # Create a new session with a name tmux rename-session -t 0 work # Rename existing session tmux attach -t work # Attach to existing session after login tmux switch -t session_name # Switches to an existing session named session_name tmux list-sessions # Lists existing tmux sessions tmux detach # (prefix + d) detach the currently attached session Workflow Recommendations \u00b6 Use a single client \u2014 Although it is possible to run multiple tmux clients by opening multiple terminal tabs or windows, I find it better to focus on one client in a single terminal window. This provides a single high-level abstraction, which is easier to reason about and interact with. One project per session \u2014 I will open each project, roughly mapping to a git repository in its own session. Typically I will have Vim in the first window along with a pane for running tests and any background processes like the rails server running in additional windows. One Vim instance per session \u2014 In order to avoid conflicts with temp files and buffers getting out of sync, I will only use a single Vim instance per tmux session. Since each session maps to a specific project, this tends to keep me safe from conflicting edits and similar complications.","title":"Tmux"},{"location":"learning/tmux/#workflow-recommendations","text":"Use a single client \u2014 Although it is possible to run multiple tmux clients by opening multiple terminal tabs or windows, I find it better to focus on one client in a single terminal window. This provides a single high-level abstraction, which is easier to reason about and interact with. One project per session \u2014 I will open each project, roughly mapping to a git repository in its own session. Typically I will have Vim in the first window along with a pane for running tests and any background processes like the rails server running in additional windows. One Vim instance per session \u2014 In order to avoid conflicts with temp files and buffers getting out of sync, I will only use a single Vim instance per tmux session. Since each session maps to a specific project, this tends to keep me safe from conflicting edits and similar complications.","title":"Workflow Recommendations"},{"location":"learning/vagrant/","text":"Vagrant is not able to run on Windows 10 + WLS2 Windows setup \u00b6 vagrant plugin install vagrant-vbguest Image should have guest additions or it will not share folder from windows inside the VM Internal Network \u00b6 Go to VirtualBox \u2192 File \u2192 Host Network Manager \u2192 Check the enabled network DHCP address Windows Features Turn On Off \u00b6 Disable \"virtual machine platform\" and \"windows hypervisor platform Installation \u00b6 Install same version of Vagrant in Windows and WSL Verify vagrant --version in both to match In windows try downloading a box and start vagrant up \u2013provider=virtualbox Vagrant \u00b6 Init with a image in vagrant cloud \u00b6 vagrant init hashicorp/precise64 Start the vm \u00b6 vagrant up SSH into the vm \u00b6 vagrant ssh Hibernate the vm \u00b6 vagrant suspend Check the status of vagrant vm \u00b6 vagrant status Stop the vm \u00b6 vagrant halt Clean up the vm \u00b6 vagrant destroy Get Status of Vagrant Machines on host vagrant global-status Get SSH Settings vagrant ssh-config Reload Virtual Machine vagrant reload Make sure the ssh key you created is stored parallel to your Vagrantfile before you execute the vagrant up command. Vagrant commands Managing Vagrant boxes \u00b6 Download a box to a machine \u00b6 vagrant box add ubuntu/trusty64 vagrant box add centos/8 List boxes on machine \u00b6 vagrant box list Update an existing box on a machine \u00b6 vagrant box outdated vagrant box update Run a downloaded box \u2192 cd into a folder \u00b6 vagrant init ubuntu/trusty64 vagrant up Remove a downloaded box from a machine \u00b6 vagrant box remove ubuntu/trusty64 Finding boxes \u00b6 vagrantboxes.es & vagrantcloud \u2192 find a box and copy the url vagrant box add vagrant init vagrant up Using Plugins \u00b6 List existing plugins \u00b6 vagrant plugin list Install Plugins \u00b6 vagrant plugin install vagrant-vbguest Update Plugin version \u00b6 vagrant plugin update vagrant-vbguest Update all plugins \u00b6 vagrant plugin update Remove Plugins \u00b6 vagrant plugin uninstall vagrant-vbguest Adding services to startup boot \u00b6 sudo chkconfig \u2013add httpd sudo chkconfig httpd on sudo service httpd stop Create symbolic link which will serve file from local on vagrant machine, ensure index html file is there in local root \u00b6 cd /var/www/html cd .. && rm -rf html sudo ln -s /vagrant /var/www/html sudo service httpd start Packaging Vagrant after baking \u00b6 Imp that VM is running, check status \u00b6 vagrant status vagrant package \u2013output .box vagrant box add .box Custom base box packaging after customization / hardening \u00b6 vagrant package \u2013base Switching of guest additions checks if the plugin is available in local \u00b6 Add line in config \u00b6 config.vbguest.auto_update = false Adding a file from local machine not in the project folder to the vm \u00b6 config.vm.provision \"file\", source: \"~/vagrant/files/git-files\", destination: \"~/.gitconfig\" If VM is running when above provisioning is done, it is not reflected \u00b6 vagrant provision Adding software at provisioning \u00b6 config.vm.provision \"shell\", inline: \"yum install -y git nano\" Adding custom scripts not in the project folder to the vm \u00b6 config.vm.provision \"shell\", path: \"~/vagrant/scripts/provision.sh\" To restart vm \u00b6 sudo shutdown -r now Restart service \u00b6 sudo systemctl restart sshd.service Update centos kernal \u00b6 sudo yum update kernel* Check and delete old kernels \u00b6 rpm -qa kernel sudo package-cleanup \u2013old-kernels \u2013count=2 Debugging Vagrant \u00b6 During Vagrant Up your Windows system tries to connect to SSH. If you type on your command line: \u00b6 set VAGRANT_LOG=INFO Debug SSH \u00b6 set VAGRANT_PREFER_SYSTEM_BIN=0 vagrant ssh \u2013debug","title":"Vagrant"},{"location":"learning/vagrant/#windows-setup","text":"vagrant plugin install vagrant-vbguest Image should have guest additions or it will not share folder from windows inside the VM","title":"Windows setup"},{"location":"learning/vagrant/#internal-network","text":"Go to VirtualBox \u2192 File \u2192 Host Network Manager \u2192 Check the enabled network DHCP address","title":"Internal Network"},{"location":"learning/vagrant/#windows-features-turn-on-off","text":"Disable \"virtual machine platform\" and \"windows hypervisor platform","title":"Windows Features Turn On Off"},{"location":"learning/vagrant/#installation","text":"Install same version of Vagrant in Windows and WSL Verify vagrant --version in both to match In windows try downloading a box and start vagrant up \u2013provider=virtualbox","title":"Installation"},{"location":"learning/vagrant/#vagrant","text":"","title":"Vagrant"},{"location":"learning/vagrant/#init-with-a-image-in-vagrant-cloud","text":"vagrant init hashicorp/precise64","title":"Init with a image in vagrant cloud"},{"location":"learning/vagrant/#start-the-vm","text":"vagrant up","title":"Start the vm"},{"location":"learning/vagrant/#ssh-into-the-vm","text":"vagrant ssh","title":"SSH into the vm"},{"location":"learning/vagrant/#hibernate-the-vm","text":"vagrant suspend","title":"Hibernate the vm"},{"location":"learning/vagrant/#check-the-status-of-vagrant-vm","text":"vagrant status","title":"Check the status of vagrant vm"},{"location":"learning/vagrant/#stop-the-vm","text":"vagrant halt","title":"Stop the vm"},{"location":"learning/vagrant/#clean-up-the-vm","text":"vagrant destroy Get Status of Vagrant Machines on host vagrant global-status Get SSH Settings vagrant ssh-config Reload Virtual Machine vagrant reload Make sure the ssh key you created is stored parallel to your Vagrantfile before you execute the vagrant up command. Vagrant commands","title":"Clean up the vm"},{"location":"learning/vagrant/#managing-vagrant-boxes","text":"","title":"Managing Vagrant boxes"},{"location":"learning/vagrant/#download-a-box-to-a-machine","text":"vagrant box add ubuntu/trusty64 vagrant box add centos/8","title":"Download a box to a machine"},{"location":"learning/vagrant/#list-boxes-on-machine","text":"vagrant box list","title":"List boxes on machine"},{"location":"learning/vagrant/#update-an-existing-box-on-a-machine","text":"vagrant box outdated vagrant box update","title":"Update an existing box on a machine"},{"location":"learning/vagrant/#run-a-downloaded-box----cd-into-a-folder","text":"vagrant init ubuntu/trusty64 vagrant up","title":"Run a downloaded box --&gt; cd into a folder"},{"location":"learning/vagrant/#remove-a-downloaded-box--from-a-machine","text":"vagrant box remove ubuntu/trusty64","title":"Remove a downloaded box  from a machine"},{"location":"learning/vagrant/#finding-boxes","text":"vagrantboxes.es & vagrantcloud \u2192 find a box and copy the url vagrant box add vagrant init vagrant up","title":"Finding boxes"},{"location":"learning/vagrant/#using-plugins","text":"","title":"Using Plugins"},{"location":"learning/vagrant/#list-existing-plugins","text":"vagrant plugin list","title":"List existing plugins"},{"location":"learning/vagrant/#install-plugins","text":"vagrant plugin install vagrant-vbguest","title":"Install Plugins"},{"location":"learning/vagrant/#update-plugin-version","text":"vagrant plugin update vagrant-vbguest","title":"Update Plugin version"},{"location":"learning/vagrant/#update-all-plugins","text":"vagrant plugin update","title":"Update all plugins"},{"location":"learning/vagrant/#remove-plugins","text":"vagrant plugin uninstall vagrant-vbguest","title":"Remove Plugins"},{"location":"learning/vagrant/#adding-services-to-startup-boot","text":"sudo chkconfig \u2013add httpd sudo chkconfig httpd on sudo service httpd stop","title":"Adding services to startup boot"},{"location":"learning/vagrant/#create-symbolic-link-which-will-serve-file-from-local-on-vagrant-machine-ensure-index-html-file-is-there-in-local-root","text":"cd /var/www/html cd .. && rm -rf html sudo ln -s /vagrant /var/www/html sudo service httpd start","title":"Create symbolic link which will serve file from local on vagrant machine, ensure index html file is there in local root"},{"location":"learning/vagrant/#packaging-vagrant-after-baking","text":"","title":"Packaging Vagrant after baking"},{"location":"learning/vagrant/#imp-that-vm-is-running-check-status","text":"vagrant status vagrant package \u2013output .box vagrant box add .box","title":"Imp that VM is running, check status"},{"location":"learning/vagrant/#custom-base-box-packaging-after-customization--hardening","text":"vagrant package \u2013base","title":"Custom base box packaging after customization / hardening"},{"location":"learning/vagrant/#switching-of-guest-additions-checks-if-the-plugin-is-available-in-local","text":"","title":"Switching of guest additions checks if the plugin is available in local"},{"location":"learning/vagrant/#add-line-in-config","text":"config.vbguest.auto_update = false","title":"Add line in config"},{"location":"learning/vagrant/#adding-a-file-from-local-machine-not-in-the-project-folder-to-the-vm","text":"config.vm.provision \"file\", source: \"~/vagrant/files/git-files\", destination: \"~/.gitconfig\"","title":"Adding a file from local machine not in the project folder to the vm"},{"location":"learning/vagrant/#if-vm-is-running-when-above-provisioning-is-done-it-is-not-reflected","text":"vagrant provision","title":"If VM is running when above provisioning is done, it is not reflected"},{"location":"learning/vagrant/#adding-software-at-provisioning","text":"config.vm.provision \"shell\", inline: \"yum install -y git nano\"","title":"Adding software at provisioning"},{"location":"learning/vagrant/#adding-custom-scripts-not-in-the-project-folder-to-the-vm","text":"config.vm.provision \"shell\", path: \"~/vagrant/scripts/provision.sh\"","title":"Adding custom scripts not in the project folder to the vm"},{"location":"learning/vagrant/#to-restart-vm","text":"sudo shutdown -r now","title":"To restart vm"},{"location":"learning/vagrant/#restart-service","text":"sudo systemctl restart sshd.service","title":"Restart service"},{"location":"learning/vagrant/#update-centos-kernal","text":"sudo yum update kernel*","title":"Update centos kernal"},{"location":"learning/vagrant/#check-and-delete-old-kernels","text":"rpm -qa kernel sudo package-cleanup \u2013old-kernels \u2013count=2","title":"Check and delete old kernels"},{"location":"learning/vagrant/#debugging-vagrant","text":"","title":"Debugging Vagrant"},{"location":"learning/vagrant/#during-vagrant-up-your-windows-system-tries-to-connect-to-ssh-if-you-type-on-your-command-line","text":"set VAGRANT_LOG=INFO","title":"During Vagrant Up your Windows system tries to connect to SSH. If you type on your command line:"},{"location":"learning/vagrant/#debug-ssh","text":"set VAGRANT_PREFER_SYSTEM_BIN=0 vagrant ssh \u2013debug","title":"Debug SSH"},{"location":"learning/docker/docker-notes/","text":"Important Links Docker Projects Master list of Docker Resources and Projects Play With Docker , a great resource for web-based docker testing and also has a library of labs built by Docker Captains and others, and supported by Docker Inc. Play With Docker Labs DockerHub Recipes Docker Cloud: CI/CD and Server Ops Docker and Pi Projects Docker Training Content Docker Mastery Bret's Podcast Bret's Youtube Docker Shell Config Docker Devops Containers vs VM ebook Docker Refernce Docker Certificated Associate Dockerfile Reference Dockerfile Best Practice Formatting Docker CLI Output Docker Image Docker Registry Config Docker Registry Garbage Collection Docker Registry as cache Docker Storage Docker secrets Docker CLI to kubectl Software Design Immutable Software 12 factor App 12 Fractured Apps Devops Roadmap Docker Best Practises Docker Best practise inside a code repo Healthcheck in Dockerfile Starting container process caused \"exec: \\\"ping\\\": executable file not found in $PATH \" : unknown apt-get update && apt-get install -y iputils-ping Starting mysql container and running ps causes \"ps: command not found\" apt-get update && apt-get install procps Creating and Using Containers Like a Boss \u00b6 Check Our Docker Install and Config \u00b6 docker version - verified cli can talk to engine docker info - most config values of engine Image vs. Container \u00b6 An Image is the application we want to run A Container is an instance of that image running as a process You can have many containers running off the same image Docker's default image \"registry\" is called Docker Hub docker container run --publish 80 :80 --detach --name webhost nginx docker container run -it # start new container interactively docker container exec -it # run additional command in existing container docker container ls -a docker container logs webhost Container VS. VM: It's Just a Process \u00b6 docker run --name mongo -d mongo docker container top - process list in one container docker stop mongo docker ps docker start mongo docker container inspect - details of one container config docker container stats - performance stats for all containers The Mighty Hub: Using Docker Hub Registry Images \u00b6 docker pull nginx docker image ls Images and Their Layers: Discover the Image Cache \u00b6 docker history nginx:latest docker image inspect nginx Image Tagging and Pushing to Docker Hub \u00b6 docker pull nginx:latest docker image ls docker image tag nginx bretfisher/nginx docker login cat .docker/config.json docker image push bretfisher/nginx docker image push bretfisher/nginx bretfisher/nginx:testing Getting a Shell Inside Containers: No Need for SSH \u00b6 docker container exec -it mysql -- bash docker container run -it alpine -- bash docker container run -it alpine -- sh Cleaning Docker images \u00b6 Use docker system df to see space usage. docker image prune to clean up just \"dangling\" images. The big one is usually docker image prune -a which will remove all images you're not using. docker volume prune to remove unused volumes docker system prune will clean up everything (Nuke everything that is not used currently). docker system prune -a wipe everything. Docker Networks: Concepts for Private and Public Comms in Containers \u00b6 Each container connected to a private virtual network \"bridge\" Each virtual network routes through NAT firewall on host IP All containers on a virtual network can talk to each other without -p Best practice is to create a new virtual network for each app: network \"my_web_app\" for mysql and php/apache containers network \"my_api\" for mongo and nodejs containers docker container run -p 80 :80 --name webhost -d nginx docker container port webhost docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost Docker Networks: CLI Management of Virtual Networks \u00b6 Show networks docker network ls Inspect a network docker network inspect Create a network docker network create --driver Attach a network to container docker network connect Detach a network from container docker network disconnect docker network ls docker network inspect bridge docker network create my_app_net docker container run -d --name new_nginx --network my_app_net nginx docker network inspect my_app_net docker network connect <new network id> <container id> docker container disconnect <new network id> <container id> Docker Networks: DNS and How Containers Find Each Other \u00b6 Create your apps so frontend/backend sit on same Docker network Their inter-communication never leaves host All externally exposed ports closed by default You must manually expose via -p , which is better default security! Containers shouldn't rely on IP's for inter-communication DNS for friendly names is built-in if you use custom networks docker container run -d --name my_nginx --network my_app_net nginx docker container exec -it my_nginx ping new_nginx docker container exec -it new_nginx ping my_nginx DNS Round Robin Testing \u00b6 docker network create dude docker container run -d --net dude --net-alias search elasticsearch:2 docker container ls docker container run --rm -- net dude alpine nslookup search docker container run --rm --net dude centos curl -s search:9200 Container Lifetime & Persistent Data: Volumes, Volumes, Volumes \u00b6 Persistent Data: Data Volumes \u00b6 Containers are usually immutable and ephemeral \"immutable infrastructure\": only re-deploy containers, never change This is the ideal scenario, but what about databases, or unique data? Docker gives us features to ensure these \"separation of concerns\" This is known as \"persistent data\" Two ways: Volumes and Bind Mounts Volumes : make special location outside of container UFS Bind Mounts : link container path to host path docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD = True -v mysql-db:/var/lib/mysql mysql docker volume ls docker volume inspect mysql-db Persistent Data: Bind Mounting \u00b6 Used for local development Usecase: When you are changing files on laptop which you want to serve in the app It can be run only during docker run as there is no explicit volume command in the dockerfile the volume will be mounted in the working directory of the container docker container run -d --name nginx -p 80 :80 -v $( pwd ) :/usr/share/nginx/html nginx docker container exec -it nginx -- bash cd /usr/share/nginx/html && ls -la docker log streaming docker container logs -f <container name> Database Passwords in Containers \u00b6 When running postgres now, you'll need to either set a password, or tell it to allow any connection (which was the default before this change). -you need to either set a password with the environment variable: POSTGRES_PASSWORD=mypasswd Or tell it to ignore passwords with the environment variable: POSTGRES_HOST_AUTH_METHOD=trust Making It Easier with Docker Compose: The Multi-Container Tool \u00b6 Why: configure relationships between containers Why: save our docker container run settings in easy-to-read file Why: create one-liner developer environment startups YAML-formatted file that describes our solution options for: containers, networks, volumes A CLI tool docker-compose used for local dev/test automation with those YAML files docker-compose.yml is default filename, but any can be used with docker-compose -f Not a production-grade tool but ideal for local development and test Two most common commands are: docker-compose up # setup volumes/networks and start all containers docker-compose down # stop all containers and remove cont/vol/net Trying Out Basic Compose Commands \u00b6 docker-compose up docker-compose up -d # Running compose in bacground docker-compose down docker-compose down -v --rmi local/all # Removes images and volumes # Compose operations docker-compose logs docker-compose ps docker-compose top docker-compose build # Build images or docker-compose up --build Swarm Intro and Creating a 3-Node Swarm Cluster \u00b6 Swarm Mode is a clustering solution built inside Docker Not enabled by default docker swarm init: What Just Happened? \u00b6 Lots of PKI and security automation Root Signing Certificate created for our Swarm Certificate is issued for first Manager node Join tokens are created Raft database created to store root CA, configs and secrets Encrypted by default on disk (1.13+) No need for another key/value system to hold orchestration/secrets Replicates logs amongst Managers via mutual TLS in \"control plane\" Create Your First Service and Scale it Locally \u00b6 docker info # swarm is down by default docker swarm init # start swarm docker node ls docker service create alpine ping 8 .8.8.8 # creates service frosty_newton docker service ls docker service ps frosty_newton docker container ls docker service update frosty_newton --replicas 3 # creates 3 replicas docker service ls docker service rm frosty_newton # deletes the service docker service ls docker container ls Creating a 3-Node Swarm Cluster \u00b6 docker-machine + VirtualBox - Free and runs locally, but requires a machine with 8GB memory docker-machine create node1 docker-machine ssh node1 docker-machine env node1 docker swarm init docker swarm init --advertise-addr node1 docker node ls docker node update --role manager node2 # Update role to existing node docker swarm join-token manager # Shows join token for manager role docker service create --replicas 3 alpine ping 8 .8.8.8 # Creates service with 3 replicas and starts ping process docker service ls docker service ps <service name> docker node ps docker node ps node2 Scaling Out with Overlay Networking \u00b6 # Create Backend network docker network create --driver overlay mydrupal docker network ls docker service create --name psql --netowrk mydrupal -e POSTGRES_PASSWORD = mypass postgres docker service ls docker service ps psql docker container logs psql <container name> # Create Frontend network docker service create --name drupal --network mydrupal -p 80 :80 drupal docker service inspect drupal Scaling Out with Routing Mesh \u00b6 docker service create --name search --replicas 3 -p 9200 :9200 elasticsearch:2 docker service ps search Create a Multi-Service Multi-Node Web App \u00b6 docker network create -d overlay backend docker network create -d overlay frontend docker service create --name vote -p 80 :80 --network frontend \\ -- replica 2 dockersamples/examplevotingapp_vote:before docker service create --name redis --network frontend \\ --replica 1 redis:3.2 docker service create --name worker --network frontend --network backend dockersamples/examplevotingapp_worker docker service create --name db --network backend \\ --mount type = volume,source = db-data,target = /var/lib/postgresql/data postgres:9.4 docker service create --name result --network backend -p 5001 :80 COPY INFO docker service ls docker service logs worker Swarm Stacks and Production Grade Compose \u00b6 Docker adds a new layer of abstraction to Swarm called Stacks Stacks accept Compose files as their declarative definition for services, networks, and volumes We use docker stack deploy rather then docker service create Stacks manages all those objects for us, including overlay network per stack. Adds stack name to start of their name Compose now ignores deploy: , Swarm ignores build: docker stack deploy -c example-voting-app-stack.yml voteapp docker stack ls docker stack services voteapp docker stack ps voteapp Using Secrets in Swarm Services \u00b6 What is a Secret? - Usernames and passwords - TLS certificates and keys - SSH keys - Any data you would prefer not be \"on front page of news\" docker secret create psql_usr psql_usr.txt echo \"myDBpassWORD\" | docker secret create psql_pass - TAB COMPLETION docker secret inspect psql_usr docker service create --name psql --secret psql_user \\ --secret psql_pass -e POSTGRES_PASSWORD_FILE = /run/secrets/psql_pass \\ -e POSTGRES_USER_FILE = /run/secrets/psql_user postgres docker exec -it <container name> bash cat /run/secrets/psql_user Swarm App Lifecycle \u00b6 Full App Lifecycle: Dev, Build and Deploy With a Single Compose Design \u00b6 Single set of Compose files for: - Local docker-compose up development environment - Remote docker-compose up CI environment - Remote docker stack deploy production environment docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d docker-compose -f docker-compose.yml -f docker-compose.prod.yml config Service Updates: Changing Things In Flight \u00b6 Provides rolling replacement of tasks/containers in a service Limits downtime (be careful with \"prevents\" downtime) Will replace containers for most changes Has many, many cli options to control the update Create options will usually change, adding -add or -rm to them Includes rollback and healthcheck options Also has scale & rollback subcommand for quicker access docker service scale web=4 and docker service rollback web Just update the image used to a newer version docker service update --image myapp:1.2.1 <servicename> Adding an environment variable and remove a port docker service update --env-add NODE_ENV=production --publish-rm 8080 Change number of replicas of two services docker service scale web=8 api=6 docker service create -p 8088 :80 --name web nginx:1.13.7 docker service scale web = 5 docker service update --image nginx:1.13.6 web docker service update --publish-rm 8088 --publish-add 9090 :80 docker service update --force web # forces rebalancing of the service without changing anything docker service rm web Healthchecks in Dockerfiles \u00b6 HEALTHCHECK was added in 1.12 Supported in Dockerfile, Compose YAML, docker run, and Swarm Services Docker engine will exec's the command in the container (e.g. curl localhost) It expects exit 0 (OK) or exit 1 (Error) Three container states: starting, healthy, unhealthy Much better then \"is binary still running?\" Options for healthcheck command --interval = DURATION ( default: 30s ) --timeout = DURATION ( default: 30s ) --start-period = DURATION ( default: 0s ) ( 17 .09+ ) --retries = N ( default: 3 ) docker container run --name p2 -d --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres docker service create --name p2 --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres Container Registries: Image Storage and Distribution \u00b6 Run a Private Docker Registry \u00b6 Secure your Registry with TLS Storage cleanup via Garbage Collection Enable Hub caching via \"\u2013registry-mirror\" # Run the registry image docker container run -d -p 5000 :5000 --name registry registry # Re-tag an existing image and push it to your new registry docker pull hello-world docker run hello-world docker tag hello-world 127 .0.0.1:5000/hello-world docker push 127 .0.0.1:5000/hello-world # Remove that image from local cache and pull it from new registry docker image remove hello-world docker image remove 127 .0.0.1:5000/hello-world docker pull 127 .0.0.1:5000/hello-world:latest # Re-create registry using a bind mount and see how it stores data docker container kill registry docker container rm registry docker container run -d -p 5000 :5000 --name registry -v $( pwd ) /registry-data:/var/lib/registry registry Using Docker Registry With Swarm \u00b6 docker node ls docker service create --name registry --publish 5000 :5000 registry docker service ps registry docker pull nginx docker tag nginx 127 .0.0.1:5000/nginx docker push 127 .0.0.1:5000/nginx docker service create --name nginx -p 80 :80 --replicas 5 --detach = false 127 .0.0.1:5000/nginx docker service ps nginx Using Docker in Production \u00b6 Focus on Dockerfiles first. Study ENTRYPOINT of Hub official images. Use it for config of images before CMD is executed. use ENTRYPOINT to set default values for all environments and then overide using ENV values. EntryPoint vs CMD FROM official distros. Make it == start, log all things in stdout/stderr, documented in file, lean and scale. Using SaaS for - Image Registry, Logging, Monitoring, Look at CNCF Landscape Using Layer 7 Reverse Proxy if port 80 and 443 are used by multiple apps Docker Security \u00b6 Docker Security Checklist Docker Engine Security Docker Security Tools Seccomp App Armor Docker Bench CIS Docker checklist Running Docker as non root user # Creating non root user in alpine RUN addgroup -g 1000 node \\ && adduser -u 1000 -G node -s /bin/sh -D node # Creating non root user in stretch RUN groupadd --gid 1000 node \\ && useradd --uid 1000 --gid node --shell /bin/bash --create-home node Sample Dockerfile with USER User Namespaces Shift Left Security Trivy - Image Scanning Sysdig Falco Appamror Profiles Seccomp Profile Docker Context \u00b6 Start a node on paly with Docker Copy the IP of the node Set the Docker Context with the Host Name of the node and port 2375 Contexts are created in the home folder of user called .docker/context docker context create --docker \"host=tcp://<Host Name>:2375\" <context-name> docker context ls docker context use <context-name> docker ps # Should show the new context of play with docker # Overriding Context to default in commandline docker -c default ps docker -c <context-name> ps # Looping through all the context and executing ps for c in ` docker context ls -q ` ; do ` docker -c $c ps ` ; done # Creates the image in all context for c in ` docker context ls -q ` ; do ` docker -c $c run hello-world ` ; done Recommendations \u00b6 To change permissions on file system (chown or chmod) use a Entrypoint script. Look up to official images for examples for Entrypoint One App or Website use one container, specially if using an orchestrator like K8s or Docker Swarm. Scaling is also a benefit due to one-one relationship. Changing Docker IP range Use Cloud DB as service instead of in containers Run one process per container Strict Separation of Config from Code. Use Env variables to achieve this. Using Development workflow in Compose Write all the ENV variables at the top of Dockerfile Using Env variables in Dockerfile Override Env variables in Docker Compose file say for Dev testing Using Env variables in Docker Entrypoint to write into Application config files during start up. Secrets and Application specific config goes into specific ENV var blocks. Tis can be changed. Defaults or data specific to SERVER or LANGUAGE goes to another ENV block and can be kept static. This avoids them being set for each ENV. Encrypting traffic for local development use Lets Encrypt ad store them in .cert folder in Home Directory. Encrypting traffic for production use Lets Encrypt and maybe Traefik as Front proxy. See example using Swarm COPY vs ADD. Use COPY to copy artefacts in the same repo to the image. Use ADD when you want to download something from the Internet or to untar or unzip. You can also replace using wget statements with ADD. Combine multiple RUN into a single statement. Delete packages which are downloaded and installed also in a single command to save image size. No secrets like configs, certificates should be saved in Image. Pass them during runtime. Always have a CMD in the image, even if its inheriting it from BASE image Version apt packages and BASE images Use multistage Dokcer builds to have Dev dependencies and Prod dependencies separate. Have healthchecks in K8s instead of Dockerfile Use DNS RoundRobin for Database inside Compose file so it switches of Virtual IP on the Overlay network and gives direct access from FrontEnd Service to Backend container. Setting resource limits inside Compose file DRY your compose files using templates","title":"Docker"},{"location":"learning/docker/docker-notes/#creating-and-using-containers-like-a-boss","text":"","title":"Creating and Using Containers Like a Boss"},{"location":"learning/docker/docker-notes/#check-our-docker-install-and-config","text":"docker version - verified cli can talk to engine docker info - most config values of engine","title":"Check Our Docker Install and Config"},{"location":"learning/docker/docker-notes/#image-vs-container","text":"An Image is the application we want to run A Container is an instance of that image running as a process You can have many containers running off the same image Docker's default image \"registry\" is called Docker Hub docker container run --publish 80 :80 --detach --name webhost nginx docker container run -it # start new container interactively docker container exec -it # run additional command in existing container docker container ls -a docker container logs webhost","title":"Image vs. Container"},{"location":"learning/docker/docker-notes/#container-vs-vm-its-just-a-process","text":"docker run --name mongo -d mongo docker container top - process list in one container docker stop mongo docker ps docker start mongo docker container inspect - details of one container config docker container stats - performance stats for all containers","title":"Container VS. VM: It's Just a Process"},{"location":"learning/docker/docker-notes/#the-mighty-hub-using-docker-hub-registry-images","text":"docker pull nginx docker image ls","title":"The Mighty Hub: Using Docker Hub Registry Images"},{"location":"learning/docker/docker-notes/#images-and-their-layers-discover-the-image-cache","text":"docker history nginx:latest docker image inspect nginx","title":"Images and Their Layers: Discover the Image Cache"},{"location":"learning/docker/docker-notes/#image-tagging-and-pushing-to-docker-hub","text":"docker pull nginx:latest docker image ls docker image tag nginx bretfisher/nginx docker login cat .docker/config.json docker image push bretfisher/nginx docker image push bretfisher/nginx bretfisher/nginx:testing","title":"Image Tagging and Pushing to Docker Hub"},{"location":"learning/docker/docker-notes/#getting-a-shell-inside-containers-no-need-for-ssh","text":"docker container exec -it mysql -- bash docker container run -it alpine -- bash docker container run -it alpine -- sh","title":"Getting a Shell Inside Containers: No Need for SSH"},{"location":"learning/docker/docker-notes/#cleaning-docker-images","text":"Use docker system df to see space usage. docker image prune to clean up just \"dangling\" images. The big one is usually docker image prune -a which will remove all images you're not using. docker volume prune to remove unused volumes docker system prune will clean up everything (Nuke everything that is not used currently). docker system prune -a wipe everything.","title":"Cleaning Docker images"},{"location":"learning/docker/docker-notes/#docker-networks-concepts-for-private-and-public-comms-in-containers","text":"Each container connected to a private virtual network \"bridge\" Each virtual network routes through NAT firewall on host IP All containers on a virtual network can talk to each other without -p Best practice is to create a new virtual network for each app: network \"my_web_app\" for mysql and php/apache containers network \"my_api\" for mongo and nodejs containers docker container run -p 80 :80 --name webhost -d nginx docker container port webhost docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost","title":"Docker Networks: Concepts for Private and Public Comms in Containers"},{"location":"learning/docker/docker-notes/#docker-networks-cli-management-of-virtual-networks","text":"Show networks docker network ls Inspect a network docker network inspect Create a network docker network create --driver Attach a network to container docker network connect Detach a network from container docker network disconnect docker network ls docker network inspect bridge docker network create my_app_net docker container run -d --name new_nginx --network my_app_net nginx docker network inspect my_app_net docker network connect <new network id> <container id> docker container disconnect <new network id> <container id>","title":"Docker Networks: CLI Management of Virtual Networks"},{"location":"learning/docker/docker-notes/#docker-networks-dns-and-how-containers-find-each-other","text":"Create your apps so frontend/backend sit on same Docker network Their inter-communication never leaves host All externally exposed ports closed by default You must manually expose via -p , which is better default security! Containers shouldn't rely on IP's for inter-communication DNS for friendly names is built-in if you use custom networks docker container run -d --name my_nginx --network my_app_net nginx docker container exec -it my_nginx ping new_nginx docker container exec -it new_nginx ping my_nginx","title":"Docker Networks: DNS and How Containers Find Each Other"},{"location":"learning/docker/docker-notes/#dns-round-robin-testing","text":"docker network create dude docker container run -d --net dude --net-alias search elasticsearch:2 docker container ls docker container run --rm -- net dude alpine nslookup search docker container run --rm --net dude centos curl -s search:9200","title":"DNS Round Robin Testing"},{"location":"learning/docker/docker-notes/#container-lifetime--persistent-data-volumes-volumes-volumes","text":"","title":"Container Lifetime &amp; Persistent Data: Volumes, Volumes, Volumes"},{"location":"learning/docker/docker-notes/#persistent-data-data-volumes","text":"Containers are usually immutable and ephemeral \"immutable infrastructure\": only re-deploy containers, never change This is the ideal scenario, but what about databases, or unique data? Docker gives us features to ensure these \"separation of concerns\" This is known as \"persistent data\" Two ways: Volumes and Bind Mounts Volumes : make special location outside of container UFS Bind Mounts : link container path to host path docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD = True -v mysql-db:/var/lib/mysql mysql docker volume ls docker volume inspect mysql-db","title":"Persistent Data: Data Volumes"},{"location":"learning/docker/docker-notes/#persistent-data-bind-mounting","text":"Used for local development Usecase: When you are changing files on laptop which you want to serve in the app It can be run only during docker run as there is no explicit volume command in the dockerfile the volume will be mounted in the working directory of the container docker container run -d --name nginx -p 80 :80 -v $( pwd ) :/usr/share/nginx/html nginx docker container exec -it nginx -- bash cd /usr/share/nginx/html && ls -la docker log streaming docker container logs -f <container name>","title":"Persistent Data: Bind Mounting"},{"location":"learning/docker/docker-notes/#database-passwords-in-containers","text":"When running postgres now, you'll need to either set a password, or tell it to allow any connection (which was the default before this change). -you need to either set a password with the environment variable: POSTGRES_PASSWORD=mypasswd Or tell it to ignore passwords with the environment variable: POSTGRES_HOST_AUTH_METHOD=trust","title":"Database Passwords in Containers"},{"location":"learning/docker/docker-notes/#making-it-easier-with-docker-compose-the-multi-container-tool","text":"Why: configure relationships between containers Why: save our docker container run settings in easy-to-read file Why: create one-liner developer environment startups YAML-formatted file that describes our solution options for: containers, networks, volumes A CLI tool docker-compose used for local dev/test automation with those YAML files docker-compose.yml is default filename, but any can be used with docker-compose -f Not a production-grade tool but ideal for local development and test Two most common commands are: docker-compose up # setup volumes/networks and start all containers docker-compose down # stop all containers and remove cont/vol/net","title":"Making It Easier with Docker Compose: The Multi-Container Tool"},{"location":"learning/docker/docker-notes/#trying-out-basic-compose-commands","text":"docker-compose up docker-compose up -d # Running compose in bacground docker-compose down docker-compose down -v --rmi local/all # Removes images and volumes # Compose operations docker-compose logs docker-compose ps docker-compose top docker-compose build # Build images or docker-compose up --build","title":"Trying Out Basic Compose Commands"},{"location":"learning/docker/docker-notes/#swarm-intro-and-creating-a-3-node-swarm-cluster","text":"Swarm Mode is a clustering solution built inside Docker Not enabled by default","title":"Swarm Intro and Creating a 3-Node Swarm Cluster"},{"location":"learning/docker/docker-notes/#docker-swarm-init-what-just-happened","text":"Lots of PKI and security automation Root Signing Certificate created for our Swarm Certificate is issued for first Manager node Join tokens are created Raft database created to store root CA, configs and secrets Encrypted by default on disk (1.13+) No need for another key/value system to hold orchestration/secrets Replicates logs amongst Managers via mutual TLS in \"control plane\"","title":"docker swarm init: What Just Happened?"},{"location":"learning/docker/docker-notes/#create-your-first-service-and-scale-it-locally","text":"docker info # swarm is down by default docker swarm init # start swarm docker node ls docker service create alpine ping 8 .8.8.8 # creates service frosty_newton docker service ls docker service ps frosty_newton docker container ls docker service update frosty_newton --replicas 3 # creates 3 replicas docker service ls docker service rm frosty_newton # deletes the service docker service ls docker container ls","title":"Create Your First Service and Scale it Locally"},{"location":"learning/docker/docker-notes/#creating-a-3-node-swarm-cluster","text":"docker-machine + VirtualBox - Free and runs locally, but requires a machine with 8GB memory docker-machine create node1 docker-machine ssh node1 docker-machine env node1 docker swarm init docker swarm init --advertise-addr node1 docker node ls docker node update --role manager node2 # Update role to existing node docker swarm join-token manager # Shows join token for manager role docker service create --replicas 3 alpine ping 8 .8.8.8 # Creates service with 3 replicas and starts ping process docker service ls docker service ps <service name> docker node ps docker node ps node2","title":"Creating a 3-Node Swarm Cluster"},{"location":"learning/docker/docker-notes/#scaling-out-with-overlay-networking","text":"# Create Backend network docker network create --driver overlay mydrupal docker network ls docker service create --name psql --netowrk mydrupal -e POSTGRES_PASSWORD = mypass postgres docker service ls docker service ps psql docker container logs psql <container name> # Create Frontend network docker service create --name drupal --network mydrupal -p 80 :80 drupal docker service inspect drupal","title":"Scaling Out with Overlay Networking"},{"location":"learning/docker/docker-notes/#scaling-out-with-routing-mesh","text":"docker service create --name search --replicas 3 -p 9200 :9200 elasticsearch:2 docker service ps search","title":"Scaling Out with Routing Mesh"},{"location":"learning/docker/docker-notes/#create-a-multi-service-multi-node-web-app","text":"docker network create -d overlay backend docker network create -d overlay frontend docker service create --name vote -p 80 :80 --network frontend \\ -- replica 2 dockersamples/examplevotingapp_vote:before docker service create --name redis --network frontend \\ --replica 1 redis:3.2 docker service create --name worker --network frontend --network backend dockersamples/examplevotingapp_worker docker service create --name db --network backend \\ --mount type = volume,source = db-data,target = /var/lib/postgresql/data postgres:9.4 docker service create --name result --network backend -p 5001 :80 COPY INFO docker service ls docker service logs worker","title":"Create a Multi-Service Multi-Node Web App"},{"location":"learning/docker/docker-notes/#swarm-stacks-and-production-grade-compose","text":"Docker adds a new layer of abstraction to Swarm called Stacks Stacks accept Compose files as their declarative definition for services, networks, and volumes We use docker stack deploy rather then docker service create Stacks manages all those objects for us, including overlay network per stack. Adds stack name to start of their name Compose now ignores deploy: , Swarm ignores build: docker stack deploy -c example-voting-app-stack.yml voteapp docker stack ls docker stack services voteapp docker stack ps voteapp","title":"Swarm Stacks and Production Grade Compose"},{"location":"learning/docker/docker-notes/#using-secrets-in-swarm-services","text":"What is a Secret? - Usernames and passwords - TLS certificates and keys - SSH keys - Any data you would prefer not be \"on front page of news\" docker secret create psql_usr psql_usr.txt echo \"myDBpassWORD\" | docker secret create psql_pass - TAB COMPLETION docker secret inspect psql_usr docker service create --name psql --secret psql_user \\ --secret psql_pass -e POSTGRES_PASSWORD_FILE = /run/secrets/psql_pass \\ -e POSTGRES_USER_FILE = /run/secrets/psql_user postgres docker exec -it <container name> bash cat /run/secrets/psql_user","title":"Using Secrets in Swarm Services"},{"location":"learning/docker/docker-notes/#swarm-app-lifecycle","text":"","title":"Swarm App Lifecycle"},{"location":"learning/docker/docker-notes/#full-app-lifecycle-dev-build-and-deploy-with-a-single-compose-design","text":"Single set of Compose files for: - Local docker-compose up development environment - Remote docker-compose up CI environment - Remote docker stack deploy production environment docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d docker-compose -f docker-compose.yml -f docker-compose.prod.yml config","title":"Full App Lifecycle: Dev, Build and Deploy With a Single Compose Design"},{"location":"learning/docker/docker-notes/#service-updates-changing-things-in-flight","text":"Provides rolling replacement of tasks/containers in a service Limits downtime (be careful with \"prevents\" downtime) Will replace containers for most changes Has many, many cli options to control the update Create options will usually change, adding -add or -rm to them Includes rollback and healthcheck options Also has scale & rollback subcommand for quicker access docker service scale web=4 and docker service rollback web Just update the image used to a newer version docker service update --image myapp:1.2.1 <servicename> Adding an environment variable and remove a port docker service update --env-add NODE_ENV=production --publish-rm 8080 Change number of replicas of two services docker service scale web=8 api=6 docker service create -p 8088 :80 --name web nginx:1.13.7 docker service scale web = 5 docker service update --image nginx:1.13.6 web docker service update --publish-rm 8088 --publish-add 9090 :80 docker service update --force web # forces rebalancing of the service without changing anything docker service rm web","title":"Service Updates: Changing Things In Flight"},{"location":"learning/docker/docker-notes/#healthchecks-in-dockerfiles","text":"HEALTHCHECK was added in 1.12 Supported in Dockerfile, Compose YAML, docker run, and Swarm Services Docker engine will exec's the command in the container (e.g. curl localhost) It expects exit 0 (OK) or exit 1 (Error) Three container states: starting, healthy, unhealthy Much better then \"is binary still running?\" Options for healthcheck command --interval = DURATION ( default: 30s ) --timeout = DURATION ( default: 30s ) --start-period = DURATION ( default: 0s ) ( 17 .09+ ) --retries = N ( default: 3 ) docker container run --name p2 -d --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres docker service create --name p2 --health-cmd = \"pg_isready \\ -U postgres || exit 1\" postgres","title":"Healthchecks in Dockerfiles"},{"location":"learning/docker/docker-notes/#container-registries-image-storage-and-distribution","text":"","title":"Container Registries: Image Storage and Distribution"},{"location":"learning/docker/docker-notes/#run-a-private-docker-registry","text":"Secure your Registry with TLS Storage cleanup via Garbage Collection Enable Hub caching via \"\u2013registry-mirror\" # Run the registry image docker container run -d -p 5000 :5000 --name registry registry # Re-tag an existing image and push it to your new registry docker pull hello-world docker run hello-world docker tag hello-world 127 .0.0.1:5000/hello-world docker push 127 .0.0.1:5000/hello-world # Remove that image from local cache and pull it from new registry docker image remove hello-world docker image remove 127 .0.0.1:5000/hello-world docker pull 127 .0.0.1:5000/hello-world:latest # Re-create registry using a bind mount and see how it stores data docker container kill registry docker container rm registry docker container run -d -p 5000 :5000 --name registry -v $( pwd ) /registry-data:/var/lib/registry registry","title":"Run a Private Docker Registry"},{"location":"learning/docker/docker-notes/#using-docker-registry-with-swarm","text":"docker node ls docker service create --name registry --publish 5000 :5000 registry docker service ps registry docker pull nginx docker tag nginx 127 .0.0.1:5000/nginx docker push 127 .0.0.1:5000/nginx docker service create --name nginx -p 80 :80 --replicas 5 --detach = false 127 .0.0.1:5000/nginx docker service ps nginx","title":"Using Docker Registry With Swarm"},{"location":"learning/docker/docker-notes/#using-docker-in-production","text":"Focus on Dockerfiles first. Study ENTRYPOINT of Hub official images. Use it for config of images before CMD is executed. use ENTRYPOINT to set default values for all environments and then overide using ENV values. EntryPoint vs CMD FROM official distros. Make it == start, log all things in stdout/stderr, documented in file, lean and scale. Using SaaS for - Image Registry, Logging, Monitoring, Look at CNCF Landscape Using Layer 7 Reverse Proxy if port 80 and 443 are used by multiple apps","title":"Using Docker in Production"},{"location":"learning/docker/docker-notes/#docker-security","text":"Docker Security Checklist Docker Engine Security Docker Security Tools Seccomp App Armor Docker Bench CIS Docker checklist Running Docker as non root user # Creating non root user in alpine RUN addgroup -g 1000 node \\ && adduser -u 1000 -G node -s /bin/sh -D node # Creating non root user in stretch RUN groupadd --gid 1000 node \\ && useradd --uid 1000 --gid node --shell /bin/bash --create-home node Sample Dockerfile with USER User Namespaces Shift Left Security Trivy - Image Scanning Sysdig Falco Appamror Profiles Seccomp Profile","title":"Docker Security"},{"location":"learning/docker/docker-notes/#docker-context","text":"Start a node on paly with Docker Copy the IP of the node Set the Docker Context with the Host Name of the node and port 2375 Contexts are created in the home folder of user called .docker/context docker context create --docker \"host=tcp://<Host Name>:2375\" <context-name> docker context ls docker context use <context-name> docker ps # Should show the new context of play with docker # Overriding Context to default in commandline docker -c default ps docker -c <context-name> ps # Looping through all the context and executing ps for c in ` docker context ls -q ` ; do ` docker -c $c ps ` ; done # Creates the image in all context for c in ` docker context ls -q ` ; do ` docker -c $c run hello-world ` ; done","title":"Docker Context"},{"location":"learning/docker/docker-notes/#recommendations","text":"To change permissions on file system (chown or chmod) use a Entrypoint script. Look up to official images for examples for Entrypoint One App or Website use one container, specially if using an orchestrator like K8s or Docker Swarm. Scaling is also a benefit due to one-one relationship. Changing Docker IP range Use Cloud DB as service instead of in containers Run one process per container Strict Separation of Config from Code. Use Env variables to achieve this. Using Development workflow in Compose Write all the ENV variables at the top of Dockerfile Using Env variables in Dockerfile Override Env variables in Docker Compose file say for Dev testing Using Env variables in Docker Entrypoint to write into Application config files during start up. Secrets and Application specific config goes into specific ENV var blocks. Tis can be changed. Defaults or data specific to SERVER or LANGUAGE goes to another ENV block and can be kept static. This avoids them being set for each ENV. Encrypting traffic for local development use Lets Encrypt ad store them in .cert folder in Home Directory. Encrypting traffic for production use Lets Encrypt and maybe Traefik as Front proxy. See example using Swarm COPY vs ADD. Use COPY to copy artefacts in the same repo to the image. Use ADD when you want to download something from the Internet or to untar or unzip. You can also replace using wget statements with ADD. Combine multiple RUN into a single statement. Delete packages which are downloaded and installed also in a single command to save image size. No secrets like configs, certificates should be saved in Image. Pass them during runtime. Always have a CMD in the image, even if its inheriting it from BASE image Version apt packages and BASE images Use multistage Dokcer builds to have Dev dependencies and Prod dependencies separate. Have healthchecks in K8s instead of Dockerfile Use DNS RoundRobin for Database inside Compose file so it switches of Virtual IP on the Overlay network and gives direct access from FrontEnd Service to Backend container. Setting resource limits inside Compose file DRY your compose files using templates","title":"Recommendations"},{"location":"learning/k8s/k8s-notes/","text":"Tailing logs from multiple containers on laptop K8s Tutorials K8s DNS Kubectl Usage Convention K8s API Reference Operator Hub Awesome Operator List Creating EKS using Terraform Why Kubernetes \u00b6 Orchestration: Next logical step in journey to faster DevOps First, understand why you may need orchestration Not every solution needs orchestration Servers + Change Rate = Benefit of orchestration K8s Learning Resources \u00b6 Play with k8s Katacoda Install Kubernetes \u00b6 Linux - Microk8s Install SNAP first using apt-get sudo snap install microk8s --classic --channel = 1 .17/stable # Install specific k8s version microk8s.enable dns # Enbale DNS microk8s.status # Check status Windows - Minikube minikube start --kubernetes-version = '1.17.4' # Install specific k8s version minikube ip # IP of the machine minikube status # Check status minikube stop # Stop minkube service Kubernetes Container Abstractions \u00b6 Pod: one or more containers running together on one Node. Basic unit of deployment. Containers are always in pods Controller: For creating/updating pods and other objects. Many types of Controllers inc. Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob, etc. Service: network endpoint to connect to a pod Namespace: Filtered group of objects in cluster - Secrets, ConfigMaps, and more. Our First Pod With Kubectl run \u00b6 Two ways to deploy Pods (containers): Via commands, or via YAML Object hieracrhy - Pods -> ReplicaSet -> Deployment kubectl run my-nginx --image nginx # Creates a single pod kubectl run nginx-pod --generator = run-pod/v1 -- image nginx # Another way to create pod kubectl get pods # list the pod kubectl create deployment nginx --image nginx # Creates a deployment kubectl deployment deployment nginx # Deletes a deployment kubectl create deployment nginx --image nginx --dry-run --port 80 -- expose # Using Dry run option Scaling ReplicaSets \u00b6 kubectl create deployment my-apache --image httpd kubectl scale deploy/my-apache --replicas 2 # Scale up by 2 kubectl scale deployment my-apache --replicas 2 # Scale up by 2 kubectl get all Inspecting Kubernetes Objects \u00b6 kubectl get deploy,pods # Get multiple resources in one line kubectl get pods -o wide # Get all pods, in wide format ( gives more info ) kubectl get pods --show-labels # Get all pods and show labels kubectl logs deployment/my-apache kubectl logs deployment/my-apache --follow --tail 1 # Show the last line kubectl logs -l run = my-apache # Show logs using label kubectl describe pod/my-apache-<pod id> # Shows the pod configuration including events kubectl get pods -w # Watches the pods in real time kubectl delete pod/my-apache-<pod id> # Deletes a single instance Exposing Kubernetes Ports \u00b6 A service is a stable address for pod(s) If we want to connect to pod(s), we need a service CoreDNS allows us to resolve services by name There are different types of services ClusterIP NodePort LoadBalancer ExternalName ClusterIP and NodePort services are always available in Kubernetes kubectl expose creates a service for existing pods Basic Service Types \u00b6 ClusterIP (default) Single, internal virtual IP allocated Only reachable from within cluster (nodes and pods) Pods can reach service on apps port number NodePort High port allocated on each node Port is open on every node\u2019s IP Anyone can connect (if they can reach node) Other pods need to be updated to this port LoadBalancer Controls a LB endpoint external to the cluster Only available when infra provider gives you a LB (AWS ELB, etc) Creates NodePort+ClusterIP services, tells LB to send to NodePort ExternalName Adds CNAME DNS record to CoreDNS only Not used for Pods, but for giving pods a DNS name to use for something outside Kubernetes # To show how to reach a ClusterIP deployment which is only accessible from the cluster in a Laptop kubectl create deployment httpenv --image = bretfisher/httpenv # simple http server kubectl scale deployment/httpenv --replicas = 5 kubectl expose deployment/httpenv --port 8888 # Create a ClusterIP service ( default ) kubectl get service # Shows services # Uses Generator option and launches the pod and gives BASH terminal kubectl run --generator run-pod/v1 tmp-shell --rm -it --image bretfisher/netshoot -- bash # Launch another pod to run curl curl httpenv:8888 curl [ ip of service ] :8888 # Creating a NodePort and LoadBalancer Service \u00b6 Nodeport Port Range: 30000 to 32767 Did you know that a NodePort service also creates a ClusterIP? These three service types are additive, each one creates the ones above it: ClusterIP NodePort LoadBalancer If you're on Docker Desktop, it provides a built-in LoadBalancer that publishes the \u2013port on localhost If you're on kubeadm, minikube, or microk8s No built-in LB You can still run the command, it'll just stay at LoadBalancer recieves the packet on 8888, then transfers it to the Nodeport of the Node and then to the ClusterIP of the service. \"pending\" (but its NodePort works) kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort kubectl get services curl localhost:<Node Port> # Get this from svc output kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer kubectl get services curl localhost:8888 # Pod Port kubectl delete service/httpenv service/httpenv-np kubectl delete service/httpenv-lb deployment/httpenv Kubernetes Services DNS \u00b6 Internal DNS is provided by CoreDNS Services also have a FQDN curl <hostname>.<namespace>.svc.cluster.local curl <hostname> kubectl get namespaces curl <hostname>.<namespace>.svc.cluster.local Kubernetes Management Techniques \u00b6 Run, Expose and Create Generators \u00b6 These commands use helper templates called \"generators\" Every resource in Kubernetes has a specification or \"spec\" You can output those templates with \u2013dry-run -o yaml kubectl create deployment sample \u2013image nginx \u2013dry-run -o yaml You can use those YAML defaults as a starting point Generators are \"opinionated defaults\" Generator Examples \u00b6 \u2022 Using dry-run with yaml output we can see the generators kubectl create deployment test \u2013image nginx \u2013dry-run -o yaml kubectl create job test \u2013image nginx \u2013dry-run -o yaml kubectl expose deployment/test \u2013port 80 \u2013dry-run -o yaml - You need the deployment to exist before this works Imperative vs. Declarative \u00b6 Imperative : Focus on how a program operates Declarative : Focus on what a program should accomplish - Example: \"I'd like a cup of coffee\" Imperative : I boil water, scoop out 42 grams of medium-fine grounds, poor over 700 grams of water, etc. Declarative : \"Barista, I'd like a a cup of coffee\". (Barista is the engine that works through the steps, including retrying to make a cup, and is only finished when I have a cup) Kubernetes Imperative \u00b6 Examples: kubectl run, kubectl create deployment, kubectl update We start with a state we know (no deployment exists) We ask kubectl run to create a deployment Different commands are required to change that deployment Different commands are required per object Imperative is easier when you know the state Imperative is easier to get started Imperative is easier for humans at the CLI Imperative is NOT easy to automate Kubernetes Declarative \u00b6 Example: kubectl apply -f my-resources.yaml We don't know the current state We only know what we want the end result to be (yaml contents) Same command each time (tiny exception for delete) Resources can be all in a file, or many files (apply a whole dir) Requires understanding the YAML keys and values More work than kubectl run for just starting a pod The easiest way to automate The eventual path to GitOps happiness Three Management Approaches \u00b6 Imperative commands: run, expose, scale, edit, create deployment Best for dev/learning/personal projects Easy to learn, hardest to manage over time Imperative Commands Imperative objects: create -f file.yml, replace -f file.yml, delete\u2026 Good for prod of small environments, single file per command Store your changes in git-based yaml files Hard to automate Imperative Config File Declarative objects: apply -f file.yml or dir, diff Best for prod, easier to automate Harder to understand and predict changes Declarative Config File Recommendations \u00b6 Most Important Rule : Don't mix the three approaches Recommendations: Learn the Imperative CLI for easy control of local and test setups Move to apply -f file.yml and apply -f directory for prod Store yaml in git, git commit each change before you apply This trains you for later doing GitOps (where git commits are automatically applied to clusters) Moving to Declarative Kubernetes YAML \u00b6 Using kubectl apply \u00b6 create/update resources in a file kubectl apply -f myfile.yaml create/update a whole directory of yaml kubectl apply -f myyaml/ create/update from a URL kubectl apply -f https://bret.run/pod.yml Be careful, lets look at it first (browser or curl) # Using Shell curl -L https://bret.run/pod # Using Windows CMD Win PoSH? start https://bret.run/pod.yml Kubernetes Configuration YAML \u00b6 Kubernetes configuration file (YAML or JSON) Each file contains one or more manifests Each manifest describes an API object (deployment, job, secret) Each manifest needs four parts (root key:values in the file) apiVersion: kind: metadata: spec: Building Your YAML Files \u00b6 kind : We can get a list of resources the cluster supports kubectl api-resources Notice some resources have multiple API's (old vs. new) apiVersion : We can get the API versions the cluster supports kubectl api-versions metadata : only name is required spec : Where all the action is at! Building Your YAML spec - explain Command \u00b6 We can get all the keys each kind supports kubectl explain services \u2013recursive Oh boy! Let's slow down kubectl explain services.spec We can walk through the spec this way kubectl explain services.spec.type spec: can have sub spec: of other resources kubectl explain deployment.spec.template.spec.volumes.nfs.server Use kubectl api-versions or kubectl api-resources along with kubectl explain as documentation on explain could be old We can also use docs kubernetes.io/docs/reference/#api-reference Dry Runs With Apply YAML \u00b6 dry-run a create (client side only) kubectl apply -f app.yml \u2013dry-run dry-run a create/update on server kubectl apply -f app.yml \u2013server-dry-run see a diff visually kubectl diff -f app.yml Difference between dry-run and diff Labels and Label Selectors \u00b6 Labels goes under metadata: in your YAML Simple list of key: value for identifying your resource later by selecting, grouping, or filtering for it Common examples include tier: frontend, app: api, env: prod, customer: acme.co Not meant to hold complex, large, or non- identifying info, which is what annotations are for filter a get command kubectl get pods -l app=nginx apply only matching labels kubectl apply -f myfile.yaml -l app=nginx Label Recommendation Label Selectors (Use case for Labels) \u00b6 The \"glue\" telling Services and Deployments which pods are theirs Many resources use Label Selectors to \"link\" resource dependencies You'll see these match up in the Service and Deployment YAML Using Label selectors Use Labels and Selectors to control which pods go to which nodes Assigning Pods to Nodes Taints and Tolerations also control node placement Taints and Tolerations Your Next Steps, and The Future of Kubernetes \u00b6 Storage in Kubernetes \u00b6 Storage and stateful workloads are harder in all systems Containers make it both harder and easier than before StatefulSets is a new resource type, making Pods more sticky Recommendation : avoid stateful workloads for first few deployments until you're good at the basics Use db-as-a-service whenever you can Volumes in Kubernetes \u00b6 Creating and connecting Volumes: 2 types Volumes Tied to lifecycle of a Pod All containers in a single Pod can share them PersistentVolumes Created at the cluster level, outlives a Pod Separates storage config from Pod using it Multiple Pods can share them CSI plugins are the new way to connect to storage Ingress \u00b6 None of our Service types work at OSI Layer 7 (HTTP) How do we route outside connections based on hostname or URL? Example Usecase: app1.com and app2.com are 2 different deployments in the cluster and both listen on port 443. You will need Ingress to understand the DNS and route traffic to those apps Ingress Controllers (optional) do this with 3 rd party proxies Nginx is popular, but Traefik, HAProxy, F5, Envoy, Istio, etc. Recommendation: Check out Traefik Implementation is specific to Controller chosen Why Controller - To configure LB which is outside the cluster CRD's and The Operator Pattern \u00b6 You can add 3 rd party Resources and Controllers This extends Kubernetes API and CLI A pattern is starting to emerge of using these together Operator : automate deployment and management of complex apps e.g. Databases, monitoring tools, backups, and custom ingresses Higher Deployment Abstractions \u00b6 All our kubectl commands just talk to the Kubernetes API Kubernetes has limited built-in templating, versioning, tracking, and management of your apps Helm is the most popular Compose on Kubernetes comes with Docker Desktop Remember these are optional, and your distro may have a preference Most distros support Helm Templating YAML \u00b6 Many of the deployment tools have templating options You'll need a solution as the number of environments/apps grow Helm was the first \"winner\" in this space, but can be complex Official Kustomize feature works out-of-the-box (as of 1.14) docker app and compose-on-kubernetes are Docker's way Kubernetes Dashboard \u00b6 Default GUI for \"upstream\" Kubernetes Clouds don't have it by default Let's you view resources and upload YAML Safety first! Namespaces and Context \u00b6 Namespaces limit scope, aka \"virtual clusters\" Not related to Docker/Linux namespaces Won't need them in small clusters There are some built-in, to hide system stuff from kubectl \"users\" kubectl get namespaces kubectl get all --all-namespaces - Context changes kubectl cluster and namespace - See ~/.kube/config file kubectl config get-contexts # Selectively show output of Kube config kubectl config get-contexts -o name kubectl config set*","title":"Kubernetes"},{"location":"learning/k8s/k8s-notes/#why-kubernetes","text":"Orchestration: Next logical step in journey to faster DevOps First, understand why you may need orchestration Not every solution needs orchestration Servers + Change Rate = Benefit of orchestration","title":"Why Kubernetes"},{"location":"learning/k8s/k8s-notes/#k8s-learning-resources","text":"Play with k8s Katacoda","title":"K8s Learning Resources"},{"location":"learning/k8s/k8s-notes/#install-kubernetes","text":"Linux - Microk8s Install SNAP first using apt-get sudo snap install microk8s --classic --channel = 1 .17/stable # Install specific k8s version microk8s.enable dns # Enbale DNS microk8s.status # Check status Windows - Minikube minikube start --kubernetes-version = '1.17.4' # Install specific k8s version minikube ip # IP of the machine minikube status # Check status minikube stop # Stop minkube service","title":"Install Kubernetes"},{"location":"learning/k8s/k8s-notes/#kubernetes-container-abstractions","text":"Pod: one or more containers running together on one Node. Basic unit of deployment. Containers are always in pods Controller: For creating/updating pods and other objects. Many types of Controllers inc. Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob, etc. Service: network endpoint to connect to a pod Namespace: Filtered group of objects in cluster - Secrets, ConfigMaps, and more.","title":"Kubernetes Container Abstractions"},{"location":"learning/k8s/k8s-notes/#our-first-pod-with-kubectl-run","text":"Two ways to deploy Pods (containers): Via commands, or via YAML Object hieracrhy - Pods -> ReplicaSet -> Deployment kubectl run my-nginx --image nginx # Creates a single pod kubectl run nginx-pod --generator = run-pod/v1 -- image nginx # Another way to create pod kubectl get pods # list the pod kubectl create deployment nginx --image nginx # Creates a deployment kubectl deployment deployment nginx # Deletes a deployment kubectl create deployment nginx --image nginx --dry-run --port 80 -- expose # Using Dry run option","title":"Our First Pod With Kubectl run"},{"location":"learning/k8s/k8s-notes/#scaling-replicasets","text":"kubectl create deployment my-apache --image httpd kubectl scale deploy/my-apache --replicas 2 # Scale up by 2 kubectl scale deployment my-apache --replicas 2 # Scale up by 2 kubectl get all","title":"Scaling ReplicaSets"},{"location":"learning/k8s/k8s-notes/#inspecting-kubernetes-objects","text":"kubectl get deploy,pods # Get multiple resources in one line kubectl get pods -o wide # Get all pods, in wide format ( gives more info ) kubectl get pods --show-labels # Get all pods and show labels kubectl logs deployment/my-apache kubectl logs deployment/my-apache --follow --tail 1 # Show the last line kubectl logs -l run = my-apache # Show logs using label kubectl describe pod/my-apache-<pod id> # Shows the pod configuration including events kubectl get pods -w # Watches the pods in real time kubectl delete pod/my-apache-<pod id> # Deletes a single instance","title":"Inspecting Kubernetes Objects"},{"location":"learning/k8s/k8s-notes/#exposing-kubernetes-ports","text":"A service is a stable address for pod(s) If we want to connect to pod(s), we need a service CoreDNS allows us to resolve services by name There are different types of services ClusterIP NodePort LoadBalancer ExternalName ClusterIP and NodePort services are always available in Kubernetes kubectl expose creates a service for existing pods","title":"Exposing Kubernetes Ports"},{"location":"learning/k8s/k8s-notes/#basic-service-types","text":"ClusterIP (default) Single, internal virtual IP allocated Only reachable from within cluster (nodes and pods) Pods can reach service on apps port number NodePort High port allocated on each node Port is open on every node\u2019s IP Anyone can connect (if they can reach node) Other pods need to be updated to this port LoadBalancer Controls a LB endpoint external to the cluster Only available when infra provider gives you a LB (AWS ELB, etc) Creates NodePort+ClusterIP services, tells LB to send to NodePort ExternalName Adds CNAME DNS record to CoreDNS only Not used for Pods, but for giving pods a DNS name to use for something outside Kubernetes # To show how to reach a ClusterIP deployment which is only accessible from the cluster in a Laptop kubectl create deployment httpenv --image = bretfisher/httpenv # simple http server kubectl scale deployment/httpenv --replicas = 5 kubectl expose deployment/httpenv --port 8888 # Create a ClusterIP service ( default ) kubectl get service # Shows services # Uses Generator option and launches the pod and gives BASH terminal kubectl run --generator run-pod/v1 tmp-shell --rm -it --image bretfisher/netshoot -- bash # Launch another pod to run curl curl httpenv:8888 curl [ ip of service ] :8888 #","title":"Basic Service Types"},{"location":"learning/k8s/k8s-notes/#creating-a-nodeport-and-loadbalancer-service","text":"Nodeport Port Range: 30000 to 32767 Did you know that a NodePort service also creates a ClusterIP? These three service types are additive, each one creates the ones above it: ClusterIP NodePort LoadBalancer If you're on Docker Desktop, it provides a built-in LoadBalancer that publishes the \u2013port on localhost If you're on kubeadm, minikube, or microk8s No built-in LB You can still run the command, it'll just stay at LoadBalancer recieves the packet on 8888, then transfers it to the Nodeport of the Node and then to the ClusterIP of the service. \"pending\" (but its NodePort works) kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort kubectl get services curl localhost:<Node Port> # Get this from svc output kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer kubectl get services curl localhost:8888 # Pod Port kubectl delete service/httpenv service/httpenv-np kubectl delete service/httpenv-lb deployment/httpenv","title":"Creating a NodePort and LoadBalancer Service"},{"location":"learning/k8s/k8s-notes/#kubernetes-services-dns","text":"Internal DNS is provided by CoreDNS Services also have a FQDN curl <hostname>.<namespace>.svc.cluster.local curl <hostname> kubectl get namespaces curl <hostname>.<namespace>.svc.cluster.local","title":"Kubernetes Services DNS"},{"location":"learning/k8s/k8s-notes/#kubernetes-management-techniques","text":"","title":"Kubernetes Management Techniques"},{"location":"learning/k8s/k8s-notes/#run-expose-and-create-generators","text":"These commands use helper templates called \"generators\" Every resource in Kubernetes has a specification or \"spec\" You can output those templates with \u2013dry-run -o yaml kubectl create deployment sample \u2013image nginx \u2013dry-run -o yaml You can use those YAML defaults as a starting point Generators are \"opinionated defaults\"","title":"Run, Expose and Create Generators"},{"location":"learning/k8s/k8s-notes/#generator-examples","text":"\u2022 Using dry-run with yaml output we can see the generators kubectl create deployment test \u2013image nginx \u2013dry-run -o yaml kubectl create job test \u2013image nginx \u2013dry-run -o yaml kubectl expose deployment/test \u2013port 80 \u2013dry-run -o yaml - You need the deployment to exist before this works","title":"Generator Examples"},{"location":"learning/k8s/k8s-notes/#imperative-vs-declarative","text":"Imperative : Focus on how a program operates Declarative : Focus on what a program should accomplish - Example: \"I'd like a cup of coffee\" Imperative : I boil water, scoop out 42 grams of medium-fine grounds, poor over 700 grams of water, etc. Declarative : \"Barista, I'd like a a cup of coffee\". (Barista is the engine that works through the steps, including retrying to make a cup, and is only finished when I have a cup)","title":"Imperative vs. Declarative"},{"location":"learning/k8s/k8s-notes/#kubernetes-imperative","text":"Examples: kubectl run, kubectl create deployment, kubectl update We start with a state we know (no deployment exists) We ask kubectl run to create a deployment Different commands are required to change that deployment Different commands are required per object Imperative is easier when you know the state Imperative is easier to get started Imperative is easier for humans at the CLI Imperative is NOT easy to automate","title":"Kubernetes Imperative"},{"location":"learning/k8s/k8s-notes/#kubernetes-declarative","text":"Example: kubectl apply -f my-resources.yaml We don't know the current state We only know what we want the end result to be (yaml contents) Same command each time (tiny exception for delete) Resources can be all in a file, or many files (apply a whole dir) Requires understanding the YAML keys and values More work than kubectl run for just starting a pod The easiest way to automate The eventual path to GitOps happiness","title":"Kubernetes Declarative"},{"location":"learning/k8s/k8s-notes/#three-management-approaches","text":"Imperative commands: run, expose, scale, edit, create deployment Best for dev/learning/personal projects Easy to learn, hardest to manage over time Imperative Commands Imperative objects: create -f file.yml, replace -f file.yml, delete\u2026 Good for prod of small environments, single file per command Store your changes in git-based yaml files Hard to automate Imperative Config File Declarative objects: apply -f file.yml or dir, diff Best for prod, easier to automate Harder to understand and predict changes Declarative Config File","title":"Three Management Approaches"},{"location":"learning/k8s/k8s-notes/#recommendations","text":"Most Important Rule : Don't mix the three approaches Recommendations: Learn the Imperative CLI for easy control of local and test setups Move to apply -f file.yml and apply -f directory for prod Store yaml in git, git commit each change before you apply This trains you for later doing GitOps (where git commits are automatically applied to clusters)","title":"Recommendations"},{"location":"learning/k8s/k8s-notes/#moving-to-declarative-kubernetes-yaml","text":"","title":"Moving to Declarative Kubernetes YAML"},{"location":"learning/k8s/k8s-notes/#using-kubectl-apply","text":"create/update resources in a file kubectl apply -f myfile.yaml create/update a whole directory of yaml kubectl apply -f myyaml/ create/update from a URL kubectl apply -f https://bret.run/pod.yml Be careful, lets look at it first (browser or curl) # Using Shell curl -L https://bret.run/pod # Using Windows CMD Win PoSH? start https://bret.run/pod.yml","title":"Using kubectl apply"},{"location":"learning/k8s/k8s-notes/#kubernetes-configuration-yaml","text":"Kubernetes configuration file (YAML or JSON) Each file contains one or more manifests Each manifest describes an API object (deployment, job, secret) Each manifest needs four parts (root key:values in the file) apiVersion: kind: metadata: spec:","title":"Kubernetes Configuration YAML"},{"location":"learning/k8s/k8s-notes/#building-your-yaml-files","text":"kind : We can get a list of resources the cluster supports kubectl api-resources Notice some resources have multiple API's (old vs. new) apiVersion : We can get the API versions the cluster supports kubectl api-versions metadata : only name is required spec : Where all the action is at!","title":"Building Your YAML Files"},{"location":"learning/k8s/k8s-notes/#building-your-yaml-spec---explain-command","text":"We can get all the keys each kind supports kubectl explain services \u2013recursive Oh boy! Let's slow down kubectl explain services.spec We can walk through the spec this way kubectl explain services.spec.type spec: can have sub spec: of other resources kubectl explain deployment.spec.template.spec.volumes.nfs.server Use kubectl api-versions or kubectl api-resources along with kubectl explain as documentation on explain could be old We can also use docs kubernetes.io/docs/reference/#api-reference","title":"Building Your YAML spec - explain Command"},{"location":"learning/k8s/k8s-notes/#dry-runs-with-apply-yaml","text":"dry-run a create (client side only) kubectl apply -f app.yml \u2013dry-run dry-run a create/update on server kubectl apply -f app.yml \u2013server-dry-run see a diff visually kubectl diff -f app.yml Difference between dry-run and diff","title":"Dry Runs With Apply YAML"},{"location":"learning/k8s/k8s-notes/#labels-and-label-selectors","text":"Labels goes under metadata: in your YAML Simple list of key: value for identifying your resource later by selecting, grouping, or filtering for it Common examples include tier: frontend, app: api, env: prod, customer: acme.co Not meant to hold complex, large, or non- identifying info, which is what annotations are for filter a get command kubectl get pods -l app=nginx apply only matching labels kubectl apply -f myfile.yaml -l app=nginx Label Recommendation","title":"Labels and Label Selectors"},{"location":"learning/k8s/k8s-notes/#label-selectors-use-case-for-labels","text":"The \"glue\" telling Services and Deployments which pods are theirs Many resources use Label Selectors to \"link\" resource dependencies You'll see these match up in the Service and Deployment YAML Using Label selectors Use Labels and Selectors to control which pods go to which nodes Assigning Pods to Nodes Taints and Tolerations also control node placement Taints and Tolerations","title":"Label Selectors (Use case for Labels)"},{"location":"learning/k8s/k8s-notes/#your-next-steps-and-the-future-of-kubernetes","text":"","title":"Your Next Steps, and The Future of Kubernetes"},{"location":"learning/k8s/k8s-notes/#storage-in-kubernetes","text":"Storage and stateful workloads are harder in all systems Containers make it both harder and easier than before StatefulSets is a new resource type, making Pods more sticky Recommendation : avoid stateful workloads for first few deployments until you're good at the basics Use db-as-a-service whenever you can","title":"Storage in Kubernetes"},{"location":"learning/k8s/k8s-notes/#volumes-in-kubernetes","text":"Creating and connecting Volumes: 2 types Volumes Tied to lifecycle of a Pod All containers in a single Pod can share them PersistentVolumes Created at the cluster level, outlives a Pod Separates storage config from Pod using it Multiple Pods can share them CSI plugins are the new way to connect to storage","title":"Volumes in Kubernetes"},{"location":"learning/k8s/k8s-notes/#ingress","text":"None of our Service types work at OSI Layer 7 (HTTP) How do we route outside connections based on hostname or URL? Example Usecase: app1.com and app2.com are 2 different deployments in the cluster and both listen on port 443. You will need Ingress to understand the DNS and route traffic to those apps Ingress Controllers (optional) do this with 3 rd party proxies Nginx is popular, but Traefik, HAProxy, F5, Envoy, Istio, etc. Recommendation: Check out Traefik Implementation is specific to Controller chosen Why Controller - To configure LB which is outside the cluster","title":"Ingress"},{"location":"learning/k8s/k8s-notes/#crds-and-the-operator-pattern","text":"You can add 3 rd party Resources and Controllers This extends Kubernetes API and CLI A pattern is starting to emerge of using these together Operator : automate deployment and management of complex apps e.g. Databases, monitoring tools, backups, and custom ingresses","title":"CRD's and The Operator Pattern"},{"location":"learning/k8s/k8s-notes/#higher-deployment-abstractions","text":"All our kubectl commands just talk to the Kubernetes API Kubernetes has limited built-in templating, versioning, tracking, and management of your apps Helm is the most popular Compose on Kubernetes comes with Docker Desktop Remember these are optional, and your distro may have a preference Most distros support Helm","title":"Higher Deployment Abstractions"},{"location":"learning/k8s/k8s-notes/#templating-yaml","text":"Many of the deployment tools have templating options You'll need a solution as the number of environments/apps grow Helm was the first \"winner\" in this space, but can be complex Official Kustomize feature works out-of-the-box (as of 1.14) docker app and compose-on-kubernetes are Docker's way","title":"Templating YAML"},{"location":"learning/k8s/k8s-notes/#kubernetes-dashboard","text":"Default GUI for \"upstream\" Kubernetes Clouds don't have it by default Let's you view resources and upload YAML Safety first!","title":"Kubernetes Dashboard"},{"location":"learning/k8s/k8s-notes/#namespaces-and-context","text":"Namespaces limit scope, aka \"virtual clusters\" Not related to Docker/Linux namespaces Won't need them in small clusters There are some built-in, to hide system stuff from kubectl \"users\" kubectl get namespaces kubectl get all --all-namespaces - Context changes kubectl cluster and namespace - See ~/.kube/config file kubectl config get-contexts # Selectively show output of Kube config kubectl config get-contexts -o name kubectl config set*","title":"Namespaces and Context"},{"location":"server/","text":"Server Details \u00b6 Linux Package Management Basics DNS Basics","title":"Server Details"},{"location":"server/#server-details","text":"Linux Package Management Basics DNS Basics","title":"Server Details"},{"location":"server/install/","text":"Installation \u00b6 Centos \u00b6 Identifying Version Download the latest version of Centos from centos.org Creating Live USB Format a USB as NTFS. Download Fedora LiveUSB Creator or read the supported software for Centos in the downloads page. See the instructions to write the ISO into the USB Setting Up PC Format the HDD (optional) Ensure - Advanced Option, Legacy USB Support is Enabled In Boot priority, USB is the first drive Ensure the Boot option has Boot from USB selected above HDD Configuring Centos Full Installation Steps Create Custom partitions for /boot, /, /var, /tmp, /home Initial Server Setup Update Centos with latest patches yum update -y && yum upgrade -y yum clean all # To clean downloaded packages in /var/cache/yum Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Adding Non Root User Add and Delete Users Creating Volume Groups Securing Server Security based on articles mentioned here or here or DigitalOcean -keygen -t rsa -b 4096 It will create two files: id_rsa and id_rsa.pub in the ~/.ssh directory. When you add a passphrase to your SSH key, it encrypts the private key using 128-bit AES so that the private key is useless without the passphrase to decrypt it. cat ~/.ssh/id_rsa.pub For example, if we had a Linux VM named myserver with a user azureuser, we could use the following command to install the public key file and authorize the user with the key: ssh-copy-id -i ~/.ssh/id_rsa.pub azureuser@myserver Connect with SSH ssh jim@137.117.101.249 Try executing a few Linux commands ls -la / to show the root of the disk ps -l to show all the running processes dmesg to list all the kernel messages lsblk to list all the block devices - here you will see your drives Initialize data disks Any additional drives you create from scratch need to be initialized and formatted. dmesg | grep SCSI - list all the messages from the kernel for SCSI devices. Use fdisk to initialize the drive We can use the following command to create a new primary partition: (echo n; echo p; echo 1; echo ; echo ; echo w) | sudo fdisk /dev/sdc We need to write a file system to the partition with the mkfs command. sudo mkfs -t ext4 /dev/sdc1 We need to mount the drive to the file system. sudo mkdir /data & sudo mount /dev/sdc1 /data Always make sure to lock down ports used for administrative access. An even better approach is to create a VPN to link the virtual network to your private network and only allow RDP or SSH requests from that address range. You can also change the port used by SSH to be something other than the default. Keep in mind that changing ports is not sufficient to stop attacks. It simply makes it a little harder to discover.","title":"Installation"},{"location":"server/install/#installation","text":"","title":"Installation"},{"location":"server/install/#centos","text":"Identifying Version Download the latest version of Centos from centos.org Creating Live USB Format a USB as NTFS. Download Fedora LiveUSB Creator or read the supported software for Centos in the downloads page. See the instructions to write the ISO into the USB Setting Up PC Format the HDD (optional) Ensure - Advanced Option, Legacy USB Support is Enabled In Boot priority, USB is the first drive Ensure the Boot option has Boot from USB selected above HDD Configuring Centos Full Installation Steps Create Custom partitions for /boot, /, /var, /tmp, /home Initial Server Setup Update Centos with latest patches yum update -y && yum upgrade -y yum clean all # To clean downloaded packages in /var/cache/yum Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Adding Non Root User Add and Delete Users Creating Volume Groups Securing Server Security based on articles mentioned here or here or DigitalOcean -keygen -t rsa -b 4096 It will create two files: id_rsa and id_rsa.pub in the ~/.ssh directory. When you add a passphrase to your SSH key, it encrypts the private key using 128-bit AES so that the private key is useless without the passphrase to decrypt it. cat ~/.ssh/id_rsa.pub For example, if we had a Linux VM named myserver with a user azureuser, we could use the following command to install the public key file and authorize the user with the key: ssh-copy-id -i ~/.ssh/id_rsa.pub azureuser@myserver Connect with SSH ssh jim@137.117.101.249 Try executing a few Linux commands ls -la / to show the root of the disk ps -l to show all the running processes dmesg to list all the kernel messages lsblk to list all the block devices - here you will see your drives Initialize data disks Any additional drives you create from scratch need to be initialized and formatted. dmesg | grep SCSI - list all the messages from the kernel for SCSI devices. Use fdisk to initialize the drive We can use the following command to create a new primary partition: (echo n; echo p; echo 1; echo ; echo ; echo w) | sudo fdisk /dev/sdc We need to write a file system to the partition with the mkfs command. sudo mkfs -t ext4 /dev/sdc1 We need to mount the drive to the file system. sudo mkdir /data & sudo mount /dev/sdc1 /data Always make sure to lock down ports used for administrative access. An even better approach is to create a VPN to link the virtual network to your private network and only allow RDP or SSH requests from that address range. You can also change the port used by SSH to be something other than the default. Keep in mind that changing ports is not sufficient to stop attacks. It simply makes it a little harder to discover.","title":"Centos"},{"location":"server/mobile/","text":"Converting Android Device Into Linux Server \u00b6 Centos \u00b6 Pre-requisites Identify an Android mobile phone Factory reset to remove any unwanted apps and data Remove any apps or services from the device to free up space and memory Install Termux and AnLinux from Playstore Installation Start AnLinux and search for the distro. Select Centos Copy the installation command Open Termux terminal and paste the command Begin installation Access Mobile Configure Static Ip to Mobile Add Ip address and remote-hostname to Laptop's hosts file. Install SSH on Termux apt install openssh Start the ssh server sshd . This will also generate the private and public keys. Test the service locally ssh localhost -p 8022 . Accept the public key. ssh-copy-id copies the local-host\u2019s public key to the remote-host\u2019s authorized_keys file. ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-hostname> -p 8022 . Enter remote user password to complete transaction. Login from Laptop ssh <hostname> -p 8022 . Enter the passphrase. Install Python Configuring Centos Update Centos with latest patches yum install update -y && yum install upgrade -y Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Init Centos yum -y install initscripts & yum clean all Change Root Password passwd root Install Sudo yum install sudo -y The difference between wheel and sudo . In CentOS, a user belonging to the wheel group can execute su and directly ascend to root. Meanwhile, a sudo user would have use the sudo su first. Essentially, there is no real difference except for the syntax used to become root, and users belonging to both groups can use the sudo command. * Adding Non Root User bmmnb Add and Delete Users Securing Server Security based on articles mentioned here or here or DigitalOcean Configures a Hostname Reconfigures the Timezone Updates the entire System Creates a New Admin user so you can manage your server safely without the need of doing remote connections with root. Helps user Generate Secure RSA Keys, so that remote access to your server is done exclusive from your local pc and no Conventional password Configures, Optimize and secures the SSH Server (Some Settings Following CIS Benchmark) Configures IPTABLES Rules to protect the server from common attacks Disables unused FileSystems and Network protocols Protects the server against Brute Force attacks by installing a configuring fail2ban Installs and Configure Artillery as a Honeypot, Monitoring, Blocking and Alerting tool Installs PortSentry Installs RootKit Hunter Secures Root Home and Grub Configuration Files Installs Unhide to help Detect Malicious Hidden Processes Installs Tiger, A Security Auditing and Intrusion Prevention system Restrict Access to Apache Config Files Disables Compilers Creates Daily Cron job for System Updates Kernel Hardening via sysctl configuration File (Tweaked) /tmp Directory Hardening PSAD IDS installation Enables Process Accounting Enables Unattended Upgrades MOTD and Banners for Unauthorized access Disables USB Support for Improved Security (Optional) Configures a Restrictive Default UMASK Configures and enables Auditd Configures Auditd rules following CIS Benchmark Sysstat install ArpWatch install Additional Hardening steps following CIS Benchmark Secures Cron Automates the process of setting a GRUB Bootloader Password Secures Boot Settings Sets Secure File Permissions for Critical System Files","title":"Mobile"},{"location":"server/mobile/#converting-android-device-into-linux-server","text":"","title":"Converting Android Device Into Linux Server"},{"location":"server/mobile/#centos","text":"Pre-requisites Identify an Android mobile phone Factory reset to remove any unwanted apps and data Remove any apps or services from the device to free up space and memory Install Termux and AnLinux from Playstore Installation Start AnLinux and search for the distro. Select Centos Copy the installation command Open Termux terminal and paste the command Begin installation Access Mobile Configure Static Ip to Mobile Add Ip address and remote-hostname to Laptop's hosts file. Install SSH on Termux apt install openssh Start the ssh server sshd . This will also generate the private and public keys. Test the service locally ssh localhost -p 8022 . Accept the public key. ssh-copy-id copies the local-host\u2019s public key to the remote-host\u2019s authorized_keys file. ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-hostname> -p 8022 . Enter remote user password to complete transaction. Login from Laptop ssh <hostname> -p 8022 . Enter the passphrase. Install Python Configuring Centos Update Centos with latest patches yum install update -y && yum install upgrade -y Verify the OS Version cat /etc/os-releases net-tools package provides the ifconfig command yum install net-tools -y Init Centos yum -y install initscripts & yum clean all Change Root Password passwd root Install Sudo yum install sudo -y The difference between wheel and sudo . In CentOS, a user belonging to the wheel group can execute su and directly ascend to root. Meanwhile, a sudo user would have use the sudo su first. Essentially, there is no real difference except for the syntax used to become root, and users belonging to both groups can use the sudo command. * Adding Non Root User bmmnb Add and Delete Users Securing Server Security based on articles mentioned here or here or DigitalOcean Configures a Hostname Reconfigures the Timezone Updates the entire System Creates a New Admin user so you can manage your server safely without the need of doing remote connections with root. Helps user Generate Secure RSA Keys, so that remote access to your server is done exclusive from your local pc and no Conventional password Configures, Optimize and secures the SSH Server (Some Settings Following CIS Benchmark) Configures IPTABLES Rules to protect the server from common attacks Disables unused FileSystems and Network protocols Protects the server against Brute Force attacks by installing a configuring fail2ban Installs and Configure Artillery as a Honeypot, Monitoring, Blocking and Alerting tool Installs PortSentry Installs RootKit Hunter Secures Root Home and Grub Configuration Files Installs Unhide to help Detect Malicious Hidden Processes Installs Tiger, A Security Auditing and Intrusion Prevention system Restrict Access to Apache Config Files Disables Compilers Creates Daily Cron job for System Updates Kernel Hardening via sysctl configuration File (Tweaked) /tmp Directory Hardening PSAD IDS installation Enables Process Accounting Enables Unattended Upgrades MOTD and Banners for Unauthorized access Disables USB Support for Improved Security (Optional) Configures a Restrictive Default UMASK Configures and enables Auditd Configures Auditd rules following CIS Benchmark Sysstat install ArpWatch install Additional Hardening steps following CIS Benchmark Secures Cron Automates the process of setting a GRUB Bootloader Password Secures Boot Settings Sets Secure File Permissions for Critical System Files","title":"Centos"},{"location":"server/proxy/","text":"Nginx Reverse Proxy Setup","title":"Proxy"},{"location":"server/service/","text":"Create a systemd unit file for starting the application: \u00b6 Example service file can be found here: $ wget https://gist.githubusercontent.com/Artemmkin/ce82397cfc69d912df9cd648a8d69bec/raw/7193a36c9661c6b90e7e482d256865f085a853f2/raddit.service Move it to the systemd directory $ sudo mv raddit.service /etc/systemd/system/raddit.service Now start the application and enable autostart: $ sudo systemctl start raddit $ sudo systemctl enable raddit Verify that it's running: $ sudo systemctl status raddit","title":"Create a systemd unit file for starting the application:"},{"location":"server/service/#create-a-systemd-unit-file-for-starting-the-application","text":"Example service file can be found here: $ wget https://gist.githubusercontent.com/Artemmkin/ce82397cfc69d912df9cd648a8d69bec/raw/7193a36c9661c6b90e7e482d256865f085a853f2/raddit.service Move it to the systemd directory $ sudo mv raddit.service /etc/systemd/system/raddit.service Now start the application and enable autostart: $ sudo systemctl start raddit $ sudo systemctl enable raddit Verify that it's running: $ sudo systemctl status raddit","title":"Create a systemd unit file for starting the application:"},{"location":"server/volume-groups/","text":"Configuration \u00b6 Centos \u00b6 Prerequisites Install EPEL Packages yum install epel-release Install ntfs-3g and fuse packages to support NTFS drives yum install ntfs-3g fuse -y Format the new NTFS filestsystem to xfs mkfs.xfs -f /dev/sda Identifying Additional HDD Run blkid to verify if HDD is detected. On detection of HDD, follow the below steps to configure as XFS file system. * Run fdisk -l to list the devices, partitions and volume groups * Scan your hard drive for Bad Sectors or Bad Blocks 1. Partitioning HDD (Optional and for Primary HDD) * On identifying the device(HDD), say /dev/sda as the target device to partition, run fdisk /dev/sda * These steps below are not for secondary HDD as partitioning will not utilize the entire space * Option m to list the operations, Option p to print the current partitions * Note the start and end block ids of the current partitions * Option d to delete the current partitions, select the partition to delete 2 * Option n to create new partition, select only 1 partition for the HDD, select default start and end block id, it should match the one printed above * Option t to select the type of partition, followed by 8e to select as Linux LVM * Format the new partition to xfs mkfs.xfs -f /dev/sda1 as only one partition was created * Mount the new partition to test mount -t xfs /dev/sda1 /data * Make entry in /etc/fstab to make it permanent * Verify the mounted partition by running df -Th to see free and used space Securing Drive and adding Redundancy Software RAID and LVM For the partitions that are needed as Raid 1, use fdsik /dev/sdb3 ,Press \u2018t\u2019 to change the partition type. Press \u2018fd\u2019 to choose Linux Raid Auto. Install Raid 1 on /dev/sdb3 and /dev/sda1 using mdadm --create /dev/data --level=1 --raid-devices=2 /dev/sdb3 /dev/sda1 Encrypting Raid devices Volume Group Setup Logical Volume Manager (LVM) is a software-based RAID-like system that lets you create \"pools\" of storage and add hard drive space to those pools as needed. * LVM Basic * Create, expand, and encrypt storage pools Volume Group Lifecycle Migrate Data from one LV to another Identify the current Physical Volume and Volume Group mapping pvscan or pvs List the partitions and volumes in the Volume Groups vgdisplay centos-digital -v Logical volume and device mapping lvs -o+devices List all devices and device mapping lsscsi Before Reducing logical volume, move to another drive Format /dev/sdb3 to a physical volume: pvcreate /dev/sdb3 Give /dev/sdb3 to the volume group centos-digital: vgextend centos-digital /dev/sdb3 Migrate our logical volume Users from /dev/sdb2 to /dev/sdb3, pvmove -n Users /dev/sdb2 /dev/sdb3 Verify if the data is migrated lsblk Reduce your logical volume to make place for your new partition lvresize -L 25G /dev/centos-digital/home Use fdisk to reduce the size of /dev/sdb2 to 80G as total for volume group centos-digital was 75G (yes, a little bit bigger as we used in lvresize!) Use fdisk to create a new LVM partition, /dev/sdb3, with the new available space (it will be around 400G). Check if the kernel could automatically detect the partition change. See if /proc/partition matches the new state (thus, /dev/sdb3 is visible). If not, you need to reboot. (Probably it will.) Make /dev/sdb2 to be as big again pvresize /dev/sdb2","title":"Configuration"},{"location":"server/volume-groups/#configuration","text":"","title":"Configuration"},{"location":"server/volume-groups/#centos","text":"Prerequisites Install EPEL Packages yum install epel-release Install ntfs-3g and fuse packages to support NTFS drives yum install ntfs-3g fuse -y Format the new NTFS filestsystem to xfs mkfs.xfs -f /dev/sda Identifying Additional HDD Run blkid to verify if HDD is detected. On detection of HDD, follow the below steps to configure as XFS file system. * Run fdisk -l to list the devices, partitions and volume groups * Scan your hard drive for Bad Sectors or Bad Blocks 1. Partitioning HDD (Optional and for Primary HDD) * On identifying the device(HDD), say /dev/sda as the target device to partition, run fdisk /dev/sda * These steps below are not for secondary HDD as partitioning will not utilize the entire space * Option m to list the operations, Option p to print the current partitions * Note the start and end block ids of the current partitions * Option d to delete the current partitions, select the partition to delete 2 * Option n to create new partition, select only 1 partition for the HDD, select default start and end block id, it should match the one printed above * Option t to select the type of partition, followed by 8e to select as Linux LVM * Format the new partition to xfs mkfs.xfs -f /dev/sda1 as only one partition was created * Mount the new partition to test mount -t xfs /dev/sda1 /data * Make entry in /etc/fstab to make it permanent * Verify the mounted partition by running df -Th to see free and used space Securing Drive and adding Redundancy Software RAID and LVM For the partitions that are needed as Raid 1, use fdsik /dev/sdb3 ,Press \u2018t\u2019 to change the partition type. Press \u2018fd\u2019 to choose Linux Raid Auto. Install Raid 1 on /dev/sdb3 and /dev/sda1 using mdadm --create /dev/data --level=1 --raid-devices=2 /dev/sdb3 /dev/sda1 Encrypting Raid devices Volume Group Setup Logical Volume Manager (LVM) is a software-based RAID-like system that lets you create \"pools\" of storage and add hard drive space to those pools as needed. * LVM Basic * Create, expand, and encrypt storage pools Volume Group Lifecycle Migrate Data from one LV to another Identify the current Physical Volume and Volume Group mapping pvscan or pvs List the partitions and volumes in the Volume Groups vgdisplay centos-digital -v Logical volume and device mapping lvs -o+devices List all devices and device mapping lsscsi Before Reducing logical volume, move to another drive Format /dev/sdb3 to a physical volume: pvcreate /dev/sdb3 Give /dev/sdb3 to the volume group centos-digital: vgextend centos-digital /dev/sdb3 Migrate our logical volume Users from /dev/sdb2 to /dev/sdb3, pvmove -n Users /dev/sdb2 /dev/sdb3 Verify if the data is migrated lsblk Reduce your logical volume to make place for your new partition lvresize -L 25G /dev/centos-digital/home Use fdisk to reduce the size of /dev/sdb2 to 80G as total for volume group centos-digital was 75G (yes, a little bit bigger as we used in lvresize!) Use fdisk to create a new LVM partition, /dev/sdb3, with the new available space (it will be around 400G). Check if the kernel could automatically detect the partition change. See if /proc/partition matches the new state (thus, /dev/sdb3 is visible). If not, you need to reboot. (Probably it will.) Make /dev/sdb2 to be as big again pvresize /dev/sdb2","title":"Centos"}]}